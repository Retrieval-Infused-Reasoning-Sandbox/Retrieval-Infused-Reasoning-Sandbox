id	instruction	CoT	concepts	answer	topic	token_count	noise_doc_count	related_doc_count
recuTUxvLKuZuC	In a three-dimensional periodic crystal, if the target distances $\hat{d}_{ij}$ between all pairs of atoms in the equilibrium state and the corresponding lattice matrix $\hat{L}$ are known, please write the global optimization objective function for solving the equilibrium atomic coordinate matrix $\hat{R} \in \mathbb{R}^{N\times 3}$. This function should minimize the deviation between the predicted distances and the actual distances of the coordinates, while satisfying the periodic boundary conditions.	{'step_1': 'Given the lattice matrix $\\hat{L}=\\big[\\mathbf{\\hat{l}}_1,\\mathbf{\\hat{l}}_2,\\mathbf{\\hat{l}}_3\\big]\\in\\mathbb{R}^{3\\times 3}$, and the atomic coordinate matrix to be solved $\\hat{R}=\\big[\\hat{R}_1^\\top,\\dots,\\hat{R}_N^\\top\\big]^\\top\\in\\mathbb{R}^{N\\times 3}$. Under **Periodic Boundary Conditions (PBC)**, equivalent images outside the unit cell are determined by integer vectors $\\mathbf{k}=(k_1,k_2,k_3)\\in\\mathbb{Z}^3$ and lattice vectors. For any atomic pair $(i,j)$, the relative displacement including periodic images is expressed as $\\Delta R_{ij}(\\mathbf{k})=\\hat{R}_i-\\big(\\hat{R}_j+k_1\\mathbf{\\hat{l}}_1+k_2\\mathbf{\\hat{l}}_2+k_3\\mathbf{\\hat{l}}_3\\big).$ (Corresponding concepts: PBC; lattice-coordinate consistency)', 'step_2': 'Given target equilibrium distances $\\hat{d}_{ij}$ (defined for atomic pairs in the edge set $E$), **Euclidean Distance Geometry (EDG)** requires that actual distances closely approximate the target distances. First, the basic geometric loss is formulated as: $L_{\\text{EDG}}(\\hat{R})=\\sum_{(i,j,\\mathbf{k})\\in E}\\left|\\;\\big\\|\\Delta R_{ij}(\\mathbf{k})\\big\\|-\\hat{d}_{ij}\\right|.$ (Corresponding concept: EDG)', 'step_3': 'If distances have uncertainty $\\hat{b}_{ij}$, introduce upper and lower bounds: $\\hat{d}^l_{ij}=\\hat{d}_{ij}-e^{\\hat{b}_{ij}},\\qquad \\hat{d}^u_{ij}=\\hat{d}_{ij}+e^{\\hat{b}_{ij}}.$ Accordingly, replace absolute deviations with **out-of-interval penalties** (hinge loss): $\\ell_{ij\\mathbf{k}}(\\hat{R})= \\max\\!\\left(0,\\;\\big\\|\\Delta R_{ij}(\\mathbf{k})\\big\\|-\\hat{d}^u_{ij}\\right) +\\max\\!\\left(0,\\;\\hat{d}^l_{ij}-\\big\\|\\Delta R_{ij}(\\mathbf{k})\\big\\|\\right).$ (Corresponding concept: BED)', 'step_4': 'Synthesizing Steps 1–3, sum over all atomic pairs in $E$ and their considered periodic images to obtain the global optimization objective: $L_g(\\hat{R})=\\sum_{(i,j,\\mathbf{k})\\in E} \\max\\!\\left(0,\\;\\big\\|\\hat{R}_i-(\\hat{R}_j+k_1\\mathbf{\\hat{l}}_1+k_2\\mathbf{\\hat{l}}_2+k_3\\mathbf{\\hat{l}}_3)\\big\\|-\\hat{d}^u_{ij}\\right) +\\max\\!\\left(0,\\;\\hat{d}^l_{ij}-\\big\\|\\hat{R}_i-(\\hat{R}_j+k_1\\mathbf{\\hat{l}}_1+k_2\\mathbf{\\hat{l}}_2+k_3\\mathbf{\\hat{l}}_3)\\big\\|\\right).$ The minimizer of this objective, $\\hat{R}^\\star=\\arg\\min_{\\hat{R}}L_g(\\hat{R})$, yields the equilibrium coordinates satisfying periodicity and distance interval constraints (ignoring non-uniqueness from rigid translations/rotations). (Corresponding concepts: PBC; EDG; lattice-coordinate consistency; BED)'}	{'concept_1': 'Euclidean Distance Geometry (EDG) For a given set of target interatomic distances, the solution to the 3D coordinates needs to minimize the deviation between the actual distances and the target distances: $L_g = \\sum_{(u,v)\\in E} \\left| \\| \\hat{R}_u - \\hat{R}_v \\| - \\hat{d}_{uv} \\right|$', 'concept_2': 'Periodic Boundary Conditions (PBCs) An atom $v$ is equivalent to the corresponding atom in the unit cell after being translated by the vector $k_1\\mathbf{l}_1 + k_2\\mathbf{l}_2 + k_3\\mathbf{l}_3$, and distance calculations must take this translation into account: $\\hat{R}_v \\to \\hat{R}_v + k_1 \\mathbf{\\hat{l}}_1 + k_2 \\mathbf{\\hat{l}}_2 + k_3 \\mathbf{\\hat{l}}_3$', 'concept_3': 'Lattice Matrix and Coordinate Consistency Condition The lattice vectors $\\mathbf{\\hat{l}}_1, \\mathbf{\\hat{l}}_2, \\mathbf{\\hat{l}}_3$ must be consistent with the predicted lattice matrix $\\hat{L}$, and periodic translations are implemented through this matrix.', 'concept_4': 'Optimization with Bounded Constraints (Bounded Euclidean Distance, BED) If there is prediction uncertainty $\\hat{b}_{ij}$ in the target distance, upper and lower bounds should be introduced: $\\hat{d}^l_{ij} = \\hat{d}_{ij} - e^{\\hat{b}_{ij}}, \\quad \\hat{d}^u_{ij} = \\hat{d}_{ij} + e^{\\hat{b}_{ij}}$ Only the deviation of the actual distance beyond this range is penalized.'}	The optimization objective function for equilibrium coordinates is: $L_g = \sum_{(i,j,\mathbf{k}) \in E} \left[ \max\left( 0, \| \hat{R}_i - (\hat{R}_j + k_1 \mathbf{\hat{l}}_1 + k_2 \mathbf{\hat{l}}_2 + k_3 \mathbf{\hat{l}}_3 ) \| - \hat{d}^u_{ij} \right) + \max\left( 0, \hat{d}^l_{ij} - \| \hat{R}_i - (\hat{R}_j + k_1 \mathbf{\hat{l}}_1 + k_2 \mathbf{\hat{l}}_2 + k_3 \mathbf{\hat{l}}_3 ) \| \right) \right]$ where: - $(i,j,\mathbf{k})$ denotes the atomic pair $(i,j)$ and its periodic translation vector $\mathbf{k} = (k_1,k_2,k_3) \in \mathbb{Z}^3$; - $\mathbf{\hat{l}}_1,\mathbf{\hat{l}}_2,\mathbf{\hat{l}}_3$ are the column vectors of the lattice matrix $\hat{L}$; - $\hat{d}^l_{ij} = \hat{d}_{ij} - e^{\hat{b}_{ij}}$ and $\hat{d}^u_{ij} = \hat{d}_{ij} + e^{\hat{b}_{ij}}$ represent the lower and upper bounds of the target distance, respectively; - $E$ is the set of all atomic pairs and their periodic images to be considered. This objective function optimizes the atomic coordinates $\hat{R}$ under periodic boundary conditions by penalizing actual distances that fall outside the range $[\hat{d}^l_{ij}, \hat{d}^u_{ij}]$.	理论交叉学科-计算材料学-晶体结构	138375	1	3
recuTOH1R0sx7z	Let T be a smooth toric Deligne-Mumford stack, there is a well-established mirror Z_T as a Lefschetz fibration over C. Does the general fiber of Z_T have enough Lagrangian spheres? (Definition 1: An (stable infinite) category C, which is an appropriate version of Fukaya category, is said to have enough lagrangian spheres if the full subcategory split-generated by embedded lagrangian spheres contains all pseudo-perfect objects, i.e. objects A such that Hom(M,A) is of finite dimensions for any M. Also, for a proper algebraic stack Y, the subcategory of pseudo-perfect objects in IndCoh(Y) is actually Perf(Y).)	{'step_1': 'Connect Fuk(F) to IndCoh(∂T) via HMS (Theorem 1) The homological mirror symmetry equivalence in Theorem 1 gives IndCoh(∂T)≅Fuk(F). Under this equivalence: Pseudo-perfect objects in Fuk(F) correspond to pseudo-perfect objects in IndCoh(∂T).', 'step_2': 'For a proper algebraic stack Y, pseudo-perfect objects in IndCoh(Y) coincide with Perf(Y) (perfect complexes). Assuming ∂T is proper (natural for boundaries of toric DM stacks), we have: Pseudo-perfect objects in IndCoh(∂T)≅Perf(∂T).', 'step_3': 'Generators of Perf(∂T) (Theorems 2 and 3) By theorem 2 and 3, Perf(∂T) is generated by restrictions of these line bundles to ∂T.', 'step_4': 'Lagrangian spheres as mirrors of restricted line bundles (Theorem 1) Theorem 1 states that the mirror of line bundles on T restricted to ∂T are Lagrangian spheres on F. Under the equivalence IndCoh(∂T)≅Fuk(F), these restricted line bundles correspond to Lagrangian spheres in F.', 'step_5': 'The Lagrangian spheres on F correspond to the generators of Perf(∂T), which (by Step 2) are exactly the pseudo-perfect objects in Fuk(F). Thus, the subcategory split-generated by Lagrangian spheres contains all pseudo-perfect objects.'}	{'Theorem 1 [Gammage-Shende]': 'There is a homological mirror symmetry between T and Z_T fitting into the commutative diagram \\[\\xymatrix{ IndCoh(T) \\ar[d]^{i^\\ast} \\ar[r]^\\sim & Fuk(({\\mathbb C}^\\ast)^n,F)\\ar[d]\\\\ IndCoh(\\partial T) \\ar[r]^\\sim & Fuk(F) }\\] Where Fuk(-,-) means the Fukaya-Seidel category of Z_T with respect to the general fiber F. $i$ is the natural inclusion. This diagram is such that the mirror of line bundles on T restricted to the boundary of T are Lagrangian spheres on F.', 'Theorem 2 [Orlov]': 'If IndCoh(X) is generated by a collection of vector bundles, and $Y\\subset X$ is closed, then Perf(Y) is generated by restrictions of these vector bundles to Y.', 'Theorem 3 [Hanlon-Hicks-Lazarev]': 'Given a toric Deligne-Mumford stack X, there is a finite collection of line bundles that generate IndCoh(X).'}	"Yes (Checklist: 1. The answer heavily depends on homological mirror symmetry; direct ""argument"" on Fukaya category of Lefschetz fibrations cannot be correct. 2. Indicate that the demand generation is equivalent to that restriction of line bundles on T to the boundary ∂T generates Perf(∂T). In particular, lagrangian spheres correspond to restriction of line bundles. The answer cannot be correct without mentioning the boundary ∂T.)"	理论基础学科-基础数学-几何	364373	2	3
recuSTVHHAlGEz	Let $r$ be a positive number, and let $\BF_{p^r}$ be the finite field with $p^r$ elements. Let $f(x_1,...,x_m)=\sum_{i=1}^mx_i^{p^{k_i}+1}$, where $k_i$ satisfies that $r/\gcd(r,k_i)$ is an odd number. Then how many elements are there in the set $\{(x_1,...,x_m)\in \BF_{p^r}}^m|f(x_1,...,x_m)=0\}$?	{'step_1': 'Transform the zero point problem into an exponential sum problem, that is, the number of elements in the set $\{(x_1,...,x_m)\in \BF_{p^r}}^m|f(x_1,...,x_m)=0\}$ is equivalent to calculating $\frac{1}{p^r}\sum_{y\in \mathbb{F}_{p^r}}\sum_{(x_1,...,x_m)\in \mathbb{F}_{p^r}^m}\chi(y(\sum_{i=1}^mx_i^{p^{k_i}+1}))$.', 'step_2': 'Decompose the above exponential sum into the product of the exponential sums in the Theorem, that is, $\frac{1}{p^r}\sum_{y\in \mathbb{F}_{p^r}}\sum_{(x_1,...,x_m)\in \mathbb{F}_{p^r}^m}\chi(y(\sum_{i=1}^mx_i^{p^{k_i}+1}))=p^{r(m-1)}+\sum_{y\in \mathbb{F}_{p^r}\prod_{i=1}^mS_{k_i}(y).$', 'step_3': 'Use Theorem 2 and Theorem 3 to discuss the problem by cases, that is, it is necessary to discuss $p=2$ and $p\ne 2$, and also discuss whether $m$ is an odd number or an even number.', 'step_4': 'Integrate the results to draw the conclusion.'}	{'concept_1': 'Let $q$ be a prime power, $\mathbb{F}_q$ be the finite field with $q$ elements, $g$ be a primitive element (i.e., a multiplicative generator) of $\mathbb{F}_q$, and $\chi$ be the standard additive character on $\mathbb{F}_q$. Define the exponential sum $S_{\alpha}(a):=\sum_{x\in \mathbb{F}_q}\chi(ax^{p^\alpha+1}).$', 'theorem_2': 'Let $r$ and $\alpha$ be positive integers such that $r/\gcd(r,\alpha)$ is an odd number, $p$ be an odd prime, and $q=p^r$. Then we have $S_{\alpha}(a)=\begin{cases} (-1)^{r-1}\sqrt{q}\eta(a),& \mbox{if}p\equiv 1\mod 4,\\\\ (-1)^{r-1}\sqrt{-1}^r\sqrt{q}\eta(a),& \mbox{if}p\equiv 3\mod 4. \end{cases}$ where $\eta$ is the quadratic multiplicative character on $\BF_q$.', 'theorem_3': 'Let $r$ and $\alpha$ be positive integers, and $q=2^r$. Suppose $d:=\gcd(r,\alpha)$ such that $r/\gcd(r,\alpha)$ is an odd number. Then we have $S_{\alpha}(a)=\begin{cases} q,& \mbox{if}\ a=0,\\\\ 0,& \mbox{otherwise}. \end{cases}$', 'theorem_4': 'Suppose that $f(x)=a^{p^\alpha}x^{p^{2\alpha}}+ax$ is not a permutation polynomial over $\mathbb{F}_{p^r}$, and suppose $c\in \BF_{p^r}^*$ such that $f(c)=0$. Then $\sum_{\beta\in \BF_{p^{2\gcd(r,\alpha)}}\chi(a(\beta c)^{p^\alpha+1})=p^{2\gcd(r,\alpha)}$.'}	$\begin{cases} p^{r(m-1)}, &\mbox{if}p=2\mbox{or}2\nmid m\\ p^{r(m-1)}+(p^r-1)p^{r(\frac{m}{2}-1)}, &\mbox{if}p\equiv 1\mod 4\mbox{and}2\mid m\\ p^{rm-1}+(-1)^{\frac{mr}{2}}(p^r-1)p^{r(\frac{m}{2}-1)}, &\mbox{if}p\equiv 3\mod 4\mbox{and}2\mid rm \end{cases}$	理论基础学科-基础数学-代数	35678	1	2
recuT5sTo3djUD	Let $q$ be an odd prime power, and let $f(x)=x^4+bx^2+cx+d$ be a monic polynomial over $\mathbb{F}_q$. If $f(x)$ is a permutation polynomial over $\mathbb{F}_q$, find all values of $q$ and the corresponding forms of $f(x)$?	{'step_1': 'Theorem 5 shows that for an odd prime power $q$, there are no exceptional polynomials of degree 4 over $\mathbb{F}_q$;', 'step_2': 'Combining the above conclusion with Theorem 4, we obtain that for an odd prime power $q$, a necessary condition for the existence of permutation polynomials of degree 4 over $\mathbb{F}_q$ is $q\\le 11$, thus we only need to consider $q=3,5,7,9,11$;', 'step_3': 'Use Theorem 2 to rule out the cases of $q=5$ and $q=9$;', 'step_4': 'Use Theorem 1 to rule out the case of $q=11$ (note that in this step, $a_i$ usually adopts the exhaustive search method, but only a few examples are tested and then such cases are excluded by guesswork);', 'step_5': 'Then use Theorem 3 to give two permutation polynomials over $\mathbb{F}_7$, and use Theorem 1 to rule out other possibilities;', 'step_6': 'Finally, give the construction for $q=3$ by exhaustive search or Theorem 1.'}	{'concept/theorem_1': 'Theorem 1(Hermite’s criterion): Let $q$ be a prime power, and let $f(x)$ be a polynomial over the finite field $\mathbb{F}_q$. $f(x)$ is a permutation polynomial over $\mathbb{F}_q$ if and only if the following two properties hold: (1) $f(x)$ has exactly one root in $\mathbb{F}_q$; (2) For any positive integer $1\le i\le q-2$ satisfying $p\nmid i$, the degree of $[f(x)]^j \modulo x^q - x$ is at most $q-2$.', 'concept/theorem_2': 'Theorem 2: Let $q$ be a prime power. If $n\mid q-1$, then there are no permutation polynomials of degree $n$ over the finite field $\mathbb{F}_q$.', 'concept/theorem_3': 'Theorem 3: Let $q$ be an odd prime power. $x^{\frac{q+1}{2}}+ax$ is a permutation polynomial over $\mathbb{F}_q$ if and only if $a^2-1$ is a non-zero square number in $\mathbb{F}_q$.', 'concept/theorem_4': 'Theorem 4: Let $q$ be a prime power and $n$ be a positive integer. If there exists a permutation polynomial of degree $n$ that is not an exceptional polynomial over the finite field $\mathbb{F}_q$, then $q\le (\frac{(n-2)(n-3)+\sqrt{(n-2)^2(n-3)^2+8n-12}}{2})^2$. ', 'concept/theorem_5': 'Theorem 5: Let $q$ be a prime power and $n$ be a positive integer. There exists an exceptional polynomial of degree $n$ over the finite field $\mathbb{F}_q$ if and only if $\gcd(n,q-1)=1$. '}	$q=3$：$f(x)=x^4+2x^2+cx+d(c\ne 0, d\in \mathbb{F}_3)$; $q=7$：$f(x)=x^4+cx+d(c=3,4, d\in \mathbb{F}_7)$	理论基础学科-基础数学-代数	123323	1	3
recuTJaLaBXcTj	The first-order phase transition of the universe produces a large number of bubbles. Particles passing through the bubble wall into the bubble gain mass, and some particles are blocked outside the bubble due to insufficient kinetic energy. Assume the phase transition temperature is $T$, the mass of particles inside the bubble is $m_d$, and the degrees of freedom are $g_d$. Consider a bubble wall with a velocity $v_w$ in the z-direction. How many particles can pass through the bubble wall? Give a specific expression.	"{'step_1': ""The momentum of a particle is $\mathbf{p}=\left(p_x, p_y, p_z\right)$. Outside the bubble, the particle is massless, and its energy is $E=|\mathbf{p}|=\sqrt{p_x^2+p_y^2+p_z^2}$. After passing through the bubble wall, the particle gains mass $m_d$, and its energy is $E=\sqrt{p_x^2+p_y^2+p_z^2+m_d^2}$. Energy conservation requires that the total energy of the particle remains unchanged when passing through the bubble wall: $\sqrt{p_x^2+p_y^2+p_z^2}=\sqrt{p_x^{' 2}+p_y^{' 2}+p_z^{' 2}+m_d^2}$. The bubble wall does not change $p_x$ and $p_y$ (transverse momentum conservation), so: $\sqrt{p_x^2+p_y^2+p_z^2}=\sqrt{p_x^2+p_y^2+p_z^{' 2}+m_d^2}$, from which we get $p_z^2=p_z^{' 2}+m_d^2$. $p_z^{' 2}=\sqrt{p_z^2-m_d^2}$ must be a real number, that is: $p_z^2 \geq m_d^2 \Longrightarrow\left|p_z\right| \geq m_d$. Thus, the condition for particle penetration is obtained: $|p_z| \geq m_d$"", 'step_2': 'Write the number of particles $\Delta N_{\rm in}$ passing through area $\Delta A$ in time $\Delta t$ in the bubble wall frame: $\frac{\Delta N_{\rm in}}{\Delta A} &=& \frac{g_d}{(2 \pi)^3} \, \int d^3\vec{p} \, \int^{r_0-\frac{p_z \, \Delta t}{|\vec{p}|}}_{r_o} dr \, \mathcal{T}(\vec{p}) \, \Theta(-p_z) \, f(\vec{p};\vec{x}) \\\\ &=& \frac{g_d}{(2\pi)^3}\int d^3\vec{p} \, \Theta(-p_z-m_d) \, \Theta(-p_z) \, (-\frac{p_z \, \Delta t}{|\vec{p}|}) \, f(\vec{p};\vec{x}).', 'step_3': 'The incident particle flux can be expressed as the number of particles passing through per unit time and per unit area, $J_w\equiv\frac{dN}{dAdt}\simeq\frac{\Delta N_{\rm in}}{\Delta A \, \Delta t}$: $J_w &=& \frac{g_d}{(2\pi)^3} \int d^3\vec{p} \, \Theta(-p_z-m_\chi) \, \Theta(-p_z) \, f(\vec{p};\vec{x}) \, (-\frac{p_r}{|\vec{p}|}) \\\\ &=& \frac{g_d}{(2\pi)^3} \int^{\pi}_{0} d\theta \, \sin\theta \, \int^{2\pi}_0 d\phi \, \int^{\infty}_{0} dp \, p^2 (-\cos\theta) \, \Theta(-p\cos\theta-m_d) \, \Theta(-\cos\theta) \, f(\vec{p})', 'step_4': 'Adopt the Boltzmann approximate distribution for the incident particle distribution function to facilitate subsequent analytical integration: $f(\vec{p}) = \frac{1}{e^{\gamma_w \, (E \, - \, \vec{v_w} \cdot \vec{p})/T} \pm 1 } \simeq\frac{1}{e^{\gamma_w\, (p \, + \, v_w \, p \, \cos\theta )/T}}.$ where the Lorentz factor $\gamma_w=1/\sqrt{1-v_w^2}$ and $T$ is the temperature of the particle.', 'step_5': 'Perform analytical integration and transform back to the plasma frame through Lorentz transformation: $n^{\rm in}&=& \frac{g_d}{(2\pi)^2 \gamma_w v_w} \int^{-1}_0 d\cos\theta \, \cos\theta \int^{\infty}_{-\frac{m_d}{\cos\theta}} dp \, \frac{p^2}{e^{\gamma_w \, (1 \, + \, v_w \, \cos\theta ) \, p / T}} \\\\ &=& \frac{g_d T^3}{\gamma_w v_w}\frac{1 \, + \, \gamma_w \, m_d \, (1-v_w)/T }{4 \, \pi^2 \, \gamma_w^3 \, (1-v_w)^2} \, e^{-\gamma_w\, m_d \, (1-v_w)/T}'}'"	{'concept_1': 'Energy conservation: a basic condition that must be satisfied in any reference frame (bubble wall frame)', 'concept_2': 'Lorentz transformation: converting physical quantities from one reference frame to the reference frame to be solved', 'concept_3': 'The number of particles $\Delta N_{\rm in}$ passing through area $\Delta A$ in time $\Delta t$ in the bubble wall frame: $\frac{\Delta N_{\rm in}}{\Delta A} &=& \frac{g_d}{(2 \pi)^3} \, \int d^3\vec{p} \, \int^{r_0-\frac{p_z \, \Delta t}{|\vec{p}|}}_{r_o} dr \, \mathcal{T}(\vec{p}) \, \Theta(-p_z) \, f(\vec{p};\vec{x}) \\\\ &=& \frac{g_d}{(2\pi)^3}\int d^3\vec{p} \, \Theta(-p_z-m_d) \, \Theta(-p_z) \, (-\frac{p_z \, \Delta t}{|\vec{p}|}) \, f(\vec{p};\vec{x}).', 'concept_4': 'The distribution function adopts the Boltzmann approximation $f(p) = \exp(-\gamma_w(E-\vec{v_w}\dot \vec{p})/T)$'}	The final number density of particles passing through the bubble wall into the bubble is: $n^{\rm in}= \frac{g_d T^3}{\gamma_w v_w}\frac{1 \, + \, \gamma_w \, m_d \, (1-v_w)/T }{4 \, \pi^2 \, \gamma_w^3 \, (1-v_w)^2} \, e^{-\gamma_w\, m_d \, (1-v_w)/T}$	理论基础学科-理论物理-分析力学	101944	2	2
recuTKmYlCFlBN	Under the quasi-topological pure gravity theory, without considering the cosmological constant, if n=2, it reduces to the Einstein-Gauss-Bonnet theory. What is the equation of motion under the 5-dimensional spherically symmetric static metric?	{'step_1': 'First, starting from the Gauss–Bonnet action in theorem_1, by varying the metric $g_{\mu\nu}$, the gravitational field equation $G_{\mu\nu}+\alpha H_{\mu\nu}=0\,.$ in concept_1 can be obtained.', 'step_2': 'Next, specifically consider the 5-dimensional static spherically symmetric metric and gauge potential in theorem_2, and focus on the (t,t) component. $G_{tt}=-\frac{3 f(r)\bigl(r f'(r)+2f(r)-2\bigr)}{2r^2},\qquad G^{t}{}_{t}=g^{tt}G_{tt}=\frac{3\bigl(r f'(r)+2f(r)-2\bigr)}{2r^2},$ while concept_2 gives $H^{t}{}_{t}=-\frac{6\,(f(r)-1)\,f'(r)}{r^{3}}.$', 'step_3': 'Substitute both into the (t,t) component $G^{t}{}_{t}+\alpha H^{t}{}_{t}=0$ of the field equation and simplify to get the answer: $\frac{3\,(r f'+2f-2)}{2r^2}-\frac{6\alpha\,(f-1)f'}{r^3}=0$.'}	{'concept/theorem_1': 'Gauss–Bonnet Action | \\begin{equation} I[g_{\mu\nu},A_{\mu}] = \frac{1}{16\pi}\int d^{D}x\,\sqrt{-g}\, \Bigl[R + \alpha\bigl(R_{\mu\nu\rho\sigma}R^{\mu\nu\rho\sigma} - 4R_{\mu\nu}R^{\mu\nu} + R^{2}\bigr)\Bigr]\,, \\end{equation} where \\alpha is the coupling constant of Gauss–Bonnet.', 'concept/theorem_2': 'Now specifically consider the 5-dimensional static spherically symmetric (SSS) spacetime. The metric and gauge potential are taken as \\begin{equation} ds^{2} =- f(r)\,dt^{2}+ \frac{dr^{2}}{f(r)} + r^{2} d \Omega_3\,. \\end{equation}', 'concept_1': 'By varying the metric $g_{\mu\nu}$, we obtain \\begin{equation}\\label{eq:EGBM_einstein} \\delta g_{\mu\nu}:\qquad G_{\mu\nu} + \alpha\,H_{\mu\nu} = 0\,, \\end{equation} where \\begin{align} G_{\mu\nu} &\equiv R_{\mu\nu}-\tfrac12 g_{\mu\nu}R\,,\\nonumber\\\\ H_{\mu\nu} &\equiv 2RR_{\mu\nu}-4R_{\mu\rho}R_{\nu}{}^{\rho} -4R_{\mu\rho\nu\sigma}R^{\rho\sigma} +2R_{\mu}{}^{\rho\sigma\lambda}R_{\nu\rho\sigma\lambda}\\nonumber\\\\ &\qquad - \frac{1}{2}g_{\mu\nu}\bigl(R_{\alpha\beta\gamma\delta}R^{\alpha\beta\gamma\delta} -4R_{\alpha\beta}R^{\alpha\beta}+R^{2}\bigr)\,. \\end{align}', 'concept_2': 'The (t,t) component of the gravitational field equation can be written as a first-order differential equation \\begin{align} H^{t}{}_{t} = -\frac{6\,(f(r)-1)\,f'(r)}{r^{3}}\\,. \\end{align}'}	\frac{3\,(r f'+2f-2)}{2r^2}-\frac{6\alpha\,(f-1)f'}{r^3}=0	理论基础学科-理论物理-几何	193691	2	5
recuTPmUpdGG24	What’s the mian driving forces by which the Indian Plate has subducted beneath Tibet？	{'step_1': 'Firstly, the subducting Indian continental lithosphere is too buoyant to exert a driving force sufficient to explain the ongoing India–Asia convergence (①). So other external forces have had to be invoked (②).', 'step_2': 'However, both ocean ridge and pull of slabs are proved to be insufficient to explain the present-day India–Asia convergence(③④).', 'step_3': 'Based on precious seismic evidence, the deep-rooted mantle currents may influence continental lithospheric deformation stretching from the Tibetan to the Baikal-Mongolia Plateau by the asthenosphere flowing underneath(⑤).', 'step_4': 'Consistent with this, geodynamc model has simulated a plume developed in the mantle beneath the southern Neo-Tethys Ocean which produces the northward-directed mantle flow(⑥). Ultimately, the mantle flow is verified to be sourced from Réunion plume (⑦) to provide a main driving force of Indian plate.'}	{'concept_1': 'The subducting Indian continental lithosphere is too buoyant to exert a driving force sufficient to explain the ongoing India–Asia convergence.', 'concept_2': 'Other external forces, such as push by the Indian Ocean ridge and pull of slabs, have had to be invoked', 'concept_3': 'The northward Indian slab pull should have represented a role in modulating the motion of the initial India–Asia collisional convergence, but not sustain the process.', 'concept_4': 'The far-field ridge push forces appears insufficient to explain the present-day deformation associated with the India–Asia convergence.', 'concept_5': 'Seismic evidence reveals the deep-rooted mantle currents may influence the widely distributed continental lithospheric deformation stretching from the Himalaya to the Baikal-Mongolia Plateau by the asthenosphere flowing underneath.', 'concept_6': 'A plume naturally developed in the mantle beneath the southern Neo-Tethys Ocean, producing the northward-directed mantle flow.', 'concept_7': 'The rapid northward motions of the Indian Plate during the early Cenozoic have long been attributed to a driving force originating from the Réunion plume head'}	Mantle flow	其他理论学科-地质力学与地球动力学-板块运动理论	175316	4	7
recuTUUPJRgInk	It is known that the closer the 2p band center of pre-adsorbed oxygen species is to the Fermi level, the stronger their Lewis basicity (i.e., the ability to capture H) becomes, thereby promoting methane activation. When the d-band center of the dopant is higher than the 2p band center of pre-adsorbed oxygen species, the dopant will excessively adsorb oxygen species and exhibit inhibitory effects. Therefore, for single-atom alloy structures formed on Ag(111) surfaces doped with different transition metals (Pt, Rh, Au, Cu, Mn, Cr), will pre-adsorbed oxygen species promote or inhibit methane activation?	{'step_1': 'Rh and Pt doping can significantly enhance the methane activation activity on Ag(111) surfaces, but pre-adsorbed oxygen species inhibit C-H bond cleavage on metals with high activity, thus pre-adsorbed oxygen species will inhibit methane activation on Rh and Pt doped Ag(111) surfaces.', 'step_2': 'Considering the d-band center theory, the d-band centers of Au and Cu are very low, far lower than the 2p band center of pre-adsorbed oxygen species, thus showing a promoting effect on methane activation. Rh and Pt with higher d-band centers show an inhibitory effect.', 'step_3': 'For Mn and Cr doping, although their d-band centers are relatively high, making them easy to combine with O and lead to deactivation, doping significantly improves the O2 activation ability, optimizes the 2p band center of O* and stabilizes the dual-site five-center transition state, thereby promoting methane activation and thus also showing a promoting effect.'}	{'concept_1': 'Pre-adsorbed oxygen species promote C-H bond cleavage on metals with lower activity, while inhibiting C-H bond cleavage on metals with higher activity', 'concept_2': 'Rh and Pt doping significantly enhance the methane activation activity on Ag(111) surfaces, and Mn and Cr doping significantly enhance the O2 activation activity on Ag(111) surfaces'}	Inhibit methane activation on Pt and Rh doped Ag(111) surfaces, and promote methane activation on Au, Cu, Mn and Cr doped Ag(111) surfaces.	理论基础学科-理论化学-计算化学	363372	3	4
recuTVLTMDCBFT	Consider problem (P): minimize $ \varphi(x, u) $ subject to $ u = 0 $, where $ \varphi(x, u) = f_0(x) + g(F(x) + u) $, where $ g $ is a proper closed convex function, $ f_0 $ is twice continuously differentiable, and $ F $ is once continuously differentiable. Define $ l_r(x, y) = \inf_u \{\varphi(x, u) - y \cdot u + (r/2)|u|^2\} $ for $ r > 0 $, where $ |u| = ||u||_2 $. The variational sufficient condition for local optimality in (P) holds with respect to $ x^* $ and $ y^* $ satisfying the first-order condition if there exists $ r > 0 $ such that $ \varphi_r $ is variationally convex with respect to the pair $ ((x^*, 0), (0, y^*)) $ in $ \text{gph } \partial \varphi_r $, where $ \varphi_r(x, u) = \varphi(x, u) + (r/2)|u|^2 $. The strong variational sufficient condition holds if $ \varphi_r $ is variationally strongly convex. Question: The strong variational sufficient condition for local optimality holds for (P) if and only if $ l_r(x,y) $ satisfies what conditions respectively with respect to $ (x,y) $? Note the range of $ (x,y) $ and the value of r. Hint: The convexity of x is independent of the concavity of y.	"{'step_1': 'Analyze the relationship between $l_r(x,y)$ and $\\varphi_r(x,u)$. By definition, $l_r(x,y) = \\inf_u \\{\\varphi_r(x,u) - y \\cdot u\\}$, where $\\varphi_r(x,u) = \\varphi(x,u) + (r/2)|u|^2$. Therefore, $l_r(x,y)$ is the infimum function of $\\varphi_r(x,u)$ with respect to the variable $u$ (taking inf over $u$).', 'step_2': 'Analyze the properties of $\\varphi_r$ using the definition of variational strong convexity (concept_1). The strong variational sufficient condition requires $\\varphi_r$ to be variationally strongly convex, i.e., there exists an open convex neighborhood containing $((x^*, 0), (0, y^*))$, and there exists a strongly convex lsc function $\\hat{\\varphi}_r \\leq \\varphi_r$ that satisfies the graph condition and the value equality condition. This means $\\varphi_r$ has local strong convexity characteristics.', 'step_3': 'Derive the conditions for $l_r(x,y)$ with respect to $x$. For a fixed $y$, the strong convexity of $\\varphi_r(x,u) - y \\cdot u$ with respect to $x$ can be transferred to the infimum function $l_r(x,y)$. Since $\\varphi_r$ is variationally strongly convex, its strong convexity with respect to $x$ is preserved locally (for $x \\in X$, where $X$ is a closed convex neighborhood of $x^*$), so $l_r(x,y)$ is strongly convex with respect to $x$ in $X$ (when $y \\in Y$, where $Y$ is a closed convex neighborhood of $y^*$).', 'step_4': 'Derive the conditions for $l_r(x,y)$ with respect to $y$. For a fixed $x$, $\\varphi_r(x,u) - y \\cdot u$ is a linear function with respect to $y$ (since $-y \\cdot u$ is linear in $y$), and linear functions are concave. According to concept_2 (the infimum of concave functions is still concave), after taking inf over $u$, $l_r(x,y)$ remains concave with respect to $y$. Combined with the locality of variational strong convexity, this concavity holds in $Y$ (when $x \\in X$).', 'step_5': 'Determine the range of $r$. The strong variational sufficient condition requires the existence of $r > 0$ such that $\\varphi_r$ is variationally strongly convex, and the ""if and only if"" in the problem requires this condition to hold for sufficiently large $r$ (since the strong convexity of $(r/2)|u|^2$ increases with $r$, ensuring overall strong convexity).', 'step_6': 'Summarize the conditions. Based on the above analysis, the strong variational sufficient condition holds if and only if: for sufficiently large $r > 0$, there exists a closed convex neighborhood $X \\times Y$ of $(x^*, y^*)$ such that $l_r(x,y)$ is strongly convex with respect to $x$ when $x \\in X$ (for $y \\in Y$), and concave with respect to $y$ when $y \\in Y$ (for $x \\in X$).'}"	{'concept_1': 'Definition (Variational convexity) The lsc function \\( f \\) will be called variationally convex at \\( \\bar{x} \\) for \\( \\bar{v} \\in \\partial f (\\bar{x}) \\) if for some open convex neighborhood \\( X \\times V \\) of \\( (\\bar{x}, \\bar{v}) \\) there is a convex lsc function \\( \\hat{f} \\leq f \\) on \\( X \\) such that, for some \\( \\varepsilon > 0 \\), \\([X_\\varepsilon \\times V] \\cap \\text{gph } \\partial f = [X \\times V] \\cap \\text{gph } \\partial \\hat{f}\\) and \\( f(x) = \\hat{f}(x) \\) at the common elements \\( (x, v) \\). It will be called variationally strongly convex at \\( \\bar{x} \\) for \\( \\bar{v} \\) with modulus \\( \\sigma > 0 \\) if this holds with \\( \\hat{f} \\) strongly convex on \\( X \\) with that modulus.', 'concept_2': 'Theorem：Suppose that \\( \\{f_i\\}_{i\\in I} \\) is a set of concave function, then function \\( f(x):=\\inf_{i\\in I} f_i(x) \\) is also a concave function.'}	for r > 0 sufficiently large, there is a closed convex neighborhood X × Y of (x*, y*) such that lr(x, y) is strongly convex in x ∈ X when y ∈ Y as well as concave in y ∈ Y when x ∈ X	理论基础学科-基础数学-最优化理论	179666	2	2
recuTXI7RKHToP	How can phosphotungstenic acid (HPW) modification modify the structure and properties of MnCeOₓ nanowire aerogel catalysts to achieve excellent anti-SO₂ toxicity under low temperature and high sulfur flue gas? You need to answer from two aspects, on the one hand, why it changes the crystal lattice structure and increases a certain elemental vacancy, and on the other hand, it produces a special nitrate species, and you have to answer what kind of nitrate species it is, so as to produce superior SO2 resistance.	{'step_1': 'Understand the fundamental properties of HPW and its interaction with MnCeOₓ. HPW (phosphotungstic acid, H₃PW₁₂O₄₀) is a heteropolyacid with a Keggin structure with strong Brønsted acidity, high redox and good thermal stability. When HPW is modified onto MnCeOₓ nanowire aerogels, it interacts strongly with metal oxide (Mn and Ce) surfaces, mainly through hydrogen bonding, electrostatic action, or chemical bonding. MnCeOₓ nanowire aerogels themselves have a high specific surface area, porous structure, and abundant oxygen vacancy, but the introduction of HPW can further modulate these properties.', 'step_2': 'Apply Concept_1 analysis of valence regulation and structural optimization. Rare earth metal doping (e.g., Nd) can reduce the average valence state of Mn, increase the proportion of low-valence Mn oxides (e.g., Mn₂O₃, Mn₃O₄, MnO), reduce MnO₂, and optimize low-temperature reducibility through high dispersion. Similarly, HPW, as a strong electron acceptor, can extract electrons from MnCeOₓ, resulting in a decrease in the Mn valence state. This is in line with mathematical logic: according to electron transfer theory, the electron-deficiency of HPW (due to the high oxidation state of W⁶) attracts electrons from Mn or Ce species, reducing Mn⁴ to Mn³ or Mn². XPS or XANES analysis confirms this valence change. At the same time, the introduction of HPW drives lattice rearrangement, increasing oxygen vacancies (Vo) because charge compensation is required (oxygen vacancies are reduced). This enhances the redox ability of the catalyst and the low-temperature NH₃-SCR activity (e.g., H₂-TPR shows improved reducibility). In addition, the high dispersion of HPW prevents Mn species agglomeration and increases the number of active sites. Therefore, HPW modification achieved an Nd-doped effect through electronic structure regulation, but may be more significant due to the strong acidity of HPW.', 'step_3': 'Apply Concept_2 to analyze surface reaction pathways and anti-SO₂ mechanisms. Single tooth and bridge nitrates are key intermediates in the SCR reaction, but are susceptible to SO₂ toxication (sulfate formation). HPW modifications enhance the production of another nitrate, Bidentate Nitrate. Bidentate forms a stable chelating structure with two oxygen atoms and metal sites (such as Ce⁴), which has high bonding energy, strong chemical inertness, and is not easily attacked by SO₂. This is in line with surface chemistry: the highly acidic site of HPW promotes NO₂ adsorption and oxidation, preferentially forming bidentate over single-toothed/bridge species. The in situ DRIFTS experiment verifies this transition. Didentate nitrate reacts with NH₃ to form NH₄⁺-NO₃⁻ intermediates, which are then decomposed into N₂ and H₂O, and this pathway is not affected by SO₂, because SO₂ is difficult to replace stable bidentate coordination. Therefore, HPW modification avoids the deposition of ammonium sulfate or metal sulfate by reconstructing the surface reaction path, thereby imparting anti-SO₂ properties.'}	{'concept_1': 'According to literature reports, researchers have found that if Nd is added to a manganese-based catalyst (30% Mn - 3% RE/TiO₂) modified with titanium dioxide as a carrier and modified by rare earth metals, it can reduce the average valence state of Mn elements. The XANES linear combination analysis further confirmed that Nd doping significantly increased the proportion of low-valence Mn oxides (Mn₂O₃, Mn₃O₄, MnO) and decreased the content of MnO₂. This valence rearrangement and the high dispersion of Mn species on the TiO₂ support (XRD and SEM results) optimize the low-temperature reduction of the catalyst, thereby improving its low-temperature NH₃-SCR activity.', 'concept_2': 'Other researchers have found that monodonate and bridgen nitrate are the key reactants in the traditional catalyst reaction, which react with NH3 to form NH4-NO3- intermediates.'}	The HPW-MnCeOx-N catalyst effectively reduced the average oxidation state of Mn elements, increased the surface oxygen vacancy concentration, and significantly promoted the generation of active didentate nitrate species by introducing HPW. In the presence of SO2, the single-toothed and bridge-type nitrate species on the surface of the catalyst were inhibited, while the key bidentate active sites remained highly reactive, thus exhibiting excellent anti-SO2 performance.	理论基础学科-理论化学-材料表界面	43183	1	2
recuUkfhG1fX9f	Does the addition of a 2,2′-bipyridine coating on Pt/C and Ir/C electrodes enhance or weaken the HER?	"{'step_1': 'Analyzing the Core Problem | The central question in the Instruction is to determine whether the 2,2′-bipyridine overlayer enhances or suppresses the hydrogen evolution reaction (HER) activity of Pt/C and Ir/C catalysts, and to elucidate the underlying mechanism.', 'step_2': ""Applying Core Strategy | Introducing 2,2′-bipyridine molecules onto catalyst surfaces constitutes a classic 'Interfacial Overlayer Modulation Strategy' (Concept_1). This strategy aims not at physical blocking but directly modulates intrinsic catalytic activity by altering the electronic structure of catalyst surfaces."", 'step_3': ""Linking Reaction Principles | HER activity is governed by 'Hydrogen Binding' (Concept_2) strength. According to the Sabatier Principle, an ideal catalyst should exhibit moderate adsorption toward hydrogen intermediates (*H). Pt and Ir, as top-tier catalysts, exhibit slightly strong hydrogen adsorption. Thus, further enhancing their activity requires appropriately weakening hydrogen adsorption energy."", 'step_4': ""Introducing Electronic Structure Theory | Per 'd-band Center Theory' (Concept_3), the energy position of a transition metal's d-band center directly determines its bonding strength with adsorbates (e.g., *H). A lower d-band center energy corresponds to weaker adsorbate bonding."", 'step_5': ""Analyzing Overlayer's Electronic Effects | When 2,2′-bipyridine adsorbs as an overlayer on Pt or Ir surfaces, its π-backbonding effect induces electron donation from metal d-orbitals to the ligand. This electron depletion reduces the filling of metal d-orbitals, consequently lowering the d-band center energy."", 'step_6': ""Synthesis and Conclusion | The 2,2′-bipyridine overlayer causes downward shifting of Pt/Ir d-band centers (Step_5). According to d-band theory, this weakens metal-*H adsorption strength (Step_4). Given Pt/Ir's originally strong hydrogen adsorption, this weakening brings adsorption strength closer to the optimal point described by the Sabatier Principle (Step_3). Therefore, the 2,2′-bipyridine overlayer ultimately 'enhances' HER activity.""}"	{'concept_1': 'Interfacial Overlayer Modulation Strategy | A strategy that precisely regulates the microenvironment of the electrode-electrolyte interface and the electronic structure of catalyst surfaces by introducing strongly adsorbed functional molecules or thin films stably present at HER-relevant potentials. This strongly adsorbed overlayer can directly alter the intrinsic activity of catalysts rather than merely acting as a physical barrier.', 'concept_2': 'Hydrogen Binding | According to the Sabatier Principle, an ideal catalyst should exhibit moderate adsorption strength toward reaction intermediates (denoted as *H in this context).', 'concept_3': 'd-band Center Theory | A fundamental theory describing the catalytic activity of transition metals. It posits that the energy center of metal d-orbitals (d-band center) governs the strength of bonding with adsorbates.'}	Enhance	理论基础学科-理论化学-催化化学	206820	4	10
recuUhtDlBvR0z	Determine the lower bound for the relative distance δ of a Geometric Arrays for Polynomials (GAP_{m,d,t}) code. This code evaluates m-variate polynomials of degree at most d, and is constructed from a set of t hyperplanes in general position, where t is defined as m+d+εd for a given ε > 0. Relative distance is the ratio of the minimum distance of the code to its block length.	{'step_1': 'The objective is to calculate the lower bound for the relative distance δ of a specific GAP code where the number of hyperplanes is t = m+d+εd.', 'step_2': 'Based on the definition (concept_2), the relative distance δ is the minimum distance divided by the block length. We can express its lower bound as δ ≥ (Minimum Distance Bound / Block Length).', 'step_3': 'Using theorem_1, we establish the lower bound for the minimum distance as (t-d choose m).', 'step_4': 'Using theorem_2, we identify the block length as (t choose m).', 'step_5': 'We substitute these into the inequality from Step_2, yielding δ ≥ ((t-d choose m) / (t choose m)).', 'step_6': 'Now, we substitute the specific value t = m+d+εd into this expression. The numerator becomes ((m+d+εd)-d choose m) = (m+εd choose m), and the denominator becomes (m+d+εd choose m).', 'step_7': 'The inequality for the relative distance is now δ ≥ ((m+εd choose m) / (m+d+εd choose m)).', 'step_8': 'To simplify, we expand the binomial coefficients, which results in the product form: δ ≥ ∏_{i=0}^{m-1}((m+εd-i) / (m+d+εd-i)).', 'step_9': 'By analyzing each term in the product, we observe that for 0 ≤ i < m, the term ((m+εd-i) / (m+d+εd-i)) is minimized as i increases but is always greater than the limiting behavior, which approaches (εd / (d+εd)) = (ε / (1+ε)). Therefore, we can bound each term below by (ε / (1+ε)).', 'step_10': 'Since the expression is a product of m such terms, the final lower bound is the product of their individual bounds, resulting in δ ≥ (ε / (1+ε))^m.'}	{'concept_1': 'A Geometric Arrays for Polynomials (GAP) code is a type of polynomial evaluation code where the evaluation points are the m-wise intersection points of a given set of hyperplanes in general position.', 'concept_2': 'The relative distance δ of a code is defined as the ratio of its minimum distance to its total block length.', 'theorem_1': 'For a GAP_{m,d,t} code constructed using t hyperplanes where t > m+d, its minimum distance is at least (t-d choose m).', 'theorem_2': 'The block length of a GAP_{m,d,t} code constructed from t hyperplanes is exactly (t choose m).'}	(ε / (1+ε))^m	理论基础学科-理论计算机科学-算法理论	716648	5	4
recuUcQt0bLL6m	What is the improved time complexity for solving TENSOR ISOMORPHISM-complete problems over a finite field \(\mathbb{F}_q\), such as CUBIC FORM EQUIVALENCE and ALGEBRA ISOMORPHISM, by utilizing the novel linear-length reductions presented in the thesis? The previous state-of-the-art was the brute-force bound of \(q^{O(n^2)}\). TENSOR ISOMORPHISM asks if two 3-way arrays (tensors) \(A, A'\) are equivalent up to invertible linear transformations on each of their three vector spaces, i.e., if there exist invertible matrices P, Q, R such that \(a'_{i',j',k'} = \sum_{i,j,k} P_{i'i}Q_{j'j}R_{k'k}a_{ijk}\).	{'step_1': 'The objective is to find a faster algorithm for TENSOR ISOMORPHISM-complete problems (Concept_2) over a finite field \\(\\mathbb{F}_q\\), improving upon the existing \\(q^{O(n^2)}\\) brute-force method.', 'step_2': 'The thesis leverages recent progress in a related, but more structured, problem called ALTERNATING MATRIX SPACE ISOMETRY (AMSI) (Concept_3).', 'step_3': 'Based on a breakthrough for p-group isomorphism, an improved algorithm for AMSI exists. For an AMSI instance defined by matrices of size roughly L \\times L, this algorithm runs in time \\(q^{\\tilde{O}(L^{1.5})}\\) (Theorem_1).', 'step_4': 'The critical step is to connect the general TI problem to the specific AMSI problem efficiently. If a reduction from TI to AMSI significantly increases the problem size, the algorithmic gains for AMSI would be lost. For instance, a quadratic blow-up from size n to n^2 would result in a runtime of \\(q^{\\tilde{O}((n^2)^{1.5})} = q^{\\tilde{O}(n^3)}\\), which is worse than brute force.', 'step_5': 'The main technical novelty of the thesis is the creation of a new reduction from TENSOR ISOMORPHISM to AMSI that causes only a linear increase in the size parameters (Theorem_2).', 'step_6': 'Using this new reduction, an instance of TENSOR ISOMORPHISM on tensors with side lengths of order n (total length L=O(n)) can be transformed into an equivalent AMSI instance where the new matrix dimension is also of order O(n).', 'step_7': 'By applying the faster AMSI algorithm (from Theorem_1) to this new, linearly-sized instance, the overall time complexity becomes \\(q^{\\tilde{O}((O(n))^{1.5})}\\), which simplifies to \\(q^{\\tilde{O}(n^{3/2})}\\). This provides the improved runtime for TI and its complete problems.'}	{'concept_1': 'TENSOR ISOMORPHISM (TI) is the problem of determining if two 3-way arrays are equivalent under the action of the product of general linear groups on each of the three indices.', 'concept_2': 'A TI-complete problem is an isomorphism problem that is polynomial-time reducible to TENSOR ISOMORPHISM, and TENSOR ISOMORPHISM is also polynomial-time reducible to it. Examples include CUBIC FORM EQUIVALENCE and ALGEBRA ISOMORPHISM.', 'concept_3': 'ALTERNATING MATRIX SPACE ISOMETRY (AMSI) is the problem of deciding if two tuples of alternating matrices, \\((A_1, ..., A_m)\\) and \\((B_1, ..., B_m)\\), are isometric, which is equivalent to testing the isomorphism of p-groups of class 2 and exponent p.', 'theorem_1': 'Isomorphism of p-groups of class 2 and exponent p of order \\(N\\) can be tested in time \\(N^{\\tilde{O}((\\log N)^{1/2})}\\).', 'theorem_2': 'There exists a polynomial-time reduction from TENSOR ISOMORPHISM for \\(n \\times m \\times l\\) tensors to ALTERNATING MATRIX SPACE ISOMETRY, where the resulting matrices have a size that is a linear function of \\(n, m,\\) and \\(l\\).'}	\(q^{\tilde{O}(n^{3/2})}\)	理论基础学科-理论计算机科学-形式语言与自动机	249653	4	4
recuUbuDFVXupE	What is the optimal space complexity, as a function of time $t(n)$, for simulating a multitape Turing machine that runs in time $t(n)$ by reducing the computation to a series of Tree Evaluation instances and applying the space-efficient algorithm by Cook and Mertz? The time function $t(n)$ is assumed to be greater than or equal to $n$.	"{'step_1': 'The initial problem is to find the minimum space required to simulate a multitape Turing machine running in $t(n)$ time. The chosen strategy is to reduce this simulation to the Tree Evaluation problem.', 'step_2': 'First, using Concept_1, the arbitrary $t(n)$-time multitape Turing machine is converted into an equivalent $O(t(n))$-time block-respecting machine. The computation is divided into $B = O(t(n)/b(n))$ time blocks, each of length $b(n)$, where $b(n)$ is a parameter to be optimized later.', 'step_3': 'Next, the flow of information between these blocks is modeled using a Computation Graph (Concept_2). The state of the computation at the end of the final time block, $B$, depends on the states of previous blocks. This dependency structure forms a directed acyclic graph.', 'step_4': ""This computation graph is then transformed into an instance of the Tree Evaluation problem (Concept_3). The root of the tree is tasked with computing the final state of the machine (the content of the relevant tape block in time block $B$). The children of a node in the tree correspond to the nodes in the computation graph that are required to compute the value of the parent node. The height of this tree, $h$, is at most the number of time blocks, so $h = O(t(n)/b(n))$. The bit-length of the values at each node, $b$, is the information needed to represent a tape block's content, so $b = O(b(n))$. The fan-in, $d$, is a constant determined by the number of tapes."", 'step_5': ""To solve this Tree Evaluation instance, we apply the space-efficient Cook-Mertz algorithm from Theorem_1. Substituting the parameters from our construction into the theorem's space complexity formula, we get a total space bound of $S(n) = O(d \\cdot b + h \\log(d \\cdot b)) = O(b(n) + \\frac{t(n)}{b(n)}\\log(b(n)))$. This expression also accounts for the space needed to store and enumerate the computation graphs."", 'step_6': 'The final step is to minimize this space complexity by choosing an optimal value for the block size parameter $b(n)$. The two terms in the space complexity are balanced when $b(n) \\approx \\frac{t(n)}{b(n)}\\log(t(n))$. This gives $b(n)^2 \\approx t(n)\\log t(n)$, so the optimal choice is $b(n) = \\sqrt{t(n)\\log t(n)}$.', 'step_7': 'Substituting this optimal $b(n)$ back into the space complexity formula yields $S(n) = O(\\sqrt{t(n)\\log t(n)} + \\frac{t(n)}{\\sqrt{t(n)\\log t(n)}}\\log(\\dots)) = O(\\sqrt{t(n)\\log t(n)})$. This is the final answer for the space complexity of the simulation.'}"	"{'concept_1': 'Block-Respecting Turing Machines. A multitape Turing machine where the computation is partitioned into time blocks of length $b(n)$, and each tape is partitioned into tape blocks of length $b(n)$. Tape heads only cross between tape blocks at the end of a time block.', 'concept_2': 'Computation Graph. A directed acyclic graph used to model the information flow of a computation. For a block-respecting machine, nodes can represent the contents of tape blocks at specific time blocks, and edges represent the dependencies required to compute the contents of a later block from earlier ones.', 'concept_3': 'The Tree Evaluation Problem. A computational problem where one is given a tree. Each leaf is labeled with a value (a b-bit string), and each internal node is labeled with a function. The value of an internal node is determined by applying its function to the values of its children. The goal is to compute the value of the root node.', 'theorem_1': ""Space-Efficient Algorithm for Tree Evaluation (Cook & Mertz). The Tree Evaluation problem on a tree with a maximum height of $h$, a maximum fan-in of $d$, and where each node's value is a b-bit string, can be solved in $O(d \\cdot b + h \\log(d \\cdot b))$ space.""}"	$O(\sqrt{t(n)\log t(n)})$	理论基础学科-理论计算机科学-形式语言与自动机	148995	5	4
recuU5qv72o22y	What is the expected communication cost, in bits, of the one-round algorithm for $l_p$ heavy hitter estimation ($p \ge 2$) in the coordinator model, which aims to estimate each frequency $v_j$ with an additive error bounded by $2\epsilon l_{p}^{\prime}(v)$? In this context, the coordinator model involves $k$ sites, each holding a frequency vector, and $l_{p}^{\prime}(v)$ is defined as $(\sum_{i=1}^{k}\sum_{j=1}^{n}v_{ij}^{p})^{1/p}$.	"{'step_1': 'The primary goal is to determine the communication cost for the $l_p$ heavy hitter algorithm. The overall strategy is to leverage the provided $l_2$ heavy hitter algorithm as a core subroutine.', 'step_2': 'A naive reduction from $l_p$ to $l_2$ is inefficient due to the potentially large factor relating the $l_p$ and $l_2$ norms, which depends on the universe size $n$. To address this, the first step is to apply the thresholding technique (concept_3). Each site creates a sparsified vector, $\\tilde{v}$, by zeroing out all local frequencies $v_{ij}$ below the threshold $\\frac{\\epsilon l_{p}^{\\prime}(v)}{k}$. The $l_2$ algorithm will be run on this sparsified vector.', 'step_3': ""The next step is to determine the correct accuracy parameter to use for the $l_2$ subroutine. Let's call this parameter $\\epsilon'$. According to the $l_2$ algorithm's guarantee (concept_2), running it on the sparsified vector $\\tilde{v}$ will yield an estimation error of $|\\hat{v}_j - \\tilde{v}_j| \\le \\epsilon' l_2'(\\tilde{v})$."", 'step_4': ""However, the final error must be expressed in terms of the original accuracy parameter $\\epsilon$ and the $l_p'$ norm. We need this error to be bounded by $\\epsilon l_p'(\\tilde{v})$. Therefore, we must choose $\\epsilon'$ such that the two error bounds are equivalent: $\\epsilon' l_2'(\\tilde{v}) \\approx \\epsilon l_p'(\\tilde{v})$."", 'step_5': ""From this relationship, we can express the required parameter $\\epsilon'$ as: $\\epsilon' \\approx \\epsilon \\cdot \\frac{l_p'(\\tilde{v})}{l_2'(\\tilde{v})}$. To find a value for $\\epsilon'$, we need to bound the ratio of the norms."", 'step_6': ""This is where the norm inequality for sparsified vectors (theorem_2) becomes crucial. The thresholding step (concept_3) ensures that every non-zero element in the sparsified vector $\\tilde{v}$ satisfies the condition of the theorem with $\\beta \\approx \\frac{\\epsilon}{k}$. Plugging this into the inequality gives: $l_2'(\\tilde{v})^2 \\le \\frac{1}{(\\epsilon/k)^{p-2}} l_p'(\\tilde{v})^2$."", 'step_7': ""By rearranging this inequality, we find a bound for the norm ratio: $\\frac{l_p'(\\tilde{v})}{l_2'(\\tilde{v})} \\ge \\frac{\\epsilon^{(p-2)/2}}{k^{(p-2)/2}}$. We can now substitute this back into our expression for $\\epsilon'$ from step 5, which gives $\\epsilon' \\approx \\epsilon \\cdot \\frac{\\epsilon^{(p-2)/2}}{k^{(p-2)/2}} = \\frac{\\epsilon^{p/2}}{k^{p/2 - 1}}$. This is the derived accuracy parameter needed for the subroutine."", 'step_8': ""Finally, we calculate the total communication cost by using the cost formula for the base $l_2$ algorithm (theorem_1), which is $O(\\frac{k \\log n}{(\\epsilon')^2})$, and substituting our derived value for $\\epsilon'$."", 'step_9': 'The final cost is $O\\left(\\frac{k \\log n}{\\left(\\frac{\\epsilon^{p/2}}{k^{p/2 - 1}}\\right)^2}\\right) = O\\left(\\frac{k \\log n}{\\frac{\\epsilon^p}{k^{p-2}}}\\right) = O\\left(\\frac{k \\cdot k^{p-2} \\log n}{\\epsilon^p}\\right) = O\\left(\\frac{k^{p-1} \\log n}{\\epsilon^p}\\right)$.'}"	{'concept_1': 'The $l_2$ heavy hitter algorithm is a sampling-based method where each site $i$ sends its frequency value $v_{ij}$ for element $j$ to a coordinator with a probability proportional to its squared value, specifically $p_{ij} = min\\{1, \\frac{3v_{ij}^{2}}{\\epsilon^{2}F_{2}(v^{(i)})}\\}$, where $F_{2}(v^{(i)})$ is the local second frequency moment at site $i$.', 'concept_2': 'A key feature of the proposed $l_2$ heavy hitter algorithm is its error guarantee. It estimates each frequency $v_j$ with an additive error of $\\epsilon l_{2}^{\\prime}(v)$, where $l_{2}^{\\prime}(v) = (\\sum_{i=1}^{k}||v^{(i)}||_{2}^{2})^{1/2}$. This is an improvement over standard methods that guarantee an error of $\\epsilon l_{2}(v)$.', 'concept_3': 'To generalize from $l_2$ to $l_p$ heavy hitters without incurring a dependency on the universe size $n$, a thresholding technique is applied. Each site $i$ sparsifies its local frequency vector by setting any value $v_{ij}$ to zero if it is below a certain threshold, specifically if $v_{ij} < \\frac{\\epsilon l_{p}^{\\prime}(v)}{k}$.', 'theorem_1': 'The expected communication cost of the proposed $l_2$ heavy hitter algorithm (concept_1) is $O(\\frac{k \\log n}{\\epsilon^{2}})$ bits.', 'theorem_2': 'For a non-negative vector $x$ where each non-zero element $x_i$ satisfies $x_i \\ge \\beta l_p(x)$, the $l_2$ norm and $l_p$ norm are related by the inequality $l_{2}^{2}(x) \\le \\frac{1}{\\beta^{p-2}}l_{p}^{2}(x)$. This is crucial for analyzing the sparsified vectors.'}	$O(\frac{k^{p-1}\log n}{\epsilon^{p}})$	理论工程学科-信息论和通信理论-通信理论	990461	5	5
recuU3kgNP3HA9	In a static dictionary where `n` keys are partitioned into `L` buckets, the contents of each bucket are encoded as a variable-length spillover representation `(m_i, k_i)`. What is the dominant redundancy term, in words, for the final data structure that jointly stores all `L` of these representations, given that the size `s_i` of a bucket's content can fall into one of `S` distinct types?	"{'step_1': 'The primary objective is to determine the dominant source of redundancy when jointly storing `L` Spillover Representations (`Concept_1`). The information about the initial hashing method (`Concept_4`) is a distractor and is irrelevant to this specific sub-problem of concatenating the already-formed bucket representations.', 'step_2': 'The core challenge is the non-uniform nature of the spills. They are ""heterogeneous"" (`Concept_2`), meaning they are drawn from `S` different integer universes `[K^{(s)}]`. This is more formally described as a ""Representational Incompatibility"" (`Concept_5`) between the integer spills and the finite field elements `GF(2^w)` used for other data.', 'step_3': 'Because of this incompatibility, a single, monolithic storage structure cannot simultaneously handle spills from `S` different types of universes. The logical inference is that the problem must be broken down: a separate, dedicated storage structure must be created for each of the `S` spill types.', 'step_4': 'The chosen tool for storing each of these `S` spill groups is the Auxiliary-Data Retrieval structure (`Concept_3`). This means the overall solution involves instantiating this structure `S` times.', 'step_5': 'Before storage, the incompatibility for each type `s` must be resolved. Theorem_2 (Prime Number Gaps) is applied to convert the integer universe `[K^{(s)}]` into a finite field based on a nearby prime `P^{(s)}`. This step has a negligible space cost.', 'step_6': 'A second incompatibility then arises. The auxiliary data required by the retrieval structure is in base `GF(2^w)`, but the retrieval structure for spills of type `s` must now operate in the prime field `\\mathbb{F}_{P^{(s)}}`.', 'step_7': 'Theorem_1 (Base Conversion) provides the solution to this problem. For each of the `S` spill structures that must be built, a base conversion operation is required. The theorem states that each of these conversion operations incurs a redundancy of `$O(\\log L)$` words.', 'step_8': 'The dominant redundancy is the total cost of these necessary, high-overhead operations. Since the `$O(\\log L)$-cost base conversion must be performed once for each of the `S` distinct spill types, the total dominant redundancy is the product of the two: `$S \\times O(\\log L) = O(S \\log L)$` words.'}"	"{'concept_1': 'Spillover Representation: A method to encode a data structure using a pair `(m, k)`, which consists of a main bit string `m` of length `M` and an integer ""spill"" `k` from a universe `[K]`. This allows for a fine-grained, fractional space measurement of `$M + \\log K$` bits.', 'concept_2': 'Handling Heterogeneous Spills: The spills `k_i` are heterogeneous; they are drawn from `S` different integer universes `[K^{(s)}]` depending on their type `s`. The storage solution must correctly handle values from these distinct and type-dependent domains.', 'concept_3': 'Auxiliary-Data Retrieval: A storage technique where the retrieval of primary data is made highly efficient (constant time, no redundancy) by leveraging a separate, large block of auxiliary data that is encoded alongside it.', 'concept_4': 'Initial Key Partitioning (Distractor): The dictionary construction begins by using a k-wise independent hash function to pseudo-randomly partition the `n` keys from a universe `U` into the `L` buckets. The choice of hash function affects key distribution but not the final concatenation method.', 'concept_5': 'Representational Incompatibility: A challenge in joint storage is that the numeric spills `k_i` (integers in `[K^{(s)}]`) and the word-based data `m_i` (elements in `GF(2^w)`) exist in different algebraic structures, requiring a common framework for unified storage and retrieval.', 'theorem_1': 'Base Conversion with Low Redundancy: A sequence of elements can be converted from base `p` to base `q` with a redundancy of only `$O(\\log q)$` words. In this context, converting data for a structure of size `~L` incurs `$O(\\log L)$` words of redundancy *per conversion operation*.', 'theorem_2': 'Prime Number Gaps: For any sufficiently large integer `n`, there exists a prime number in the interval `$[n, n + n^{7/11}]$`. This guarantees that the universe `$K^{(s)}$` for a spill can be rounded up to a nearby prime `$P^{(s)}$` for field operations with negligible space cost.'}"	$O(S \log L)$	理论基础学科-理论计算机科学-算法理论	219497	5	7
recuSZqZ0KN1aV	Let $\mathbb{C}$ be the complex number field. For a positive real number $y$, $c>1$, and a positive real number $X$ tending to infinity, consider the function $P_+(y, X, c)$ defined as follows: it is obtained by summing $\frac{\chi(\lceil yX \rceil^{\mathfrak{p}})}{\tau(\chi)}$ over all prime numbers $N$ in the interval $[X, cX]$ and all primitive even Dirichlet characters $\chi$ modulo $N$ (i.e., primitive Dirichlet characters with $\chi(-1)=1$ and conductor $N$), and then multiplying the sum by the normalization factor $\frac{\log X}{X}$ (where $\lceil yX \rceil^{\mathfrak{p}}$ denotes the smallest prime not less than $yX$, and $\tau(\chi)$ is the Gauss sum of the Dirichlet character $\chi$, defined as $\tau(\chi)=\sum_{b=1}^{N}\chi(b)e^{2\pi ib/N}$). Find the limit value of $P(y, X, c)$ as $X \to \infty$.	{'step_1': 'To calculate the limit of $P(y, X, c)$, starting from its definition, the expression is $\frac{\log X}{X}\sum_{\substack{N \in [X, cX] \\\\ N\text{ prime}}}\sum_{\chi \in \mathcal{D}_+(N)}\frac{\chi(\lceil yX \rceil^{\mathfrak{p}})}{\tau(\chi)}$.', 'step_2': 'First, use theorem_2 to handle $\frac{1}{\tau(\chi)}$ in the inner sum. For $\chi \in \mathcal{D}_+(N)$ (satisfying $\chi(-1)=1$), we have $\frac{1}{\tau(\chi)} = \frac{1}{N} \tau(\overline{\chi})$, so the inner sum can be rewritten as $\frac{1}{N} \sum_{\chi \in \mathcal{D}_+(N)} \tau(\overline{\chi}) \chi(p)$ (where $p = \lceil yX \rceil^{\mathfrak{p}}$).', 'step_3': 'Combining with theorem_1, for a prime $N$, $\phi(N) = N - 1$, thus it reduces to $\cos\left(\frac{2\pi p}{N}\right) = \frac{-1}{N - 1} + \frac{1}{N - 1} \sum_{\substack{\chi \mod N \\\\ \chi \neq \chi_0, \chi(-1)=1}} \tau(\overline{\chi}) \chi(p)$. Since $\mathcal{D}_+(N)$ are non-principal primitive even characters modulo $N$, we have $\sum_{\chi \in \mathcal{D}_+(N)} \tau(\overline{\chi}) \chi(p) = (N - 1)\cos\left(\frac{2\pi p}{N}\right) + 1$. Substituting into the inner sum gives $\sum_{\chi \in \mathcal{D}_+(N)} \frac{\chi(p)}{\tau(\chi)} = \left(\frac{N - 1}{N}\right)\cos\left(\frac{2\pi p}{N}\right) + \frac{1}{N}$.', 'step_4': 'By theorem_3 (result in [BHP]), as $X \to \infty$, $\lceil yX \rceil^{\mathfrak{p}} - yX < (yX)^{\theta}$ ($\theta < 1$), and since $N \geq X$, we have $\frac{\lceil yX \rceil^{\mathfrak{p}} - yX}{N} \to 0$, i.e., $p \sim yX$, thus $\cos\left(\frac{2\pi p}{N}\right) \sim \cos\left(\frac{2\pi yX}{N}\right)$. Meanwhile, the sum term of $\frac{1}{N}$ tends to 0 after normalization.', 'step_5': 'Finally, according to theorem_5, Weyl criterion, the sequence $\frac{N}{X}$ is uniformly distributed on $(1, c)$, and the sum $\sum \cos\left(\frac{2\pi yX}{N}\right)$ converts to the Riemann integral $\int_{1}^{c}\cos\left(\frac{2\pi y}{x}\right)dx$, thus obtaining the limit value.'}	{'concept/theorem_1': 'Lemma 2.5: Let $N$ be a positive integer and $p$ be a prime number coprime to $N$, then $\cos\left(\frac{2\pi p}{N}\right) = \frac{-1}{\phi(N)} + \frac{1}{\phi(N)} \sum_{\substack{\chi \mod N \\\\ \chi \neq \chi_0, \chi(-1)=1}} \tau(\overline{\chi}) \chi(p)$, where $\phi(N)$ is Euler's totient function (the number of positive integers less than or equal to $N$ and coprime to $N$), $\chi_0$ is the principal Dirichlet character (satisfying $\chi_0(a)=1$ when $(a,N)=1$), and $\tau(\overline{\chi})$ is the Gauss sum of the character $\overline{\chi}$.', 'concept/theorem_2': 'Formula (2.13): For a Dirichlet character $\chi$ modulo $N$, we have $\frac{1}{\tau(\chi)} = \frac{\chi(-1)}{N} \tau(\overline{\chi})$, where $\overline{\chi}$ is the complex conjugate character of $\chi$, and $\tau(\overline{\chi})$ is the Gauss sum of $\overline{\chi}$.', 'concept/theorem_3': 'Result in [BHP]: For any positive real number $x$, there exists a constant $\theta < 1$ such that the smallest prime number $\lceil x \rceil^{\mathfrak{p}}$ not less than $x$ satisfies $\lceil x \rceil^{\mathfrak{p}} - x < x^{\theta}$ (cited from the reference of Lemma 2.8).', 'concept/theorem_5': 'Equidistribution Property: Weyl Criterion'}	\(\int_{1}^{c} \cos\left(\frac{2\pi y}{x}\right)dx\)	理论基础学科-基础数学-数论	918383	2	4
recuTdJFVyVpDJ	Is the Big Ramsey degree of finite $K_4$-free graph $G$ in the countable homogeneous $K_4$-free graph $R_4$ infinite? Here $K_4$ is the $4$ vetices clique.	{'step_1': 'Describe the $K_4$-free diaries, this will indicate the extension of $K_4$ free graph.', 'step_2': 'Define the type(extension) of graphs.', 'step_3': 'Show that the $K_4$-free diaries correspond to a type of some $K_4$ free graph.', 'step_4': 'The big Ramsey degree is |T(G)|*|Aut(G)| thus finite. |T(G)| is the number of diaries related with $G$.'}	{'concept_1': 'Poset-diary is an anti chain in a partial order set (Can be seen as tree) such that each level has certain way to extend to next level.', 'concept_2': 'Given a countable partial order Q, we let T(Q) be the set of all poset-diaries S such that (S, ⪯) is isomorphic to Q.', 'theorem_1': 'For every finite partial order Q, the big Ramsey degree of Q in the generic partial order P equals |T (Q)| · |Aut(Q)|.'}	Finite	理论基础学科-基础数学-组合	192414	4	1
recuTPQIDJCVhQ	With standard Brownian Motion as the prior for Gaussian Process (GP), its kernel function is \(k(x,x') = \min(x,x')\) (\(x,x' \in [0,T]\)). Given \(N\) observations \(f(x_1),\ldots,f(x_N)\), it is necessary to predict the function value \(f(x^*)\) at a new point \(x^*\) and estimate its uncertainty. To correct the variance estimation, it is assumed that the true function follows a Brownian Motion with a scale parameter \(\sigma\), i.e., the kernel function is \(k_\sigma(x,x') = \sigma^2 k(x,x')\), where \(\sigma^2\) is the scale parameter to be estimated.	{'step_1': '明确交叉验证尺度参数估计量的期望表达式，即\\(\\mathbb{E}\\hat{\\sigma}_{\\text{CV}}^2 = \\frac{1}{N} \\sum_{n=1}^N \\mathbb{E}\\left[ \\frac{[f(x_n) - m_{\\backslash n}(x_n)]^2}{k_{\\backslash n}(x_n)} \\right]\\)，其中\\(f\\)服从H=0.6的iFBM，\\(m_{\\backslash n}\\)和\\(k_{\\backslash n}\\)基于布朗运动先验。', 'step_2': '分析中间项（\\(1 \\leq n < N\\)）：\\(m_{\\backslash n}(x_n)\\)为相邻点的加权平均，\\(k_{\\backslash n}(x_n) = \\Theta(N^{-1})\\)（拟均匀划分，concept 2）。利用iFBM核函数（theorem 1）展开\\(\\mathbb{E}[f(x_n) - m_{\\backslash n}(x_n)]^2\\)，其主导项为\\(\\Theta((N^{-1})^{2H+2})\\)，故中间项每一项期望为\\(\\Theta(N^{-2.2})\\)，总和为\\(\\Theta(N^{-1.2})\\)。', 'step_3': '分析边界项（\\(n = N\\)）：根据定义，\\(m_{\\backslash N}(x_N) = f(x_{N-1})\\)，\\(k_{\\backslash N}(x_N) = \\Theta(N^{-1})\\)。利用theorem 3计算得\\(\\mathbb{E}[f(x_N) - f(x_{N-1})]^2 = \\Theta((N^{-1})^2)\\)，故边界项期望为\\(\\Theta(N^{-1})\\)。', 'step_4': '比较中间项与边界项总和的阶：由concept 3，\\(\\Theta(N^{-1})\\)主导\\(\\Theta(N^{-1.2})\\)，总和为\\(\\Theta(N^{-1})\\)。', 'step_5': '整体期望为总和除以\\(N\\)，得\\(\\mathbb{E}\\hat{\\sigma}_{\\text{CV}}^2 = \\Theta(N^{-2})\\)。'}	"{'concept_1': ""Integrated Fractional Brownian Motion (iFBM): Obtained by integrating a Fractional Brownian Motion with Hurst parameter \\(H\\), with a mean function \\(m(x) = 0\\). Its kernel function is \\(k_{1,H}(x,x') = \\int_0^x \\int_0^{x'} k_{0,H}(z,z')dzdz'\\)."", 'concept_2': 'Quasi-Uniform Partition: A sequence of partitions \\((P_N)\\) satisfies the condition that there exists a constant \\(C_{\\text{qu}} \\geq 1\\) such that \\sup_N \\frac{\\max_n \\Delta x_{N,n}}{\\min_n \\Delta x_{N,n}} = C_{\\text{qu}}. In this case, \\Delta x_{N,n} = \\Theta(N^{-1}).', 'concept_3': 'Asymptotic Order Comparison Rule: For two terms of the form \\Theta(N^a) and \\Theta(N^b) (where \\(a\\) and \\(b\\) are real numbers), when \\(N \\to \\infty\\): if \\(a > b\\), then \\Theta(N^a) is the dominant term (decays more slowly); if \\(a = b\\), the two terms are of the same order.', 'theorem_1': ""\\Explicit Expression for the Kernel Function of Integrated Fractional Brownian Motion (iFBM):\n\\[\\begin{aligned}\nk_{1,H}(x,x') &= \\int_{0}^{x} \\int_{0}^{x'} \\frac{1}{2}\\left(|z|^{2H} + |z'|^{2H} - |z-z'|^{2H}\\right)dzdz' \\\\\n&= \\frac{1}{2(2H+1)}\\left(x'x^{2H+1} + x(x')^{2H+1}\\right) - \\frac{1}{4(2H+1)(H+1)}\\left[x^{2H+2} + (x')^{2H+2} - |x-x'|^{2H+2}\\right]\n\\end{aligned}\\]"", 'theorem_2': 'Generalized Binomial Theorem: For a real number \\alpha and a small quantity \\delta,\n\\[(a - \\delta)^\\alpha = a^\\alpha - \\alpha a^{\\alpha-1}\\delta + \\frac{\\alpha(\\alpha-1)}{2}a^{\\alpha-2}\\delta^2 + O(\\delta^3)\n\\], which holds only when \\delta <<a.'}"	Answer：\(\mathbb{E}\hat{\sigma}_{\text{CV}}^2 = \Theta(N^{-2})\)。	理论基础学科-基础数学-统计学	120109	2	4
recuTKIAnvc2Ao	We aim to train a large language model (LLM), denoted as $\pi_\theta$, on its ability to answer a prompt $x$ using reinforcement learning. We employ the SPEED algorithm, which enhances the RLOO algorithm with an initial screening phase. First, we generate $N_{init}$ responses for the prompt $x$ and evaluate their corresponding rewards, $r(y)$, which are binary (1 for a correct response, 0 for incorrect). If the rewards for all these initial responses are identical (all 0s or all 1s), the prompt $x$ is discarded for the current update step. Otherwise, an additional $N_{cont}$ responses are generated. A reinforcement learning update is then performed using all $N=N_{init}+N_{cont}$ responses with the RLOO advantage estimator. Here we want to compute the policy gradient when $N_{init}$ responses are all incorrect.    Formally, we want to compute:  $$\nabla_{\theta}\overline{J}_{x}(\theta)=\mathbb{E}_{y_{1},...,y_{N}}\left[\mathbb{I}\left(\sum_{i=1}^{N_{init}}r(y_{i})=0\right)\cdot\frac{1}{N}\sum_{j=1}^{N}\hat{\mathcal{A}}(y_{j})\nabla_{\theta} \log \pi_{\theta}(y_{j}|x)\right]$$  where $\hat{\mathcal{A}}(y_j)$ is the RLOO advantage estimator, $r(y)\in\{0,1\}$, $y_j$ are responses generated by LLM $\pi_\theta(\cdot|x)$. Please compute the final expression for $\nabla_{\theta}\overline{J}_{x}(\theta)$.	"{'Step_1': ""Apply the 'Law of Total Expectation' to the target expression. This separates the problem into two parts: calculating the probability of the screening event A (where A is the event that all N_init initial responses are incorrect), and calculating the conditional expectation of the gradient estimator given that event A occurred."", 'Step_2': ""Calculate the probability of the screening event, P(A). Using the 'Probability of Independent Events' concept, since each of the N_init responses is an independent Bernoulli trial with a failure probability of (1 - P_x(θ)), the probability of all N_init failing is P(A) = (1 - P_x(θ))^(N_init)."", 'Step_3': ""Focus on the conditional expectation term, E[gradient estimator | A]. Use the 'RLOO Advantage Estimator' concept to substitute its definition into the expression. Then, simplify the outer sum by recognizing that under event A, the reward r(y_j) is 0 for all j <= N_init. This means the sum over j only runs from N_init+1 to N."", 'Step_4': ""Apply the 'Linearity of Expectation' concept to the simplified expression from Step 3. This splits the conditional expectation into two separate terms: one involving the term r(y_j) * ∇log π, and another involving the term -r(y_j) * (sum of other ∇log π)."", 'Step_5': ""Calculate the first of the two terms from Step 4. For this term, use the 'Statistical Independence' concept to note that a continuation sample (e.g., y_N) is independent of the screening event A. Then, apply the 'Log-Derivative Trick' to the resulting unconditional expectation, which simplifies to (N_cont / N) * ∇_θ P_x(θ)."", 'Step_6': ""Calculate the second term from Step 4. Use 'Statistical Independence' again to separate the expectation of the reward from the expectation of the sum of score functions (∇log π). The expectation of the sum of score functions is then broken down using 'Linearity of Expectation'."", 'Step_7': ""Evaluate the sum of conditional expectations of the score functions from Step 6. For terms where the sample is from the continuation phase, 'Statistical Independence' implies the expectation is zero. For the N_init terms from the screening phase, apply the 'Conditional Score Function' concept, which yields -N_init * (∇_θ P_x(θ) / (1 - P_x(θ)))."", 'Step_8': 'Combine the results from steps 5, 6, and 7 to get the final expression for the conditional expectation E[gradient estimator | A]. This expression will be the sum of the results from the two terms analyzed.', 'Step_9': 'Assemble the final answer. Multiply the probability P(A) calculated in Step 2 with the final expression for the conditional expectation calculated in Step 8. This yields the target answer.'}"	{'Concept 1: RLOO Advantage Estimator': '$$ \\hat{\\mathcal{A}}(y_j) = r(y_j) - \\frac{1}{N-1} \\sum_{k \\neq j} r(y_k). $$', 'Concept 2: Pass Rate': '$\\mathcal{P}_{x}(\\theta) = \\mathbb{E}_{y \\sim \\pi_{\\theta}(\\cdot|x)}[r(y)]$.', 'Concept 3: Law of Total Expectation': '$\\mathbb{E}[X \\cdot \\mathbb{I}_A] = P(A)\\mathbb{E}[X|A]$.', 'Concept 4: Linearity of Expectation': '$\\mathbb{E}[X+Y] = \\mathbb{E}[X] + \\mathbb{E}[Y]$.', 'Concept 5: Probability of Independent Events': '$P(A \\cap B) = P(A)P(B)$.', 'Concept 6: Statistical Independence': 'If a random variable $X$ is statistically independent of an event $A$, its conditional expectation given the event equals its unconditional expectation ($\\mathbb{E}[X|A,B] = \\mathbb{E}[X|B]$).', 'Concept 7: The Log-Derivative Trick': '$\\nabla_{\\theta}\\mathbb{E}_{x \\sim p_{\\theta}}[f(x)] = \\mathbb{E}_{x \\sim p_{\\theta}}[f(x) \\nabla_{\\theta} \\log p_{\\theta}(x)]$.', 'Concept 8: Conditional Score Function': '$\\mathbb{E}[\\nabla_{\\theta} \\log \\pi_{\\theta}(y_1 | x) | r(y_1) = 0] = - \\frac{\\nabla_{\\theta} \\mathcal{P}_x(\\theta)}{1 - \\mathcal{P}_x(\\theta)}$.'}	$$(1-\mathcal{P}_{x}(\theta))^{N_{init}}\left(\frac{N_{cont}}{N}+\frac{N_{cont}N_{init}}{N(N-1)}\cdot\frac{\mathcal{P}_{x}(\theta)}{1-\mathcal{P}_{x}(\theta)}\right)\cdot\nabla_{\theta}\mathcal{P}_{x}(\theta)$$	理论基础学科-基础数学-概率论	89197	2	5
recuTNd0zBAuOe	In holographic defect conformal field theory, simplify the two-point function of operators far from the defect insertion in position space under the large AdS radius limit, so that it can be written as a formula in terms of the flat spacetime form factor in momentum space, and provide the integral expression. It should be noted that, due to the presence of the defect, do not substitute any equations related to momentum conservation to avoid mistakes.	{'step_1': 'Write the initial integral form of the two-point function of operators far from the defect (based on concept_1)', 'step_2': 'Decompose the integral variables into parallel and perpendicular components (based on concept_2)', 'step_3': 'Substitute the Schwinger parametrization of the bulk-to-boundary propagator (based on concept_4)', 'step_4': 'Apply the large AdS radius limit (flat-space limit, based on concept_3)', 'step_5': 'Perform Fourier transformation on the flat measure variables (based on concept_5)', 'step_6': 'Arrange the integral variables and exponential terms to obtain the final expression'}	{'concept_1': 'Two-Point Function of Operators Far from Defects in Holographic Defect Conformal Field Theory | Refers to the correlation function of two local operators inserted at positions far from the defect in holographic conformal field theories (defect CFTs) containing conformal defects. Its form can be expressed as $G\left(P_{1}, P_{2}\right)=\int_{AdS_{d+1}} d X_{1} \int_{AdS_{d+1}} d X_{2} G_{B \partial}\left(P_{1}, X_{1}\right) G_{B \partial}\left(P_{2}, X_{2}\right) G\left(X_{1}, X_{2}\right)$, where $G_{B \partial}$ is the bulk-to-boundary propagator, and $G\left(X_{1}, X_{2}\right)$ is the bulk two-point function in the presence of holographic defects. It has the physical meaning of a form factor describing the scattering of particles and extended objects in AdS. The integral measure $dX$ in this formula can be divided into parallel and perpendicular components', 'concept_2': 'Parallel and Perpendicular Components | The integral variables $X_1$ and $X_2$ can be divided into components parallel to the defect and components perpendicular to the defect. $G\left(X_{1}, X_{2}\right)$ has translational invariance for components parallel to the defect, but no translational invariance for perpendicular components.', 'concept_3': 'Large AdS Radius Limit (Flat-Space Limit) | The limit case when the radius $R$ of AdS space is much larger than the intrinsic length scale $\ell_s$ of bulk physics (i.e., $R/\ell_s \to \infty$). At this time, $G\left(X_{1}, X_{2}\right)$ is approximated as a flat spacetime form factor in position space, and the parallel component of $X_1$ is the $AdS_{d+1}$ measure (Fourier transformation cannot be performed, and the final result must retain the integral over the parallel component of $X_1$), while the perpendicular component of $X_1$ and all components of $(X_1 - X_2)$ are flat spacetime measures.', 'concept_4': 'Schwinger Parametrization | A method to rewrite the propagator into an exponential form, which is convenient for calculations such as integration. For the bulk-to-boundary propagator $G_{B \partial}^{\Delta}(X, P)$, its Schwinger parametrization form is $G_{B \partial}^{\Delta}(X, P)=\frac{(-i)^{\Delta} R^{\Delta}}{\Gamma(\Delta) R^{(d-1) / 2} \ell_{s}^{\Delta}} \int_{0}^{\infty} \frac{d \beta}{\beta} \beta^{\Delta} e^{-2 i \beta P^{\| } \cdot X^{\| } / \ell_{s}} e^{-2 i \beta P^{\perp} \cdot X^{\perp} / \ell_{s}}$, where $\beta$ is the Schwinger parameter. This form decomposes the propagator into exponential terms parallel and perpendicular to the defect direction, facilitating subsequent momentum space transformation and other operations.', 'concept_5': 'Fourier Transformation and Momentum Conservation | If the measure is flat spacetime, position space and momentum space are connected by Fourier transformation. If the measure is $AdS_{d+1}$, Fourier transformation cannot be performed.'}	G(P_1,P_2)=\left(\prod_{i=1}^2\frac{(-i)^{\Delta_i}R^{\Delta_i}}{\Gamma(\Delta_i)R^{(d-1)/2}\ell_s^{\Delta_i}}\right)\int_{\te{AdS}_{p+1}}dX_1^{\parallel}\int_0^{\infty}\prod_{i=1}^2\frac{d\beta_i}{\beta_i}\beta_i^{\Delta_i}e^{-2i\sum_{i=1}^2\beta_i P_i^{\parallel}\cdot X_1^{\parallel}/\ell_s}F_{\text{flat}}(k_1^{\parallel}=-k_2^{\parallel}=2\beta_2 P_2^{\parallel}/\ell_s,k_1^{\perp}=-2\beta_1P_1^{\perp}/\ell_s,k_2^{\perp}=-2\beta_2 P_2^{\perp}/\ell_s)	理论基础学科-理论物理-分析力学	171721	4	4
recuTPXViEsUKT	What is the cumulative regret upper bound for the Phased Elimination (PE) algorithm in a noiseless Gaussian Process (GP) bandit setting when using a Squared Exponential (SE) kernel, assuming the RKHS norm B, dimensionality d, lengthscale l, and smoothness v are constants ( $\Theta(1)) ?$	{'step_1': 'The objective is to determine the cumulative regret $R_T$ for the PE algorithm (Concept_3) in a noiseless setting (Concept_2) using an SE kernel (Concept_4).', 'step_2': 'The PE algorithm divides the total time horizon T into a logarithmic number of batches, approximately $\\log _2 T$. The total regret is the sum of regrets incurred in each batch.', 'step_3': 'We first analyze the regret within a single batch $i$. According to the standard analysis of PE and using the deterministic confidence bounds for the noiseless setting (Theorem_1), the regret for batch $i$ (of size $N_i$ ) can be bounded by the maximum posterior standard deviation from the previous batch $i-1$. This relationship is given by the inequality: $\\sum_{j=1}^{N_i} f\\left(x^*\\right)-f\\left(x_j^{(i)}\\right) \\leq 4 B N_i \\max _{x \\in \\mathcal{X}_{i-1}} \\sigma_{\\lambda^2 I_{N_{i-1}}}\\left(x ; X_{N_{i-1}}^{(i-1)}\\right) .', 'step_4': 'The core of the analysis is to apply the novel upper bound on this maximum posterior standard deviation. Since each batch of PE runs an MVR-style selection, we can apply Theorem_2. For the SE kernel, Theorem_2 states that the maximum posterior standard deviation after $N_{i-1}$ steps is bounded by $O\\left(\\sqrt{\\exp \\left(-N_{i-1}^{\\frac{1}{d+1}} \\ln -\\alpha N_{i-1}\\right.}\\right)$.', 'step_5': 'Substituting this bound into the per-batch regret expression shows that the regret term $N_{i-1} \\times \\sqrt{\\exp \\left(-N_{i-1}^{\\frac{1}{d+1}} \\ln -\\alpha N_{i-1}\\right)}$ converges to 0 as $N_{i-1}$ increases. Therefore, for sufficiently large batches, the regret accumulated within one batch is bounded by a constant, $O(1)$.', 'step_6': 'Finally, by summing the constant regret $O(1)$ over all batches, the total cumulative regret $R_T$ is the product of the number of batches $(O(\\ln T))$ and the per-batch regret $(O(1))$. This yields a total cumulative regret of $O(\\ln T)$.'}	{'concept_1': 'Cumulative Regret. In a GP bandit problem, the cumulative regret $R_T$ is the sum of the difference between the reward of the optimal action $x^*$ and the chosen action $x_t$ over T steps: $R_T=\\sum_{t \\in[T]} f\\left(x^*\\right)-f\\left(x_t\\right)$.', 'concept_2': 'Noiseless GP Bandit Setting. A sequential decision-making problem where at each step $t$, a learner selects a point $x_t$ and observes the reward directly without noise, i.e., $y_t=f\\left(x_t\\right)$. The function $f$ is assumed to be an element of a known Reproducing Kernel Hilbert Space (RKHS).', 'concept_3': 'Phased Elimination (PE) Algorithm. An algorithm for GP bandits that divides time into batches. In each batch, it performs Maximum Variance Reduction (MVR) to select points. After each batch, it eliminates candidate points whose upper confidence bound (UCB) is lower than the maximum lower confidence bound (LCB), shrinking the search space for the next batch. The number of batches is logarithmic with respect to the total time horizon T, i.e., $O(\\log T)$.', 'concept_4': 'Squared Exponential (SE) Kernel. A commonly used kernel in GP models defined as $k_{S E}(x, \\tilde{x})=\\exp \\left(-\\frac{\\|x-\\tilde{x}\\|_2^2}{2 l^2}\\right)$, where $l$ is the lengthscale parameter.', 'theorem_1': 'Deterministic Confidence Bound for Noiseless Setting. In a noiseless setting ( $\\lambda=0$ ), the true function $f(x)$ is deterministically bounded by the posterior mean $\\mu_{0 I_t}$ and the posterior standard deviation $\\sigma_{0 I_t}$ scaled by the RKHS norm bound B. Specifically, $\\left|f(x)-\\mu_{0 I_t}\\left(x ; X_t, f_t\\right)\\right| \\leq B \\sigma_{0 I_t}\\left(x ; X_t\\right)$ holds for all $x$ and $t$.', 'theorem_2': 'Upper Bound of Posterior Variance for SE Kernel. For the MVR algorithm in a low-noise or noiseless setting with an SE kernel, the maximum posterior standard deviation after T steps, $\\max _{x \\in \\bar{X}} \\sigma_{\\bar{\\lambda}_T^2 I_T}\\left(x ; X_{T, T}\\right)$, decreases exponentially with T. Specifically, it is bounded by $O\\left(\\sqrt{\\exp \\left(-T^{\\frac{1}{d+1}} \\ln -\\alpha T\\right)}\\right)$ for any $\\alpha>0$ and for all T greater than some constant $\\bar{T}$.'}	$O(\ln T)$	理论基础学科-理论计算机科学-算法理论	297669	5	4
recuT9JBySRLud	Consider the observation model $X(x(i)) = m_X(x(i)) + Z(x(i)) + \epsilon(x(i))$, where:  - $x(i)\in [0,1)^d$.  - $m_X$ is a smooth mean function.  - $Z$ is an isotropic Gaussian random field with mean 0, and its covariance function is the Matérn function $K_M(x,y)=\frac{\sigma^2(\alpha\| x - y\| )^{v}}{2^{v - 1} \Gamma(v)} \mathcal{K}_{v}(\alpha\| x - y\| )$ (where $\sigma^2$ is the variance, $\alpha$ is the scale parameter, $v$ is the smoothness parameter, $\Gamma(\cdot)$ is the gamma function, and $\mathcal{K}_{v}(\cdot)$ is the modified Bessel function).  - $\epsilon$ is an independent and identically distributed measurement error (nugget), following a Gaussian distribution with mean 0 and variance $\tau > 0$, and is independent of $Z$.    To construct an estimator for the nugget parameter $\tau$, we introduce the following definitions:  1. **Design of sample points $x(i)$**    We select sample points using stratified random sampling. Let $i=(i_1,\dots,i_d)'$, where $i_j$ are integers with $1 \leq i_1,\dots,i_d \leq n$. The sample points $x(i)=(x_1(i),\dots,x_d(i))'$ are points in $[0,1)^d$ satisfying:    \[ x_k(i) = \frac{i_k - 1}{n} + \frac{\delta_{i;k}}{n}, \quad 0 \leq \delta_{i;k} < 1, \quad k=1,\dots,d \]    The observed samples are $\{(x(i), X(x(i))): i_1,\dots,i_d \in \{1,\dots,n\}\}$ with a sample size of $n^d$.  2. **Definitions of parameters $\overline{\ell}$ and $\omega_n$**    - Let $\ell \in \mathbb{Z}_+$, define:    \[ \overline{\ell} = \sum_{l=1}^{\ell} \binom{l + d - 1}{d - 1} = \binom{\ell + d}{d} - 1 \]    - $\omega_n$ is an integer depending on $n$, satisfying $\omega_n \to \infty$ and $\omega_n = O(n^{\gamma_0})$ (where $\gamma_0 \in (0,1)$ is a constant).  3. **Definition of the deleted neighborhood $N(x_{i,0})$**    For $x_{i,0}=x(i)$ (where $i_1,\dots,i_d \in \{1,\dots,n - \ell \omega_n\}$), its deleted neighborhood is $N(x_{i,0}) = \{x_{i,1},\dots,x_{i,\overline{\ell}}\}$, where:    - For $l=1,\dots,\ell$, the subset $\mathcal{S}_{i,d,l}$ is defined as:    \[ \mathcal{S}_{i,d,l} = \left\{x_{i,j}: j = 1 + \sum_{k=1}^{l-1} \binom{k + d - 1}{d - 1}, \dots, \sum_{k=1}^l \binom{k + d - 1}{d - 1}\right\} \]    - The points in $\mathcal{S}_{i,d,l}$ are $x(i_1 + k_1 \omega_n, \dots, i_d + k_d \omega_n )$ satisfying $k_1,\dots,k_d \in \{0,\dots,\ell\}$ and $k_1 + \dots + k_d = l$, arranged in the lexicographical order of $(k_1,\dots,k_d)$.  4. **Definitions of vector $a^{<d,l>}(s)$ and parameter $d_{i,j;k}$**    - For $s=(s_1,\dots,s_d)' \in \mathbb{R}^d$, the elements of the $1 \times \binom{l + d - 1}{d - 1}$ row vector $a^{<d,l>}(s)$ are $\prod_{k=1}^d \frac{s_k^{l_k}}{l_k!}$ satisfying $l_1 + \dots + l_d = l$, arranged in the lexicographical order of $(l_1,\dots,l_d)$.    - Define:    \[ d_{i,j;k} = \frac{n}{\omega_n} \left(x_{i,j;k} - x_{i,0;k}\right), \quad j=1,\dots,\overline{\ell}, \quad k=1,\dots,d \]   5. **Definitions of matrix $A_{i,d,\ell}$ and its inverse matrix**    Let $y_{i,j} = \frac{n}{\omega_n}(x_{i,j} - x_{i,0})$, define the $\overline{\ell} \times \overline{\ell}$ matrix:    \[ A_{i,d,\ell} = \begin{pmatrix} a^{<d,1>}(y_{i,1}) & a^{<d,2>}(y_{i,1}) & \dots & a^{<d,\ell>}(y_{i,1}) \\ \vdots & \vdots & \ddots & \vdots \\ a^{<d,1>}(y_{i,\overline{\ell}}) & a^{<d,2>}(y_{i,\overline{\ell}}) & \dots & a^{<d,\ell>}(y_{i,\overline{\ell}}) \end{pmatrix} \]    If $|A_{i,d,\ell}| \neq 0$, its inverse matrix is $A_{i,d,\ell}^{-1} = (\alpha_{i,d,\ell}^{j,k})_{1 \leq j,k \leq \overline{\ell}}$.  6. **Definition of coefficient $c_{i,d,\ell}(j)$**    \[ c_{i,d,\ell}(j) = \begin{cases} \alpha_{i,d,\ell}^{\overline{\ell},j}, & j=1,\dots,\overline{\ell}, \\ -\sum_{k=1}^{\overline{\ell}} \alpha_{i,d,\ell}^{\overline{\ell},k}, & j=0. \end{cases} \]   7. **Definition of parameter $c_{i,d,\ell}^{k_1,\dots,k_d}$**    For $j \in \{0,\dots,\overline{\ell}\}$, if $j \Leftrightarrow (k_1,\dots,k_d)$ (i.e., $x_{i,j} = x(i_1 + k_1 \omega_n, \dots, i_d + k_d \omega_n)$), then:    \[ c_{i,d,\ell}^{k_1,\dots,k_d} = \begin{cases} c_{i,d,\ell}(j), & (k_1,\dots,k_d) \Leftrightarrow j \in \{0,\dots,\overline{\ell}\}, \\ 0, & \text{when there is no corresponding relationship}. \end{cases} \]   8. **Definition of discrete differential operator $\nabla_{d,\ell} X(x(i))$**    For indices $i=(i_1,\dots,i_d)'$ satisfying $1 \leq i_1,\dots,i_d \leq n - \ell \omega_n$, define the discrete differential operator:    \[ \nabla_{d,\ell} X(x(i)) = \sum_{0 \leq k_1,\dots,k_d \leq \ell} c_{i,d,\ell}^{k_1,\dots,k_d} X\left(x\left(i_1 + k_1 \omega_n, \dots, i_d + k_d \omega_n \right)\right) \]    where $c_{i,d,\ell}^{k_1,\dots,k_d}$ are the coefficients defined in step 7.  9. **Definition of index set $\Xi_{n}$**    Define the index set $\Xi_{n}$ as:    \[ \Xi_{n} = \left\{i: 1 \leq i_1, \dots, i_d \leq n - \ell \omega_n\right\} \]    This set ensures that all design points involved in the calculation are within the domain $[0,1)^d$.  10. **Definition of high-order quadratic variation $V_{d,\ell}$**    Based on the discrete differential operator, define the $\ell$-th order quadratic variation:    \[ V_{d,\ell} = \sum_{i \in \Xi_{n}} \left\{\nabla_{d,\ell} X(x(i))\right\}\left\{\nabla_{d,\ell} X\left(x\left(i\right)\right)\right\} \]     The estimator for the nugget parameter is constructed as $\hat{\tau}_{\ell} = \frac{V_{d,\ell}}{\sum_{i \in \Xi_{0,n}} \sum_{0 \leq k_1,\cdots,k_d \leq \ell} (c_{i,1,d,\ell}^{k_1,\cdots,k_d})^2}$.    Given two conditions:  1. Let $m_{X} \in C^{N}(\mathbb{R}^{d})$ (where $C^{N}(\mathbb{R}^{d})$ denotes the set of N-times continuously differentiable functions on $\mathbb{R}^{d}$), and $N$ is an integer satisfying $N \geq v+d/2$.  2. $\nu = \ell$.    What is the order of the mean squared error of this estimator? (Note that the answer may contain multiple terms; do not add additional conditions to the parameters; the calculation result will include $n,\omega_n$ and not $\gamma_0$)	{'step_1': 'Clarify the composition of the mean squared error. According to concept_1, the mean squared error is the sum of the squared bias and the variance, i.e., $\mathbb{E}\\{(\hat{\tau}_{\ell}-\tau)^2\\} = [\text{Bias}(\hat{\tau}_{\ell})]^2 + \text{Var}(\hat{\tau}_{\ell})$, and the orders of the bias and variance need to be analyzed separately.', 'step_2': 'Analyze the source of bias. The estimator $\hat{\tau}_{\ell}$ is based on the quadratic variation $V_{d,\ell}$ (concept_2), and $V_{d,\ell}$ includes contributions from the mean function $m_X$, the random field $Z$, and the measurement error $\epsilon$. By theorem_3, the coefficients $c_{i,d,\ell}^{k_1,\dots,k_d}$ eliminate the influence of low-order terms of the mean and covariance, and the bias mainly comes from the high-order terms of the covariance of $Z$.', 'step_3': 'Determine the order of the bias. When $\nu = \ell$, according to theorem_1, the Matérn covariance function $K_M$ contains the term $G_{\ell}(\|x-y\|) = \|x-y\|^{2\ell}\log(\|x-y\|)$, and the distance between sample points is $O(\omega_n/n)$, so the high-order term of the covariance is $O((\omega_n/n)^{2\ell}|\log(\omega_n/n)|)$. The bias is the result of the combination of this high-order term and the coefficients, with an order of $O(\omega_n^{2\ell}|\log n|/n^{2\ell})$, and the order of the squared bias is $O((\omega_n^{2\ell}\log n/n^{2\ell})^2)$.', 'step_4': 'Analyze the source of variance. The variance comes from the contribution of $\epsilon$, where $\epsilon$ is independent and identically distributed and independent of $Z$. The variance of the discrete differential operator $\nabla_{d,\ell}\epsilon$ is related to the sum of squared coefficients, and the variance of the quadratic variation $V_{d,\ell}$ is determined by the sample size $n^d$, so the order of the estimator variance is $O(1/n^d)$.', 'step_5': 'Synthesize the order of the mean squared error. The mean squared error is the sum of the orders of the squared bias and the variance, i.e., $O((\omega_n^{2\ell}\log n/n^{2\ell})^2 + 1/n^d)$.','}	{'concept_1': 'Mean Squared Error: The expected value of the squared deviation between the estimator and the true parameter, i.e., $\mathbb{E}\\{(\hat{\tau}_{\ell}-\tau)^2\\} = [\text{Bias}(\hat{\tau}_{\ell})]^2 + \text{Var}(\hat{\tau}_{\ell})$.', 'concept_2': 'Quadratic Variation ($V_{d,\ell}$): A statistic constructed based on the linear combination of observations, used to separate the effect of the nugget parameter.', 'theorem_1': 'Relationship between the Series Expansion of the Covariance Function Based on $K_M$ and $G_s(t)$: Let $K_M$ be the Matérn covariance function, whose series expansion is: \\[K_M(x, y) = \sum_{j=0}^{\infty} \\left\\{ \\beta_j \\|x-y\\|^{2j} + \\beta_{\nu+j}^* G_{\nu+j}(\\|x-y\\|) \\right\\}, \\quad \\forall x, y \\in \mathbb{R}^d\\] where: $\nu > 0$ is the smoothness parameter, $\beta_j, \beta_{\nu+j}^* \in \mathbb{R}$ are constants satisfying $\beta_{\nu}^* \neq 0$, and for any integer $k$ with $0 \leq k < \nu$, $(-1)^k \beta_k > 0$; $\|x-y\|$ denotes the Euclidean distance between $x$ and $y$; the function $G_s(t)$ is defined as: \\[ G_s(t) = \\begin{cases} t^{2s} \log(t), & \text{if } s \in \mathbb{Z}_+, \\\\ t^{2s}, & \text{otherwise}, \\end{cases} \\quad \\forall t > 0 \\] and $G_s(0) = 0$.', 'theorem_2': 'Boundedness of Higher-Order Derivatives of $K_M$: When $\nu = \ell$, the $2\ell$-th order partial derivative of the covariance function can be bounded by $|\log (\|x-y\|)|$.', 'theorem_3': 'Selection of Parameters $c_{i, d, \ell}^{k_1, \dots, k_d}$ Can Avoid the Influence of Low-Order Terms in the Mean and Covariance Function on Parameter Estimation: 1. $\sum_{0 \leq k_1, \dots, k_d \leq \ell} c_{i, d, \ell}^{k_1, \dots, k_d} x_d\left(i_1 + k_1 \omega_n, \dots, i_d + k_d \omega_n\right)^{\ell} = \ell! \left(\frac{\omega_n}{n}\right)^{\ell}$ 2. For all integers $l_1, \dots, l_d$ satisfying $0 \leq l_1, \dots, l_{d-1} \leq \ell$, $0 \leq l_d \leq \ell-1$ and $0 \leq l_1 + \dots + l_d \leq \ell$, we have: \\[\\sum_{0 \leq k_1, \dots, k_d \leq \ell} c_{i, d, \ell}^{k_1, \dots, k_d} \prod_{j=1}^d x_j\left(i_1 + k_1 \omega_n, \dots, i_d + k_d \omega_n\right)^{l_j} = 0\\]'}	The order of the mean squared error of this estimator is $O\{n^{-d} + (\omega_n / n)^{4\ell}(\log (n / \omega_n) )^2\}$.	理论基础学科-基础数学-统计学	155198	2	4
recuTzgU0Kdbsh	An $F_{\sigma}$ set refers to the union of countably many closed sets; a Polish space is a separable completely metrizable space; an analytic set refers to a subset $A$ of a Polish space $X$ that is the projection of some Borel subset in $X \times X$. If $A \subseteq (\mathbb{R}, +)$ is an analytic subgroup of real numbers (i.e., $A$ is an analytic set and an additive subgroup), and there exists a real number $x$ such that $A + xA = \mathbb{R}$, then does there exist an $F_{\sigma}$ subgroup $B \subseteq A$ of $A$ such that $B + xB = \mathbb{R}$?	{'step_1': 'Define the mapping $\\phi$ and prove its graph is an analytic set\nDefine $\\phi: A \\times A \\to \\mathbb{R}$ where $\\phi(a, b) = a + xb$.\nSince $A$ is an analytic set (by the definition of an analytic subgroup), $A \\times A$ is an analytic set (the product of analytic sets is still an analytic set); moreover, $\\phi$ is a continuous mapping (addition and scalar multiplication of real numbers are continuous).\nAccording to Theorem1 (an analytic set is a continuous image of a Polish space), the graph of a continuous mapping is an analytic set, so the graph of $\\phi$, $P = \\{(a, b, z) \\mid z = \\phi(a, b)\\} \\subseteq A \\times A \\times \\mathbb{R}$, is an analytic set, and its projection onto $\\mathbb{R}$, $\\text{proj}_{\\mathbb{R}}(P) = \\mathbb{R}$ (because $A + xA = \\mathbb{R}$).', 'step_2': 'Apply Jankov, von Neumann uniformization theorem (Theorem2)\nSince $X = \\mathbb{R}$ and $Y = A \\times A$ are standard Borel spaces, $P \\subseteq X \\times Y$ is an analytic set, and $\\text{proj}_X(P) = \\mathbb{R}$. \nBy Theorem2, there exists a Σ-measurable function $f: \\mathbb{R} \\to A \\times A$ such that for all $z \\in \\mathbb{R}$, $(z, f(z)) \\in P$, i.e., $\\phi(f(z)) = z$.', 'step_3': 'Utilize the measurability of analytic sets (Theorem3)\nBy Theorem3, analytic subsets of real numbers are Lebesgue measurable, and the Σ-algebra contains analytic sets, so the function $f$ in step 3 is a Lebesgue measurable function.', 'step_4': 'Apply Lusin\'s theorem (Theorem4)\nFor the measurable function $f$, by Theorem4, there exists a compact set $P \\subseteq \\mathbb{R}$ ($\\mu(P) > 0$) such that $f$ is continuous on $P$, denoted $\\psi = f|_P$ ($\\psi$ is continuous).\nAt this point, for all $z \\in P$, $\\phi(\\psi(z)) = z$ (since $\\psi$ is a restriction of $f$).', 'step_5': 'Construct compact sets $C_0, C_1$ and generate the subgroup $B$\nLet $\\psi(z) = (a_z, b_z)$ ($a_z, b_z \\in A$), define $C_0 = \\{a_z \\mid z \\in P\\}$ and $C_1 = \\{b_z \\mid z \\in P\\}$. \nSince $\\psi$ is continuous and $P$ is a compact set, $C_0$ and $C_1$ are compact sets (the image of a compact set under a continuous mapping is still a compact set).\nLet $B$ be the additive subgroup generated by $C_0 \\cup C_1$, i.e., $B = \\{\\sum_{i=1}^n (\\pm a_i \\pm b_i) \\mid n \\in \\mathbb{N}, a_i \\in C_0, b_i \\in C_1\\}$. \nSince the countable union of finite sums and differences of compact sets is an $F_{\\sigma}$ set (a union of countably many closed sets), $B$ is an $F_{\\sigma}$ subgroup, and $B \\subseteq A$ (because $A$ is a subgroup and $C_0, C_1 \\subseteq A$).', 'step_7': 'Prove $B + xB = \\mathbb{R}$\nBy construction, $\\phi(B \\times B) \\supseteq P$ (since $\\psi(z) \\in B \\times B$, thus $z \\in \\phi(B \\times B)$), so $\\phi(B \\times B)$ has positive Lebesgue measure. \nMoreover, $\\phi(B \\times B) = B + xB$ is an additive subgroup of $\\mathbb{R}$ (because $B$ is a subgroup). By Theorem5 (a measurable proper subgroup has measure 0), if $B + xB$ were a proper subgroup, its measure would be 0, contradicting the positive measure. \nThus, $B + xB = \\mathbb{R}$, completing the proof.'}	"{""Theorem_1"": ""Equivalent Definition of Analytic Set: A is an analytic set if and only if A is the continuous image of a Polish space."", ""Theorem_2"": ""Jankov, von Neumann Uniformization Theorem: Let $X, Y$ be standard Borel spaces and $P \\subseteq X \\times Y$ be an analytic set. Then there exists a Σ-measurable function $f: X \\to Y$ such that for all $x \\in \\text{proj}_X(P)$, $(x, f(x)) \\in P$ holds."", ""Theorem_3"": ""Every analytic subset of the real numbers is Lebesgue measurable."", ""Theorem_4"": ""Lusin's Theorem: In real analysis, a measurable function is continuous on some compact set of positive measure (used to obtain the continuous function $\\psi$)."", ""Theorem_5"": ""If $A$ is a proper Lebesgue measurable subgroup of the real numbers, then the Lebesgue measure of $A$ is 0.""}"	Yes (it is still necessary to further check whether the proof process is consistent with CoT)	理论基础学科-基础数学-分析学	160938	2	2
recuTR7K0wJSWn	When calculating the band gap of the heavy-atom solid-state compound δ-UO₃, the calculation incorporating spin-orbit (SO) coupling will cause the upper limit of the valence band to increase. This phenomenon occurs because, when SO coupling is taken into account, the O-2p valence band primarily interacts with which specific orbitals? Please specify the name of the relevant orbitals.	"  {  ""Step_1"":""分析问题核心：Instruction的核心问题是在引入SO（自旋轨道）耦合后，是什么轨道与O-2p价带相互作用，导致了价带上限的升高。问题的背景是重原子固态化合物δ-UO3。"",  ""Step_2"":""识别关键元素与效应：根据Instruction，关键元素是重原子U，关键效应是SO耦合。根据Concept_1，SO耦合是重原子中的显著相对论效应，尤其对半芯层轨道如U-6p影响巨大，会造成巨大的能级分裂。"",  ""Step_3"":""关联价带能量变化：Instruction指出价带上限（主要由O-2p构成）能量升高。我们需要寻找一个能解释价带能量被“推高”的物理机制。Concept_2和Concept_3提供了这一机制：即“下层推动”效应（PFB）与芯层-价层轨道混合。该理论指出，能量较低的半芯层轨道（如U-6p）会与配体的价层轨道（O-2p）发生泡利互斥作用，从而“推高”价层轨道的能量。"",  ""Step_4"":""整合SO耦合与能量推高机制：Concept_2明确指出，PFB效应不仅推高了O-2p的能量，还将U-6p核的强SO分裂特性传递到了O-2p价带上。这是通过Concept_3描述的轨道混合实现的，即O-2p轨道因与U-6p轨道混合，而带上了一部分U-6p的属性，包括其巨大的SO分裂效应。因此，当计算中考虑SO耦合时，U-6p的巨大分裂会通过这种相互作用体现在O-2p带上，表现为价带上限的抬升。"",  ""Step_5"":""考虑晶体环境的特定影响：Concept_4补充说明了为何此效应在δ-UO3中尤为重要。δ-UO3的高对称性晶体场保护了U-6p轨道的简并性，使其固有的巨大SO分裂不会被晶体场效应所“猝灭”，从而保证了该分裂能最大程度地通过PFB机制传递给O-2p价带。"",  ""Step_6"":""得出结论：综合以上步骤，可以得出结论：在δ-UO3中，当考虑SO耦合时，具有巨大SO分裂效应的U-6p半芯层轨道，通过与O-2p价层轨道的轨道混合及随之产生的“下层推动”（泡利互斥）效应发生相互作用，从而将自身的SO分裂特性传递给O-2p价带，导致价带上限能量升高。因此，与O-2p价带作用的关键轨道是U-6p轨道。""  }"	"  {  ""Concept_1"": ""自旋轨道耦合|一个相对论效应，描述了电子的自旋角动量与其绕原子核的轨道角动量之间的相互作用。该效应的强度随原子序数的增加而急剧增强。对于像铀（U）这样的重原子，其内层和半芯层的自旋轨道耦合效应非常显著，能级分裂可达10 eV量级。 "",  ""Concept_2"": ""“下层推动”效应|该效应描述了重原子中充满电子的、能量较低的轨道与配体价层轨道之间的泡利互斥作用 (Pauli Repulsion)。根据泡利不相容原理，成键区的价层轨道必须与芯层轨道保持正交，这种正交化要求会“推高”价层轨道的能量。 PFB效应将U的强SO分裂传递到了价带上。"",  ""Concept_3"": ""芯层-价层轨道混合|在重原子体系中，半芯层轨道和价层轨道之间的能量差可能较小，且径向分布有重叠，这使得它们可以发生有效的轨道混合。这种混合导致了泡利排斥（PFB效应）。 "",  ""Concept_4"": ""晶体场对称性|原子周围的配体（或原子）会产生一个静电场（晶体场），其对称性决定了原子轨道的简并情况和轨道之间的相互作用（混合）方式。δ-[UO₃]具有很高的八面体（Oₕ）对称性。 这种高对称性使得轨道的简并性不会被晶体场轻易破坏，从而保证了其巨大的SO分裂效应可以完整地、最大程度地通过PFB机制传递给价带，而不会被低对称性带来的晶体场分裂效应所“猝灭” (quench)。""  }"	U-6p orbital	理论基础学科-理论化学-计算化学	365978	3	13
recuTUeMbocZZQ	Let u be a solution to the equation $(-Δ)^s u = e^u$ (1/2 < s < 1) satisfying that the integral $\int e^u dx$ over $\mathbb{R}$ is finite. Then, is the dimension of the null space (kernel) of its linearized operator $L_u = (-Δ)^s - e^u$ equal to 1?	"{""step_1"": ""By directly differentiating the solution family with respect to translation and scaling parameters, it can be verified that the translation generator $T_u = \\partial_x u$ and the scaling generator $R_u = x \\partial_x u + 2s$ indeed satisfy $\\mathcal{L}_u(T_u) = 0$ and $\\mathcal{L}_u(R_u) = 0$."", ""step_2"": ""The core strategy of the proof is to assume that $u$ is an even function (which can be achieved by translation), then the operator $\\mathcal{L}_u$ is also an even operator. Therefore, any function $\\psi$ in the null space can be decomposed into its even part $\\psi_e$ and odd part $\\psi_o$, and both parts must independently belong to the null space. At this point, the problem is decomposed into proving that the dimensions of the even part space and the odd part space are both 1 respectively."", ""step_3"": ""Prove that any even function $\\psi_e$ in the null space must be a constant multiple of the scaling generator $R_u$ (which is an even function). The method is to construct the function $h = \\psi_e - \\gamma R_u$, and select an appropriate constant $\\gamma$ such that $h(0) = 0$. Then, use the properties of the integral equation to prove that $h$ must be identically zero, so $\\psi_e$ can only be a multiple of $R_u$."", ""step_4"": ""For the odd function $\\psi_o$ in the null space, the equation $\\mathcal{L}_u(\\psi_o) = 0$ is cleverly transformed into an integral equation defined on the half-infinite line $(0, +\\infty)$. This integral equation has the form $g = Ag$, where $g$ is a function related to $\\psi_o$, and $A$ is a compact self-adjoint operator with a strictly positive integral kernel."", ""step_5"": ""Since the operator $A$ satisfies the conditions of the Perron-Frobenius type theorem, its maximum eigenvalue (here, 1) must be simple, that is, the corresponding eigenspace is one-dimensional. Because the function $h$ corresponding to the translation generator $T_u$ (which is an odd function) is exactly the eigenfunction with eigenvalue 1, any other odd part solution $g$ must be a constant multiple of $h$. This proves that the odd part space is one-dimensional, spanned by $T_u$.""}"	"{""concept_1"": ""Fractional Laplacian is defined as $(-\\Delta)^{s}u(x) = C_{s} \\text{ P.V. } \\int_{\\mathbb{R}} \\frac{u(x) - u(y)}{|x-y|^{1+2s}} dy$, where $C_{s}=\\frac{4^{s}}{\\sqrt{\\pi}}\\frac{\\Gamma(\\frac{1}{2}+s)}{|\\Gamma(-s)|}$."", ""concept_2"": ""For a nonlinear equation, the linearized operator describes the linear response of the system to small perturbations near a specific solution $u$. It is obtained by Fréchet differentiation of the equation. For this problem, its form is $\\mathcal{L}_{u}=(-\\Delta)^{s}-e^{u}$."", ""concept_3"": ""The null space (or kernel) of an operator $\\mathcal{L}_u$ refers to the set of all functions that are mapped to the zero function by the operator."", ""concept_4"": ""If an equation remains invariant under a family of continuous transformations (such as translation), then differentiating the solution family with respect to the transformation parameter yields functions that are the 'generators' of this symmetry. They describe the infinitesimal changes of the solution along the direction of the symmetric transformation, so they must lie in the null space of the linearized operator."", ""concept_5"": ""Compact Self-Adjoint Operator is a class of well-behaved operators in functional analysis. A 'compact operator' usually maps bounded sets to relatively compact sets (popularly speaking, it has a 'smoothing' effect), while 'self-adjoint' is a generalization of real symmetric matrices in infinite-dimensional spaces. Such operators have excellent spectral theory, with real eigenvalues that can be clearly analyzed."", ""theorem_1"": ""Perron-Frobenius Type Result is a class of theorems in spectral theory concerning operators with 'positivity' (for example, their integral kernel functions are always positive). Its classic conclusion is: the maximum eigenvalue of such operators is a positive real number, simple (i.e., the corresponding eigenspace is one-dimensional), and the corresponding eigenfunction can be taken as a strictly positive function.""}"	2	理论基础学科-基础数学-偏微分方程	1068860	6	5
recuU5KUFcTDmu	Is the first positive Steklov eigenvalue multiple for any ellipse in $\mathbb{R}^2$ that is not a disk?	"{""step_1"": ""Assumptions and Objectives: We assume that there exists an eigenfunction corresponding to $\\sigma_1$ with the property of 'even symmetry about the x-axis and odd symmetry about the y-axis'. We will prove that this assumption leads to a contradiction."", ""step_2"": ""Deriving the Lower Bound of the Eigenvalue: If the above assumption holds, then for this ellipse (let the semi-major axis be $a$ and the semi-minor axis be $b$, with $a > b$), the first eigenvalue must satisfy $\\sigma_1 > 1/b$ by applying the variational principle or comparison theorem."", ""step_3"": ""Deriving the Upper Bound of the Eigenvalue: Now we constrain $\\sigma_1$ from another perspective. First, applying Theorem_2 (Weinstock's Inequality) to the non-disk ellipse, we obtain $\\sigma_1 < 2\\pi/|\\partial \\mathcal{E}|$. Next, using Theorem_3 (Geometric Perimeter Property of Ellipses), we know that the perimeter of the ellipse $|\\partial \\mathcal{E}| > 2\\pi b$. Substituting the perimeter property into Weinstock's Inequality, we get $\\sigma_1 < 2\\pi/|\\partial \\mathcal{E}| < 2\\pi/(2\\pi b) = 1/b$."", ""step_4"": ""Discovering the Contradiction: In Step 2, we derived $\\sigma_1 > 1/b$ from the assumption. In Step 3, we obtained $\\sigma_1 < 1/b$ using classical theories and geometric facts. These two conclusions are obviously contradictory. This contradiction indicates that our initial assumption—'there exists a first-order eigenfunction with even symmetry about the x-axis and odd symmetry about the y-axis'—is false, thus completing the proof.""}"	"{""concept/theorem_1"": ""Courant's Nodal Domain Theorem: The nodal lines of k-th eigenfunction $u_k$ divide $\\Omega$ in no more than k subdomains, and no nodal line is a closed curve."", ""concept/theorem_2"": ""Weinstock's Inequality: This is a classic result in spectral geometry, which states that for any simply connected planar domain, $\\sigma_1 \\leq 2\\pi/|\\partial \\Omega|$, where $|\\partial \\Omega|$ is the perimeter of the domain. In particular, if the domain is not a disk, the inequality is strict, i.e., $\\sigma_1 < 2\\pi/|\\partial \\Omega|$."", ""concept/theorem_3"": ""Geometric Perimeter Property of an Ellipse: For an ellipse with semi-major axis $a$ and semi-minor axis $b$ (let $a > b$), its perimeter $|\\partial \\mathcal{E}|$ is strictly greater than the perimeter $2\\pi b$ of its inscribed circle (with radius $b$).""}"	No (simply guessing based on symmetry is not correct. A correct proof (checklist) should at least apply conclusions such as the Courant Nodal Domain Theorem (Theorem 1) to correctly rule out other possible eigenfunctions)	理论基础学科-基础数学-偏微分方程	208546	2	3
recuSuIjIYkDUd	Let $u$ be a Hecke-Maass cusp form, and $E_t(z)$ be an Eisenstein series with spectral parameter $t$. What is the upper bound estimate of the inner product $I= \langle u, E_t^3 \rangle = \int_{\mathbb{X}} u(z) \overline {E_t(z)^3} d\mu z$ as $t \to \infty$?	{'step_1': 'To obtain the upper bound of $I= \\langle u, E_t^3 \\rangle$, first, through small disturbance (concept_1), the non-square-integrable $E_t^3$ is transformed into a regularized inner product limit with small disturbance to facilitate subsequent analysis;', 'step_2': 'then, using Plancherel formula (concept_2), the integral is decomposed into discrete spectrum terms, continuous spectrum terms, etc.;', 'step_3': 'through Rankin-Selberg method (concept_3) and triple product formula of Eisenstein series (concept_4), these terms are associated with L-function values and transformed into the estimation of L-functions;', 'step_4': 'using Stirling formula (concept_5) to analyze the asymptotic behavior of the Γ function, determine the exponential decay range of terms related to spectral parameters, and screen out the main contribution terms;', 'step_5': 'combining the subconvexity bound (concept_6) to control the growth of L-functions, and estimating the fourth power sum of L-functions in short intervals through the fourth moment bound (concept_7), the final result $I \\ll t^{-1/3 + \\varepsilon}$ is obtained (corresponding to Theorem 1.3 in the paper).'}	"{concept_1"": ""small disturbance (Section 1-141 of the paper): To handle the non-square-integrability of $E_t(z)$, a small disturbance $t' \\to 0$ is introduced, and $I_2$ is expressed as the limit $I = \\lim_{t' \\to 0} \\langle u E_t, E_t E_{t + t'} \\rangle_{reg}$, allowing the integral to be processed via regularization methods."", ""concept_2"": ""Plancherel formula (Section 1-138 of the paper, Lemma 3.2): The regularized Plancherel formula decomposes the inner product into the sum of a constant term, discrete spectrum terms (cusp forms), continuous spectrum terms (Eisenstein series), and regularized residual terms, i.e., $\\langle F, G \\rangle_{reg} = \\langle F, \\sqrt{3/\\pi} \\rangle_{reg} \\langle \\sqrt{3/\\pi}, G \\rangle_{reg} + \\sum_j \\langle F, u_j \\rangle \\langle u_j, G \\rangle + \\frac{1}{4\\pi} \\int_{\\mathbb{R}} \\langle F, E_y \\rangle_{reg} \\langle E_y, G \\rangle_{reg} dy + \\cdots$."", ""concept_3"": ""Rankin-Selberg method (Section 1-125 of the paper): Establishes the connection between the inner product of products of automorphic functions and L-functions, for example, $\\langle u E_t, \\phi_k \\rangle = \\frac{\\rho_j(1) \\rho_k(1) \\Lambda(1/2 + it, \\phi_k \\times u_j)}{2 \\xi(1 + 2it)}$, converting integrals into estimates of L-functions."", ""concept_4"": ""triple product formula of Eisenstein series (Section 1-136 of the paper, Lemma 3.1): Zagier's regularized integral formula for the triple product of Eisenstein series, i.e., $\\int_{\\mathbb{X}}^{reg} E(z, 1/2 + s_1) E(z, 1/2 + s_2) E(z, 1/2 + s_3) d\\mu z$ can be expressed as a product form of $\\xi$-functions, used to analyze the contribution of continuous spectrum terms."", ""concept_5"": ""Stirling formula (Section 1-88 of the paper, Lemma 2.2): For a fixed $\\sigma \\in \\mathbb{R}$, $\\Gamma(\\sigma + it) = e^{-\\pi|t|/2} (1 + |t|)^{\\sigma - 1/2} \\exp\\left(it \\log \\frac{|t|}{e}\\right) (2\\pi)^{1/2} i^{\\sigma - 1/2} \\left\\{1 + \\mathcal{O}(1/|t|)\\right\\}$, used to estimate the asymptotic behavior of the Γ-function and control the exponential decay factors related to spectral parameters."", ""concept_6"": ""subconvexity bound (Section 1-104 of the paper, Lemma 2.6): For small $\\delta > 0$, $L(1/2 + 2it, \\phi_k) \\ll \\left[(1 + |2t + t_k|)(1 + |2t - t_k|)\\right]^{1/4 - \\delta}$, used to control the growth of L-functions on the critical line."", ""concept_7"": ""fourth moment bound (Section 1-90 of the paper, Lemma 2.3): For $K^{1/3} \\leq G \\leq K$, $\\sum_{K \\leq t_k \\leq K + G} |L(1/2, \\phi_k)|^4 \\ll G K^{1 + \\varepsilon}$, used to estimate the upper bound of the sum of fourth powers of L-functions over short intervals.""}"	-1/3	理论基础学科-基础数学-数论	728533	2	6
recuUg8GfKGfI2	What factors determine chemical reaction networks (CRNs) under slow driving conditions ($\epsilon \ll 1$, where the reaction rates of the external reservoir are much slower than internal reactions)?	"{""step_1"": ""Modeling the slowly driven CRN. Reactions are divided into core reactions satisfying local detailed balance and external reservoir reactions slowly coupled via parameter ε(≪1); write the relationship between mass action law rates, reservoir intake/excretion forms, and work."", ""step_2"": ""Write the rate equation and perform ε expansion. Core/boundary decomposition yields $∂t n = S0·J0(n) + ε[r(z^C − n/Ω)]$; let $n = n0 + ε n1 + …$. The leading order is a closed system, monotonically relaxing to the thermal equilibrium manifold $J0(n0)=0$, whose general solution can be parameterized as $n0 = [neq e^{ζ·η}]$, where $ζ$ gives L conserved moieties."", ""step_3"": ""Direct perturbation fails at O(ε) order. Left-multiplying the equation for $n1$ by $ζ^T$ leads to the secular divergence $ζ^T n1(t) ~ t·ζ^T[r(z^C − n0(∞)/Ω)]$, indicating that the limit $ε→0$ is singular and requires singular perturbation treatment."", ""step_4"": ""Multiple time scales. Introduce the slow time $τ=εt$ and require the solvability condition to eliminate secular terms: $ζ^T ∂τ n0 = ζ^T[r(z^C − n0/Ω)]$, which is the conservation law form of slow dynamics in the moiety subspace."", ""step_5"": ""Project $n0$ onto the equilibrium manifold parameter $η$ to obtain $M(η)∂τη = ζ^T J^C(η)$, where $M=ζ^T[[neq e^{ζ·η}]]ζ$ and $J^C=r(z^C − neq e^{ζ·η}/Ω)$. This is the main result: the universal slow dynamics equation."", ""step_6"": ""Extract determinants and independence. The equation only contains the conserved quantity structure $ζ$ and external couplings $(r, z^C)$, independent of the activation energies or specific rates of core reactions; the steady state is also independent of core chemical potentials—thus, the dynamics are robust to details of microscopic rates, with dimension reduced from N to L."", ""step_7"": ""Thermodynamic consistency and dissipation. Even though the system is always approximately in a certain closed equilibrium state, entropy is still generated through slow fluxes: $T·Ṡ = W·(r^+−r^−) + O(ε^2)$, which can be evaluated on the slow solution."", ""step_8"": ""Steady state structure and network topology. Using the rank-nullity relation, derive $MC = L_br + C_C$, indicating that the emergence of loops is a necessary condition for reaching non-equilibrium steady states; the steady state of the slow equation can be obtained solely from reservoir parameters without core rates."", ""step_9"": ""Stochastic extension and large deviations. Derived from the Doi–Peliti/Hamilton–Jacobi formalism in the macroscopic limit, the condition to avoid divergence also gives $ζ^T(∂τ λ − J^C)=0$, thus yielding the same slow dynamics conclusion in the presence of fluctuations."", ""step_10"": ""Inductive answer. Therefore, under $ϵ≪1$ slow driving, both the slow dynamics and final steady state of the CRN are independent of details of reaction rates, determined entirely by (i) the subspace spanned by the conserved quantities $ζ$ of the closed system and (ii) the external couplings $(r, z^C)$.""}"	{'concept_1': 'Definition and Local Detailed Balance of Slow-Driven CRNs: Core reactions satisfy $k+_α/k−_α = exp{−ΔG_α/RT}$; the reservoir is slowly coupled with $ε$ and follows the law of mass action and the energetic relationship of chemical potential doing work.', 'concept_2': 'Conserved Quantities/Moiety and Parameterization: The equilibrium solution of a closed system is $n0=[neq e^{ζ·η}]$, where $ζ$ is the cokernel basis of $(S0)^T$, usually corresponding to element conservation.', 'concept_3': 'Singular Perturbation and Multiple Time Scales: Direct $ε$ expansion leads to secular divergence at $O(ε)$, requiring the introduction of $τ=εt$ and the application of the solvability condition $ζ^T∂τ n0=…$.', 'concept_4': 'Universal Slow Dynamical Equation: $M(η)∂τη = ζ^T J^C(η)$, where $M=ζ^T[[neq e^{ζ·η}]]ζ$ and $J^C=r(z^C − neq e^{ζ·η}/Ω)$.','concept_5': 'Independence from Core Rate Details and Dimensionality Reduction: The slow equation does not contain core activation energies and rates, and the steady state is also independent of core chemical potentials; $N$ species → $L$ degrees of freedom of conserved quantities.', 'concept_6': 'Entropy Production and Thermodynamic Consistency: $T·Ṡ = −G·S·(k+−k−) + W·(r^+−r^−)$, and the dominant term under slow driving is $W·(r^+−r^−)+O(ε^2)$.', 'concept_7': 'Steady-State Condition and Emergent Loops: $MC = L_br + C_C$ (number of reservoirs = number of broken conserved quantities + number of emergent loops); the existence of emergent loops is a necessary condition for reaching NESS.', 'concept_8': 'Stochastic Extension: The Doi–Peliti and Hamilton–Jacobi formalisms give the same solvability condition $ζ^T(∂τ λ − J^C)=0$ for the slow equation, ensuring consistency at the large deviation level.', 'theorem_1': 'Local Detailed Balance/Microscopic Reversibility: $k+_α/k−_α = exp{−ΔG_α/RT}$ is a necessary condition for the existence of thermal equilibrium in a physical CRN and is used to constrain rate parameterization.', 'theorem_2': 'Rank-Nullity Relationship (Topological Counting) for Open Systems: Derived from rank-nullity, $MC = L_br + C_C$ links reservoirs, broken conservation laws, and emergent loops.'}	Under slow driving conditions (ϵ ≪ 1), the long-term evolution of Chemical Reaction Networks (CRNs) is dominated by two types of factors: first, the conserved quantities followed by the closed system (their number is usually much smaller than the number of species in the system), and second, the coupling characteristics with external reservoirs (e.g., the reaction rates or concentrations of the reservoirs). This slow dynamics is not sensitive to the internal microscopic details of the network (such as specific reaction rate constants or activation energies), and the system behavior can be accurately described even in the absence of these detailed information.	理论基础学科-理论化学-化学反应动力学理论	476027	2	13
recuU0wMcv6cHB	What is the relationship between the momentum diffusion coefficient $\kappa$ and the spatial diffusion coefficient $D_s$ in the Langevin equation?	"{""Step_1"": ""Objective: Derive the relationship between the momentum diffusion coefficient $\\kappa$ and the spatial diffusion coefficient $D_s$ within the Langevin framework for heavy quarks; adopt natural units with $k_B=1$."", ""Step_2"": ""Establish Langevin dynamics and definition of noise (Concept_1): $\\tfrac{d\\mathbf{p}}{dt}=-\\eta_D(p)\\,\\mathbf{p}+\\boldsymbol{\\xi}$, where the isotropic white noise satisfies $\\langle\\xi_i(t)\\xi_j(t')\\rangle=\\kappa\\,\\delta_{ij}\\,\\delta(t-t')$, and $\\kappa$ characterizes the intensity of thermal random forces."", ""Step_3"": ""Apply the fluctuation-dissipation theorem (Concept_2): $\\eta_D(p)=\\frac{\\kappa}{2TE}$, to ensure the equilibrium distribution (Maxwell–Jüttner) at temperature $T$."", ""Step_4"": ""Adopt the Einstein–Smoluchowski relation for spatial diffusion in the small momentum limit (Concept_3): $D_s \\equiv \\frac{T}{M\\,\\eta_D(0)}$."", ""Step_5"": ""Take the small momentum limit $p\\to0\\Rightarrow E\\to M$, yielding $\\eta_D(0)=\\kappa/(2TM)$."", ""Step_6"": ""Substitute $\\eta_D(0)=\\kappa/(2TM)$ into the Einstein relation: $D_s=\\frac{T}{M\\,\\eta_D(0)}=\\frac{T}{M\\,(\\kappa/(2TM))}=\\frac{2T^2}{\\kappa}$."", ""Step_7"": ""Answer (consistent with the given result) — The relationship between momentum and spatial diffusion in the Langevin equation is: $\\boxed{D_s = \\frac{2T^2}{\\kappa}}$. Assumptions: natural units with $k_B=1$, local thermal equilibrium, isotropic Markovian white noise, and the small momentum limit.""}"	{'concept_1': 'The Fluctuation-Dissipation Theorem (relativistic Einstein relation) gives the relationship between the damping coefficient $\\eta_D(p)$ and $\\kappa$', 'concept_2': 'Local thermal equilibrium and isotropic Markovian white noise with the noise correlation \\[ \\langle \\xi_i(t)\\,\\xi_j(t\') \\rangle \\;=\\; \\kappa\\,\\delta_{ij}\\,\\delta(t-t\')\\,, \\] and the equilibrium distribution is in the Maxwell–Jüttner form $f_{\\mathrm{eq}}\\propto e^{-E/T}$ (natural units $k_B=1$).', 'concept_3': 'Einstein–Smoluchowski relation for spatial diffusion (defined in the small momentum limit): $\\displaystyle D_s \\equiv \\frac{T}{M\\,\\eta_D(0)}$.', 'concept_4': 'Damping coefficient in the small momentum limit $E\\to M$: $\\displaystyle \\eta_D(0)'}	\[ \boxed{D_s = \frac{2T^2}{\kappa}} \]	理论基础学科-理论物理-几何	103739	2	4
recuU0q3re659f	Under the action $S = \int \sqrt{-g} \, d^{4}x \left[ R - 2\kappa^{2} \left( \mathcal{L}(\varphi) - \mathcal{L}_{\mathrm{NLED}}(F) \right) \right]$, for a dyonic black bounce with the metric written as $ds^{2} = A(r)\, dt^{2} - \frac{1}{A(r)}\, dr^{2} - \Sigma^{2}(r)\, d\Omega^{2}$, where the magnetic charge is $q_m$ and the electric charge is $q_e$, what is the specific form of its electromagnetic field scalar $F$?	{'step_1': 'First, according to \\textbf{theorem_1}, take the Ansatz for the dyonic spherically symmetric field, \\[ F=dA=F_{tr}(r)\\,dt\\wedge dr+q_m\\sin\\theta\\,d\\theta\\wedge d\\phi, \\] where \\(E(r)\\equiv F_{tr}(r)\\) is denoted.', 'step_2': 'Next, from the radial Gauss\'s law of \\textbf{concept_1} (NLED field equation \\(\\nabla_\\mu(\\mathcal L_{\\mathcal F}F^{\\mu\\nu})=0\\)), we have \\[ \\mathcal L_{\\mathcal F}\\,\\Sigma^{2}(r)\\,E(r)=q_e \\;\\Rightarrow\\; E(r)=\\frac{q_e}{\\mathcal L_{\\mathcal F}\\,\\Sigma^{2}(r)}. \\]', 'step_3': 'Finally, using the definition of the electromagnetic field scalar in \\textbf{theorem_2}: \\(\\displaystyle \\mathcal F\\equiv \\tfrac14 F_{\\mu\\nu}F^{\\mu\\nu}\\), and raising indices under \\[ ds^{2}=A(r)\\,dt^{2}-\\frac{dr^{2}}{A(r)}-\\Sigma^{2}(r)\\,d\\Omega^{2} \\] we have \\[ F^{tr}=-E,\\qquad F^{\\theta\\phi}=\\frac{q_m}{\\Sigma^{4}(r)\\sin\\theta}, \\] thus \\[ F_{\\mu\\nu}F^{\\mu\\nu}=2\\!\\left(-E^{2}+\\frac{q_m^{2}}{\\Sigma^{4}(r)}\\right) \\;\\Rightarrow\\; \\boxed{\\; \\mathcal F=\\frac12\\!\\left(\\frac{q_m^{2}}{\\Sigma^{4}(r)}-E^{2}\\right) =\\frac{q_m^{2}\\,\\mathcal L_{\\mathcal F}^{2}(r)-q_e^{2}} {2\\,\\mathcal L_{\\mathcal F}^{2}(r)\\,\\Sigma^{4}(r)}\\; }. \\]'}	"{""concept_1"": ""Maxwell field equation of NLED, giving the relationship between $F_{tr}$, electric charge $q_e$, and $L_{\\mathcal F}$: $\\nabla_\\mu\\!\\left(\\mathcal L_{\\mathcal F} F^{\\mu\\nu}\\right)=0$, where $\\mathcal{L}_{\\mathcal F} = \\partial \\mathcal{L}_{\\mathrm{NLED}}(F)/ \\partial F$"", ""theorem_1"": ""Standard Ansatz for dyonic spherically symmetric fields: $F=dA=F_{tr}(r)\\,dt\\wedge dr+q_m\\sin\\theta\\,d\\theta\\wedge d\\phi$."", ""theorem_2"": ""The electromagnetic field scalar is defined as $F=1/4 F_{\\mu \\nu}F^{\\mu \\nu}$""}"	F=\frac{q_m^{2}\,\mathcal{L}_{\mathcal F}^{\,2}(r)-q_e^{2}} {2\,\mathcal{L}_{\mathcal F}^{\,2}(r)\,\Sigma^{4}(r)}\,.	理论基础学科-理论物理-电磁学	110531	2	3
recuU5Cxq7MO7M	During the execution of the zero-knowledge proof protocol, the verifier observes the commitment values $Com(a)=C_1$, $Com(b)=C_2$, $Com(\tilde{R}_\pi)=C_3$, $Com(f_\pi)=C_4$, and knows that $a+b=s \pmod{m}$. $f_\pi$ satisfies $wt_L(f_\pi)=w$ and $\sum_{j=1}^{n\ell} f_{\pi(j)} =0$. Given the modulus $m = 9$, the code length of the balance problem $\left( \bar{e} \mid -\bar{e} \right) = 24$, the upper bound of the Lee weight $w = 20$, and the verifier checks that $f_\pi \tilde{R}_\pi = a \pmod{m}$. Try to deduce the original code length $n$.	"{""step1"": ""Calculate the Lee metric parameter $\\ell$ (based on concept_1)\\ According to $m = 2\\ell + 1$ (where $m$ is an odd integer and $m \\geq 3$). Given $m = 9$. Calculate $\\ell = \\left\\lfloor \\frac{m}{2} \\right\\rfloor = \\left\\lfloor \\frac{9}{2} \\right\\rfloor = \\lfloor 4.5 \\rfloor = 4.$ Obtain $\\ell = 4$."", ""step2"": ""Calculate the extended code length $\\bar{n}$ (based on theorem_1)\\ According to the balanced problem code length $\\bar{n}' = (\\bar{e} \\mid -\\bar{e}) = 2\\bar{n}$, given $\\bar{n}' = 24$. Calculate $\\bar{n}' = 2\\bar{n} \\implies 24 = 2\\bar{n} \\implies \\bar{n} = \\frac{24}{2} = 12.$ Obtain $\\bar{n} = 12$."", ""step3"": ""Apply the reduction formula to solve for $n$ (based on concept_2)\\ Weights need to be compatible during the reduction process, requiring $n\\ell \\leq \\bar{n}(\\ell - 1)$. Given $\\bar{n} = 12$ and $\\ell = 4$, thus $\\ell - 1 = 3$. Then derive that $n$ satisfies the equation $n + \\left\\lceil \\frac{n}{3} \\right\\rceil = 12.$ Solve the equation (consider three cases of $n \\bmod 3$):\\ Case 1: $n$ is divisible by 3 (let $n = 3k$): $\\left\\lceil \\frac{3k}{3} \\right\\rceil = k \\implies 3k + k = 4k = 12 \\implies k = 3 \\implies n = 9.$\\ Case 2: $n \\equiv 1 \\pmod{3}$ (let $n = 3k + 1$): $\\left\\lceil \\frac{3k + 1}{3} \\right\\rceil = k + 1 \\implies (3k + 1) + (k + 1) = 4k + 2 = 12 \\implies k = 2.5 \\quad \\text{(non-integer, invalid)}.$\\ Case 3: $n \\equiv 2 \\pmod{3}$ (let $n = 3k + 2$): $\\left\\lceil \\frac{3k + 2}{3} \\right\\rceil = k + 1 \\implies (3k + 2) + (k + 1) = 4k + 3 = 12 \\implies k = 2.25 \\quad \\text{(non-integer, invalid)}.$\\ Obtain the unique solution: $n = 9$."", ""step4"": ""Verify the weight constraints (based on theorem_2 and concept_2)\\ Weight compatibility condition (reduction validity): According to $n\\ell \\leq \\bar{n}(\\ell - 1)$ (to ensure the maximum weight of the original vector $n\\ell$ does not exceed the new weight upper limit). $n\\ell = 9 \\times 4 = 36, \\quad \\bar{n}(\\ell - 1) = 12 \\times 3 = 36 \\implies 36 \\leq 36 \\quad \\text{(satisfied)}.$\\ Balanced problem weight constraint: According to $w \\leq \\bar{n}(\\ell - 1)$. $\\bar{n}(\\ell - 1) = 12 \\times 3 = 36, \\quad w = 20 \\leq 36 \\quad \\text{(satisfied)}.$\\ Conclusion: The original code length satisfying all constraints is $n = 9$.""}"	{'concept_1': '$m=2\\ell+1, \\ell⩾2$ defines the Lee metric parameters corresponding to the modulus $m$, where $m$ is an odd integer and $m \\geq 3$.', 'theorem_1': 'Defining $\\bar{n}'=\\left( \\bar{e} \\mid -\\bar{e} \\right) = 2\\bar{n}$ describes the relationship between the code length of the balance problem $\\left( \\bar{e} \\mid -\\bar{e} \\right)$ and the extended code length $\\bar{n}$, which is determined by the matrix structure $H_{\\pm} = \\left( \\begin{array}{c} \\bar{H} \\\\ \\hline -\\bar{H} \\end{array} \\right) $.', 'theorem_2': '$w \\leq \\bar{n}(\\ell - 1)$ is the weight constraint condition for the balanced Lee syndrome decoding problem, ensuring that the original vector $e$ can be effectively converted into the binary form $f$, where $f \\in \\{-1,0,1\\}^{n\\ell}$.', 'concept_2': 'To ensure weight compatibility during the reduction process, the upper weight bound of the solution vector is required to be $\\bar{n}(\\ell - 1)$, so the following must be satisfied: $n\\ell \\leq \\bar{n}(\\ell - 1)$. Otherwise, even if the original problem has a solution, the newly constructed $g$ may exceed the upper weight bound, leading to reduction failure.'}	n=9	理论基础学科-理论计算机科学-密码学	182108	2	3
recuTKxErJ9PO5	Under the premise that the Exponential Time Hypothesis (ETH) holds, for a fixed pattern graph $H$, assume that in the reduction from 3-Coloring to $\mathrm{ColSub}(H)$, the selected blowup parameter is $t = \frac{n}{2\gamma(H)}$, and the size of the generated instance satisfies $s \leq c \cdot 3^t$ (where $c$ is a constant). Please write the lower bound expression of the time complexity of $\mathrm{ColSub}(H)$ in the form of $n$ and $\gamma(H)$ according to these conditions.	"{'step1': '3-Coloring of graphs with maximum degree 4 requires $2^{\\Omega(n)}$ time under ETH, which is equivalent to the non-existence of $2^{o(n)}$ algorithms, and this is invoked as the lower bound foundation.', 'step2': 'Define the strong product (blowup) $H \\boxtimes K_t$: replace each vertex of $H$ with a block of size $t$, and add complete bipartite edges between corresponding blocks; Note: Marx\'s notation $H(t)$ is equivalent to $H \\boxtimes K_t$.', 'step3': 'Introduce the matching-linked set, and based on this, define the linking capacity $\\gamma(H)=\\liminf_{t\\to\\infty}\\ell(H\\boxtimes K_t)/t$, and give its basic range $1\\le \\gamma(H)\\le |V(H)|$.', 'step4': 'For any graph $G$ with $n$ vertices and maximum degree $\\Delta$, when $t \\ge \\dfrac{3\\Delta\\,n}{\\gamma(H)}$, $H\\boxtimes K_t$ contains a topological minor of $G$, and the embedding can be explicitly constructed in $O(n\\cdot f(k))$ time ($f(k)$ depends only on $k=|V(H)|$).', 'step5': 'If $G \\subseteq H\\boxtimes K_t$, a colorful graph $X$ can be constructed in time $9^t\\!\\cdot\\!\\mathrm{poly}(k,t)$ with $k\\cdot 3^t$ vertices, such that the $H$-colorful copies in $X$ are in one-to-one correspondence with the 3-assignments of $G$.', 'step6': 'From the ETH lower bound in step1, the embedding threshold in step4, and the reduction size in step5, it can be concluded that there exists a polynomial-time reduction from 3-Coloring (maximum degree 4) to $\\mathrm{ColSub}(H)$ that preserves ""yes/no"" equivalence; therefore, if there exists an $n^{o(\\gamma(H))}$ time algorithm for $\\mathrm{ColSub}(H)$, then 3-Coloring will have a $2^{o(n)}$ algorithm, violating ETH.', 'step7': 'From the above, it can be obtained: under the validity of ETH, there exists a constant $\\alpha>0$ such that for any fixed $H$, there is no $O\\!\\big(n^{\\alpha\\cdot\\gamma(H)}\\big)$ algorithm for $\\mathrm{ColSub}(H)$; equivalently, $\\mathrm{ColSub}(H)$ requires at least $n^{\\Omega(\\gamma(H))}$ time (where $n$ is the size of the target instance).'}"	{'concept/theorem_1': 'ETH Lower Bound for 3-Coloring: Under the Exponential Time Hypothesis (ETH), 3-Coloring of size $n$ requires time $2^{\\Omega(n)}$. The lower bound here is for the original input size $n$, not the intermediate reduction parameters.', 'concept/theorem_2': 'Selection of $t$ in the Reduction. In the reduction from 3-Coloring to $\\mathrm{ColSub}(H)$, select $t = \\frac{n}{2\\gamma(H)}$. This $t$ is a blowup parameter that affects the size of the constructed $\\mathrm{ColSub}(H)$ instance but does not change the fact that the ETH lower bound is transmitted in terms of $n$.', 'concept/theorem_3': 'Relationship between Instance Size and $t$. The size of the generated $\\mathrm{ColSub}(H)$ instance satisfies $s \\leq c \\cdot 3^t$, where $c$ is a constant. The exponent $t$ here changes with $\\gamma(H)$, thereby affecting the size of $s$.', 'concept/theorem_4': 'Transmission Principle of Complexity Lower Bounds. Reductions only affect the time complexity by a polynomial factor, and the core exponential part of the lower bound $2^{\\Omega(n)}$ is still transmitted according to the original problem size $n$, without moving $\\gamma(H)$ to the denominator of the exponent.', 'concept/theorem_5': 'Position of $\\gamma(H)$ in the Formula. $\\gamma(H)$ changes the blowup size through $t$, thereby affecting the instance size $s$, but it does not directly enter the denominator of the exponent. The correct form should retain $2^{\\Omega(n)}$, while $\\gamma(H)$ only appears in the polynomial factor.'}	T_{\mathrm{ColSub}(H)}(n,\gamma(H)) \ge \left( 3^{\frac{n}{2\gamma(H)}} \right)^{\Omega(\gamma(H))} = 3^{\Omega(n)} = 2^{\Omega(n)}.	理论基础学科-理论计算机科学-计算复杂度	198534	2	7
recuTLpE9sKOPt	In the strongly stably stratified shear turbulence of low Prandtl number (Pr ≪ 1) fluids, what is the scaling relation between the vertical length scale $H^*$ and the vertical velocity $W^*$ when in the non-diffusive strongly stratified turbulence regime ($Pe_b \gtrsim O(1)$)?	{'step_1': 'Determine physical interval conditions: It is known that the Instruction specifies a low Prandtl number fluid ($Pr \\ll 1$) and $Pe_b \\gtrsim O(1)$. According to Concept 3 (Buoyancy Péclet Number), $Pe_b \\gtrsim O(1)$ indicates a non-diffusion-dominated region where diffusion effects are secondary. Focus on the mutual constraints of inertia, viscosity, and stratification, rather than the diffusion-dominated case.', 'step_2': 'Stratification intensity and vertical length scale: According to Concept 1 (Definition and Physical Significance of Froude Number), under strong stratification ($Fr \\ll 1$), the vertical length scale is proportional to the horizontal velocity: $H^* \\propto Fr \\, L^*$. This gives the initial scaling relationship between $H^*$, $Fr$, and $L^*$.', 'step_3': 'Can viscous effects be neglected: According to Concept 2 (Buoyancy Reynolds Number $Re_b = a^2 Re$), if $Re_b \\gg 1$, viscosity can be neglected. For low $Pr$, if $Pe_b \\gtrsim O(1)$ and $Pr \\ll 1$, $Re_b$ is often large (since $Pe_b = Pr \cdot Re_b$). Therefore, the current interval can be regarded as inertia-stratification dominated, with viscous effects being secondary.', 'step_4': 'Stratified energy balance constraint on vertical kinetic energy: According to Concept 5, under strong stratification, the ratio of the square of vertical velocity to the square of horizontal velocity satisfies: $\\frac{w^{*2}}{U^{*2}} \\sim Fr^2$. Taking the square root gives: $W^* \\propto Fr \\, U^*$.', 'step_5': 'Supplementary role of wave dissipation mechanism: According to Concept 4, in non-diffusive strongly stratified turbulence, vertical velocity is not only determined by geometric constraints (Concept 1) and energy balance (Concept 5), but also affected by energy exchange between small-scale internal waves and turbulence. Here, this mechanism ensures the validity of the scaling $W^* \\propto Fr^{1/2} U^*$ in Step 4, that is, wave dissipation provides physical support for this scaling, rather than being purely determined by $H^* N^*$.'}	"{""concept_1"": ""Froude number $Fr = U^*/(N^* L^*)$ describes the inhibitory effect of stable stratification on flow: it characterizes the strength of the inhibitory effect of stable stratification on flow. $Fr \\ll 1$ indicates strong stratification, inhibiting vertical motion; $Fr \\sim 1$ indicates that stratification and inertial effects are equivalent; otherwise, it indicates weak stratification. Thin-layer structures form under strong stratification, where $H^*$ is proportional to $U^*$."", ""concept_2"": ""Buoyancy Reynolds number $Re_b = \\alpha^2 Re$ serves as a parameter to determine the importance of viscous effects: where $\\alpha = H^*/L^*$ is the anisotropic aspect ratio, and $Re$ is the large-scale Reynolds number. Physical significance: It represents the ratio of vertical inertial forces to viscous forces in turbulence, and is a key parameter for judging whether viscous effects are important. When $Re_b \\gg 1$, viscous effects can be neglected; when $Re_b \\lesssim 1$, viscosity begins to control the flow."", ""concept_3"": ""Buoyancy Péclet number $Pe_b = Pr \\, Re_b$ serves as a parameter to determine the importance of diffusion effects: Physical significance: It represents the relative importance of convection and diffusion in buoyancy transport (heat or mass transport) in turbulence. When $Pe_b \\ll 1$, buoyancy diffusion is important at all scales (diffusion-dominated); when $Pe_b \\gtrsim 1$, diffusion effects are secondary (non-diffusion-dominated)."", ""concept_4"": ""Energy balance relation for vertical velocity controlled by wave dissipation mechanism in strongly stratified non-diffusive turbulence Physical significance: In the non-diffusive strongly stratified regime ($Pe_b \\gtrsim O(1)$) and when the buoyancy Reynolds number is sufficiently large, the formation of vertical velocity is not only subject to geometric constraints but also needs to consider the energy exchange between small-scale internal waves and turbulence. This wave dissipation mechanism changes the source and dissipation balance of vertical kinetic energy, making the vertical velocity scaling no longer simply determined by the product of $H^*$ and $N^*$, but requiring a solution combining the combined effects of energy input, wave dissipation, and turbulent kinetic energy transfer."", ""concept_5"": ""Energy balance constraint for vertical kinetic energy suppressed by stratification In strongly stratified non-diffusive turbulence, vertical kinetic energy is mainly derived from the energy transfer from horizontal motion to the vertical direction via tilted vortex structures. The efficiency of this transfer is related to the stratification strength, satisfying that the ratio of the square of vertical velocity to the square of horizontal velocity is $\\sim Fr$. Physical significance: Under strong stratification, vertical velocity is much smaller than horizontal velocity, and the ratio is determined by the Froude number.""}"	Non-diffusive strongly stratified turbulence region ($Pe_b \gtrsim O(1)$): $H^* \propto Fr \, L^*, \quad W^* \propto Fr^{1/2} U^*$	理论工程学科-理论工程力学-流体力学	193085	5	6
recuTXzS2cdMei	Define the complexity class $C$ as follows: A decision problem $L$ belongs to $C$ if and only if there exists a Probabilistically Checkable Proof (PCP) system $(P, V)$ satisfying: - For $x \in L$, it is always accepted; for $x \notin L$, it is rejected with a constant upper bound error probability; - There exists an expected polynomial-time simulator $\mathrm{Sim}$ such that for any **adaptive** polynomial-time verifier $V^{*}$ and any $x \in L$, the distribution of $\mathrm{Sim}^{V^{*}}(x)$ is exactly the same as the observation view of $V^{*}$ when accessing the real proof (distributionally consistent); - The honest verifier is non-adaptive; - Both the proof length and verifier resources are polynomial. Question: Does the decision version of #SAT belong to $C$? (Equivalently: Does #𝑃⊆𝐶 hold?)	"{'step_1': 'Foundation: Reduce #SAT to Polynomial Summation Problem (Concept 1) The core of the decision version of #SAT is to determine whether ""the number of satisfying assignments of a CNF formula is equal to γ"", which can be transformed into the summation verification of an arithmetized polynomial: Let Φ be a CNF formula, and its arithmetized polynomial $P:\\{0,1\\}^m \\to \\mathbb{F}$ satisfies "" $P(a)=1$ if a is a satisfying assignment of Φ, otherwise 0"". It is necessary to verify $\\sum_{a\\in\\{0,1\\}^m} P(a)=\\gamma$. According to Concept 1, the Sumcheck protocol decomposes the global summation into layer-by-layer consistency checks (such as $g_1(0)+g_1(1)=\\gamma$, $g_i(c_i)=g_{i+1}(0)+g_{i+1}(1)$, etc.) by defining chain polynomials $g_i(X) = \\sum_{a_{i+1},\\dots,a_m\\in\\{0,1\\}} P(c_1,\\dots,c_{i-1},X,a_{i+1},\\dots,a_m)$, and writes the evaluation tables of these polynomials into the proof string to form the PCP framework. This step provides the basic structure for subsequent verification, which is consistent with the definition of Sumcheck as ""arithmetizing #SAT and expanding it into a PCP framework"".', 'step_2': 'Zero-Knowledge Guarantee: Masking and Zero-Sum Polynomials (Concept 2 + Concept 3) Class C requires ""the view of the adaptive verifier can be perfectly simulated"" (zero-knowledge), so it is necessary to avoid the proof directly leaking information about $P$. According to Concept 2, introduce a random $\\alpha \\in \\mathbb{F}$ and a zero-sum polynomial $R$ (satisfying $\\sum_{a} R(a)=0$), and change the verification object to $\\alpha P + R$. At this time, the summation formula to be verified becomes $\\sum_{a} (\\alpha P + R)(a) = \\alpha \\gamma$, which not only maintains correctness but also masks $P$ through $R$. To construct the required $R$, the vanishing polynomial technique in Concept 3 is used: Let $R(X) = Q(X) - Q(X_{\\text{rev}}) + \\sum_{i=1}^m X_i(1-X_i)T_i(X)$, where $X_i(1-X_i)$ is a vanishing term on the hypercube (guaranteed by the Combinatorial Nullstellensatz, which is identically zero on $\\{0,1\\}^m$). Thus, $R(a)=0$ for all $a\\in\\{0,1\\}^m$, satisfying the zero-sum condition; at the same time, $R$ is pseudorandom at other points in the field to avoid leaking information about $P$.', 'step_3': 'Locally Simulable: Deal with Adaptive Verifiers (Concept 4) Class C requires ""the simulator can reproduce the view of the adaptive verifier"", so it is necessary to ensure that the encoding of the proof can be locally simulated. According to Concept 4, encode $\\alpha P + R$, intermediate polynomials $g_i$, and auxiliary polynomials $T_i$ into ""Random Low-Degree Extension (LDE)"": that is, randomly extract an extension polynomial $\\tilde{f}$ consistent with the boundary function (such as the value of $\\alpha P + R$ on $\\{0,1\\}^m$) from the Reed-Muller code $\\text{RM}[\\mathbb{F}, m, d]$. The key of this encoding is ""local simulability"": for any adaptive query sequence $(q_1,\\dots,q_t)$, the simulator can output results with the same distribution as the evaluations of the real $\\tilde{f}$ at these points. This ensures that no matter how the malicious verifier adaptively queries, its view can be reproduced by the simulator, satisfying the zero-knowledge requirement.', 'step_4': 'Efficient Checking by Verifier: Low-Degree and Consistency (Concept 5) Class C requires ""the honest verifier is non-adaptive and has polynomial resources"", so it is necessary to efficiently verify the correctness of the proof. According to Concept 5, use the Low Individual Degree Test (LIDT): for polynomials such as $\\alpha P + R$, $g_i$, and $T_i$, randomly sample along the coordinate axes, fix all variables except one, check whether the resulting univariate polynomial has degree ≤ d (the individual degree of $P$), and spot-check consistency. The query complexity of LIDT is $O(md \\cdot \\text{poly}(1/\\epsilon) \\cdot \\log(1/\\delta))$ (polynomial order), which can verify that the polynomial belongs to the low-degree code, ensure the consistency of the proof, and naturally match the non-adaptive query mode of the honest verifier (pre-fixing query positions), satisfying the completeness and soundness of PCP.', 'step_5': 'Satisfy All Conditions of Class C - **Completeness and Soundness**: For $x\\in L$ (the #SAT instance is true), the honest verifier passes the consistency check of Sumcheck and LIDT verification; for $x\\notin L$, the false proof will be rejected by LIDT with constant probability. - **Zero-Knowledge**: Through the masking in Concept 2, the construction of $R$ in Concept 3, and the locally simulable LDE in Concept 4, the simulator can perfectly reproduce the view of the adaptive verifier. - **Resource Bounds**: The proof length (LDE encoding), verification time (LIDT queries), and simulator time (local simulation) are all polynomial. Therefore, the decision version of #SAT belongs to Class C, i.e., $\\#P \\subseteq C$.'}"	"{""concept_1"": ""Sumcheck Protocol → Arithmetize #SAT and 'unfold' it into a PCP skeleton, sumcheck → PCP skeleton: Arithmetize the counting assertion of CNF: Let $F$ be a field, and let $P:\{0,1\}^m \to F$ be the arithmetized polynomial of $\Phi$ (individual degree $\leq d$). It is necessary to verify $\sum_{a \in \{0,1\}^m} P(a) = \gamma$. Sumcheck (interactive) defines a chain of polynomials for $i=1..m$ as $g_i(X) = \sum_{a_{i+1},\dots,a_m \in \{0,1\}} P(c_1,\dots,c_{i-1},X,a_{i+1},\dots,a_m)$, and checks $g_1(0) + g_1(1) = \gamma$, $g_i(c_i) = g_{i+1}(0) + g_{i+1}(1)$, $g_m(c_m) = P(c_1,\dots,c_m)$. Write the 'answer table/coefficient table for all challenge prefixes' (evaluation tables of $g_i$ and $P$) into the proof string at one time to obtain the baseline PCP; its checkability comes from the above consistency equations, but the bare unfolding will leak information (subsequent masking and simulability are required)."", ""concept_2"": ""Masking and Constraint Detection in ZK IOP, masking $\alpha P + R$ & constraint detection: Select a random $\alpha \in F$ and a polynomial $R$ satisfying the zero-sum condition ($\sum_{a \in \{0,1\}^m} R(a) = 0$), and run Sumcheck on $\alpha P + R$ instead: declare $\sum_{a \in \{0,1\}^m} (\alpha P + R)(a) = \alpha \gamma$, and take the evaluation tables/commitments of $(\alpha P + R)$ and its intermediate polynomials as queryable proof components (oracle tables)."", ""concept_3"": ""Combinatorial Nullstellensatz | Ensures the construction of polynomial terms that are identically zero on $\{0,1\}^m$, used for masking without breaking verification, thereby supporting perfect zero-knowledge.: Use the Combinatorial Nullstellensatz to construct vanishing terms on the hypercube, such as $Z(X) = \sum_{i=1}^m X_i(1-X_i)T_i(X)$ or $Z_H(X) = \prod_{a \in H} (X - a)$, let $R$ take the form $R(X) = Q(X) - Q(X_{rev}) + \sum_{i=1}^m X_i(1-X_i)T_i(X)$, thereby ensuring $R(a) = 0$ for all $a \in \{0,1\}^m$ (the 'zero-sum/zero-value' holds identically within the hypercube), while maintaining a sufficiently 'pseudorandom' distribution at other points in the field; substitute this $R$ into the $\alpha P + R$ masking in concept_2 to ensure that the summation and consistency checks are not broken, and at the same time weaken the recoverable information about $P$."", ""concept_4"": ""Encode proof components as 'random low-degree extensions (random LDE) consistent with boundary functions': Given a boundary $f:\{0,1\}^m \to F$, uniformly sample $\tilde{f}$ in the polynomial family $RM[F, m, d]$ such that $\tilde{f}|_{\{0,1\}^m} = f$, and provide a local simulator that outputs answers distributed identically to the evaluations of the 'true random $\tilde{f}$' at these points for any adaptive query sequence $(q_1,\dots,q_t)$; the PCP side uniformly embeds $\alpha P + R$ and several auxiliary tables (such as $T_i$) into this encoding, so that the local view seen by the malicious verifier (adaptive) is reproduced by the simulator, and the honest verifier can pre-fix queries (non-adaptive)."", ""concept_5"": ""Low Individual Degree Test (LIDT). As a verification subroutine, it detects low-degree polynomials with a small number of queries, which is naturally suitable for the query mode of non-adaptive honest verifiers: On the field $F$, perform random sampling and one-dimensionalization on candidate polynomials expressed as $F^m \to F$ (such as $\alpha P + R$, each $g_i$, $T_i$) along the coordinate axis direction (randomly fix all variables except one coordinate), check whether the degree of the obtained univariate polynomial is $\leq d$ and spot-check consistency at multiple points; LIDT provides a decision threshold for 'belonging to/being close to the Reed–Muller low-degree code' with a small number of queries (the typical query complexity is on the order of $O(md \cdot poly(1/\varepsilon) \cdot log(1/\delta))$), thereby implementing the verification side as a low-degree/compatibility check with fixed query positions.""}"	Yes. Equivalent expression: $#P \subseteq C$, $#P \subseteq C$, $#P \subseteq C$.	理论基础学科-理论计算机科学-计算复杂度	571484	2	6
recuU2y0f7KjIO	For d-regular Abelian Cayley graphs generated by Abelian groups (such graphs are typical examples with polynomial volume growth, whose doubling constant satisfies $\gamma_G \leq 2^{O(d)}$), what is the upper bound on the multiplicity of the second smallest eigenvalue $\lambda_2$ (the second smallest eigenvalue of the normalized Laplacian matrix, which is related to the graph conductance $\phi(G)$ via Cheeger's inequality $\frac{\lambda_2}{2} \leq \phi(G) \leq \sqrt{2 \lambda_2}$)? (Hint: A preliminary upper bound can be obtained using the volume growth bound, and then the upper bound can be tightened by analyzing the collision probability ratio through Stirling's approximation.)	{'step_1': 'Using theorem_1 (Cheeger\'s inequality), we know that for a graph G, there holds \\[\\frac{\\lambda_2}{2} \\leq \\phi(G) \\leq \\sqrt{2 \\lambda_2}\\], which connects the second smallest eigenvalue $\\lambda_2$ with the graph\'s conductance $\\phi(G)$. This provides a relational foundation between the spectrum and cuts for the subsequent analysis of the multiplicity of $\\lambda_2$, thereby allowing us to indirectly bound the properties of the eigenvalue through conductance.', 'step_2': 'According to concept_1, Abelian Cayley graphs have polynomial growth, that is, their doubling constant satisfies \\[\\gamma_G = \\max_{t \\geq 0} \\frac{|B(2t)|}{|B(t)|} \\leq 2^{O(d)}\\], where $B(t)$ is the ball of radius $t$. This describes the volume growth rate of the graph and provides a key parameter for applying volume growth bounds.', 'step_3': 'Applying theorem_2, through volume growth bounds, we obtain the upper bound for the multiplicity of the second smallest eigenvalue $\\lambda_2$ as \\[mul_{\\lambda_2} \\leq \\gamma^{O(\\log \\gamma)}\\]. Substituting $\\gamma_G \\leq 2^{O(d)}$ from concept_1, we calculate \\[mul_{\\lambda_2} \\leq (2^{O(d)})^{O(\\log 2^{O(d)})} = (2^{O(d)})^{O(d)} = 2^{O(d^2)}\\], which gives a preliminary upper bound but requires further improvement.', 'step_4': 'To improve the upper bound, we introduce theorem_3 (Stirling\'s approximation), which states that for all $t \\in \\mathbb{N} \\setminus \\{0\\}$, there holds \\[\\sqrt{2 \\pi t} (t/e)^t \\leq t! \\leq 2 \\sqrt{2 \\pi t} (t/e)^t\\]. This is used for the refined analysis of bounds on collision probability under polynomial distribution, such as by estimating polynomial coefficients or probability ratios, thereby proving that the collision probability ratio $\\gamma_{CP} \\leq 2^{O(d)}$. Combining the spectral-cut relationship from theorem_1 and the framework from theorem_2, the upper bound of the multiplicity is finally tightened to \\[2^{O(d)}\\].'}	"{""theorem_1"": ""Cheeger's Inequality | For a graph $G$, $\frac{\lambda_2}{2} \leq \phi(G) \leq \sqrt{2 \lambda_2}$ holds. This is a standard inequality in spectral graph theory, connecting the second smallest eigenvalue and the graph conductance."", ""concept_1"": ""Abelian Cayley graphs have polynomial growth, i.e., doubling constant $\gamma_G \leq 2^{O(d)}$ | The volume growth of Abelian Cayley graphs is bounded by the doubling constant $\gamma_G = \max_{t \geq 0} |B(2t)| / |B(t)|$, where $B(t)$ is the ball of radius $t$, cited from [LM08]."", ""theorem_2"": ""Eigenvalue multiplicity can be bounded by the volume growth bound: $mul_{\lambda_2} \leq \gamma^{O(\log \gamma)}$ | The multiplicity $mul_{\lambda_2}$ of the second smallest eigenvalue $\lambda_2$ satisfies $mul_{\lambda_2} \leq \gamma^{O(\log \gamma)}$, where $\gamma$ is the doubling constant, bounded by volume growth."", ""theorem_3"": ""Stirling's approximation: $\sqrt{2 \pi t} (t/e)^t \leq t! \leq 2 \sqrt{2 \pi t} (t/e)^t$""}"	2^{O(d)}	理论基础学科-理论计算机科学-算法理论	129437	4	4
recuVh1OQpudPk	Do respiratory viral infections (such as influenza virus and SARS-CoV-2) awaken dormant disseminated breast cancer cells (DCCs) in the lung, thereby promoting metastatic relapse?  Hints:  HINT_1: Sublethal IAV infection in MMTV-Her2 mice leads to body weight loss, with recovery at 11–12 days post-infection (dpi).  HINT_2: With increased bronchoalveolar lavage (BAL) cell counts, both wild-type and MMTV-Her2 mice display similar inflammatory responses.  HINT_3: The viral clearance kinetics are similar in wild-type and MMTV-Her2 mice. The RNA copy number of IAV peaks around 6 dpi, then decreases by 100–1000 fold between 9–15 dpi as viral load diminishes.  HINT_4: IAV infection causes an expansion of HER2+ DCCs in the lungs of MMTV-Her2 mice. These cells appear as diffuse non-epithelial-like structures, while the number of Ki67+HER2+ cells in the mammary gland does not significantly change.  HINT_5: In MMTV-PyMT mice, IAV infection increases the number of small tumor foci in the lung.  HINT_6: Compared to uninfected C57BL/6 mice (in which EO771 cells remain largely dormant), C57BL/6 mice implanted with EO771 cells and infected with IAV show increased lung metastatic burden at 17–18 dpi.  HINT_7: After IAV infection, the proportion of HER2+ cells expressing Ki67 in the lung significantly increases.  HINT_8: IAV infection causes persistent loss of mesenchymal cell markers and transient epithelial transition, eventually resulting in a stable mixed/hybrid phenotype that allows dormant cells to awaken.  HINT_9: IAV infection induces inflammation and activates IFNα, IFNγ, TNF, and IL-6–JAK–STAT3 signaling pathways.  HINT_10: IAV infection activates multiple signaling pathways in DCCs, including collagen-containing extracellular matrix (ECM) remodeling and angiogenesis. Expression of multiple collagen and collagen crosslinking genes is markedly upregulated, along with metalloproteinases and angiogenesis-related genes. Genes involved in mesenchymal or epithelial cell fate also show significant changes.  HINT_11: In DCCs, Cd274 expression increases and B2m expression decreases following IAV infection, helping evade immune clearance.  HINT_12: After IAV infection, high IL-6 levels but low IL-1β levels are detected in the BAL fluid (BALF) of both wild-type and MMTV-Her2 mice.  HINT_13: The IL-6 signaling pathway is strongly activated in DCCs following IAV infection.  HINT_14: Primary murine tracheal epithelial cells infected with IAV in vitro show significantly increased IL-6 secretion.  HINT_15: In IL-6 knockout (IL-6-KO) MMTV-Her2 mice infected with IAV, the number of HER2+ lung cells and Ki67+HER2+ cells is significantly reduced compared to IAV-infected MMTV-Her2 mice.  HINT_16: In MMTV-PyMT mice, IAV-induced proliferation of PyMT+ microlesions and formation of lung micrometastases are suppressed by IL-6 deficiency; a similar IL-6 dependency is observed in the EO771 model.  HINT_17: In the lungs of IL-6-KO MMTV-Her2 mice, most HER2+ cells retain vimentin expression and remain EpCAM-negative.  HINT_18: Treatment with IL-6 significantly enlarges mammary organoid volume, and similar enlargement is observed in the EO771 cell model.  HINT_19: IAV infection triggers the formation of inducible bronchus-associated lymphoid tissue (iBALT) in the lung, consisting mainly of CD4+ and B cells.  HINT_20: Few CD8+ cells are present in iBALT compared to CD4+ cells in both wild-type and MMTV-Her2 mice.  HINT_21: Compared with wild-type mice, MMTV-Her2 mice show selective collagen deposition in iBALT following IAV infection.  HINT_22: CD4+ cells help sustain the awakening of DCCs after IAV infection.  HINT_23: Depleting neutrophils with anti-Ly6G antibodies during IAV infection does not alter the number or Ki67 positivity of HER2+ lung cells.  HINT_24: Depleting CD8+ cells before IAV infection does not affect the presence of DCCs.  HINT_25: Depleting CD4+ cells after IAV infection increases lung CD8+ cell accumulation.  HINT_26: CD4+ cell depletion significantly reduces HER2+ lung cells after infection, while dual depletion of CD4+ and CD8+ cells partially restores HER2+ cell numbers.  HINT_27: IAV infection induces type I and type II interferon responses and activates other innate immune pathways across multiple immune cell types.  HINT_28: In CD4+ and CD8+ T cells of IAV-infected MMTV-Her2 mice, survival and memory phenotypes increase, but effector functions are weakened.  HINT_29: In HER2+ mice infected with IAV, expression of genes critical for T cell activation decreases. CD4+ cells isolated from MMTV-Her2 mice show significantly reduced expression of many mitochondrial-related genes, with reduced mitochondrial content.  HINT_30: Interferon responses decrease across multiple immune cell types in IAV-infected MMTV-Her2 mice.  HINT_31: The ratio of M2/M1 macrophages increases in IAV-infected MMTV-Her2 mice, accompanied by major gene expression changes.  HINT_32: Oxidative phosphorylation pathways in macrophages and T cells are significantly suppressed in IAV-infected MMTV-Her2 mice.  HINT_33: When CD4+ cells are depleted in IAV-infected MMTV-Her2 mice, the proliferation and activity of effector and memory CD8+ cells improve, with restoration of mitochondrial and respiratory phenotypes.  HINT_34: Lung CD8+ cells isolated from IAV-infected, CD4+-depleted MMTV-Her2 mice show increased IFNγ synthesis after anti-CD3/anti-CD28 stimulation in vitro, and exhibit stronger cytotoxicity against HER2+ tumor cells.  HINT_35: Infection with the mouse-adapted SARS-CoV-2 strain MA10 induces high levels of IL-6 and IFNα in the lung, along with low levels of IFNβ, IFNγ, and IL-1β in BALF.  HINT_36: MA10 infection of MMTV-Her2 mice gradually increases HER2+ and Ki67+HER2+ cells. EpCAM positivity rises transiently, while vimentin positivity transiently decreases — similar to effects observed with IAV infection.  HINT_37: UK Biobank data show that cancer survivors infected with SARS-CoV-2 have significantly increased risks of all-cause mortality, non-COVID mortality, and cancer-related mortality.  HINT_38: Flatiron Health data show that breast cancer patients with COVID-19 have a significantly increased risk of lung metastasis (HR = 1.44).	  1. 使用亚致死量 IAV 感染后，小鼠体重短暂下降，支气管肺泡灌洗液（BALF）中炎症细 胞增多，病毒载量 6 天达峰后迅速清除。由于病毒激活肺部固有免疫，引发局部炎症， 但免疫清除与病毒复制动力学无关，说明炎症本身是关键信号。   2. IAV 诱导炎症因子 IL-6、IFNγ、TNF 等以及 IL-6-JAK-STAT3 通路激活，同时 DCCs 中胶原蛋白、血管生成和 EMT 相关基因上调。说明病毒感染后，激活炎症因子（尤其 是 IL-6）通过 STAT3 信号直接作用于 DCCs，驱动其从休眠的间充质状态（vimentin+） 向混合表型（EpCAM+ /vimentin+）转化，促进增殖和转移。   3. IAV 感染后，肺上皮细胞分泌 IL-6，IL-6-KO 小鼠中 DCCs 扩增被完全阻断；体外实验 发现，IL-6 直接刺激乳腺类器官或 EO771 细胞增大，证明其可直接激活休眠细胞增殖。 由上述实验结果可得出结论，IL-6 是病毒炎症中驱动 DCCs 唤醒的“开关”，不依赖病 毒的持续存在。   4. IAV 诱导 iBALT 形成（CD4+ /B 细胞聚集），但 CD8+ T 细胞较少。清除 CD4+ T 细胞后， CD8+ T 细胞浸润增加，DCCs 数量减少；双重清除 CD4+ /CD8+ T 细胞后，DCCs 部分 恢复，说明 CD4+ T 细胞通过抑制 CD8+ T 细胞毒性以维持转移灶。推测原因，可能是 CD4+ T 细胞通过分泌免疫抑制因子（如 TGF-β）或竞争性耗竭细胞因子，阻碍 CD8+ T 细胞杀伤功能。   5. 使用鼠源化的 SARS-CoV-2 病毒（MA10）感染，同样有依赖 IL-6 扩增的 DCCs。结合 UK Biobank 和 Flatiron Health 数据库中乳腺癌患者在 SARS-CoV-2 或 COVID-19 感染 后死亡及肺转移风险的数据，可得出结论：DCCs 唤醒与病毒类型无关，呼吸道病毒通 过 IL-6-CD4+ T 细胞轴，系统性促进癌细胞转移，临床数据支持该文献中所用小鼠模型 的普适性。   ---  Translation:  1. After sublethal IAV infection, mice exhibited a transient loss of body weight, with increased inflammatory cells in bronchoalveolar lavage fluid (BALF). The viral load peaked at day 6 and was rapidly cleared thereafter. Since the virus activated innate immunity in the lung and triggered local inflammation, but immune clearance was independent of viral replication dynamics, this indicates that inflammation itself is the key signal.  2. IAV induced inflammatory factors such as IL-6, IFNγ, and TNF, as well as activation of the IL-6-JAK-STAT3 pathway, while collagen, angiogenesis, and EMT-related genes were upregulated in DCCs. This suggests that following viral infection, the induction of inflammatory factors (especially IL-6) acts directly on DCCs through STAT3 signaling, driving their transition from a dormant mesenchymal state (vimentin⁺) to a hybrid phenotype (EpCAM⁺/vimentin⁺), thereby promoting proliferation and metastasis.  3. Following IAV infection, lung epithelial cells secreted IL-6. In IL-6-KO mice, expansion of DCCs was completely blocked; in vitro experiments showed that IL-6 directly stimulated mammary organoids or EO771 cells to enlarge, confirming its ability to directly activate dormant cell proliferation. From these experimental results, it can be concluded that IL-6 is the “switch” that drives the awakening of DCCs during viral inflammation, independent of the continued presence of the virus.  4. IAV induced iBALT formation (CD4⁺/B cell aggregation), but relatively few CD8⁺ T cells were observed. Upon CD4⁺ T-cell depletion, CD8⁺ T-cell infiltration increased and the number of DCCs decreased; however, dual depletion of CD4⁺/CD8⁺ T cells partially restored DCC numbers. This indicates that CD4⁺ T cells maintain metastatic foci by suppressing CD8⁺ T-cell cytotoxicity. The underlying mechanism may involve CD4⁺ T cells secreting immunosuppressive factors (such as TGF-β) or competitively depleting cytokines, thereby impairing CD8⁺ T-cell killing function.  5. Infection with a mouse-adapted SARS-CoV-2 virus (MA10) similarly led to IL-6-dependent expansion of DCCs. Combined with UK Biobank and Flatiron Health datasets showing increased mortality and lung metastasis risk in breast cancer patients following SARS-CoV-2 or COVID-19 infection, it can be concluded that DCC awakening is independent of virus type. Respiratory viruses systemically promote cancer cell metastasis through the IL-6-CD4⁺ T-cell axis, and clinical data support the universality of the mouse model used in this study.	  Concept_1：DCCs：Disseminated Cancer Cells，播散性癌细胞，是指从原发性肿瘤脱离，并 通过血液或淋巴系统播散到身体其他远端器官（如肺、骨、肝）的癌细胞。其最关键的特征 是休眠（Dormancy）状态，这些细胞在到达远端器官后，并不会立即增殖形成可检测的转移 瘤；相反，它们可以进入一种持续数年、甚至数十年的“休眠”状态，保持代谢活性但不进 行细胞分裂。 Concept_2：上皮-间质转化（EMT）与间质-上皮转化（MET）：EMT 是癌细胞获得迁移和侵 袭能力的关键过程，其特征是上皮标志物（如 E-cadherin，EpCAM）下调、间质标志物（如 Vimentin 波形蛋白）上调；而 MET 则相反，被认为与休眠细胞被“唤醒”和形成转移灶有 关。 Concept_3：IAV，甲型流感病毒（Influenza A virus）为常见流感病毒，甲型流感病毒极易发 生变异。本研究中特指所用的实验毒株 A/PR/8/34（H1N1），简称 PR8；PR8 是经典的鼠适 应性甲型流感病毒，广泛用于小鼠呼吸道感染模型。本研究通过鼻内滴注（50μl PBS 含 500EIU 病毒），模拟人类呼吸道自然感染途径。 Concept_4：BALF 是 Bronchoalveolar Lavage Fluid 的缩写，即支气管肺泡灌洗液。它通过支 气管肺泡灌洗术（BAL）采集，将无菌生理盐水经支气管镜注入肺泡，随后负压回收，所得 液体即为 BALF，可反映肺部真实微环境。 Concept_5：MMTV-Her2（FVB/C57BL/6 背景）：MMTV-Her2 模型是 MMTV-LTR 驱动鼠 Erbb2（Neu，HER2 同源基因）的高表达，形成 HER2+乳腺癌。FVB 背景肿瘤发生早、生长 快；C57BL/6 背景肿瘤潜伏期略长，但免疫微环境更完整，适合免疫治疗研究。 Concept_6：MMTV-PyMT：MMTV-LTR 驱动多瘤病毒中间 T 抗原表达，激活 c-Src/PI3K/Akt 等信号通路。 Concept_7：iBALT（inducible bronchus-associated lymphoid tissue），即诱导性支气管相关淋 巴组织，是一种由炎症或感染诱导的三级淋巴结构，并非胚胎期就存在，而是在出生后肺受 到抗原（如病毒、细菌、过敏原）刺激时才在支气管周围或血管周围快速形成。 Concept_8：CD274，即 PD-L1，编码 I 型跨膜糖蛋白，含 IgV 样和 IgC 样胞外域；与 PD-1 受体结合后，抑制 T 细胞增殖、细胞毒性以及细胞因子释放，是肿瘤免疫逃逸的关键检查 点。 Concept_9：B2m，编码 12kDa 的可溶性蛋白，β-折叠片结构。若无 B2M，则 MHC-I 无法 折叠并转运至细胞膜表面，导致 CD8+ T 细胞无法识别靶细胞；肿瘤下调 B2m 或缺失突变可 逃避免疫监视。 Concept_10：细胞因子风暴与 IL-6/JAK/STAT3 通路：病毒感染会引发强烈的炎症反应，释 放大量细胞因子，如 IL-6。IL-6 通过激活 JAK/STAT3 信号通路，在促进细胞增殖、存活和 免疫调节中发挥核心作用，该通路也与癌症相关。 Concept_11：IL-6-CD4⁺ T 细胞轴，指 IL-6 与其受体复合物通过 JAK-STAT3 信号通路，直 接调控初始 CD4⁺ T 细胞的命运决定、增殖、效应功能及记忆形成的一套完整分子-细胞网 络。该轴在感染、肿瘤、自身免疫中均发挥重要作用。 IL-6最初激活DCC从间充质状态转化为混合表型，随后CD4+ T细胞主要发挥作用，通过抑制CD8+ T细胞等维持DCC活性状态，从而实现肿瘤转移。Concept_12：GSEA：一种生物信息学方法，用于判断一组先定义的基因集合（如某个信号 通路的所有基因）在差异表达基因列表中是否集中出现。GSEA 不提前设定差异阈值，可发 现表达量变化微小但方向一致的通路。   ----  Translation:  Concept_1: DCCs: Disseminated Cancer Cells, are cancer cells that detach from the primary tumor and disseminate to other distant organs of the body (such as lung, bone, liver) through the blood or lymphatic system. Their most critical feature is the dormant (Dormancy) state. After reaching distant organs, these cells do not immediately proliferate to form detectable metastases; instead, they can enter a “dormant” state lasting for years or even decades, remaining metabolically active but not undergoing cell division.  Concept_2: Epithelial-Mesenchymal Transition (EMT) and Mesenchymal-Epithelial Transition (MET): EMT is a key process by which cancer cells acquire migratory and invasive abilities, characterized by the downregulation of epithelial markers (such as E-cadherin, EpCAM) and upregulation of mesenchymal markers (such as Vimentin). MET, on the other hand, is the opposite process and is thought to be related to the “awakening” of dormant cells and the formation of metastatic lesions.  Concept_3: IAV, Influenza A virus, is a common influenza virus that is highly prone to mutation. In this study, the experimental strain A/PR/8/34 (H1N1), abbreviated as PR8, was specifically used; PR8 is a classical mouse-adapted influenza A virus, widely used in mouse respiratory infection models. In this study, intranasal instillation (50 μl PBS containing 500 EIU virus) was performed to mimic the natural infection route of the human respiratory tract.  Concept_4: BALF is the abbreviation for Bronchoalveolar Lavage Fluid. It is collected through bronchoalveolar lavage (BAL), in which sterile saline is introduced into the alveoli via a bronchoscope and then recovered by negative pressure. The resulting fluid is BALF, which reflects the true microenvironment of the lung.  Concept_5: MMTV-Her2 (FVB/C57BL/6 background): The MMTV-Her2 model is driven by MMTV-LTR to overexpress murine Erbb2 (Neu, the homolog of human HER2), forming HER2+ breast cancer. On the FVB background, tumor onset occurs early and growth is rapid; on the C57BL/6 background, tumor latency is slightly longer, but the immune microenvironment is more intact, making it suitable for immunotherapy studies.  Concept_6: MMTV-PyMT: MMTV-LTR drives the expression of the polyomavirus middle T antigen, which activates signaling pathways such as c-Src/PI3K/Akt.  Concept_7: iBALT (inducible bronchus-associated lymphoid tissue) is a tertiary lymphoid structure induced by inflammation or infection. It does not exist during embryonic development but forms rapidly around the bronchi or blood vessels after birth when the lung is exposed to antigens (such as viruses, bacteria, allergens).  Concept_8: CD274, also known as PD-L1, encodes a type I transmembrane glycoprotein containing IgV-like and IgC-like extracellular domains. Upon binding to the PD-1 receptor, it inhibits T cell proliferation, cytotoxicity, and cytokine release, and is a key immune checkpoint for tumor immune evasion.  Concept_9: B2m encodes a 12 kDa soluble protein with a β-sheet structure. Without B2M, MHC-I cannot fold or be transported to the cell membrane surface, preventing CD8+ T cells from recognizing target cells. Tumors can evade immune surveillance by downregulating B2m or through loss-of-function mutations.  Concept_10: Cytokine storm and the IL-6/JAK/STAT3 pathway: Viral infection can trigger a strong inflammatory response, releasing large amounts of cytokines such as IL-6. IL-6 plays a central role in promoting cell proliferation, survival, and immune regulation by activating the JAK/STAT3 signaling pathway, which is also associated with cancer.  Concept_11: The IL-6–CD4⁺ T cell axis refers to a complete molecular-cellular network in which IL-6 and its receptor complex regulate the fate decision, proliferation, effector function, and memory formation of naïve CD4⁺ T cells via the JAK-STAT3 signaling pathway. This axis plays important roles in infection, cancer, and autoimmunity. IL-6 initially activates DCCs to transform from a mesenchymal state into a hybrid phenotype, after which CD4⁺ T cells play a major role in maintaining the active state of DCCs by suppressing CD8⁺ T cells, thereby enabling tumor metastasis.  Concept_12: GSEA: A bioinformatics method used to determine whether a predefined set of genes (such as all genes in a given signaling pathway) is significantly enriched in a ranked list of differentially expressed genes. GSEA does not require predefining a differential threshold and can detect pathways where expression changes are subtle but consistent in direction.	Checklist:  Respiratory viral infections (such as influenza and COVID-19) can:  ① awaken dormant breast cancer cells in the lung through an IL-6–dependent mechanism (5 points)  ② maintain the survival of dormant breast cancer cells via CD4⁺ T-cell–mediated immunosuppression, ultimately promoting breast cancer metastasis and recurrence. (5 points)	理论基础学科-生物学-细胞生物学	342858	3	11
recuUDt2UA2NuB	In order to improve the antitumor activity of T cells in vivo, researchers conducted a series of experiments and made the following observations:  hints:  HINT1: RNAseq found that the following metabolic pathways were enriched in tumor-infiltrating T cells (TIL): beta-oxidation very long chain fats; Glycerolipid Metabolism; Oxidation of Branched Chain FAs; De novo Triacylglycerol Biosynthesis; Fatty acid metabolism; Mitochondrial beta-oxidation of short/medium/long chain FAs.  HINT2: Through UPLC-MS detection, it was found that the conclusion was consistent with RNAseq.  HINT3: In TIL, lipids such as TAG, DAG, PC, PG, PS were all enriched.  HINT4: Through immunofluorescence experiments in MCA-205, MC-38, and B16 cell lines, compared with the control group, the Neutral Lipid fluorescence signal was stronger in all three cell lines than in the control group.  HINT5: In vitro experiments showed that T cells in the tumor microenvironment infiltration microenvironment were consistent with the phenomenon observed in HINT4.  HINT6: After MCA-205 fibrosarcoma implantation in mice for 1 week and 2 weeks (n = 5/5), dimensionality reduction and UMAP projection and quantification of neutral lipid content were assessed by high-dimensional spectral flow cytometry, and it was found that T cell lipid accumulation in mice bearing tumors for two weeks was higher than in mice bearing tumors for one week.  HINT7: High-dimensional spectral flow cytometry clustering analysis of phenotypic markers of MCA-205 sarcoma showed abnormal PD-1/Tim-3 expression.  HINT8: When ND-646 was used in vitro in OT-1 cells, the level of Malonyl-CoA decreased, the number of surviving OT-1 cells showed no significant difference from the non-added group, TAG, DAG, PC, PG, PS, PI, PE levels decreased, but SM, LPC levels increased significantly.  HINT9: After in vitro expansion of OT-1 cell line for 7 days, metabolomic analysis was performed on two groups of cells with or without ND-646. A total of 213 metabolites were analyzed, and 69 metabolites showed differences. After the addition of ND-646, ATP content increased, Acetyl-CoA, NAD, NADH, FAD contents all increased.  HINT10: Bodipy-C16 staining showed that in the group without ND-646 addition, mitochondrial and fluorescence co-localization was weak, but in the group with ND-646 addition, the co-localization phenomenon was obvious.  HINT11: In order to determine which metabolic pathway was affected, researchers co-cultured T cells with tumor cells to simulate the tumor microenvironment, and observed that compared with normal T cells, ATP produced by glycolysis decreased, total ATP production decreased, mitochondrial ATP production decreased. After adding [Substance A], total ATP content increased, mitochondrial ATP production increased, ATP produced by glycolysis increased. After adding [Substance B], total ATP content decreased again, mitochondrial ATP production decreased.  HINT12: BCL-2 was enriched after the addition of ND-646.  HINT13: Mice were implanted with the B16F1-OVA cell line, with the following three groups of experiments:  Group a: only PBS was added  Group b: OT-1 CD8+ cell line was implanted  Group c: ND-646 and OT-1 CD8+ were added  It was observed that tumor size decreased in groups b and c, with group c being more significant, and the survival rate was also obvious.  checklist:  The above experimental phenomena indicate: (10 points)  Speculate on the properties of substance A and substance B (4 points)  Speculate on which step of metabolism of T cells in the tumor microenvironment is affected (2 points)  The experiments indicate which gene(s) should be (activated/inhibited) in order to achieve enhanced antitumor efficacy of T cells (4 points)	  1. 在体内的肿瘤微环境中，因为肿瘤的代谢是高需糖的，所以在肿瘤微环境中的免疫细胞处于一个低葡萄糖含量的环境中。而能量缺乏的状态会抑制细胞的活性。因此自然想到如果可以提高T细胞的代谢活性，可以提高T细胞的抗肿瘤功能  2. “体外实验表明，在肿瘤微环境浸润的微环境会导致T细胞中脂质的异常积累”的原因主要是肿瘤微环境中的低氧和葡萄糖缺乏  3. TIL伴随肿瘤的进程，其细胞内的脂质积累逐渐升高，并且在第七条观察中，PD-1/Tim-3是肿瘤进程的标志物，于是到此可以推得T细胞中脂质的积累与肿瘤的进程相关，而抑制其脂质的积累对于抑制肿瘤的进程具有积极作用  4. 实验现象“对OT-1细胞体外使用ND-646，发现Malonyl-CoA水平下降，OT-1存活细胞数量与非添加组数目无明显差异，TAG、DAG、PC、PG、PS、PI、PE水平下降，但是SM、LPC水平却显著上升”证明了ND-646是一种ACC1/2的有效抑制剂，同时对于细胞的活性没有明显的影响，正是这个特性保证了下面的实验结果有意义。  5. 实验现象“在OT-1细胞系体外扩增7天后，对是否添加ND-646的两组细胞进行代谢组学分析，共分析了213个代谢物，观察到有差异的有69种代谢物，添加ND-646之后ATP含量升高，Acetyl-CoA，NAD，NADH， FAD含量都有所升高”中，使用ACC1/2的共同抑制剂ND-646，发现脂质的积累下降，说明ACC1/2在脂质积累的进程中起到关键作用  6. etomoxir的添加可以将ACC的作用局限于脂肪酸的beta氧化。说明ACC抑制剂的添加可以通过脂肪酸的beta 氧化来降低脂质的积累  7. 实验“为了锁定受到影响的代谢通路是哪一条，研究人员将T细胞与肿瘤细胞共培养，意图模拟肿瘤微环境，观察到与正常T细胞相比，糖酵解产生ATP的含量下降，ATP产生的总量下降，线粒体ATP产量下降，添加【物质A】之后，总ATP含量上升，线粒体ATP产量上升，糖酵解产生ATP量上升，添加【物质B】，发现ATP总量重新下降，线粒体ATP产量下降”，物质A和物质B分别是ND-646和etomoxir，首先解释一下逻辑：我们观察到补充ACC的抑制剂，体外培养的肿瘤微环境T细胞能量代谢情况改善，而这种改善可以被etomoxir逆转，进一步说明了ACC抑制剂的添加可以通过脂肪酸的beta 氧化来降低脂质的积累，提高能量代谢效率  8. 实验“将小鼠植入B16F1-OVA细胞系，有如下三组实验：  第一组只添加PBS  第二组植入OT-1 CD8+细胞系  第三组添加ND-646和OT-1 CD8+  观察到bc组Tumor size下降，其中c组更为明显，存活率同样”做了动物实验，验证了抑制ACC的活性确实可以直接降低肿瘤的大小，并且提高动物的存活时间。  综上，我们得出：   在肿瘤微环境中，T细胞有脂质的富集情况，同时表现出了产能的低下  ACC1/2与脂质的异常积累相关  抑制ACC1/2可以提高细胞的代谢产能  ACC1/2所引起的产能变化主要通过脂肪酸的beta氧化发生作用  外源补充ACC1/2抑制剂可以降低荷瘤小鼠的肿瘤大小，同时延长荷瘤小鼠的存活时间  回到本问题，第一题我们需要推测A和B的性质，经过上文的推导我们知道我们的目标基因是ACC基因。所以我们要首先营造该基因产物的缺失模型，同时，由第二步实验现象得知，添加物质B之后实验现象被逆转，说明此物质可以部分恢复由ACC缺失造成的脂质积累。而ACC促进脂肪酸合成，抑制ACC脂肪酸合成减少，由此前体物质Acetyl-CoA，NAD，NADH， FAD含量都有所升高，而这些物质可以促进beta氧化，为细胞供能，而逆转这一过程需要的是脂肪酸beta氧化的抑制剂，也就是物质B所需要的性质。  第二小问同理，抑制ACC之后供能情况改善，说明主要发生异常的是脂肪酸的beta氧化  第三小问综合离体实验和在体实验可知，添加ACC的抑制剂可以改善肿瘤细胞的供能情况，从而改善T细胞抗肿瘤活性  ---  Translation:  1. In the in vivo tumor microenvironment, because tumor metabolism is highly glucose-demanding, immune cells in the tumor microenvironment are in a low-glucose environment. The state of energy deficiency suppresses cellular activity. Therefore, it is natural to consider that if T cell metabolic activity can be enhanced, T cell antitumor function can be improved.  2. “The reason why in vitro experiments show that the tumor microenvironment infiltration leads to abnormal lipid accumulation in T cells” is mainly due to hypoxia and glucose deficiency in the tumor microenvironment.  3. As TILs accompany the progression of tumors, intracellular lipid accumulation gradually increases, and in observation 7, PD-1/Tim-3 are markers of tumor progression. Thus, it can be inferred that lipid accumulation in T cells is related to tumor progression, and inhibiting lipid accumulation plays a positive role in suppressing tumor progression.  4. Experimental phenomenon 8 proved that ND-646 is an effective inhibitor of ACC1/2, and at the same time, it does not significantly affect cell activity. This characteristic ensures the meaningfulness of the following experimental results.  5. In experimental phenomenon 9, using the common ACC1/2 inhibitor ND-646, lipid accumulation was found to decrease, indicating that ACC1/2 plays a key role in the process of lipid accumulation.  6. The addition of etomoxir can confine the function of ACC to fatty acid beta-oxidation. This indicates that the addition of ACC inhibitors can reduce lipid accumulation through fatty acid beta-oxidation.  7. In experiment 11, substances A and B are ND-646 and etomoxir, respectively. First, to explain the logic: we observed that supplementation with an ACC inhibitor improved the energy metabolism of T cells cultured in the tumor microenvironment in vitro, and this improvement could be reversed by etomoxir. This further indicates that the addition of an ACC inhibitor can reduce lipid accumulation through fatty acid beta-oxidation and improve energy metabolism efficiency.  8. Experiment 13 carried out animal experiments, verifying that inhibition of ACC activity can indeed directly reduce tumor size and increase animal survival time.  In summary, we conclude:  In the tumor microenvironment, T cells exhibit lipid accumulation as well as reduced energy production.  ACC1/2 are related to abnormal lipid accumulation.  Inhibition of ACC1/2 can improve cellular energy metabolism.  The energy production changes caused by ACC1/2 mainly occur through fatty acid beta-oxidation.  Exogenous supplementation of ACC1/2 inhibitors can reduce tumor size in tumor-bearing mice and prolong their survival time.  Returning to this problem:  For the first question, we need to infer the properties of A and B. From the above derivation, we know that our target gene is the ACC gene. Therefore, we must first create a knockout model of this gene product. At the same time, from the second experimental phenomenon, we know that after adding substance B, the experimental phenomenon was reversed, indicating that this substance can partially restore the lipid accumulation caused by ACC deficiency. Since ACC promotes fatty acid synthesis, inhibition of ACC reduces fatty acid synthesis, thereby increasing precursor substances Acetyl-CoA, NAD, NADH, and FAD, which can promote beta-oxidation and provide energy for cells. To reverse this process requires an inhibitor of fatty acid beta-oxidation, which is the property needed for substance B.  For the second sub-question, similarly, after inhibiting ACC, the energy supply improved, indicating that the main abnormality occurred in fatty acid beta-oxidation.  For the third sub-question, combining the ex vivo and in vivo experiments, it can be seen that adding an ACC inhibitor can improve the energy supply of tumor-infiltrating cells, thereby improving the antitumor activity of T cells.	  Concept_1: B16-OVA-Tumor: 在B16细胞系中外源过表达鸡的卵清蛋白，鸡的卵清蛋白具有高免疫原性，可以更好地被免疫细胞识别，是免疫学的重要研究工具  Concept_2: OT-I T cells: CD8+ T细胞系，过表达一个识别OVA的肽段，可以配合OVA过表达细胞系使用  Concept_3: ACC：乙酰辅酶A羧化酶（ACC）是关键酶，它催化胞质乙酰辅酶A不可逆地羧化形成丙二酰辅酶A。该反应是从头脂肪酸合成的第一步和限速步骤 。哺乳动物表达两种不同的ACC同工酶：ACC1（也称为ACC-α）和ACC2（也称为ACC-β），它们具有约80%的氨基酸序列同源性，但在细胞定位和特定功能上有所不同 。ACC1： 主要定位于细胞质，并在脂肪生成组织（如肝脏、脂肪组织和哺乳期乳腺）中高表达。它是负责为脂肪酸合成提供丙二酰辅酶A的主要酶 。ACC2： 定位于线粒体外膜，尤其在心脏和肌肉中含量丰富。虽然它也产生丙二酰辅酶A，但其主要作用是通过抑制肉碱棕榈酰转移酶1（CPT1）来调节脂肪酸氧化，CPT1是控制脂肪酰辅酶A进入线粒体进行β-氧化的酶 。两种同工酶都受到饮食、激素（例如，胰岛素上调ACC1，胰高血糖素下调）和细胞能量状态的高度调节，特别是通过AMP激活的蛋白激酶（AMPK），当细胞能量（ATP）低时，AMPK会磷酸化并使ACC1和ACC2失活 。ACC1（合成）和ACC2（氧化调节）独特的细胞定位和主要功能揭示了一种复杂的代谢控制机制。  Concept_4: OCR：OCR（Oxygen Consumption Rate）：OCR是指细胞对氧气的消耗速率。Seahorse能量代谢分析仪通过测量细胞周围培养基中的溶解氧含量的变化来确定OCR。在测量过程中，细胞被置于特定的耗氧试剂中，如噻唑类化合物（例如，琥珀酸、马拉酸和氰化钠）。这些试剂会与细胞内的线粒体呼吸链相互作用，导致氧气消耗速率的变化。Seahorse能量代谢分析仪通过测量耗氧试剂与细胞周围培养基中溶解氧含量的变化，计算细胞的OCR。  Concept_5: 先前的实验证明，在体外环境下，如果能提高T细胞脂肪酸氧化代谢的速率，对于维持T细胞抗肿瘤这种免疫作用是有促进作用的，但是体内是否有作用尚不清楚  Concept_6: ND-646是一种ACC1和ACC2的抑制剂，可以结合ACC的biotin carboxylation domain，使得ACC无法二聚化  Concept_7: Bodipy-C16是检测16C的棕榈酸酯染色剂  Concept_8: etomoxir是一个CPT-1的不可逆抑制剂，阻止了乙酰CoA的转运，从而抑制脂肪酸的代谢途径  ---  Translation:  Concept_1: B16-OVA-Tumor: In the B16 cell line, exogenous overexpression of chicken ovalbumin, which has high immunogenicity, can be better recognized by immune cells and is an important research tool in immunology.  Concept_2: OT-I T cells: CD8+ T cell line, overexpressing a peptide that recognizes OVA, can be used together with OVA-overexpressing cell lines.  Concept_3: ACC: Acetyl-CoA carboxylase (ACC) is a key enzyme that catalyzes the irreversible carboxylation of cytosolic acetyl-CoA to form malonyl-CoA. This reaction is the first and rate-limiting step of de novo fatty acid synthesis. Mammals express two different ACC isoenzymes: ACC1 (also called ACC-α) and ACC2 (also called ACC-β), which share about 80% amino acid sequence homology but differ in cellular localization and specific functions. ACC1: Mainly localized in the cytoplasm and highly expressed in lipogenic tissues (such as liver, adipose tissue, and lactating mammary glands). It is the main enzyme responsible for providing malonyl-CoA for fatty acid synthesis. ACC2: Localized on the outer mitochondrial membrane and is abundant especially in the heart and muscle. Although it also produces malonyl-CoA, its main role is to regulate fatty acid oxidation by inhibiting carnitine palmitoyltransferase 1 (CPT1), the enzyme that controls the entry of acyl-CoA into mitochondria for β-oxidation. Both isoenzymes are highly regulated by diet, hormones (e.g., insulin upregulates ACC1, glucagon downregulates), and cellular energy status, particularly through AMP-activated protein kinase (AMPK). When cellular energy (ATP) is low, AMPK phosphorylates and inactivates ACC1 and ACC2. The distinct cellular localization and main functions of ACC1 (synthesis) and ACC2 (oxidation regulation) reveal a complex mechanism of metabolic control.  Concept_4: OCR: OCR (Oxygen Consumption Rate): OCR refers to the rate of oxygen consumption by cells. The Seahorse metabolic analyzer determines OCR by measuring changes in dissolved oxygen content in the culture medium around cells. During measurement, cells are placed in specific oxygen-consuming reagents, such as thiazole compounds (e.g., succinate, malonate, and sodium cyanide). These reagents interact with the mitochondrial respiratory chain inside the cells, leading to changes in the rate of oxygen consumption. The Seahorse metabolic analyzer calculates cellular OCR by measuring the change in dissolved oxygen content in the medium surrounding the cells in the presence of oxygen-consuming reagents.  Concept_5: Previous experiments have demonstrated that in vitro, increasing the rate of fatty acid oxidation metabolism in T cells promotes the maintenance of the antitumor immune function of T cells, but whether it has an effect in vivo is still unclear.  Concept_6: ND-646 is an inhibitor of ACC1 and ACC2, which binds to the biotin carboxylation domain of ACC, preventing ACC dimerization.  Concept_7: Bodipy-C16 is a staining reagent for detecting 16C palmitate esters.  Concept_8: Etomoxir is an irreversible inhibitor of CPT-1, which blocks the transport of acetyl-CoA, thereby inhibiting the fatty acid metabolic pathway.	Checklist:  1. Substance A can inhibit ACC1/2 and suppress lipid accumulation; in the experiment, the substance used was ND-646. Substance B can inhibit fatty acid β-oxidation; in the experiment, the substance used was etomoxir.(4 points)  2. ACC catalyzes the conversion of acetyl-CoA to malonyl-CoA.(2 points)  3. Inhibit the activity of ACC1/2 in T cells.(4 points)	理论基础学科-生物学-免疫学	58214	2	4
recuTON94osGxY	Determine if the isolated weighted homogeneous Gorenstein 3-fold singularity defined by an affine hypersurface equation x^2+y^3+yz^3+zw^{14} have a crepant resolution. Is it a terminal singularity?	{'step_1': 'By short verification one finds that the singularity is terminal.', 'step_2': 'By direct computations in Hochschild cohomology of the mirror one finds that symplectic cohomology of the Milnor fiber indeed has constant ranks for all negative degrees. By the conjecture of Evans and Lekili, one concludes that the singularity really admits crepant resolution.', 'step_3': 'In fact one may notice that the singularity is related to the canonical cE_7 singularity x^2+y^3+yz^3+w^{18} by weight-one deformations which do not change weight properties of the singularity. The latter however admits crepant resolution by an old result of Brieskorn in 1960s.'}	{'concept_1': 'Conjecture 1 [Evans-Lekili]: An isolated compound Du Val singularity has crepant resolution if the symplectic cohomology groups of its Milnor fiber have constant ranks at all negative degree. The conjecture is expected to be true by various calculations. Symplectic cohomology is a symplectic geometric invariant for exact symplectic manifolds with boundary, which can be defined on the Milnor fiber of a singularity.', 'concept_2': 'Definition 2 [Berglund-Hubsch-Krawitz]: The Berglund-Hubsch mirror for a weighted homogeneous polynomial W is defined as follows. Suppose W^\\check contains n+1 monomials in n+1 variables. Collect the exponents of monomials in W^\\check and package them into an integral (n+1)\\times (n+1) matrix. Suppose the determinant of the matrix is nonzero. Then you take the tranpose of the matrix, call it A, and write down the corresponding polynomial W, such that the exponent matrix of W is precisely this new matrix A. Define a group $Γ_W :=\\{ (t_0,t_1,...,t_{n+1}) \\in (\\mathbb C^\\ast)^{n+2}: \\sum_{j=1}^{n+1}t_j^{A_{ij}} =t_0t_1 ···t_{n+1}, for i = 1,...,n+1\\}$. This is a finite extension of $\\mathbb C^\\ast$. The Milnor fiber of W^\\check=0 is mirror to the Landau-Ginzburg orbifold (W, Γ_W) in an appropriate sense.', 'concept_3': 'Theorem 3 [Evans-Lekili]: Given a weighted homogeneous isolated singularity X by equation $W^\\check=0$ which is an affine hypersurface in $\\mathbb C^{n+1}$, denote its Milnor fiber by V. One can compute $(W, Γ_W)$ by definition 2. Assuming that $HH^2(mf(A_{n+2},Γ_W,w))=0$, one has \\[SH^∗(V ) \\simeq HH^∗(mf(A_{n+2},Γ_W,W))\\] Here HH denotes Hochschild cohomology and SH denotes symplectic cohomology.', 'concept_4': 'Theorem 4 [Dyckerhoff, see also Lekili-Ueda, and Evans-Lekili]: There is a formula for computing $HH^∗(mf(A_{n+2},Γ_W,W))$ see https://arxiv.org/abs/2004.07374, equation 5.2.', 'concept_5': 'Definition 5 [Reid]: Compound Du Val singularity is a class of 3-fold singularities such that a generic hyperplane (passing through the singular locus) intersects it on an canonical (ADE) surface singularity.', 'concept_6': 'Theorem 6 [Reid]: For three fold singularity, being isolated compound Du Val is equivalent to being terminal.'}	Yes, it has crepant resolution. Yes it is terminal.	理论基础学科-基础数学-几何	451468	3	5
recuVstW9ByyDQ	In solar seawater distillation, among the four factors — solar irradiance (q{\mathrm{in}}, W·m^{-2}), relative humidity (RH, %)，wind speed (V_{\mathrm{wind}}, m·s^{-1}), and ambient temperature (T_{\mathrm{amb}}, °C) — which two have the most significant influence on the system’s energy efficiency?	{'step_1': 'The primary goal is to determine which two environmental factors — solar irradiance (q_{\\mathrm{in}}), ambient temperature (T_{\\mathrm{amb}}), wind speed (V_{\\mathrm{wind}}), and relative humidity (RH) — have the most significant effect on the energy efficiency \\eta of solar-driven seawater distillation systems. The analysis begins with the coupled heat–mass transfer mechanisms described in concept_1.', 'step_2': 'According to concept_1, solar seawater distillation operates as a closed or semi-enclosed system. Energy efficiency \\eta depends on multiple interacting processes, including solar energy absorption, interfacial phase change, vapor diffusion, and heat dissipation. Because heat and mass transfers are strongly coupled, changes in one environmental factor may indirectly influence others, requiring a systematic evaluation.', 'step_3': 'We first analyze the effect of solar irradiance (q_{\\mathrm{in}}) (concept_5). Solar irradiance provides the primary energy input for driving interfacial evaporation. Higher q_{\\mathrm{in}} increases the available thermal energy, raising evaporation rates. However, its effective contribution depends on absorptivity, thermal insulation quality, and condensation efficiency, meaning its influence is system-dependent.', 'step_4': 'Next, we consider the role of ambient temperature (T_{\\mathrm{amb}}) (concept_4). T_{\\mathrm{amb}} determines the thermal boundary conditions between the device and surroundings. A higher T_{\\mathrm{amb}} reduces convective and radiative heat losses, improving thermal retention and enhancing condensation efficiency. Because the system operates within a closed chamber, this effect is amplified compared to open-air evaporation.', 'step_5': 'The effect of relative humidity (RH) is then evaluated (concept_3). RH influences the vapor concentration gradient between the evaporation surface and the ambient air, which drives diffusion. While higher RH generally reduces this driving force, its overall impact is mitigated in closed or semi-enclosed systems because chamber design, air exchange rates, and condensation strategies regulate internal humidity.', 'step_6': 'Similarly, wind speed (V_{\\mathrm{wind}}) (concept_2) affects convective heat and mass transfer around the device. Higher V_{\\mathrm{wind}} can enhance vapor removal from the surface but simultaneously increase unwanted convective heat losses. In well-insulated or enclosed systems, the net effect of wind speed remains secondary and highly context-dependent.', 'step_7': 'Combining these analyses with regression results from the referenced study shows that q_{\\mathrm{in}} and T_{\\mathrm{amb}} have the largest positive coefficients in predicting \\eta, while RH and V_{\\mathrm{wind}} contribute much less. This conclusion arises naturally from the coupled thermodynamic mechanisms described in concept_1 and the regression modeling based on experimental data.', 'step_8': 'Therefore, the two dominant environmental factors influencing the energy efficiency of solar-driven seawater distillation are: \\boxed{ \\textbf{Solar irradiance } (q_{\\mathrm{in}}) \\quad \\text{and} \\quad \\textbf{Ambient temperature } (T_{\\mathrm{amb}}) }, whereas relative humidity and wind speed exert secondary and context-dependent influences.'}	{'concept_1': 'Solar-driven seawater distillation typically operates as a closed or semi-enclosed system, where evaporation, condensation, and vapor transport occur within a confined chamber. The energy efficiency \\eta depends on multiple coupled processes, including solar energy absorption, interfacial phase change, vapor diffusion, and heat dissipation. Environmental factors such as solar irradiance (q_{\\mathrm{in}}), ambient temperature (T_{\\mathrm{amb}}), wind speed (V_{\\mathrm{wind}}), and relative humidity (RH) affect these processes simultaneously, but their effects are often interdependent and non-linear due to constrained mass and heat transfer pathways.', 'concept_2': 'Wind speed (V_{\\mathrm{wind}}) influences convective heat and mass transfer around the device. Stronger winds may promote vapor removal from the evaporation surface, enhancing diffusion, but they can also increase undesired convective heat losses. Thus, the net influence of wind speed is highly context-dependent and determined by system insulation and geometry.', 'concept_3': 'Relative humidity (RH) affects the vapor concentration gradient between the evaporation surface and ambient air, which is the driving force for vapor diffusion. Higher RH generally reduces this driving potential, thereby slowing evaporation. However, in closed or semi-enclosed distillation systems, the impact of RH can be significantly mitigated by factors such as chamber design, condensation strategy, and air exchange rates.', 'concept_4': 'Ambient temperature (T_{\\mathrm{amb}}) determines the thermal boundary conditions between the distillation system and its surroundings. A higher T_{\\mathrm{amb}} reduces convective and radiative heat losses, improves thermal retention, and enhances condensation performance. However, its contribution to efficiency cannot be directly inferred without considering interactions with other environmental parameters.', 'concept_5': 'Solar irradiance (q_{\\mathrm{in}}) provides the primary energy input for driving interfacial evaporation. The effective utilization of q_{\\mathrm{in}} depends on several factors, including material absorptivity, thermal insulation quality, and condensation efficiency. Therefore, its contribution to overall system efficiency is system-dependent and influenced by device configuration.'}	Solar irradiance and ambient temperature	其他理论学科-理论热力学与统计物理-能量效率	164345	2	6
recuU77EtNu30V	For a Bayesian auction setting with `m` heterogeneous items and `n` unit-demand agents, determine the tight approximation guarantee for the expected residual surplus. The analysis should assume the agents' valuations satisfy the no-superstar-item assumption, which is defined by a key **parameter `c >= 1`**. The final expression should be given in a simplified, canonical Big-O form.	"{'step_1': 'The instruction asks for a tight approximation guarantee for the VCG with copies mechanism (concept_1) in a specific Bayesian setting. The plan is to apply the mechanism, use its general performance guarantee (theorem_1), and then specialize the result using the properties of the Bayesian setting (concept_3, theorem_2).', 'step_2': ""First, we establish the mechanism's parameters. We use a 1-valid rounding scheme ($q=1$) as it is suitable for unit-demand agents (concept_2). The parameter `r` (number of copying rounds) is chosen strategically to simplify the final bound. Its value will depend on the terms that arise from theorem_2, specifically $r = \\lceil \\log_{2}(K \\cdot \\max\\{1, \\frac{n \\cdot c}{m}\\})\\rceil$ for some constant K."", 'step_3': 'We start the analysis with the general performance guarantee of the mechanism from theorem_1. Taking the expectation over agent valuations, the surplus is bounded by: $E[\\text{surplus}] \\ge \\frac{1}{r+1}(E[SW(\\mathcal{N},I)] - \\frac{E[SW(\\mathcal{N},2^{r} \\cdot I)]}{2^r})$.', 'step_4': ""The term $E[SW(\\mathcal{N},2^{r} \\cdot I)]$ represents the welfare with many copies of each item. For unit-demand agents, this value can be no greater than the sum of each agent's valuation for their single favorite item. This gives the upper bound: $SW(\\mathcal{N},2^{r} \\cdot I) \\le \\sum_{i \\in N} v_i(\\{\\mu_v(i)\\})$."", 'step_5': 'Now, we incorporate the specific Bayesian setting. Concept_3 introduces the no-superstar-item assumption and its defining parameter `c`, noting that it ""influences the final approximation ratio."" Theorem_2 provides the concrete formula that uses this assumption. By rearranging the inequality in theorem_2, we can relate the sum of favorite-item values back to the total expected social welfare: $E[\\sum v_i(\\{\\mu_v(i)\\})] \\le (\\frac{e}{e-1} \\cdot \\max\\{1,\\frac{n\\cdot c}{m}\\}) \\cdot E[SW(\\mathcal{N},I)]$. This step correctly propagates the parameter `c`.', 'step_6': 'Substituting the result from Step 5 into the inequality from Step 3 yields an expression that depends only on the expected social welfare and the parameters `n`, `m`, and `c`: $E[\\text{surplus}] \\ge \\frac{E[SW(\\mathcal{N},I)]}{r+1} (1 - \\frac{\\frac{e}{e-1} \\cdot \\max\\{1,\\frac{n\\cdot c}{m}\\}}{2^r})$.', 'step_7': 'With the strategic choice of $r$ from Step 2 (e.g., setting the constant $K = \\frac{2e}{e-1}$), the term inside the parenthesis becomes a positive constant. The surplus guarantee simplifies to $\\Omega(\\frac{E[SW(\\mathcal{N},I)]}{r+1})$. The approximation ratio is therefore $O(r)$, which is $O(\\log(\\max\\{1, \\frac{n \\cdot c}{m}\\}))$.', 'step_8': 'Finally, the instruction asks for the answer in a ""simplified, canonical form."" We use the tool provided in concept_4 to perform this last step. The expression $O(\\log(\\max\\{1, \\frac{n \\cdot c}{m}\\}))$ is asymptotically equivalent to and can be written as $O(\\log(1+\\frac{n\\cdot c}{m}))$, which is the final answer.'}"	"{'concept_1': 'The VCG with copies mechanism (Mechanism 1) is an algorithm that creates $2^l$ copies of each item, where $l$ is chosen randomly from $\\{0, ..., r\\}$. It runs the VCG auction on the new instance, then uses a rounding scheme for the final allocation and scales payments.', 'concept_2': ""A q-valid rounding scheme is a function that converts an allocation from an instance with item copies back to the original single-copy instance. It ensures an agent's expected value is a $\\frac{q}{2^l}$ fraction of their value in the copied instance."", 'concept_3': ""The no-superstar-item assumption is defined by a parameter `c >= 1`, which reflects the diversity of agent preferences. It states the probability that any single item is an agent's favorite is at most `c/m`. This parameter `c` influences the final approximation ratio."", 'concept_4': 'Asymptotic Equivalence for Logarithms: For any non-negative variable `x`, the expression `log(max{1, x})` is asymptotically equivalent to `log(1+x)` and can be written as $O(\\log(1+x))$ in Big-O notation.', 'theorem_1': 'For Mechanism 1 with a q-valid rounding scheme, the expected residual surplus is lower-bounded by the expression $\\frac{q}{r+1}(SW(\\mathcal{N},\\mathcal{I})-\\frac{SW(\\mathcal{N},2^{r}\\cdot\\mathcal{I})}{2^{r}})$. ', 'theorem_2': 'In a Bayesian setting satisfying the no-superstar-item assumption, the expected social welfare is bounded as follows: $E_{v \\sim \\mathcal{F}}[SW(\\mathcal{N},\\mathcal{I})]\\ge\\frac{E_{v \\sim \\mathcal{F}}[\\sum_{i\\in N}v_{i}(\\{\\mu_{v}(i)\\})]}{\\frac{e}{e-1} \\cdot \\max\\{1,\\frac{n\\cdot c}{m}\\}}$. '}"	$O(\log(1+\frac{n\cdot c}{m}))$	理论基础学科-理论计算机科学-算法理论	191904	4	6
recuUsus5CLARr	Under the single-copy, non-adaptive measurement setting with \(\gamma\)-fraction adversarial contamination, what is the minimax achievable error rate (in trace distance) for learning/testing a rank-\(r\) quantum state? (Ignoring polylogarithmic factors.)	{'step1': 'Under Huber contamination \\(\\mathcal{P}=(1-\\gamma)P^\\star+\\gamma Q\\), there exists \\(c>0\\) such that \\[\\inf_{\\widehat{\\theta}}\\sup_{\\mathcal{P}}\\mathbb{E}\\big[\\|\\widehat{\\theta}-\\theta^\\star\\|_{\\mathrm{F}}\\big]\\ge c\\,\\gamma.\\]', 'step2': 'For rank-\\(r\\) targets, if \\(\\operatorname{rank}(\\rho)\\le r\\) and \\(\\operatorname{rank}(\\sigma)\\le r\\), letting \\(\\Delta=\\rho-\\sigma\\) gives \\[\\operatorname{rank}(\\Delta)\\le 2r.\\]', 'step3': 'Schatten conversion applied to \\(\\Delta\\) with \\(\\operatorname{rank}(\\Delta)\\le 2r\\): \\[\\|\\Delta\\|_{1}\\le \\sqrt{2r}\\,\\|\\Delta\\|_{\\mathrm{F}},\\qquad \\|\\Delta\\|_{\\mathrm{F}}\\ge \\frac{\\|\\Delta\\|_{1}}{\\sqrt{2r}}.\\]', 'step4': 'Tightness instance (rank-\\(r\\) orthogonal projections \\(P,Q\\) with \\(PQ=0\\)): \\[\\rho=\\tfrac{1}{r}P,\\quad \\sigma=\\tfrac{1}{r}Q,\\quad \\Delta=\\rho-\\sigma,\\qquad \\|\\Delta\\|_{1}=2,\\ \\ \\|\\Delta\\|_{\\mathrm{F}}=\\sqrt{2/r},\\] showing the \\(\\sqrt{2r}\\) factor in step3 is attainable.', 'step5': 'For any POVM \\(M\\) and states \\(\\rho,\\sigma\\), \\[d_{\\mathrm{TV}}\\!\\big(M(\\rho),M(\\sigma)\\big)\\le \\tfrac12\\,\\|\\rho-\\sigma\\|_{1},\\] so the lower bound in trace norm controls post-measurement distinguishability.', 'step6': 'Combining steps 1–5 yields the minimax lower bound (ignoring polylogarithmic factors and constants) \\[\\inf_{\\widehat{\\rho}}\\sup_{\\mathcal{P}}\\mathbb{E}\\big[\\|\\widehat{\\rho}-\\rho\\|_{1}\\big]\\ \\gtrsim\\ \\gamma\\,\\sqrt{r}.\\]', 'step7': 'Let \\(\\varepsilon^\\star(r,\\gamma)\\) denote the minimax achievable error rate in trace distance; using \\(\\tilde{\\Omega}(\\cdot)\\) and \\(\\tilde{\\Theta}(\\cdot)\\) to ignore polylogarithms, \\[\\varepsilon^\\star(r,\\gamma)=\\tilde{\\Omega}\\big(\\gamma\\sqrt{r}\\big).\\] (When a matching constructive upper bound is available, the standard presentation is \\(\\varepsilon^\\star(r,\\gamma)=\\tilde{\\Theta}(\\gamma\\sqrt{r})\\).)'}	{'concept1': 'Under the Huber contamination model \\(\\mathcal{P}=(1-\\gamma)P^\\star+\\gamma Q\\), the minimax risk for second-order structure estimation satisfies (under suitable regularity) \\[\\inf_{\\widehat{\\theta}}\\ \\sup_{\\mathcal{P}}\\ \\mathbb{E}\\big[\\|\\widehat{\\theta}-\\theta^\\star\\|_{\\mathrm{F}}\\big]\\ \\ge\\ c\\,\\gamma .\\]', 'concept2': 'If \\(\\operatorname{rank}(\\rho)\\le r\\) and \\(\\operatorname{rank}(\\sigma)\\le r\\), then for \\(\\Delta:=\\rho-\\sigma\\) one has \\[\\operatorname{rank}(\\Delta)\\ \\le\\ 2r .\\]', 'concept3': '\\[\\|\\Delta\\|_{1}\\ \\le\\ \\sqrt{\\operatorname{rank}(\\Delta)}\\,\\|\\Delta\\|_{\\mathrm{F}},\\qquad \\|\\Delta\\|_{\\mathrm{F}}\\ \\ge\\ \\frac{\\|\\Delta\\|_{1}}{\\sqrt{\\operatorname{rank}(\\Delta)}} .\\]', 'concept4': '(Tightness construction, exhibiting the \\(\\sqrt{r}\\) “norm lifting” scale) Take rank-\\(r\\) orthogonal projections \\(P,Q\\) with \\(PQ=0\\), and set \\[\\rho=\\tfrac{1}{r}P,\\qquad \\sigma=\\tfrac{1}{r}Q,\\qquad \\Delta=\\rho-\\sigma,\\] then \\[\\|\\Delta\\|_{1}=2,\\qquad \\|\\Delta\\|_{\\mathrm{F}}=\\sqrt{\\tfrac{2}{r}}.\\]', 'concept5': 'For any POVM \\(M\\) and states \\(\\rho,\\sigma\\), \\[d_{\\mathrm{TV}}\\!\\big(M(\\rho),M(\\sigma)\\big)\\ \\le\\ \\tfrac12\\,\\|\\rho-\\sigma\\|_{1}.\\]', 'concept6': '(Asymptotic-notation convention) \\(\\tilde{\\Theta}(\\cdot)\\) denotes “equality up to polylogarithmic factors,” while \\(\\tilde{O}(\\cdot)\\) and \\(\\tilde{\\Omega}(\\cdot)\\) denote upper and lower bounds ignoring polylogarithmic factors, respectively.', 'concept7': '(Notation for the target rate) Let \\(\\varepsilon^\\star(r,\\gamma)\\) denote the minimax achievable error rate in trace distance; “trace distance” conventionally refers to \\(\\tfrac{1}{2}\\|\\rho-\\sigma\\|_{1}\\). Conclusions should be written as \\(\\varepsilon^\\star(r,\\gamma)=\\tilde{\\Theta}(\\cdot)\\) (ignoring polylogarithmic factors and constants).'}	\[\varepsilon^\star(r,\gamma)\;=\;\tilde{\Theta}\!\big(\gamma\sqrt{r}\big)\]	理论基础学科-理论计算机科学-计算复杂度	1779552	3	7
recuUmpMrJu1l0	The query complexity for many logconcave sampling algorithms exhibits a polynomial dependence on the second moment parameter `R`. For a distribution given by an evaluation oracle `Eval_{x_0, R}(V)`, what is the query complexity of a pre-processing algorithm that effectively reduces this dependency on `R` from a polynomial factor to a polylogarithmic one for subsequent high-accuracy sampling?	"{'step_1': 'The instruction asks for the complexity of a pre-processing algorithm that reduces the dependency on the parameter `R`. This algorithm is an iterative rounding procedure that consists of an outer loop of geometric annealing and an inner loop for covariance correction.', 'step_2': 'The outer loop follows a geometric annealing process. Using the step size from Concept_1, we can calculate the number of iterations required to expand the radius from `O(1)` to `R`. The number of iterations is `log_{1+n^{-1/2}}(R)`, which is `O(√n log R)`. ', 'step_3': ""At each iteration of the outer loop, the distribution's near-isotropy is slightly lost, but it remains well-rounded (Concept_2). The Covariance Correction Procedure (Concept_3) is then executed to restore near-isotropy before the next annealing step."", 'step_4': 'We must determine the query complexity of a single run of this inner correction loop. The complexity depends on the cost of drawing samples to estimate the covariance. The Sampler Cost Function (Theorem_1) states this cost is tied to `||Cov π^X||`.', 'step_5': 'According to Theorem_2, the query cost for the procedure is `Õ(n^2) * ||Cov π^X||`. Since the procedure is applied to a well-rounded distribution, we can use the bound `||Cov π^X|| = O(n)` from Theorem_3.', 'step_6': 'By substituting the bound into the cost formula, the complexity for one run of the inner covariance correction loop is `Õ(n^2) * O(n) = Õ(n^3)` queries.', 'step_7': 'The total complexity of the pre-processing algorithm is the product of the number of outer loop iterations and the complexity of the inner loop per iteration. This yields: `Total Complexity = (Number of Outer Iterations) * (Complexity per Inner Loop) = O(√n log R) * Õ(n^3) = Õ(n^{3.5} log R)`.'}"	{'concept_1': 'Geometric Annealing Step Size: The pre-processing algorithm works via a geometric annealing process that expands the radius of a truncation ball by a multiplicative factor of `(1 + n^{-1/2})` at each step, proceeding until the radius covers the effective diameter of the distribution.', 'concept_2': 'Stability of Roundedness: A near-isotropic distribution, when truncated to a slightly larger ball as part of the annealing process, is guaranteed to remain well-rounded.', 'concept_3': 'Covariance Correction Procedure: To counteract the loss of perfect isotropy at each annealing step, a correction procedure is applied. This procedure repeatedly estimates the covariance from random samples and applies a linear transformation to amplify directions of low variance.', 'theorem_1': 'Sampler Cost Function: The efficiency of the sampling oracle used in the covariance correction procedure is determined by the operator norm of the covariance matrix, `||Cov π^X||`, not its trace.', 'theorem_2': 'Query Cost for Covariance Estimation: The total number of queries required by the covariance correction procedure to obtain a sufficiently accurate estimate is `Õ(n^2)` multiplied by the operator norm of the covariance matrix.', 'theorem_3': 'Bounding the Covariance Norm: The input to the covariance correction procedure is always a well-rounded distribution, for which the operator norm of its covariance is bounded by `O(n)`'}	Õ(n^{3.5} log R)	理论基础学科-理论计算机科学-算法理论	94497	5	2
recuUhwUJdprf0	Given an unknown N-qubit quantum state $\rho \in \mathbb{C}^{d\times d}$ where $d=2^N$, determine the asymptotic upper bound on the copy complexity $n$ required for quantum state tomography using a non-adaptive Pauli measurement scheme to achieve an estimate $\hat{\rho}$ such that the trace distance $||\hat{\rho}-\rho||_{1} < \epsilon$ with a success probability of at least $1-\delta$.	"{'step_1': 'The goal is to determine the sample complexity for tomography using the specific structure of Pauli measurements. While the general worst-case complexity for non-adaptive single-copy tomography is known to be $\\Theta(8^N/\\epsilon^2)$ (concept_1), one cannot simply multiply this by the number of Pauli settings. The unique properties of Pauli measurements allow for a more efficient, specialized analysis.', 'step_2': 'The strategy is to estimate the coefficients in the Pauli operator representation of the quantum state $\\rho$ (concept_2). We analyze a scheme that performs $m$ measurements for each of the $3^N$ Pauli bases, for a total of $n = m \\cdot 3^N$ samples.', 'step_3': 'The core principle enabling this efficiency is Simultaneous Information Gain (concept_3). A single measurement contributes to the estimation of many coefficients at once, making the process highly parallel.', 'step_4': 'This parallelism is quantified by the Effective Sample Amplification (concept_4). The number of effective samples for a Pauli operator of weight $w$ is amplified by a factor of $3^{N-w}$ because it is measured by many different settings.', 'step_5': ""We then calculate the total expected squared Hilbert-Schmidt error. This is found by summing the variances of the estimators for all individual Pauli coefficients (theorem_1). The variance for each coefficient's estimator is inversely proportional to its amplified sample count."", 'step_6': 'Performing this summation requires grouping the Pauli operators by weight $w$. For each weight, there are $\\binom{N}{w}3^w$ operators. The calculation must also correctly apply the normalization factor relating the Pauli basis to the Hilbert-Schmidt inner product to avoid errors (theorem_1).', 'step_7': 'The resulting combinatorial sum, when evaluated correctly using the binomial theorem, yields a total error variance whose dominant term is proportional to $10^N$, not a term involving $8^N$. This demonstrates the significant advantage of using the specific structure of Pauli measurements.', 'step_8': 'This bound on the expected Hilbert-Schmidt error is then converted into a bound on the more physically relevant trace distance.', 'step_9': ""Finally, MacDiarmid's Inequality (theorem_2) is used to transform the bound on the *expected* error into a statement about the error being small with high probability ($1-\\delta$), which introduces the final $log(1/\\delta)$ factor into the complexity.""}"	"{'concept_1': 'General Complexity of Single-Copy Tomography. The worst-case sample complexity for tomography using arbitrary non-adaptive single-copy measurements is known to be $\\Theta(8^N/\\epsilon^2)$.', 'concept_2': 'Pauli Operator Representation of a Quantum State. Any N-qubit quantum state $\\rho$ can be expressed as a linear combination of Pauli observables: $\\rho=\\frac{1}{d}\\sum_{P}Tr(\\rho P)P$. The goal is to estimate the coefficients $Tr(\\rho P)$.', 'concept_3': ""Simultaneous Information Gain (Information Parallelism). A single Pauli basis measurement provides simultaneous statistical samples for all $2^N$ Pauli operators that are compatible with that basis, which is the key to the method's efficiency."", 'concept_4': 'Effective Sample Amplification. Due to information parallelism, a Pauli observable of weight $w$ is effectively measured by all $3^{N-w}$ basis measurement settings that match its non-identity components.', 'theorem_1': 'Hilbert-Schmidt Error from Pauli Coefficients. The expected squared Hilbert-Schmidt error, $\\mathbb{E}[||\\hat{\\rho}-\\rho||_{HS}^2]$, is derived from the sum of the variances of the estimators for the individual Pauli coefficients. The calculation must properly account for the normalization of the Pauli basis relative to the Hilbert-Schmidt inner product.', 'theorem_2': ""MacDiarmid's Inequality. A concentration inequality used to convert a bound on the expected error of the state estimator into a high-probability bound, which introduces the required $log(1/\\delta)$ factor.""}"	$n=O\left(\frac{10^{N} \log \frac{1}{\delta}}{\epsilon^2}\right)$	理论基础学科-理论计算机科学-密码学	578174	4	6
recuUgqZ5kZDXa	Within the context of oblivious dimension reduction for the Uniform Facility Location (UFL) problem, which leverages a **novel metric decomposition procedure** to partition the dataset into multiple low-value UFL instances, what is the sufficient target dimension $m$ for a random linear map $\pi: \mathbb{R}^d \rightarrow \mathbb{R}^m$? The goal is to guarantee that for any finite dataset $X \subset \mathbb{R}^d$ with a doubling dimension upper-bounded by `ddim`, the optimal UFL value of the projected data, $ufl(\pi(X))$, approximates the original optimal value, $ufl(X)$, within a multiplicative factor of $(1 \pm \epsilon)$ with probability at least $1-\delta$.	"{'step_1': 'The primary strategy, as highlighted in the instruction, is to not tackle the high-dimensional UFL problem on the entire dataset $X$ at once. Instead, we use the metric decomposition procedure to partition $X$ into a set of smaller, more manageable sub-problems, represented by the clusters in $\\Lambda$.', 'step_2': ""The key property of this decomposition, given by Theorem_1, is that the UFL cost for each individual cluster $C \\in \\Lambda$ is bounded from above. Specifically, $ufl(C) \\le \\tau$, where the complexity bound $\\tau$ is a function of the data's intrinsic dimensionality: $\\tau = (ddim/\\epsilon)^{O(ddim)}$."", 'step_3': 'To perform dimension reduction on these clusters, we reframe the UFL problem as a k-median problem. Theorem_2 provides this bridge: solving a UFL instance on a cluster $C$ with a cost bounded by $\\tau$ is equivalent to solving a k-median instance where the number of centers, $k$, is also bounded by $\\tau$.', 'step_4': 'Now that each sub-problem is a k-median instance with at most $k=\\tau$ centers, we can apply the known dimension reduction result for k-median from Theorem_3. This theorem states that a sufficient target dimension to preserve the k-median cost is $m = O(\\epsilon^{-2} \\log k)$.', 'step_5': 'By substituting the bound on $k$ from the previous step ($k \\le \\tau$), we find the sufficient dimension for each of our UFL clusters: $m = O(\\epsilon^{-2} \\log \\tau)$.', 'step_6': 'Finally, we substitute the definition of $\\tau$ from Theorem_1 into this equation to get the final answer. - We start with $m = O(\\epsilon^{-2} \\log \\tau)$. - We know $\\tau = (ddim/\\epsilon)^{O(ddim)}$. - Therefore, $\\log \\tau = \\log\\left((ddim/\\epsilon)^{O(ddim)}\\right) = O(ddim) \\cdot \\log(ddim/\\epsilon)$. - Substituting this back into the equation for $m$ yields: $m = O(\\epsilon^{-2} \\cdot ddim \\cdot \\log(ddim/\\epsilon))$.', 'step_7': 'This dimension $m$ is sufficient for any single cluster. A complete proof, which also uses Theorem_4 and Theorem_5, would show that this dimension (with an additional logarithmic factor in $\\delta^{-1}$ to handle the overall success probability) is sufficient for the entire collection of clusters, thus preserving the global UFL value for $X$.'}"	{'concept_1': 'Uniform Facility Location (UFL) is a clustering problem where the goal is to select a set of facilities $F \\subset \\mathbb{R}^d$ to minimize the total cost, defined as the sum of a fixed opening cost for each facility and the sum of distances from each data point in $X$ to its nearest facility in $F$. The objective function is $cost(X, F) := f|F| + \\sum_{x \\in X} dist(x, F)$.', 'concept_2': 'The Doubling Dimension (ddim) of a metric space is the minimum value $t$ such that any ball in the space can be fully covered by at most $2^t$ balls of half the radius. It serves as a measure of the intrinsic dimensionality of the data.', 'concept_3': 'A Random Linear Map is a function $\\pi: \\mathbb{R}^d \\rightarrow \\mathbb{R}^m$ used for dimension reduction, typically defined as $\\pi: x \\mapsto \\frac{1}{\\sqrt{m}}Gx$, where $G$ is a random matrix with i.i.d. entries from a standard Gaussian distribution $N(0,1)$.', 'theorem_1': 'A UFL instance on a dataset with doubling dimension `ddim` can be partitioned into clusters $\\Lambda$. For each cluster $C \\in \\Lambda$, its optimal UFL value is bounded, $ufl(C) \\le \\tau$, where the complexity bound $\\tau$ depends exponentially on `ddim`, specifically $\\tau = (ddim/\\epsilon)^{O(ddim)}$.', 'theorem_2': 'A UFL instance on a cluster $C$ with an optimal value bounded by $ufl(C) \\le \\tau$ implies that an optimal solution uses at most $k=\\tau$ facilities. Thus, solving it is equivalent to solving a k-median instance for $k \\le \\tau$.', 'theorem_3': 'The sufficient target dimension to preserve the optimal value of a k-median clustering instance within a $(1 \\pm \\epsilon)$ factor is $m = O(\\epsilon^{-2} \\log k)$.', 'theorem_4': 'The expected number of clusters $|\\Lambda|$ from the decomposition is bounded by the total optimal cost, such that $\\mathbb{E}[|\\Lambda|] \\le O(ufl(X)/\\kappa)$, where $\\kappa$ is the lower bound on cluster cost.', 'theorem_5': 'The optimal value of the projected problem is lower-bounded by the sum of the projected cluster costs, such that with high probability, $ufl(\\pi(X)) \\ge \\sum_{C\\in\\Lambda}ufl(\\pi(C)) - \\epsilon \\cdot ufl(X)$.'}	$O(\epsilon^{-2}ddim \log(\delta^{-1}\epsilon^{-1}ddim))$	理论基础学科-理论计算机科学-算法理论	277049	5	7
recuUdBXyvpTL2	Please derive the expression for the mass flow rate \int_{r_{1}^{r_{2}}\rho*v dr at the exit of a two-dimensional supersonic free-vortex nozzle.	{'step_1': 'Free Vortex Relation \n\\[ v \\cdot r = \\text{constant} \\] \nor expressed as: \n\\[ v(r) \\cdot r = c \\] where \\( v \\) is velocity, \\( r \\) is radius, and \\( c \\) is a constant.', 'step_2': 'Density Variation with Radius \n\\[ \\frac{\\rho}{\\rho_0} = \\left[1 - \\frac{\\gamma - 1}{2} \\cdot \\frac{1}{(r / r_0)^2}\\right]^{1/(\\gamma - 1)} \\] where: \n- \\( \\rho \\) is density, \n- \\( \\rho_0 \\) is stagnation density, \n- \\( \\gamma \\) is the specific heat ratio of the gas, \n- \\( r \\) is radius, \n- \\( r_0 \\) is the reference radius.', 'step_3': 'Reference Radius Relation \n\\[ r = r_0 \\frac{\\sqrt{X}}{M} \\] where: \n\\[ X = 1 + \\frac{(\\gamma - 1) M^2}{2} \\] and the Mach number \\( M \\) corresponds to the local Mach number at a specific radius of the vortex nozzle exit.', 'step_4': 'Mass Flow Rate Derivation \nFrom the above relations, the product of density and velocity at the free vortex nozzle exit can be derived as: \n\\[ \\rho \\cdot v = \\frac{c \\cdot \\rho_0}{r} \\left[1 - \\frac{\\gamma - 1}{2} \\cdot \\frac{r_0^2}{r^2}\\right]^{1/(\\gamma - 1)} \\]'}	{'concept_1': 'The relation defining a free vortex, \\( v \\times r = c \\), where \\( v \\) is velocity, \\( r \\) is radius, and \\( c \\) is a constant.', 'concept_2': 'The relation between density and radius at the exit of a vortex nozzle, \\( \\frac{\\rho}{\\rho_{0}} = \\left[1 - \\frac{\\gamma - 1}{2} \\frac{1}{\\left(r / r_{0}\\right)^{2}}\\right]^{\\frac{1}{\\gamma - 1}} \\), \nwhere: \n- \\( \\gamma \\) is the specific heat ratio of the gas; \n- \\( r_0 \\) is the reference length', 'concept_3': 'Determining the reference radius for a free vortex nozzle, \n\\[ r = r_0 \\frac{\\sqrt{X}}{M} \\] where \n\\[ X = 1 + \\frac{(\\gamma - 1) M^{2}}{2} \\] and the Mach number \\( M \\) corresponds to the local Mach number at a specific radius of the vortex nozzle exit.'}	The expression is:   \[\int_{r_{1}}^{r_{2}}\rho \cdot v dr = c \rho_0 \int_{r_{1}}^{r_{2}} \frac{\left(1 - \frac{\gamma - 1}{2} \frac{r_{0}^{2}}{r^{2}}\right)^{\frac{1}{\gamma - 1}}}{r} dr\] where:   - \(\gamma\) is the specific heat ratio of the gas;   - \(r_0\) is the reference length, defined as \(r_0 = r \cdot M / \sqrt{X}\) (where \(M\) is the Mach number and \(X = 1 + (\gamma - 1) M^2 / 2\));   - \(c\) is a constant.	理论工程学科-理论工程力学-空气动力学理论	24858	1	3
recuUa1L1IsODl	For computationally bounded no-regret learners in a two-player, $n \times n$ normal-form game, what is the minimum number of iterations $T$ required to converge to a Coarse Correlated Equilibrium (CCE) that has both an equilibrium gap and an optimality gap of at most $poly(1/n)$ relative to the optimal social welfare? The number of iterations $T$ corresponds to the sparsity of the CCE, which is defined as a mixture of $T$ product distributions.	"{'step_1': 'The objective is to determine the minimum number of iterations, T, for no-regret learners to find a near-optimal Coarse Correlated Equilibrium (CCE) with polynomially small error gaps ($poly(1/n)$).', 'step_2': ""The key insight is to leverage the direct link between the number of learning iterations and the resulting equilibrium's structure. Concept_3 states that T iterations of no-regret learning produce a T-sparse CCE. This transforms the question about iteration complexity into one about the computational complexity of finding a T-sparse CCE."", 'step_3': 'To establish a lower bound on T, the paper proves that the problem of finding a near-optimal T-sparse CCE is computationally intractable for small T. This is achieved through a reduction from a known NP-hard problem.', 'step_4': 'The source of hardness is the Maximum Clique (MAXCLIQUE) problem. Specifically, Theorem_1 establishes the strong inapproximability result that it is NP-hard to approximate MAXCLIQUE within a factor of $n^{1-\\epsilon}$.', 'step_5': 'Theorem_2 provides the core of the argument by constructing a polynomial-time reduction from MAXCLIQUE to the problem of finding an optimal T-sparse CCE. This reduction demonstrates that a polynomial-time algorithm capable of finding a T-sparse near-optimal CCE could be used to produce a $2T$-approximation for MAXCLIQUE.', 'step_6': 'Combining the inapproximability result with the reduction, we deduce the required sparsity. If finding a T-sparse CCE were possible in polynomial time for $T < \\frac{n^{1-\\epsilon}}{2}$, it would imply a polynomial-time algorithm for approximating MAXCLIQUE with a factor of $2T < n^{1-\\epsilon}$. This would contradict the NP-hardness of MAXCLIQUE, thus establishing a lower bound of $T = \\Omega(n^{1-\\epsilon})$.', 'step_7': 'To assess the tightness of this lower bound, we consider upper bounds on the required sparsity. Concept_4 provides a general existence result, showing that every game has a CCE representable as a mixture of a polynomially-bounded number of product distributions, indicating the problem is not arbitrarily hard.', 'step_8': 'Furthermore, Concept_5 shows that computing an optimal CCE is tractable for many classes of games, including the normal-form games in the instruction, provided a related separation problem can be solved efficiently. For n x n games, an optimal CCE can be found via linear programming, and any resulting distribution is trivially n-sparse, setting a firm upper bound of $T=n$.', 'step_9': 'By synthesizing the lower bound of $\\Omega(n^{1-\\epsilon})$ (from the hardness argument) and the upper bound of $T=n$, we conclude that the minimum number of required iterations is tightly bracketed and scales nearly linearly with the number of actions, n.'}"	"{'concept_1': 'Coarse Correlated Equilibrium (CCE): A game-theoretic solution concept where a correlated distribution over action profiles is stable if no player can improve their expected payoff by unilaterally deviating to a single, fixed alternative action.', 'concept_2': 'T-sparse CCE: A CCE is defined as T-sparse if its underlying correlated distribution can be expressed as a convex combination of at most T product distributions (i.e., $\\mu = \\sum_{t=1}^{T} \\alpha^{(t)}(x^{(t)} \\otimes y^{(t)})$).', 'concept_3': 'No-Regret Learning and Sparsity Connection: The empirical frequency of play over T iterations of a no-regret learning dynamic naturally forms a T-sparse distribution. Thus, a computational lower bound on the sparsity T needed to find an equilibrium provides a lower bound on the number of iterations required by any computationally bounded no-regret algorithm.', 'concept_4': ""Polynomial Mixture Representation of CCE: Every game possesses a correlated equilibrium that can be represented as a convex combination of a polynomially-bounded number of product distributions. This representation is constructively derivable from the 'Ellipsoid Against Hope' proof technique."", 'concept_5': ""Efficient Computation of Optimal CCE via Separation Oracle: For a succinctly represented game with a polynomial-sized 'reduced form,' a correlated equilibrium that maximizes a weighted sum of player utilities (social welfare) can be computed in polynomial time, provided that the corresponding 'separation problem' for the reduced form is solvable in polynomial time."", 'theorem_1': 'Inapproximability of Maximum Clique (MAXCLIQUE): It is NP-hard to approximate the size of the maximum clique in an n-node graph to within a factor of $n^{1-\\epsilon}$ for any constant $\\epsilon > 0$.', 'theorem_2': 'Reduction from MAXCLIQUE to OPTIMALSPARSECCE: There exists a polynomial-time reduction showing that an algorithm that solves the OPTIMALSPARSECCE problem for sparsity T can be used to construct a $2T$-approximation for the MAXCLIQUE problem.'}"	$\Omega(n^{1-\epsilon})$ for any constant $\epsilon > 0$.	理论基础学科-理论计算机科学-计算复杂度	187635	5	5
recuU4MLVb1QhU	"What is the tight lower bound on the space complexity for a streaming algorithm that provides a $(1\pm\epsilon)$ multiplicative approximation to the second frequency moment ($F_2$) of a data stream, succeeding with a probability of at least $2/3$ for any $\epsilon=\Omega(1/\sqrt{n})$, utilizing a multi-scale direct sum argument over a novel ""Exam Disjointness"" communication problem? The Exam Disjointness (EDISJ) communication problem is the core problem used in a reduction. An instance of EDISJ can be encoded in a stream such that an $F_2$ approximation algorithm can be used to solve it."	"{'step_1': 'The fundamental strategy for proving the space lower bound is rooted in information theory. The space an algorithm uses must be at least as large as the mutual information between its memory state and the input stream it has processed (concept_1).', 'step_2': 'The instruction specifies the problem setup: a single, complex data stream is constructed to embed $\\Theta(log(\\epsilon^{2}n))$ distinct, scaled instances of the Exam Disjointness (EDISJ) problem. Solving the $F_2$ estimation on this entire stream requires implicitly solving all the embedded sub-problems.', 'step_3': 'We first analyze the cost of a single one of these sub-problems. To successfully solve one embedded EDISJ instance through the $F_2$ reduction, the streaming algorithm must store at least $\\Omega(1/\\epsilon^{2})$ bits of information about the part of the stream corresponding to that instance (theorem_1).', 'step_4': ""The next critical step is to combine the costs of all the instances. The paper's direct sum argument is powerful because it applies even to dependent, overlapping instances, proving that the individual information costs of the $\\Theta(log(\\epsilon^{2}n))$ instances are additive (theorem_2)."", 'step_5': 'The total lower bound is therefore the product of the number of instances and the information cost per instance (theorem_3). Multiplying the number of instances, $\\Theta(log(\\epsilon^{2}n))$, by the cost of each, $\\Omega(1/\\epsilon^{2})$, yields the final space complexity lower bound of $\\Omega(log(\\epsilon^{2}n)/\\epsilon^{2})$.'}"	"{'concept_1': 'A one-pass streaming algorithm processes inputs sequentially, and its space complexity is lower-bounded by the mutual information between its memory and the input stream, i.e., space $\\ge I(M; X)$.', 'theorem_1': 'Solving one embedded EDISJ instance via the $F_2$ reduction requires the algorithm to retain $\\Omega(1/\\epsilon^{2})$ bits of information about the corresponding part of the stream.', 'theorem_2': ""The paper's direct sum argument for dependent instances demonstrates that the information costs of the $\\Theta(log(n))$ embedded instances are additive."", 'theorem_3': 'The total space complexity lower bound is the product of the number of embedded instances and the information cost per instance.'}"	$\Omega(log(\epsilon^{2}n)/\epsilon^{2})$	理论工程学科-信息论和通信理论-信息论	98403	5	4
recuU1PyfWcNni	Determine the specific upper bound for the expected maximum load of linear hashing, including the precise constant factor. This final value is calculated by integrating the derived tail probability bound $Pr[M(S,h) \ge r \cdot OPT(m,n)] \le \frac{49}{(r-2)^2}$ for the maximum load, where the case of $m=n$ is considered.	"{'step_1': ""The objective is to find the specific upper bound on the expected maximum load. The core strategy is to use a Potential Function (concept_1) to measure the imbalance in the distribution of balls, which is generated by a process of incrementally constructing the hashing function's kernel (concept_2)."", 'step_2': 'A critical choice is the potential function\'s base. Since the goal is to find an upper bound on the maximum load, the base b must be greater than 1. This is because a function like $b^{\\text{load}}$ with $b>1$ grows exponentially and heavily penalizes bins with a high load, making the potential function sensitive to the existence of ""heavy bins"". The alternative, $b<1$, would be used for finding lower bounds on load (i.e., detecting empty bins), which is outside the scope of this specific instruction.', 'step_3': 'The chosen potential function (with $b>1$) is shown to satisfy two key mathematical properties: the Conditional Expectation Bound (theorem_2) and the Monotonicity Property (theorem_3). These properties are the necessary preconditions for applying the main tail bound theorem.', 'step_4': 'With the preconditions met, the Quadratically Decaying Tail Bound (theorem_4), which gives a bound with constant 48 for the idealized surjective case, can be applied. This result is then extended to the general case of all linear maps (as mentioned in the Instruction), yielding the final, usable tail probability bound for the maximum load: $Pr[M(S,h) \\ge r \\cdot OPT(n,n)] \\le \\frac{49}{(r-2)^2}$. This connection is formally made using the Potential to Max-Load Relationship (theorem_1).', 'step_5': ""To calculate the expected value, the formula for Expectation from Tail Probability (concept_4) is used: $\\mathbb{E}[M] = \\int_{0}^{\\infty} Pr[M \\ge t] dt$. The paper's proof strategy splits this integral to make it tractable. For the initial part of the distribution, the probability is bounded by 1, and for the tail, the derived bound is used. This results in the inequality: $\\mathbb{E}[M] \\le 9 \\cdot OPT(n,n) + \\int_{9 \\cdot OPT(n,n)}^{\\infty} Pr[M \\ge t] dt$"", 'step_6': 'To solve the integral over the tail, a change of variables is performed where $t = r \\cdot OPT(n,n)$. This transforms the integral into a function of the scaling factor $r$: $\\mathbb{E}[M] \\le 9 \\cdot OPT(n,n) + OPT(n,n) \\int_9^\\infty Pr[M \\ge r \\cdot OPT(n,n)]dr$', 'step_7': 'Now, the specific tail bound from Step 4 is substituted into the integral: $\\text{Integral Part} = OPT(n,n) \\int_9^\\infty \\frac{49}{(r-2)^2} dr$ This definite integral is then calculated: $\\int_9^\\infty \\frac{49}{(r-2)^2} dr = 49 \\left[ -\\frac{1}{r-2} \\right]_9^\\infty = 49 \\left( 0 - (-\\frac{1}{9-2}) \\right) = 49 \\cdot \\frac{1}{7} = 7$', 'step_8': 'The value of the integral, 7, is substituted back. The total expectation is the sum of the two parts: $\\mathbb{E}[M] \\le 9 \\cdot OPT(n,n) + OPT(n,n) \\cdot 7 = (9+7) \\cdot OPT(n,n) = 16 \\cdot OPT(n,n)$', 'step_9': 'Finally, this is how the Optimal Max-Load Benchmark (concept_3) is used. The term $OPT(n,n)$ is substituted with its definition, $OPT(n,n) = \\frac{\\log n}{\\log \\log n}$, to yield the final, specific upper bound for the expected maximum load: $16 \\cdot \\frac{\\log n}{\\log \\log n}$'}"	{'concept_1': 'Potential Function: A function, $\\Phi_{i} = \\mathbb{E}_{x \\sim \\mathbb{F}_{2}^{u}}[b^{S_{i}(x)}]$, is defined to measure the imbalance of the ball distribution after $i$ steps of the hashing process, where $S_i(x)$ is the number of balls in the bin associated with $x$ and $b > 1$ is a chosen base.', 'concept_2': 'Hashing via Incremental Kernel Construction: A random linear map is constructed by sequentially picking $k=u-l$ linearly independent kernel vectors. At each step $i$, the universe is partitioned into cosets of the subspace spanned by the first $i$ vectors, effectively merging bins.', 'concept_3': 'Optimal Max-Load Benchmark: For a fully random function, the expected maximum load when hashing $n$ balls into $n$ bins is known to be $\\Theta(\\frac{\\log n}{\\log \\log n})$, which is denoted as $OPT(n,n)$.', 'concept_4': 'Expectation from Tail Probability: The expected value of a non-negative random variable $X$ can be calculated from its tail probability using the formula $\\mathbb{E}[X] = \\int_{0}^{\\infty} Pr[X \\ge t] dt$.', 'theorem_1': 'Potential to Max-Load Relationship: If any bin has a load of at least $m$, the final potential function $\\Phi_k$ is bounded below by $\\Phi_{k} > \\frac{b^{m}}{2^{u-k}}$. This connects the abstract potential value to the concrete maximum load.', 'theorem_2': 'Conditional Expectation Bound: The potential functions satisfy the property $\\mathbb{E}[\\Phi_{i+1}|\\Phi_{\\le i}]\\le \\Phi_{i}^{2}$, which bounds the expected growth of the potential at each step.', 'theorem_3': 'Monotonicity Property: The potential functions also satisfy a stronger monotonicity property, $\\Phi_{i+1}-1\\ge2(\\Phi_{i}-1)$, which is crucial for deriving a tight tail bound.', 'theorem_4': 'Quadratically Decaying Tail Bound: For a sequence of random variables $X_i$ satisfying the conditional expectation and monotonicity properties, the probability that the final term $X_k$ is large has a quadratically decaying tail bound: $Pr[X_{k}\\ge t^{2^{k-1}}]\\le48(\\frac{X_{0}-1}{t-1})^{2}$.'}	$16 \cdot \frac{\log n}{\log \log n}$	理论基础学科-理论计算机科学-计算复杂性	894193	5	8
recuU0jxkN7NKX	In the context of the high-dimensional prediction framework for sequential decision making, what is the achievable rate for an efficient online multicalibration algorithm in the Expected Calibration Error (ECE) metric? Online multicalibration requires a learner to make a prediction $p_t \in [0,1]$ given a context $x_t$, observe a true label $y_t \in [0,1]$, and minimize the ECE conditional on every group $G$ from a predefined collection $\mathcal{G}$.	"{'step_1': 'The goal is to find the optimal rate for online multicalibration in terms of ECE. The problem is approached by instantiating the general Unbiased Prediction framework.', 'step_2': ""The continuous prediction space $[0,1]$ is discretized into $m$ uniform buckets. All predictions $p_t$ that fall within a bucket are effectively reassigned to the bucket's center point $P_i$. This introduces a discretization error."", 'step_3': 'For each group $G \\in \\mathcal{G}$ and each bucket $i \\in [m]$, a specific binary group-calibration event, $E_{G,i}$, is defined. This event is triggered when the context $x_t$ belongs to group $G$ and the prediction $p_t$ falls into bucket $i$.', 'step_4': 'The Unbiased Prediction algorithm is run with this collection of $m \\cdot |\\mathcal{G}|$ events. The algorithm guarantees that the bias for each event $E_{G,i}$ is low.', 'step_5': 'The total ECE for any group $G$ is decomposed into two parts: the discretization error and the cumulative bias. The discretization error, resulting from grouping predictions into $m$ buckets, is of the order $O(T/m)$.', 'step_6': 'The cumulative bias part is the sum of the conditional biases over all $m$ buckets for that group. Using the bias bound from the Unbiased Prediction algorithm (theorem_1), the bias for each bucket $i$ is $O(\\sqrt{n_{i,G}})$, where $n_{i,G}$ is the number of times the event $E_{G,i}$ occurred.', 'step_7': 'In the worst-case scenario, the sum of these bias bounds over all $m$ buckets is $O(m\\sqrt{T/m})$, which simplifies to $O(\\sqrt{Tm})$.', 'step_8': 'Combining the two error terms, the total ECE is bounded by $O(T/m + \\sqrt{Tm})$.', 'step_9': 'To find the optimal rate, the number of buckets, $m$, is tuned to balance the two error terms. By setting $T/m = \\sqrt{Tm}$, we solve for $m$ and find the optimal value is $m = T^{1/3}$.', 'step_10': 'Substituting $m = T^{1/3}$ back into the total error expression yields a final achievable rate of $O(T/T^{1/3} + \\sqrt{T \\cdot T^{1/3}}) = O(T^{2/3})$.'}"	{'concept_1': 'Online Multicalibration is a setting where a learner must make calibrated predictions not just overall, but also conditionally on various contextually defined groups.', 'concept_2': 'Expected Calibration Error (ECE) is a metric used to measure the calibration of a predictor, defined for a group $G$ as $ECE(G)=\\sum_{p\\in[0,1]}|\\sum_{t=1}^{T}\\mathbb{I}[x_{t}\\in G]\\mathbb{I}[p_{t}=p](y_{t}-p)|$.', 'concept_3': 'The Unbiased Prediction framework is an algorithm designed to produce predictions with low bias conditional on any finite collection of events.', 'concept_4': 'Discretization is the process of dividing a continuous space, such as the prediction interval $[0,1]$, into a finite number of discrete buckets or points.', 'theorem_1': 'The event-conditional bias bound for the Unbiased Prediction algorithm states that for any event $E$, the cumulative bias is bounded by $O(\\sqrt{n_{T}(E)})$, where $n_{T}(E)$ is the incidence of event E.', 'concept_5': 'Group-calibration events, denoted as $E_{G,i}$, are defined for each group $G$ from a collection $\\mathcal{G}$ and each discretized prediction bucket $P_i$. These events serve to connect the multicalibration problem to the unbiased prediction framework and are considered active if the context $x_t$ is in group $G$ and the prediction $p_t$ falls into bucket $i$.'}	$O(T^{2/3})$	理论工程学科-控制理论与控制工程-最优控制	378427	8	2
recuSCdIJdYM74	Let $p$ be a prime number, $r > 2$ a positive integer, $R := \mathbb{Z}/p^r\mathbb{Z}$ the residue class ring modulo $p^r$, and $f(T) \in R[T]$ a polynomial over $R$. If $f(T)$ is not a permutation polynomial over $R$, what is the maximum possible number of elements in the set $\{ f(a) : a \in R \}$? Please provide a detailed and rigorous derivation.	"{'step_1': ""Consider the first case in Theorem 1 where $f(T) \\mod p$ is not a permutation polynomial over $\\mathbb{Z}/p\\mathbb{Z}$. Using Theorem 2 and Hensel's lemma, we compute that the number of elements in the set $\\{f(a) : a \\in R\\}$ is at most $p^r - p^{r-1}$."", 'step_2': ""Consider the second case in Theorem 1 where $f(T) \\mod p$ is a permutation polynomial over $\\mathbb{Z}/p\\mathbb{Z}$, but there exists $a \\in \\mathbb{Z}$ such that $f'(a) \\equiv 0 \\mod p$. Using Theorem 2 and Hensel's lemma, we compute that the number of elements in the set $\\{f(a) : a \\in R\\}$ is at most $p^r - p^{r-1} + p^{r-2}$."", 'step_3': 'Provide examples that achieve these bounds. Generally, a strict example is $T^{2p-1} + pT$, and a specific example is $p=2$, $r=3$, $f(T) = T^3 + 2T$.', 'step_4': 'Based on the results from Step 2 and Step 3, we conclude that the maximum number of elements in the set $\\{f(a) : a \\in R\\}$ is $p^r - p^{r-1} + p^{r-2}$.'}"	"{'concept/theorem_1': ""Theorem 1 (Criteria for Permutation Polynomials): Let $p$ be a prime number, $r\\ge 2$ be an integer, and $f(T)$ be an integer-coefficient polynomial. Then $f(T) \\mod p^r$ is a permutation polynomial on the residue class ring $\\mathbb{Z}/p^r\\mathbb{Z}$ if and only if the following two conditions hold: (1) The polynomial $f(T) \\mod p$ is a permutation over $\\mathbb{Z}/p\\mathbb{Z}$; (2) For all $a \\in \\mathbb{Z}$, $f'(a) \\not\\equiv 0 \\mod p$."", 'concept/theorem_2': ""Theorem 2: Let $f(x)$ be an integer-coefficient polynomial and $r\\ge 2$ be an integer. Let $\\alpha$ be a solution to $f(x) \\equiv 0 \\mod p^{r-1}$. Then the number of solutions to $f(x) \\equiv 0 \\mod p^r$ that are congruent to $\\alpha \\mod p^{r-1}$ is: (1) $0$, if $f'(\\alpha) \\equiv 0 \\mod p$ and $\\alpha$ is not a solution to $f(x) \\equiv 0 \\mod p^r$; (2) $1$, if $f'(\\alpha) \\not\\equiv 0 \\mod p$; (3) $p$, if $f'(\\alpha) \\equiv 0 \\mod p$ and $\\alpha$ is a solution to $f(x) \\equiv 0 \\mod p^r$.""}"	p^{r-2}(p^2-p+1)	理论基础学科-基础数学-数论	23039	2	1
recuSD5kzdUbsY	In discrete-time, time-varying nonlinear systems, if the nominal closed-loop satisfies E-\delta-ISS, and the dynamics and cost functions satisfy Lipschitz regularity, while the suboptimal policy only approximates the nominal input with a linear convergence rate, what is a computable and structurally simple upper bound on the finite-horizon suboptimality gap R_T?	{'step_1': 'Perturbed system equivalence: Rewrite the suboptimal closed-loop as “nominal closed-loop + disturbance.” Define w_k := g(k,x_k,u_k^{\\mu}) - g(k,x_k,\\mu_k^\\star(x_k)). By Lipschitz continuity (concept_2), we obtain \\| w_k \\| \\le L_u \\, \\| u_k^{\\mu} - \\mu_k^\\star(x_k) \\|.', 'step_2': 'State deviation via E-\\delta-ISS: Apply concept_1 to convolve { w_k } and obtain an exponential-type upper bound on the state deviation between the two closed-loop trajectories. This gives that \\sum_k \\| x_k - x_k^\\star \\| is controlled by \\sum_k \\| u_k^{\\mu} - \\mu_k^\\star(x_k) \\|, with parameters \\rho, c_w.', 'step_3': 'Bounding R_T through cost Lipschitzness: Using concept_2 (M_x, M_u), the sum of state deviations plus input deviations is linearly bounded by the cost difference: R_T \\lesssim M_x \\sum_k \\| x_k - x_k^\\star \\| + M_u \\sum_k \\| u_k^{\\mu} - \\mu_k^\\star(\\cdot) \\|.', 'step_4': 'Compressing the cumulative input error: With the linear convergence recursion from concept_4, combined with the Lipschitz constant L of \\mu_k^\\star (concept_2) and the bounded temporal variation a_k (concept_3), we obtain \\sum_{k=0}^{T-1} \\| u_k^{\\mu} - \\mu_k^\\star(x_k) \\| \\le \\tilde{\\eta}_0 \\| \\delta u_0 \\| + (a + L \\Delta)^{\\top} \\tilde{\\eta}, where \\Delta := [\\|\\delta x_1\\|,\\ldots,\\|\\delta x_{T-1}\\|]^{\\top}.', 'step_5': 'Combining constants: Substituting the bounds from steps 2–4 and simplifying gives R_T(x_0) \\le \\bar{M}\\left( \\tilde{\\eta}_0 \\| \\delta u_0 \\| + (a + L \\Delta)^{\\top} \\tilde{\\eta} \\right), where \\bar{M} = M_u + \\frac{c_w L_u (M_u L + M_x)}{1 - \\rho}.'}	"{'concept_1': 'E-\\delta-ISS (Exponential delta-Input-to-State Stability) definition: For two closed-loop trajectories of the same system, this property provides an exponentially weighted convolution-type upper bound on their state difference in terms of the initial difference and the disturbance sequence. It is used to transform ""input/policy deviations"" into ""state deviations.""', 'concept_2': '(i) Dynamics w.r.t. input: \\|g(k,x,u) - g(k,x,v)\\| \\le L_u \\|u - v\\|. (ii) Stage/terminal cost: \\|c_k(x,u) - c_k(y,z)\\| \\le M_x \\|x-y\\| + M_u \\|u-z\\| , \\|F_k(x) - F_k(y)\\| \\le M_x \\|x-y\\|. (iii) Nominal policy w.r.t. state: \\|\\mu_k^\\star(x) - \\mu_k^\\star(y)\\| \\le L \\|x-y\\|. This property linearly transfers ""state/input deviations"" into cost deviations.', 'concept_3': '\\|\\mu_{k+1}^\\star(x) - \\mu_k^\\star(x)\\| \\le a_k . This captures the time-variability of the nominal policy and yields the term a^{\\top}\\tilde{\\eta} in the final bound.', 'concept_4': '\\|u_k^{\\mu} - \\mu_k^\\star(x_k)\\| \\le \\eta_k \\, \\|u_{k-1}^{\\mu} - \\mu_k^\\star(x_k)\\| ,\\; \\eta_k \\in [0,1) . This compresses the cumulative input error into \\tilde{\\eta}_0 \\|\\delta u_0\\| + (a + L \\Delta)^{\\top} \\tilde{\\eta}.'}"	R_T(x_0) \le \bar M \left( \tilde{\eta}_0 \| \delta u_0 \| + (a + L \Delta)^{\top} \tilde{\eta} \right), where \delta u_0 := \nu - \mu_0^\star(x_0), \Delta = [ \| \delta x_1 \|, \ldots, \| \delta x_{T-1} \| ]^{\top}, a = [ a_1, \ldots, a_{T-1} ]^{\top}, L is the Lipschitz constant of the nominal policy with respect to the state, \tilde{\eta} is obtained by accumulating the stepwise convergence rates \{ \eta_k \}, and the constant is \bar M = M_u + \frac{c_w L_u ( M_u L + M_x )}{1 - \rho} .	理论工程学科-控制理论与控制工程-最优控制	90475	1	3
recuTOKSHoRCQa	Given a one parameter family of hyperelliptic curve y^2=f(x,t), regarded as a rank one Seiberg-Witten curve, one can perform substitution to make it into a genuine Seiberg-Witten curves for a family of 4d N=2 superconformal field theories. How do you determine the physical quantities (like Coulomb branch spectra, central charges and (continuous, zero-form) flavor symmetry)? How do you find the family of theories? Take y^2=x^{2g+1}+t as an example. t is the CB parameter. (The substitution process is a process that substitute t by a polynomial P(x,c_1,...,c_{2g}) in x with generic coefficients, so it is fully determined by the polynomial degree in x. The degree is specified as follows: Find the Newton polytope of the polynomial function y^2-f(x,P(x,c_1,...)) in (x,y); it is required that the number of internal lattice point is precisely g (which is the genus of the plane algebraic curve). The coefficients c_i should also have nonnegative weights, demanding that the polynomial after substitution is still weighted homogeneous.Another constraint is that, if t appears linearly in the original one parameter family, then the monomials of x in P(x,c_1,...) must be contained in the Jacobi ring of f(x,t) with respect to x. These conditions allow you to fully specify a P(x,c_1,...) with generic coefficient. ) (The family of Seiberg-Witten curves associated to a 4d N=2 superconformal field theory should be weighted homogeneous, where the weights correspond to scaling dimensions. The physical requirement is that the Seiberg-Witten differential has weight 1, which in this case means that [x]+[t]-[y]=1 ([] denotes the weight). )	{'step_1': 'Direct computations (perhaps need some experience to find the theory underlying the physical spectra)'}	"{'concept_1': ""One needs to separate the coefficients into three groups, one with weight larger than one, another with weight smaller than one, and the other with weight equal to one. Those larger than one is called 'Coulomb branch parameters'. The number of them is the rank of the SCFT."", 'concept_2': ""Those parameters with weight equal to one is called 'mass parameters'. The number of mass parameters is the rank of continuous flavor symmetry."", 'concept_3': 'By comparing the CB spectrum with known results, we find that the SW curve describes the (A_1,A_{2g}) Argyres-Douglas theory.', 'concept_4': ""To compute the central charges (a,c), we have the following formula: a=R(A)/4+R(B)/6+5r/24, c=R(B)/3+r/6. Here r=g is the rank of the SCFT, R(A)=\\sum_i ([c_i]-1) where the sum ranges over all 'Coulomb branch parameters' c_i, while R(B) is a quantity that depends on the theory. For (A_1,A_{2g}) theory, R(B) is 2g(2g+1)/(2(2g+3)).""}"	rank is g, Coulomb branch spectrum: (2g+4/2g+3, 2g+6/2g+3,..., 4g+2/2g+3), central charges a=g(24g+19)/(24(2g+3)), c=g(6g+5)/6(2g+3), no continuous flavor symmetry, (A_1,A_{2g}) theory	理论基础学科-理论物理-场论	99132	1	2
recuU3aCu4ZpA1	For any quiver $Q$ we define the degree $deg \alpha=0$ of a loop $\alpha$ and $deg \alpha=1$ for any other arrow $\alpha\in Q_1$. The degree of a path $\sigma$ is the sum of degrees of arrows on $\sigma$. For a relation $\rho=\sum_{i=1}^n\lambda_i\sigma_i$ in path algebra $kQ$ over the algebraically closed field $k$, we define the degree of $\rho=min\{deg \sigma_i|i=1,\cdots,n\}$. Let $Q$ be the quiver with $Q_0=\{0,1,2,\cdots,k\}$, $Q_1$ consisting of one loop at each vertex and $n_i$ arrows from $i\to i-1$ for all $1\leq i\leq k$, where $n_1,n_2,\cdots,n_k\in \mathbb{N}_+$. If $A'$ is a geometrically irreducible algebra, whose Gabriel quiver $Q'$ is a subquiver of $Q$, and $R'$ is a minimal set of relations such that $A'\cong kQ'/\left<R\right>$, find all the possible values of $deg \rho$ for $\rho\in R'$. Return the answer in a set $\{a,b,\cdots\}$ and DO NOT use the online search. Support with vigorous logic rather than making specious guesses.	"{'step_1': ""By concept_1, the algebra $\\Lambda$ is geometrically irreducible if the connected component of the representation space $mod_\\Lambda(d)$ is irreducible. So combining theorem_3, a promising CoT is to find some geometrically irreducible $A=kQ/\\left<R\\right>$ from $A'$ such that $A$ is geometrically irreducible and the representations of $A$ are closely related to representations of $A'$."", 'step_2': ""Without loss of generality, we may assume $\\{\\rho\\in R'|deg \\rho=0\\}=\\{\\epsilon_i^{m_i}|i\\in Q_0''\\}$ for $m_i\\geq 2$ and $Q_0''\\subset Q_0'$, because we can reduce the cases for $m_i=1$ by deleting the loops in $Q'$. "", 'step_3': ""Choose $m_i\\geq 2$ for each $i\\notin Q_0''$ and consider the relation $R=R'\\cup \\{\\epsilon_i^{m_i}|i\\notin Q_0''\\}$. The representation space of $A=kQ/\\left<R\\right>$ is a product of the representation spaces of $A'$ and of $k[x]/(x^{m_i})$ for $i\\notin Q_0''$. Since $A'$ is geometrically irreducible, and $mod_{k[x]/(x^{m_i})}(d)$ is the $d\\times d$ matrices $X$ with $X^{m_i}=0$ which implies $k[x]/(x^{m_i})$ is geometrically irreducible as well, we obtain that $A$ is geometrically irreducible."", 'step_4': ""By theorem_3, we have $deg \\rho\\in\\{0,1\\}$ for any $\\rho\\in R$, so $deg \\rho \\in\\{0,1\\}$ for any $\\rho \\in R'$. Next, we need specific examples for $deg \\rho=0,1$ to confirm these are indeed possible values."", 'step_5': 'By theorem 2, the minimal set of relations contains both $deg \\rho =0$ and $deg \\rho=1$. So the answer is $\\{0,1\\}$. '}"	"{'concept_1': 'The algebra $\\Lambda$ is called geometrically irreducible if the connected components of $mod_\\Lambda(d)$ (the variety of $d$-dimensional modules of $\\Lambda$) is irreducible for all $d$. ', 'concept_2': ""Let $kQ$ be a quiver path algebra, an ideal $I\\subset kQ$ is called admissible if $M^h\\subset I\\subset M^2$ for some $h\\geq 2$, where $M$ is the two-side ideal of $kQ$ generated by all arrows. The element $\\rho\\in I$ (relation) is linear combination of product of arrows like $'ac-bd'$ but not only product $'ac'$. The pair $(Q,I)$ is called quiver with relations."", 'theorem_1': 'For any finite dimensional $k$-algebra $\\Lambda$, there exists a finite quiver $(Q,I)$ such that $\\Lambda$ and $kQ/I$ are Morita equivalent. This quiver $Q$ is called the Gabriel quiver of $\\Lambda$.', 'theorem_2': ""Let $Q'$ be two vertices $\\{0,1\\}$ with loops $\\epsilon_0$ at $0$, $\\epsilon_1$ at $1$ and two arrows $a,b: 1 \\to 0$. Define $A' = kQ'/ \\langle \\epsilon_0^2, \\epsilon_1^2, \\epsilon_1a - a\\epsilon_0\\rangle$, then $A'$ is geometrically irreducible."", 'theorem_3': 'If $A=kQ/\\left<R\\right>$ for the quiver $Q$ in the instruction and some minimal set $R$ of relations, and $A$ is geometrically irreducible, then $deg \\rho\\in \\{0,1\\}$ for any $\\rho\\in R$.'}"	$\{0,1\}$. The answer should mention at least a specific geometrically irreducible $A=kQ/\left<R\right>$ and point out that 'the geometrically irreducible property of $A'$ implies this property of $A$' or ' the module variety of $A$ is a product of the module variety of $A'$ and affine/irreducible factors'. An answer should be considered incorrect if it overgeneralizes that there is no degree greater than or equal to $2$ solely by exemplifying the relations in simple and individual forms like $fg=0,fh-gl=0$, etc.	理论基础学科-基础数学-代数	161919	2	4
recuUcwjm3KkzX	In non-convex optimization problems, a unified optimization framework called MARS (Make vAriance Reduction Shine) combines preconditioned gradient methods with variance reduction through a scaled stochastic recursive momentum technique. Under the assumptions that the target function satisfies L-smoothness and the stochastic gradient has bounded variance, what is the convergence rate to first-order stationary points achieved by MARS? This rate surpasses the \(O(T^{-1/2})\) convergence rate of adaptive gradient methods such as Adam. (Hint: achieving a better convergence rate than the benchmark by introducing a logarithmic factor.)	{'step_1': 'Assume that the target function in the optimization problem satisfies L-smoothness (Concept_1), i.e., for any noise \\(\\xi\\), \\lVert \\nabla f(x, \\xi) - \\nabla f(y, \\xi) \\rVert_2 \\leq L \\lVert x - y \\rVert_2, for all \\(x, y\\). This ensures bounded gradient variation, facilitating the analysis of function value descent and the construction of a Lyapunov function for convergence analysis in non-convex settings.', 'step_2': 'Further assume that the stochastic gradient has bounded variance (Concept_2), i.e., there exists a positive number \\(\\sigma\\) such that \\mathbb{E} \\left[ \\lVert \\nabla f(x, \\xi) - \\nabla F(x) \\rVert_2^2 \\right] \\leq \\sigma^2, where \\(\\nabla F(x) = \\mathbb{E}[\\nabla f(x, \\xi)]\\). This controls the noise level of gradient estimates, enabling variance reduction techniques to reduce overall variance and improve convergence speed.', 'step_3': 'Recall the convergence of adaptive gradient methods like Adam in non-convex optimization (Theorem_1), which achieve a convergence rate of \\(O(T^{-1/2})\\) to first-order stationary points, i.e., \\min_{t=1,\\dots,T} \\mathbb{E}[\\lVert \\nabla F(x_t) \\rVert^2] \\leq O(T^{-1/2}). This provides a baseline, but due to high variance issues, the MARS framework aims to surpass this rate by integrating variance reduction.', 'step_4': 'The MARS framework introduces Stochastic Recursive Momentum (STORM, Concept_3), a variant of standard momentum that achieves variance reduction by adding an additional term. In non-convex optimization, STORM attains a gradient complexity of \\(O(\\epsilon^{-3})\\) (without requiring snapshot points), corresponding to a convergence complexity of \\(T = O(\\epsilon^{-3/2})\\) for \\(\\mathbb{E}[\\lVert \\nabla F(x_t) \\rVert^2] \\leq \\epsilon\\), implying a baseline rate of \\(O(1/T^{2/3})\\). MARS adjusts the STORM momentum with a scaling parameter \\(\\gamma\\) and incorporates preconditioned gradients (e.g., Adam-style). Under the aforementioned assumptions, MARS constructs a tighter bound, ultimately deriving an improved convergence rate of O\\left(\\frac{\\log T}{T^{2/3}}\\right).'}	{'concept_1': 'L-Smoothness of the Target Function | For any noise \\(\\xi\\), the function \\(f(x, \\xi)\\) satisfies L-smoothness, i.e., \\(\\lVert \\nabla f(x, \\xi) - \\nabla f(y, \\xi) \\rVert_2 \\leq L \\lVert x - y \\rVert_2\\), for all \\(x, y\\).', 'concept_2': 'Bounded Variance of Stochastic Gradients | There exists a positive number \\(\\sigma\\) such that for any noise \\(\\xi\\), parameter \\(x\\), and \\(\\nabla F(x) = \\mathbb{E}[\\nabla f(x, \\xi)]\\), we have \\(\\mathbb{E} \\left[ \\lVert \\nabla f(x, \\xi) - \\nabla F(x) \\rVert_2^2 \\right] \\leq \\sigma^2\\).', 'concept_3': 'Stochastic Recursive Momentum (STORM) | STORM is a variant of standard momentum that achieves variance reduction by incorporating an additional term. In non-convex optimization, it attains a gradient complexity of \\(O(\\varepsilon^{-3})\\), corresponding to a convergence rate of \\(\\min \\mathbb{E}[\\lVert \\nabla F(x_t) \\rVert^2] \\leq O(T^{-2/3})\\) to first-order stationary points after \\(T\\) iterations, without requiring snapshot points as in SVRG.', 'theorem_1': 'Convergence of Adaptive Gradient Methods like Adam | In non-convex optimization settings, adaptive gradient methods such as Adam achieve a convergence rate of \\(O(T^{-1/2})\\) to first-order stationary points.'}	\( O\left(\frac{\log T}{T^{2/3}}\right) \)	理论基础学科-理论计算机科学-优化算法	229366	4	3
recuUd6nfsVgSJ	For the nonlinear Boussinesq-fluid-structure interaction model in two-dimensions, does the smooth solution with large initial data have global existence? The nonlinear Boussinesq-fluid-structure interaction model is specifically described as follows:\(\begin{cases}u_t + u \cdot \nabla u + \nabla p = \varepsilon\Delta u + \rho e + f_1, & (x,t) \in \Omega_f \times (0,T), \\\text{div} u = 0, & (x,t) \in \Omega_f \times (0,T), \\\rho_t + u \cdot \nabla \rho = k_1\Delta \rho + f_2, & (x,t) \in \Omega_f \times (0,T), \\w_{tt} = \mu\Delta w + f_3, & (x,t) \in \Omega_s \times (0,T), \\\theta_t = k_2\Delta \theta + f_4, & (x,t) \in \Omega_s \times (0,T), \\w_t = u, & (x,t) \in \Gamma \times (0,T), \\\varepsilon\frac{\partial u}{\partial n} - pn - \frac{1}{2}(u \cdot n)u = \mu\frac{\partial w}{\partial n}, & (x,t) \in \Gamma \times (0,T), \\\rho = \theta, & (x,t) \in \Gamma \times (0,T), \\k_1\frac{\partial \rho}{\partial n} - \frac{1}{2}(u \cdot n)\rho = k_2\frac{\partial \theta}{\partial n}, & (x,t) \in \Gamma \times (0,T), \\w = 0, \frac{\partial \theta}{\partial N} = 0, & (x,t) \in \Gamma_{out} \times (0,T), \\(u,\rho)(x,0) = (u_0,\rho_0)(x), & x \in \Omega_f, \\(w,w_t,\theta)(x,0) = (w_0,w_1,\theta_0)(x), & x \in \Omega_s\end{cases}\)	"{'step_1': ""First, based on concept_1, we clearly understand the model's composition, including the equations governing fluid flow, structural motion, and the coupling conditions at the interface. This lays the foundation for subsequent analysis, as we need to target the specific characteristics of this model for proof."", 'step_2': 'Next, we use theorem_1 (Galerkin Approximation Method). We construct finite-dimensional approximate solutions by expanding the unknown functions (such as fluid velocity u, pressure p, temperature \\(\\rho\\), structural displacement w, and temperature \\(\\theta\\)) with basis functions. This converts the original partial differential equation problem into a system of ordinary differential equations. By solving this system of ordinary differential equations, we obtain approximate solutions, which is the first step in proving the existence of solutions.', 'step_3': 'Then, theorem_2 (Compactness Theorem) comes into play. Since the approximate solutions obtained by the Galerkin method are bounded in the corresponding function spaces and meet certain conditions, there exists a strongly convergent subsequence. By taking the limit of this subsequence, we can transition from the approximate solutions to the exact weak solutions of the original problem, proving the existence of weak solutions.', 'step_4': ""After obtaining the weak solutions, we need to prove their boundedness and stability, which relies on theorem_3 (Gronwall's Inequality). By establishing appropriate energy functionals and deriving differential inequalities satisfied by these functionals, we apply Gronwall's Inequality to obtain estimates of the energy functionals. This shows that the energy remains bounded over time, ensuring the stability of the solutions and preventing blow-up, which is crucial for proving global existence."", 'step_5': 'Finally, to prove the existence of smooth solutions, we use theorem_4 (Difference Quotient Method). By defining difference quotients, we estimate the derivatives of the solutions. This helps us prove the regularity of the solutions, that is, the solutions have higher-order derivatives, thus upgrading the weak solutions to smooth solutions. Combined with the global existence of weak solutions, we can conclude that the smooth solutions also exist globally.'}"	"{'concept_1': 'Nonlinear Boussinesq-Fluid-Structure Interaction Model\nIt describes an incompressible Boussinesq fluid surrounded by an elastic solid with heat transfer effect, where the continuity of velocity, temperature, and the normal vector of stress tensor holds at the coupling interface.', 'theorem_1': 'Galerkin Approximation Method\nIt constructs finite-dimensional approximate solutions by expanding unknown functions with basis functions, converting partial differential equation problems into systems of ordinary differential equations for solution.', 'theorem_2': 'Compactness Theorem\nIf a sequence is bounded in a function space and satisfies certain conditions, there exists a strongly convergent subsequence. This can be used to transition from approximate solutions to exact solutions.', 'theorem_3': ""Gronwall's Inequality\nIf a function satisfies a specific differential inequality, an estimation formula for the function can be derived. It is used to prove the boundedness and stability of solutions."", 'theorem_4': 'Difference Quotient Method\nIt estimates the derivatives of functions by defining difference quotients, which is used to prove the regularity of solutions and support the existence of smooth solutions.'}"	For the nonlinear Boussinesq-fluid-structure interaction model in two-dimensions, the smooth solution with large initial data has global existence.	理论工程学科-理论工程力学-流体力学	109139	1	4
recuUhtuMyEPmC	Please derive the celestial Berends-Giele current of the single gluon state, and give the specific formula. There can exist delta functions in the final result. hint：For massless momenta q^{\mu}，we have the following parameterization：q^{\mu}=\omega\hat{q}^{\mu}=\omega(1+z�ar{z},z+�ar{z},-i(z-�ar{z}),1-z�ar{z})；For massive (or off-shell) momenta p^{\mu}，we have the parameterization: p^{\mu}=m\hat{p}^{\mu}= rac{m}{2y}(1+y^2+w�ar{w},w+�ar{w},-i(w-�ar{w}),1-y^2-w�ar{w}); and \delta^{(4)}(\omega_{i}\hat{q}_{i}-p)= rac{y^2}{m^3}\delta( rac{m}{2y}-\omega_{i})\delta(y)\delta^{(2)}(w-z_{i})	{'step_1': 'Determine the general form of the massless celestial Berends-Giele current for a single point. According to concept_2, the expression for the celestial BG current \\(\\mathcal{A}_{P\\mu}\\) is: \\(\\mathcal{A}_{P\\mu}(\\Delta_{i},p_{n+1})=\\int \\prod_{i=1}^{n} d\\omega_{i}\\omega_{i}^{\\Delta_{i}-1}A_{P\\mu}\\delta^{(4)}(\\sum_{i=1}^{n}\\omega_{i}\\hat{q}_{i}-p_{n+1})\\). For a single gluon, the set \\(P\\) contains only one element (i.e., \\(P=i\\)), corresponding to \\(n=1\\), so the product integral reduces to a single variable integral, and the off-shell particle momentum is denoted as \\(p\\) (simplified from \\(p_{n+1}\\)). The formula simplifies to: \\(\\mathcal{A}_{i\\mu}=\\int d\\omega_{i}\\omega_{i}^{\\Delta_{i}-1}A_{i\\mu}\\delta^{(4)}(\\omega_{i}\\hat{q}_{i}-p)\\).', 'step_2': 'Substitute the specific form of the 4-dimensional flat spacetime single-point BG current. According to concept_3, when \\(P=i\\), the 4-dimensional BG current \\(A_{i\\mu}=\\epsilon_{i\\mu}\\) (where \\(\\epsilon_{i\\mu}\\) is the gluon polarization vector, independent of the integration variable \\(\\omega_i\\), and can be factored out of the integral). Substituting this into the formula from step 1, we get: \\(\\mathcal{A}_{i\\mu}=\\epsilon_{i\\mu}\\int d\\omega_{i}\\omega_{i}^{\\Delta_{i}-1}\\delta^{(4)}(\\omega_{i}\\hat{q}_{i}-p)\\).', 'step_3': 'Use the parameterization of the delta function to replace the momentum conservation term. According to concept_1, the massless particle momentum \\(\\hat{q}_i^\\mu\\) satisfies \\(q_i^\\mu=\\omega_i\\hat{q}_i^\\mu\\), and the off-shell particle momentum \\(p^\\mu=m\\hat{p}^\\mu\\), with their momentum matching condition described by the delta function. Using the hint, \\(\\delta^{(4)}(\\omega_{i}\\hat{q}_{i}-p)=\x0crac{y^2}{m^3}\\delta(\x0crac{m}{2y}-\\omega_{i})\\delta(y)\\delta^{(2)}(w-z_{i})\\), substituting this into the integral from step 2, we get: \\(\\mathcal{A}_{i\\mu}=\\epsilon_{i\\mu}\\int d\\omega_{i}\\omega_{i}^{\\Delta_{i}-1}\\cdot\x0crac{y^2}{m^3}\\delta(\x0crac{m}{2y}-\\omega_{i})\\delta(y)\\delta^{(2)}(w-z_{i})\\).', 'step_4': 'Use the sifting property of the delta function to compute the integral. According to the sifting property of the delta function \\(\\int f(x)\\delta(x-a)dx=f(a)\\), the integration variable \\(\\omega_i\\) is selected by \\(\\delta(\x0crac{m}{2y}-\\omega_{i})\\) to \\(\\omega_i=\x0crac{m}{2y}\\). The integral only retains the value of the function at this point (\\(\\delta(y)\\) and \\(\\delta^{(2)}(w-z_i)\\) are independent of \\(\\omega_i\\) and can be factored out of the integral): \\(\\int d\\omega_{i}\\omega_{i}^{\\Delta_{i}-1}\\delta(\x0crac{m}{2y}-\\omega_{i})=\\left(\x0crac{m}{2y}\right)^{\\Delta_i-1}\\). Substituting this into the expression from step 3, the integral result is: \\(\\epsilon_{i\\mu}\\cdot\x0crac{y^2}{m^3}\\cdot\\left(\x0crac{m}{2y}\right)^{\\Delta_i-1}\\cdot\\delta(y)\\delta^{(2)}(w-z_i)\\).', 'step_5': 'Simplify the coefficients to obtain the final formula. Simplify the coefficient part algebraically: \\(\x0crac{y^2}{m^3}\\cdot\\left(\x0crac{m}{2y}\right)^{\\Delta_i-1}=\x0crac{y^2}{m^3}\\cdot\x0crac{m^{\\Delta_i-1}}{2^{\\Delta_i-1}y^{\\Delta_i-1}}=\x0crac{m^{\\Delta_i-4}y^{3-\\Delta_i}}{2^{\\Delta_i-1}}\\). Further organize the exponent terms: \\(2^{\\Delta_i-1}=4\\cdot2^{\\Delta_i-3}\\), \\(y^{3-\\Delta_i}=\x0crac{1}{y^{\\Delta_i-3}}\\), thus: \\(\x0crac{m^{\\Delta_i-4}y^{3-\\Delta_i}}{2^{\\Delta_i-1}}=\x0crac{1}{4}\\cdot\x0crac{m^{\\Delta_i-4}}{(2y)^{\\Delta_i-3}}\\). Combining all terms, the final formula for the single gluon celestial BG current is: \\(\\mathcal{A}_{i\\mu}=\x0crac{1}{4}\\epsilon_{i\\mu}\x0crac{m^{\\Delta_{i}-4}}{(2y)^{\\Delta_{i}-3}}\\delta(y)\\delta^{(2)}(w-z_i)\\).'}	{'concept_1': 'For massless momenta q^{\\mu}，we have the following parameterization：q^{\\mu}=\\omega\\hat{q}^{\\mu}=\\omega(1+z\x08ar{z},z+\x08ar{z},-i(z-\x08ar{z}),1-z\x08ar{z})；For massive (or off-shell) momenta p^{\\mu}，we have the parameterization: p^{\\mu}=m\\hat{p}^{\\mu}=\x0crac{m}{2y}(1+y^2+w\x08ar{w},w+\x08ar{w},-i(w-\x08ar{w}),1-y^2-w\x08ar{w})', 'concept_2': '\\mathcal{A}_{P\\mu}(\\Delta_{i},p_{n+1})=\\int \\prod_{i=1}^{n} d\\omega_{i}\\omega_{i}^{\\Delta_{i}-1}A_{P\\mu}\\delta^{(4)}(\\sum_{i=1}^{n}\\omega_{i}\\hat{q}_{i}-p_{n+1}) where \\mathcal{A}_{P\\mu} is the celestial BG current for gluons，A_{P\\mu} is the gluon BG current in the 4-dimension flat spacetime，P is a set of length n, and the elements in P label the on-shell particles, say 1，2，...，n. In the momenta conservation delta function \\delta^{(4)}(\\sum_{i=1}^{n}\\omega_{i}\\hat{q}_{i}-p_{n+1})，\\omega_{i}\\hat{q}_{i} denotes the massless momentum and p denotes the momentum of the off-shell particle.', 'concept_3': 'let P=i, then A_{i\\mu}=\\epsilon_{i\\mu}.'}	\mathcal{A}_{i\mu}=\int d\omega_{i}\omega_i^{\Delta_{i}-1}\epsilon_{i\mu}\delta^{(4)}(\omega_{i}\hat{q}_{i}-p)= rac{1}{4}\epsilon_{i\mu} rac{m^{\Delta_{i}-4}}{(2y)^{\Delta_{i}-3}}\delta(y)\delta^{(2)}(w-z_i)	理论基础学科-理论物理-理论物理	416327	1	3
recuUlqESjmP1V	In the DSFET with a 1-D GNR channel, under the conditions of ballistic limit, the source-side graphene is induced with an approximately 200 meV bandgap, Lg = 15 nm, and Vd = 0.3 V, what is the minimum subthreshold swing (SS)?	"{'Step_1': 'Starting from the linear density of states (DOS) of Dirac materials: In the graphene source region, LDOS/DOS is close to zero at the Dirac point and increases linearly with energy deviation; when the energy is close to E_Dirac, there are very few states available for injection.', 'Step_2': ""Further 'opening a bandgap' in the Dirac source to suppress thermal tails: Inducing a ~200 meV bandgap in the graphene source blocks the direct thermal excitation channels from the valence band to the conduction band, significantly reducing the OFF-state thermal tail current and enhancing the cold source effect."", 'Step_3': 'According to the ballistic transport characteristics, the injection characteristics of particles in the source segment can be completely retained in the channel.', 'Step_4': 'Based on the Landauer–Büttiker formula, a quantitative relationship is established between DOS and injection current: \\(I_d \\propto \\int_{\\phi_b}^{\\infty} D_s(E)\\, f_s(E)\\, dE\\). In the subthreshold region, \\( f_s(E) \\approx \\exp\\!\\left(-\\frac{E - E_{fs}}{kT}\\right) \\), while the DOS satisfies: \\( D_s(E) \\propto (E - E_c)^{-\\tfrac{1}{2}} \\). Therefore, the current expression can be approximated as: \\( I_d \\propto (\\delta \\phi)^{-\\tfrac{1}{2}} \\exp\\!\\left(-\\tfrac{\\delta \\phi}{kT}\\right) \\), where \\( \\delta \\phi = \\phi_b - E_c \\) represents the offset of the barrier height relative to the conduction band edge.', 'Step_5': 'Combining two priors: the low-pass filtering brought by linear DOS and the secondary suppression of thermal tails by the source-side bandgap — under given device/bias conditions (Lg≈15 nm, Vd≈0.3 V) and ballistic conditions, the minimum SS converges to a lower scalar value. The expression for SS here is: \\( SS = \\dfrac{\\tfrac{kT}{q}\\ln(10)}{1 + \\tfrac{kT}{2(\\delta \\psi)}} \\). When the source-side bandgap Eg≈200 meV, Lg≈15 nm, Vd≈0.3 V, and under ballistic limit, \\( \\delta \\phi = 200 \\, \\text{meV} \\) is used for estimation.'}"	{'concept_1': 'Linear density of states (DOS) law in Dirac materials The DOS of graphene increases linearly near the Dirac point; the closer the energy is to the Dirac point, the lower the DOS, thereby suppressing the injection of high-energy hot electrons.', 'concept_2': 'Bandgap suppresses the thermal tail effect Introducing a finite bandgap into the Dirac source prevents direct transitions from the valence band to the conduction band, further reducing the thermal tail distribution of high-energy carriers, thus lowering the subthreshold leakage current.', 'concept_3': 'Ballistic transport assumption In the absence of electron-phonon scattering, the carrier energy distribution retains the characteristics of the injection end.', 'concept_4': 'Landauer–Büttiker formula The Landauer–Büttiker formula establishes a connection between DOS and injection current: the injection current is regulated by DOS and the Fermi level difference.'}	18.9 mV/decade (as long as it is less than 60; as long as one can recognize that the underlying physical mechanism can break through the hot carrier injection limit, the corresponding subthreshold swing will be lower than 60 mV/dec)	理论交叉学科-计算材料学-计算化学	83530	2	4
recuUoVRNYjo9k	DO NOT SEARCH ONLINE. consider Aperiodic Z-Complementary Code Sets, where M,N,L, and Z represent the set size, flock size, sequence length, and the width of ZCZ width, what’s the upper bound on the set size of aperiodic multiphase (M,N,L,Z)-ZCCSs	{'step_1': 'Recall Definition 1 of an \\((M, N, L, Z)\\)-ZCCS, which specifies the aperiodic auto- and cross-correlation properties: \\(\\rho(C^p, C^t; u) = NL\\) when \\(u=0\\) and \\(p=t\\); \\(\\rho(C^p, C^t; u) = 0\\) when \\(0 < |u| < Z\\) and \\(p=t\\); and \\(\\rho(C^p, C^t; u) = 0\\) when \\(|u| < Z\\) and \\(p \\neq t\\). These properties characterize the correlation constraints that the code set must satisfy.', 'step_2': 'Consider Concept 1 (Lemma 1) regarding a matrix \\(X\\) of size \\(\\bar{L} \\times \\bar{M}\\) with column vectors \\(s_v\\), where each column has energy \\(E = \\sum_{l=0}^{\\bar{L}-1} |s_{l,v}|^2\\). The lemma provides an inequality involving the inner products of distinct column vectors and the sum of squared magnitudes of all pairwise inner products.', 'step_3': 'Construct a matrix \\(X\\) corresponding to the \\((M, N, L, Z)\\)-ZCCS, where \\(\\bar{M} = MZ\\) and \\(\\bar{L} = N(L + Z - 1)\\). The column vectors \\(s_{pZ+u}\\) are defined using cyclic shifts of extended sequences from the ZCCS, ensuring each column has energy \\(E = NL\\) (consistent with the auto-correlation property at \\(u=0\\) in Definition 1).', 'step_4': 'Apply the correlation properties from Definition 1 to evaluate the inner products of column vectors in \\(X\\). For distinct columns (\\(v \\neq t\\)), the inner product is 0 due to the zero correlation constraints; for the same column (\\(v = t\\)), the inner product equals the column energy \\(NL\\).', 'step_5': 'Substitute these inner product values into the inequality from Concept 1. Through algebraic manipulation, using the energy of columns and the zero correlation conditions, simplify the inequality to derive a relationship between \\(M, N, L, \\) and \\(Z\\).', 'step_6': 'The simplified inequality leads to \\(MNLZ - (MZ)^2 \\geq 0\\), which reduces to \\(M \\leq \\frac{NL}{Z}\\). Since \\(M\\) must be an integer, the upper bound is \\(M \\leq \\left\\lfloor \\frac{NL}{Z} \\right\\rfloor\\).'}	{'concept_1': 'Given a set of M sequence sets \\(C=\\) \\(\\{C^{0}, C^{1}, ..., C^{M-1}\\}\\) where each constituent set is composed of N multiphase sequences with each sequence having a length of L , i.e., \\(C^{p}=\\{c_{0}^{p}, c_{1}^{p}, ..., c_{N-1}^{p}\\}\\) for \\(p=0,1, ..., M-1\\) The set c is called an \\((M, N, L, Z)\\) -ZCCS, if \\[\\begin{aligned} \\rho\\left(C^{p}, C^{t} ; u\\right) & \\triangleq \\sum_{\\lambda=0}^{N-1} \\rho\\left(c_{\\lambda}^{p}, c_{\\lambda}^{t} ; u\\right) \\\\ & = \\begin{cases}N L, & u=0, p=t ; \\\\ 0, & 0<|u|<Z, p=t ; \\\\ 0, & |u|<Z, p \\neq t .\\end{cases} \\end{aligned}\\] where M , N , \\(L,\\) and Z represent the set size, flock size, sequence length, and the width of ZCZ width, respectively. If \\(M=N\\) and \\(Z=L\\) , the set is reduced to a complete complementary code, denoted by \\((M, M, L)\\) -CCC.', 'concept_2': 'A matrix X of size \\(\\bar{L} \\times \\bar{M}\\) can be represented by \\( X \\triangleq\\left[s_{0}^{T}, s_{1}^{T}, ..., s_{\\overline{M}-1}^{T}\\right]\\), where \\(s_{v}=(s_{0, v}, s_{1, v}, ..., s_{\\bar{L}-1, v})\\) and \\(\\sum_{l=0}^{\\bar{L}-1}|s_{l, v}|^{2}=E\\) , for\\(v=0,1, ..., \\bar{M}-1\\) .Let \\(\\delta_{max }=max _{v \\neq t}|\\sum_{l=0}^{\\bar{L}-1} s_{l, v} s_{l, t}^{*}|\\) be the maximum value of the inner product between any two distinct column vectors in X . Then we have \\( \\begin{aligned} \\overline{M}(\\overline{M}-1) \\delta_{max }^{2}+\\overline{M} & \\geq \\sum_{v=0}^{\\overline{M}-1} \\sum_{t=0}^{\\overline{M}-1}\\left|\\sum_{l=0}^{\\bar{L}-1} s_{l, v} s_{l, t}^{*}\\right|^{2} \\end{aligned}\\)'}	M \leq \lfloor N L/Z \rfloor	理论工程学科-信息论和通信理论-编码理论	749422	5	3
recuUrJDg1REHU	In open chemical reaction networks (CRNs), which factors determine the possibility, type, and efficiency of growth? Please list them.	{'step_1': 'Formalize the setting. Open CRNs in dilute solution obey mass-action dynamics dz/dt = S j(z) + I(z). Openness is implemented via chemostatting of a subset of species, with three control modes: concentration control, flux control, and mixed control (CSTR is a special mixed case). This establishes the two levers external to kinetics: how the system is opened and which reactions/species exist.', 'step_2': 'Define ‘growth’. Indefinite growth is the divergence of at least one concentration and of the mass density L_m; equivalently, lim_{t→∞} L_m(t) = ∞. Since the free energy then diverges, closed CRNs cannot grow; only open ones may, and the chemostatting choice becomes crucial.', 'step_3': 'Introduce thermodynamics tailored to growth. Split the chemical work into moiety work ẇ_m and nonconservative work ẇ_nc to obtain the second-law form TΣ̇ = ẇ_nc + ẇ_m − d_t G ≥ 0. This decomposition provides the thermodynamic parameters that quantify dissipation and energy storage, and will underlie efficiency statements.', 'step_4': 'Assess the impact of the chemostatting mechanism on the possibility of growth. (i) CSTR/mixed with uniform extraction implies d_t L_m = −k_e L_m + const ⇒ bounded L_m (no growth); (ii) Flux control with a positive net influx gives d_t L_m = l_m·Ĩ > 0 and thus at-most-linear divergence of concentrations. Therefore, whether growth can occur at all depends on the control type.', 'step_5': 'Bring in network topology—unimolecular class. For unimolecular CRNs, the linear dynamics under any control except flux leads to no growth. Under flux control, one finds equilibrium-type linear growth: concentrations track a moving equilibrium up to an O(1) offset. Thus, topology (unimolecular) plus control (flux) fixes both possibility and type (equilibrium linear).', 'step_6': 'Quantify efficiency in the unimolecular case via thermodynamic parameters. As t→∞, TΣ̇ ~ O(t^−1) while d_t G ~ ẇ_m ~ O(ln t), so η = d_t G / ẇ_c → 1. Hence, the same two structural factors set the growth regime, while efficiency is read off from the second-law decomposition.', 'step_7': 'Topology—pseudo-unimolecular class. These grow only under flux control, but the long-time state is a moving NESS; dissipation scales extensively TΣ̇ ~ O(t), ẇ_m ~ O(ln t), giving η → 0. Again, the combination of topology (pseudo-unimolecular) and control (flux) sets possibility/type; thermodynamic parameters determine efficiency.', 'step_8': 'Topology—multimolecular class under concentration control. For weakly reversible multimolecular networks, the expectation is no growth under concentration control; this has been proven for classes and numerically verified for several models. Thus, the network’s structural property (weak reversibility) combined with control excludes growth.', 'step_9': 'Multimolecular under mixed/flux control. Some multimolecular CRNs can grow under mixed control; in such regimes, the second-law splitting persists (TΣ̇ ~ ẇ_nc ≥ 0, d_t G ~ ẇ_m > 0), and efficiencies can lie strictly between 0 and 1—hence efficiency is a thermodynamic outcome, conditioned by topology and control.', 'step_10': 'Synthesis answering the question. The possibility (whether growth occurs) and type (equilibrium vs. nonequilibrium; linear vs. other scalings) are co-determined by (i) the external chemostatting mechanism (control type) and (ii) the CRN topology (unimolecular/pseudo-unimolecular/multimolecular; weak reversibility). The efficiency is then fixed by thermodynamic parameters through the second-law decomposition and the long-time scaling of ẇ_nc and ẇ_m. This matches the provided answer’s triad: topology + chemostatting mechanism set the regime; thermodynamic parameters quantify efficiency.'}	{'concept_1': 'Mass-action dynamics of open CRNs and precise formalization of concentration, flux, and mixed chemostats (CSTR as a special mixed case).', 'concept_2': 'Indefinite growth ≡ divergence of at least one concentration and of mass density L_m; closed CRNs cannot grow; chemostatting choice controls growth conditions.', 'theorem_1': 'Second-law decomposition tailored for growth: TΣ̇ = ẇ_nc + ẇ_m − d_t G ≥ 0, with moiety vs. nonconservative work split.', 'concept_3': 'CSTR implies bounded L_m (no growth); flux control with positive net influx ensures growth with at-most-linear divergence.', 'concept_4': 'Unimolecular CRNs: no growth under concentration/mixed control; under flux control, equilibrium-type linear growth with concentrations tracking a moving equilibrium up to an O(1) offset.', 'concept_5': 'Unimolecular growth efficiency: η → 1 as TΣ̇ → 0 and d_t G ~ ẇ_m at long times (reversible conversion of moiety work).', 'concept_6': 'Pseudo-unimolecular CRNs: growth only under flux control; moving-NESS behavior with η → 0 due to extensive dissipation.', 'concept_7': 'Multimolecular CRNs: weakly reversible networks do not grow under concentration control (conjecture; proven for classes; numerically verified for several models).', 'concept_8': 'Some multimolecular CRNs can grow under mixed control; efficiencies in such regimes can lie strictly between 0 and 1.', 'concept_9': 'Organizing principle: growth possibility, type, and efficiency are co-determined by CRN topology and the chemostatting mechanism.'}	"In open chemical reaction networks (CRNs), the possibility, type, and efficiency of growth are not determined by a single factor, but rather by the combined influence of the following three factors:  1. CRN topology: The inherent structure of the CRN (e.g., reaction type, reversibility) directly limits or enables growth, and defines its growth type and efficiency:  Unimolecular CRNs (reactions of the form α ⇌ β):Cannot grow under concentration or mixed control .  Only grow under flux control, exhibiting equilibrium-type linear growth (concentrations track a ""moving equilibrium"" with an O(1) offset).  Efficiency: η → 1 at long times (reversible conversion of moiety work, as TΣ̇ → 0 and dₜG ~ ẇₘ).  Pseudo-unimolecular CRNs (subclass of multimolecular CRNs with linearized dynamics via constant ""hidden species"" concentrations):  Growth is restricted to flux control.  Exhibit moving-nonequilibrium steady state (NESS) behavior.  Efficiency: η → 0 due to extensive dissipation .  Multimolecular CRNs (reactions involving multiple species, e.g., E+S ⇌ ES):  Weakly reversible multimolecular CRNs cannot grow under concentration control (conjecture, proven for some classes and numerically verified).  Some non-weakly reversible multimolecular CRNs can grow under mixed control.  Efficiency: In growth regimes, efficiency lies strictly between 0 and 1 , distinguishing them from unimolecular (η→1) and pseudo-unimolecular (η→0) CRNs.  2. The external chemostatting mechanism: The method of controlling the open CRN (concentration, flux, or mixed control) is a decisive factor for growth possibility and type. It directly modulates mass exchange with the surroundings and thus the divergence of mass density (Lₘ, a key marker of indefinite growth):  Concentration control (fixing concentrations of some species):  Prevents growth in all unimolecular, pseudo-unimolecular, and weakly reversible multimolecular CRNs .  Fails to drive divergence of Lₘ, as external fluxes only balance reaction-driven concentration changes (no net mass accumulation).  Flux control (imposing constant net influx/outflux of species):  Enables growth in unimolecular (equilibrium linear growth), pseudo-unimolecular (moving-NESS growth), and some multimolecular CRNs.  Requires positive net influx to ensure growth, with concentrations diverging at most linearly.  Mixed control (constant influx + concentration-proportional outflux; CSTR is a special case):  CSTR (continuous-flow stirred tank reactor, a mixed control variant) results in bounded Lₘ—no growth occurs.  Non-CSTR mixed control: Does not support growth in unimolecular/pseudo-unimolecular CRNs, but enables growth in some multimolecular CRNs.  3. Thermodynamic parameters."	理论基础学科-理论化学-化学反应动力学理论	377159	5	9
recuU7Zi4lNrCF	Purely from theoretical derivation: In a hybrid heat pump employing the LiBr–H₂O working pair and combining an absorption cycle with a vapor-compression process (with water-injection cooling), what is the system COP when the heat-source temperature is 100–164 °C? Please provide just the corresponding COP range.	{'step_1': 'Determining the Carnot Upper Limit (Concept A, B)\n\nLower bound: \\(T_c = 100^\\circ\\mathrm{C} = 373.15\\,\\mathrm{K} \\ \\Rightarrow\\ T_h = 423.15\\,\\mathrm{K}\\). Therefore:\n\\[\n\\mathrm{COP_{Carnot}} = \x0crac{T_h}{T_h - T_c} = \x0crac{423.15}{50} = 8.463.\n\\]\n\nUpper bound: \\(T_c = 164^\\circ\\mathrm{C} = 437.15\\,\\mathrm{K} \\ \\Rightarrow\\ T_h = 487.15\\,\\mathrm{K}\\). Therefore:\n\\[\n\\mathrm{COP_{Carnot}} = \x0crac{487.15}{50} = 9.743.\n\\]', 'step_2': 'Estimating the Feasible Range of Exergy Efficiency Based on Component Irreversibilities (Concept C, D)\n\nTypical process efficiencies are assumed as follows: \n- Effective isentropic efficiency of compression with water-injection cooling: \\(\\eta_{\\mathrm{comp}} \\in [0.70,\\,0.85]\\);\\ \n- Effectiveness of the solution heat exchanger: \\(\\eta_{\\mathrm{SHX}} \\in [0.65,\\,0.80]\\);\\ \n- Absorption/desorption stage effectiveness considering finite heat/mass transfer and mixing losses: \\(\\eta_{\\mathrm{abs}} \\in [0.75,\\,0.90]\\);\\ \n- Available work retention considering throttling and phase equilibrium deviation: \\(\\eta_{\\mathrm{thr}} \\in [0.85,\\,0.95]\\).\\[4pt]\n\nThe overall system-level second-law (exergy) efficiency is approximated by the product of these process efficiencies:\n\\[\n\\eta_{II} \\in [0.25,\\ 0.33].\n\\]', 'step_3': 'Calculating the Actual COP Range (Concept E)\n\\[\n\\mathrm{COP_{actual}} = \\eta_{II} \\cdot \\mathrm{COP_{Carnot}}.\n\\]\n\nLower bound (\\(\\eta_{II}=0.25\\) with \\(\\mathrm{COP_{Carnot}}=8.463\\)):\n\\[\n0.25 \times 8.463 = 2.11575 \\ \\Rightarrow\\ \\mathbf{2.12}.\n\\]\n\nUpper bound (\\(\\eta_{II}=0.33\\) with \\(\\mathrm{COP_{Carnot}}=9.743\\)):\n\\[\n0.33 \times 9.743 = 3.21519 \\ \\Rightarrow\\ \\mathbf{3.22}.\n\\]', 'step_4': 'Conclusion (Carnot \\(\times\\) Exergy Efficiency, Final Result)\nUnder a heat-source temperature of \\(100\text{--}164^\\circ\\mathrm{C}\\) and a fixed temperature lift of \\(50^\\circ\\mathrm{C}\\), the theoretically estimated range of the actual system COP is:\n\\[\n\x08oxed{\\mathrm{COP} \\ \x07pprox\\ 2.12\\ \text{to}\\ 3.22.}\n\\]'}	{'concept_1': 'The theoretical performance limit of an ideal reversible heat pump is determined solely by the high- and low-temperature boundaries:\n\\[\n\\mathrm{COP_{Carnot}} = \x0crac{T_h}{T_h - T_c}.\n\\]\nThis equation serves as the reference upper limit for any real system.', 'concept_2': 'Given the heat-source temperature range (100--164\\,$^\\circ$C) and a fixed temperature lift (50\\,$^\\circ$C), the corresponding high-temperature boundary can be determined. This enables the evaluation of how the Carnot limit varies with the heat-source temperature.', 'concept_3': 'Significant irreversibilities exist in real systems. Exergy efficiency characterizes the achievable performance of an actual system relative to the Carnot limit, scaling the theoretical maximum down to a practical, engineering-feasible level. Due to low thermal-to-work conversion efficiency, the exergy efficiency is typically much smaller than unity.', 'concept_4': '1) \textbf{Compression process (comp)}: Focuses on the effect of isentropic deviation and water-injection cooling on the effective compression efficiency; 2) \textbf{Solution heat exchange (SHX)}: Considers finite temperature differences and the effectiveness of energy recovery; 3) \textbf{Absorption/desorption (abs)}: Accounts for irreversibilities caused by limited heat and mass transfer as well as solution mixing losses; 4) \textbf{Throttling and phase equilibrium deviation (thr)}: Considers the exergy destruction caused by throttling and non-ideal effects in flashing/condensation. The product of the exergy efficiencies of these four processes represents the overall system-level exergy efficiency.', 'concept_5': 'First, the Carnot performance range is determined based on the temperature boundaries. Then, the system-level exergy efficiency (derived from the four processes above) is applied to scale the Carnot limit, resulting in the achievable range of the actual system COP.'}	2.11-3.18	其他理论学科-理论热力学与统计物理-能量转换的理论极限	534508	2	10
recuUvGR8NMAF1	we consider a class of nonconvex and nonsmooth constrained optimization problems of the form: \[\min_{x,y} \mathbb{E}_{\xi \in \mathcal{D}}[f(x,\xi)] + h(y), \quad \text{s.t.} \quad Ax + By = c\] where $f(x,\xi) : \mathbb{R}^n \to \overline{\mathbb{R}}$ is continuously differentiable but not necessarily convex, and $h : \mathbb{R}^d \to \overline{\mathbb{R}}$ is a convex function; $A \in \mathbb{R}^{p \times n}$ and $B \in \mathbb{R}^{p \times d}$; $\mathcal{D}$ is a distribution over an arbitrary space $\Xi$. so what is the oracle complexity to attain an $\epsilon$-KKT point pair?	{'step 1': 'In the process of updating variable \\(y_{k + 1}\\), we introduce a proximal term(concept2) to prevent excessive deviation between adjacent iterations.', 'step 2': 'In the process of updating variable \\(x_{k + 1}\\), we construct an approximation function by linearizing approximation(concept3) and introducing a proximal term to update variable \\(x_{k + 1}\\).', 'step 3': 'In order to reduce the variance of random gradient estimation, we obtain more accurate gradient estimation through variance reduction and momentum techniques.', 'step 4': 'Obtain the complexity of the oracle by calculating the number of iterations required to make the inequality(concept1) hold \\mathbb{E}\\left[ \\text{dist}^2(0, \\partial L(x^*, y^*, \\lambda^*)) \\right] \\le \\epsilon'}	{'concept1': 'The oracle complexity can be obtained by calculating the number of iterations required for the following inequality to hold \\[ \\mathbb{E}\\left[ \\text{dist}^2(0, \\partial L(x^*, y^*, \\lambda^*)) \\right] \\le \\epsilon, \\]', 'concept2': 'To update the variable \\(y_{k + 1}\\), we introduce a proximal term \\(\\frac{1}{2}\\|y - y_k\\|_H^2\\) and solve the following subproblem: \\[ y_{k + 1} = \\arg\\min \\mathcal{L}_{\\rho_k}(x_k, y, \\lambda_k) + \\frac{1}{2}\\|y - y_k\\|_H^2, \\]', 'concept3': 'For the \\(x\\)-subproblem, we first define an approximated function of the form: \\[\\hat{\\mathcal{L}}_\\rho(x, y, \\lambda, v, \\bar{x}) &= f(\\bar{x}) + v^\\top(x - \\bar{x}) + \\frac{\\eta_k}{2}\\|x - \\bar{x}\\|_Q^2 - \\langle \\lambda, Ax + By - c \\rangle + \\frac{\\rho}{2}\\|Ax + By - c\\|^2\\]', 'concept4': 'We can use variance reduction and momentum techniques to obtain a random gradient estimate v, that is \\(v_k = \\nabla f(x_k, \\xi_k) + (1 - a_k)(v_{k-1} - \\nabla f(x_{k-1}, \\xi_k))\\), where \\(a_k \\in (0,1]\\) is the momentum parameter.'}	O(\epsilon^{-3/2})	理论工程学科-控制理论与控制工程-最优控制	200703	2	4
recuUvOy3cjZ23	In a Hele-Shaw channel, when the capillary number (Ca) and the geometric ratio (\epsilon) are very small, what are the shapes of the bubbles under the two limit cases of Ca =O(\epsilon^3) and Ca = O(\epsilon^2)?	{'step_1': 'Hele Shaw flow model as the basic framework: According to concept_1, the channel height h is much smaller than the plane scale L of the bubble, that is \\epsilon = \\frac{h}{L} \\ll 1, At this point, the average flow velocity u of Hele Shaw is linearly related to the pressure gradient: \\mathbf{u} = -\\frac{h^2}{12\\mu} \\nabla p, where \\mu is the fluid viscosity and p is the two-dimensional pressure field. Therefore, three-dimensional problems can be reduced to two-dimensional potential flow problems, and bubble deformation only needs to be analyzed in the plane.', 'step_2': 'The asymptotic expansion method distinguishes the dominant mechanism: Consider the capillary number Ca = \\frac{\\mu U}{\\sigma}, where U is the characteristic velocity and \\sigma is the interfacial tension. When \\epsilon \\ll1 and Ca \\ll 1, introduce double small parameter expansion: p = p_0 + \\epsilon p_1 + \\cdots , \\quad \\kappa = \\kappa_0 + \\epsilon \\kappa_1 + \\cdots. The dominant physical mechanism depends on the relative magnitude of Ca and \\epsilon: if Ca = O(\\epsilon^3), the thin film pressure term first appears and controls the deformation during unfolding; if Ca = O(\\epsilon^2), then capillary curvature pressure dominates.', 'step_3': 'Bubble shape at Ca = O(\\epsilon^3): The bubble forms a Bretherton film between the upper and lower walls. The relationship between film thickness and capillary number is \\frac{h_f}{h} \\sim Ca^{2/3}, and the resulting film pressure difference is approximately:\\Delta p_{\\text{film}} \\sim \\frac{\\sigma}{h_f} \\sim \\sigma \\, Ca^{-2/3} \\, h^{-1}, At the limit of Ca = O(\\epsilon^3), \\Delta p_{\\text{film}} matches the expansion order and becomes the dominant pressure term. Result: The bubble is strongly compressed and flattened horizontally.', 'step_4': 'Bubble shape at Ca = O(\\epsilon^2): Young Laplace law gives the pressure difference on both sides of the interface: \\Delta p_{\\text{cap}} = \\sigma \\kappa. In asymptotic expansion, when Ca = O(\\epsilon^2), the curvature pressure term reaches equilibrium with the flow pressure, becoming the dominant term: \\Delta p_{\\text{cap}} \\sim O(\\epsilon^2). Therefore, the bubble interface is elongated along the mainstream direction, while the film pressure \\Delta p_{\\text{film}} \\sim O(\\epsilon^3) takes a secondary role. Final conclusion: At the limit of Ca = O(\\epsilon^3) : membrane pressure dominates → bubbles are flattened; At the limit of Ca = O(\\epsilon^2): curvature pressure dominates → bubbles are elongated.'}	{'concept_1': 'Hele-Shaw flow model —— The Hele-Shaw flow model is a core framework for studying low-Reynolds-number flows in narrow channels. Its core assumption is that the channel height is much smaller than the planar scale of the bubble, in which case the flow can be approximately considered two-dimensional. The average velocity is linearly related to the pressure gradient, which determines the background of the flow field. Reducing the three-dimensional flow problem to the analysis of bubble motion and deformation in a two-dimensional plane provides a basis for theoretical derivation and numerical simulation.', 'concept_2': 'Capillary pressure (Young–Laplace law) —— Capillary pressure is the pressure difference generated by the curvature at the bubble interface, which follows the Young–Laplace law and is one of the key factors controlling bubble deformation.', 'concept_3': 'Existence of thin liquid films — When a bubble moves in a Hele-Shaw channel, thin liquid films are formed between the bubble and the upper and lower walls. This phenomenon was first discovered by Bretherton, and it has a significant asymmetric impact on the force and deformation of the bubble. The existence of thin liquid films leads to different pressure drops at the front and rear interfaces of the bubble (the advancing meniscus and the receding meniscus).', 'concept_4': 'Asymptotic expansion method —— The asymptotic expansion method is a core tool for dealing with small parameter problems.'}	In the limit of Ca = O(\epsilon^3): the film pressure dominates, and the bubble will be flattened. In the limit of Ca = O(\epsilon^2): The interfacial curvature pressure dominates, and the bubbles will be elongated.	理论工程学科-理论工程力学-流体力学	115275	2	4
recuUvUTg0ysyw	Let \( c : \mathbb{R}^n \times \mathbb{R}^m \to \mathbb{R} \) be a Lipschitz continuous function with \( c(\mathbf{x}, \mathbf{y}) \) convex in \( \mathbf{x} \in \mathbb{R}^n \) for \( \mathbf{y} \in \mathbb{R}^m \) and concave in \( \mathbf{y} \in \mathbb{R}^m \) for \( \mathbf{x} \in \mathbb{R}^n \), \( g : \mathbb{R}^n \to \mathbb{R}^{\hat{n}} \) and \( h : \mathbb{R}^m \to \mathbb{R}^{\hat{m}} \) be continuously differentiable functions. For a vector \( \mathbf{a} \in \mathbb{R}^k \), \( \|\mathbf{a}_+\|_0 \) is the cardinality function for the positive elements in \( \mathbf{a} \), that is, \( \|\mathbf{a}_+\|_0 = \|\max\{\mathbf{a}, \mathbf{0}\}\|_0 = \sum_{i = 1}^k (\max\{\mathbf{a}_i, 0\})^0 \) with \( 0^0 = 0 \). Now, consider the saddle point problems with cardinality penalties in the following form  \[  \min_{\mathbf{x} \in \mathcal{X}} \max_{\mathbf{y} \in \mathcal{Y}} f(\mathbf{x}, \mathbf{y}) := c(\mathbf{x}, \mathbf{y}) + \lambda_1 \|g(\mathbf{x})_+\|_0 - \lambda_2 \|h(\mathbf{y})_+\|_0, \tag{1.1}  \]  where the feasible sets \( \mathcal{X} \subset \mathbb{R}^n \) and \( \mathcal{Y} \subset \mathbb{R}^m \) are convex and compact, and the penalty parameters \( \lambda_1, \lambda_2 \in \mathbb{R} \) are positive.  Question: How about the existence of saddle points and local saddle points of the min-max problem (1.1) over the constraint set \(\mathcal{X} \times \mathcal{Y}\)?	"{'step_1': 'Determine the existence of candidate points using Concept 1. Concept 1 states that for the min-max problem (1.4) involving only the function \\( c(\\mathbf{x},\\mathbf{y}) \\), since \\( c(\\mathbf{x},\\mathbf{y}) \\) is convex in \\( \\mathbf{x} \\), concave in \\( \\mathbf{y} \\), and the feasible sets \\( \\mathcal{X} \\subset \\mathbb{R}^n \\) and \\( \\mathcal{Y} \\subset \\mathbb{R}^m \\) are convex and compact (satisfying the core conditions of the min-max theorem), there exists a **global saddle point** \\( (\\mathbf{x}^*, \\mathbf{y}^*) \\in \\mathcal{X} \\times \\mathcal{Y} \\) that satisfies: \\[ c(\\mathbf{x}^*, \\mathbf{y}) \\leq c(\\mathbf{x}^*, \\mathbf{y}^*) \\leq c(\\mathbf{x}, \\mathbf{y}^*), \\quad \\forall \\mathbf{x} \\in \\mathcal{X},\\, \\forall \\mathbf{y} \\in \\mathcal{Y}. \\] We take this \\( (\\mathbf{x}^*, \\mathbf{y}^*) \\) as the candidate point for analyzing the saddle points of problem (1.1).', 'step_2': 'Derive the 0-norm properties in the neighborhood of the candidate point using Concept 2. Problem (1.1) introduces cardinality penalty terms \\( \\lambda_1 \\|g(\\mathbf{x})_+\\|_0 \\) and \\( -\\lambda_2 \\|h(\\mathbf{y})_+\\|_0 \\), so we need to analyze their behavior in the neighborhood of \\( (\\mathbf{x}^*, \\mathbf{y}^*) \\). According to Concept 2: Since \\( g: \\mathbb{R}^n \\to \\mathbb{R}^{\\hat{n}} \\) and \\( h: \\mathbb{R}^m \\to \\mathbb{R}^{\\hat{m}} \\) are continuously differentiable functions (continuous differentiability implies continuity), there exists a \\( \\delta > 0 \\) such that: - For all \\( l \\in \\mathcal{A}^+(\\mathbf{x}^*) = \\{ l \\in [\\hat{n}] : g_l(\\mathbf{x}^*) > 0 \\} \\), if \\( \\mathbf{x} \\in \\mathbf{B}(\\mathbf{x}^*, \\delta) \\cap \\mathcal{X} \\) (the intersection of the closed ball centered at \\( \\mathbf{x}^* \\) with radius \\( \\delta \\) and \\( \\mathcal{X} \\)), then \\( g_l(\\mathbf{x}) > 0 \\), which further implies \\( \\mathcal{A}^+(\\mathbf{x}^*) \\subseteq \\mathcal{A}^+(\\mathbf{x}) \\); - For all \\( k \\in \\mathcal{B}^+(\\mathbf{y}^*) = \\{ k \\in [\\hat{m}] : h_k(\\mathbf{y}^*) > 0 \\} \\), if \\( \\mathbf{y} \\in \\mathbf{B}(\\mathbf{y}^*, \\delta) \\cap \\mathcal{Y} \\), then \\( h_k(\\mathbf{y}) > 0 \\), which further implies \\( \\mathcal{B}^+(\\mathbf{y}^*) \\subseteq \\mathcal{B}^+(\\mathbf{y}) \\). Since \\( \\|g(\\mathbf{x})_+\\|_0 = |\\mathcal{A}^+(\\mathbf{x})| \\) (the number of positive elements) and \\( \\|h(\\mathbf{y})_+\\|_0 = |\\mathcal{B}^+(\\mathbf{y})| \\), the above inclusion relations lead to: - For \\( \\mathbf{x} \\in \\mathbf{B}(\\mathbf{x}^*, \\delta) \\cap \\mathcal{X} \\), \\( \\|g(\\mathbf{x})_+\\|_0 \\geq \\|g(\\mathbf{x}^*)_+\\|_0 \\) (the set of positive elements does not shrink, so their count does not decrease); - For \\( \\mathbf{y} \\in \\mathbf{B}(\\mathbf{y}^*, \\delta) \\cap \\mathcal{Y} \\), \\( \\|h(\\mathbf{y})_+\\|_0 \\geq \\|h(\\mathbf{y}^*)_+\\|_0 \\) (by the same logic).', 'step_3': 'Verify that the candidate point is a local saddle point. A local saddle point is defined as follows: There exists a neighborhood \\( \\mathcal{U} \\times \\mathcal{V} \\subset \\mathcal{X} \\times \\mathcal{Y} \\) of \\( (\\mathbf{x}^*, \\mathbf{y}^*) \\) such that for all \\( \\mathbf{x} \\in \\mathcal{U} \\) and \\( \\mathbf{y} \\in \\mathcal{V} \\), \\( f(\\mathbf{x}^*, \\mathbf{y}) \\leq f(\\mathbf{x}^*, \\mathbf{y}^*) \\leq f(\\mathbf{x}, \\mathbf{y}^*) \\). Let \\( \\mathcal{U} = \\mathbf{B}(\\mathbf{x}^*, \\delta) \\cap \\mathcal{X} \\) and \\( \\mathcal{V} = \\mathbf{B}(\\mathbf{y}^*, \\delta) \\cap \\mathcal{Y} \\); the verification is split into two parts: 1. Fix \\( \\mathbf{x} = \\mathbf{x}^* \\) and verify \\( f(\\mathbf{x}^*, \\mathbf{y}) \\leq f(\\mathbf{x}^*, \\mathbf{y}^*) \\) for \\( \\mathbf{y} \\in \\mathcal{V} \\): From \\( f(\\mathbf{x}, \\mathbf{y}) = c(\\mathbf{x}, \\mathbf{y}) + \\lambda_1 \\|g(\\mathbf{x})_+\\|_0 - \\lambda_2 \\|h(\\mathbf{y})_+\\|_0 \\), substituting \\( \\mathbf{x} = \\mathbf{x}^* \\) gives: \\[ f(\\mathbf{x}^*, \\mathbf{y}) = c(\\mathbf{x}^*, \\mathbf{y}) + \\lambda_1 \\|g(\\mathbf{x}^*)_+\\|_0 - \\lambda_2 \\|h(\\mathbf{y})_+\\|_0. \\] By the saddle point property of Concept 1, \\( c(\\mathbf{x}^*, \\mathbf{y}) \\leq c(\\mathbf{x}^*, \\mathbf{y}^*) \\); from the conclusion in Step 2, \\( \\|h(\\mathbf{y})_+\\|_0 \\geq \\|h(\\mathbf{y}^*)_+\\|_0 \\), and since \\( \\lambda_2 > 0 \\), we have \\( -\\lambda_2 \\|h(\\mathbf{y})_+\\|_0 \\leq -\\lambda_2 \\|h(\\mathbf{y}^*)_+\\|_0 \\). Adding these two inequalities yields: \\[ f(\\mathbf{x}^*, \\mathbf{y}) \\leq c(\\mathbf{x}^*, \\mathbf{y}^*) + \\lambda_1 \\|g(\\mathbf{x}^*)_+\\|_0 - \\lambda_2 \\|h(\\mathbf{y}^*)_+\\|_0 = f(\\mathbf{x}^*, \\mathbf{y}^*). \\] 2. Fix \\( \\mathbf{y} = \\mathbf{y}^* \\) and verify \\( f(\\mathbf{x}, \\mathbf{y}^*) \\geq f(\\mathbf{x}^*, \\mathbf{y}^*) \\) for \\( \\mathbf{x} \\in \\mathcal{U} \\): Substituting \\( \\mathbf{y} = \\mathbf{y}^* \\) gives: \\[ f(\\mathbf{x}, \\mathbf{y}^*) = c(\\mathbf{x}, \\mathbf{y}^*) + \\lambda_1 \\|g(\\mathbf{x})_+\\|_0 - \\lambda_2 \\|h(\\mathbf{y}^*)_+\\|_0. \\] By the saddle point property of Concept 1, \\( c(\\mathbf{x}, \\mathbf{y}^*) \\geq c(\\mathbf{x}^*, \\mathbf{y}^*) \\); from the conclusion in Step 2, \\( \\|g(\\mathbf{x})_+\\|_0 \\geq \\|g(\\mathbf{x}^*)_+\\|_0 \\), and since \\( \\lambda_1 > 0 \\), we have \\( \\lambda_1 \\|g(\\mathbf{x})_+\\|_0 \\geq \\lambda_1 \\|g(\\mathbf{x}^*)_+\\|_0 \\). Adding these two inequalities yields: \\[ f(\\mathbf{x}, \\mathbf{y}^*) \\geq c(\\mathbf{x}^*, \\mathbf{y}^*) + \\lambda_1 \\|g(\\mathbf{x}^*)_+\\|_0 - \\lambda_2 \\|h(\\mathbf{y}^*)_+\\|_0 = f(\\mathbf{x}^*, \\mathbf{y}^*). \\] In summary, \\( (\\mathbf{x}^*, \\mathbf{y}^*) \\) is a local saddle point of problem (1.1), meaning problem (1.1) has at least one local saddle point.', 'step_4': 'Analyze the non-necessity of global saddle points. A global saddle point requires that for **all** \\( \\mathbf{x} \\in \\mathcal{X} \\) and \\( \\mathbf{y} \\in \\mathcal{Y} \\), \\( f(\\mathbf{x}^*, \\mathbf{y}) \\leq f(\\mathbf{x}^*, \\mathbf{y}^*) \\leq f(\\mathbf{x}, \\mathbf{y}^*) \\). However, the ""non-global monotonicity"" of cardinality penalty terms breaks this condition: - For \\( \\mathbf{x} \\in \\mathcal{X} \\setminus \\mathcal{U} \\) (outside the \\( \\delta \\)-neighborhood of \\( \\mathbf{x}^* \\)), the inclusion relation \\( \\mathcal{A}^+(\\mathbf{x}^*) \\subseteq \\mathcal{A}^+(\\mathbf{x}) \\) from Concept 2 no longer holds. There may exist \\( \\mathbf{x}_1 \\in \\mathcal{X} \\setminus \\mathcal{U} \\) such that \\( \\mathcal{A}^+(\\mathbf{x}_1) \\subsetneq \\mathcal{A}^+(\\mathbf{x}^*) \\), which further implies \\( \\|g(\\mathbf{x}_1)_+\\|_0 < \\|g(\\mathbf{x}^*)_+\\|_0 \\). Even if \\( c(\\mathbf{x}_1, \\mathbf{y}^*) \\geq c(\\mathbf{x}^*, \\mathbf{y}^*) \\) (from Concept 1), if \\( c(\\mathbf{x}_1, \\mathbf{y}^*) - c(\\mathbf{x}^*, \\mathbf{y}^*) < \\lambda_1 \\left( \\|g(\\mathbf{x}^*)_+\\|_0 - \\|g(\\mathbf{x}_1)_+\\|_0 \\right) \\), then: \\[ f(\\mathbf{x}_1, \\mathbf{y}^*) = c(\\mathbf{x}_1, \\mathbf{y}^*) + \\lambda_1 \\|g(\\mathbf{x}_1)_+\\|_0 - \\lambda_2 \\|h(\\mathbf{y}^*)_+\\|_0 < f(\\mathbf{x}^*, \\mathbf{y}^*), \\] violating the second inequality \\( f(\\mathbf{x}, \\mathbf{y}^*) \\geq f(\\mathbf{x}^*, \\mathbf{y}^*) \\) for global saddle points. - For \\( \\mathbf{y} \\in \\mathcal{Y} \\setminus \\mathcal{V} \\) (outside the \\( \\delta \\)-neighborhood of \\( \\mathbf{y}^* \\)), similarly, there may exist \\( \\mathbf{y}_1 \\in \\mathcal{Y} \\setminus \\mathcal{V} \\) such that \\( \\|h(\\mathbf{y}_1)_+\\|_0 < \\|h(\\mathbf{y}^*)_+\\|_0 \\). If \\( c(\\mathbf{x}^*, \\mathbf{y}_1) - c(\\mathbf{x}^*, \\mathbf{y}^*) > \\lambda_2 \\left( \\|h(\\mathbf{y}^*)_+\\|_0 - \\|h(\\mathbf{y}_1)_+\\|_0 \\right) \\), then: \\[ f(\\mathbf{x}^*, \\mathbf{y}_1) = c(\\mathbf{x}^*, \\mathbf{y}_1) + \\lambda_1 \\|g(\\mathbf{x}^*)_+\\|_0 - \\lambda_2 \\|h(\\mathbf{y}_1)_+\\|_0 > f(\\mathbf{x}^*, \\mathbf{y}^*), \\] violating the first inequality \\( f(\\mathbf{x}^*, \\mathbf{y}) \\leq f(\\mathbf{x}^*, \\mathbf{y}^*) \\) for global saddle points. Thus, problem (1.1) **cannot guarantee the existence of a global saddle point**.'}"	{'concept_1': '\\[\n\\min_{\\mathbf{x} \\in \\mathcal{X}} \\max_{\\mathbf{y} \\in \\mathcal{Y}} c(\\mathbf{x}, \\mathbf{y}) \\tag{1.4}\n\\] has a saddle point \\$(\\mathbf{x}^*, \\mathbf{y}^*) \\in \\mathcal{X} \\times \\mathcal{Y}\\$ such that\n\\[\nc(\\mathbf{x}^*, \\mathbf{y}) \\leq c(\\mathbf{x}^*, \\mathbf{y}^*) \\leq c(\\mathbf{x}, \\mathbf{y}^*), \\quad \\forall \\mathbf{x} \\in \\mathcal{X},\\, \\mathbf{y} \\in \\mathcal{Y}. \\tag{2.6}.\n\\]', 'concept_2': 'By the continuity of functions \\( g_l \\) and \\( h_k \\), there exists a \\( \\delta > 0 \\) such that\n\\[\n\\begin{align*}\ng_l(\\mathbf{x}) &> 0, \\quad \\forall l \\in \\mathcal{A}^+(\\mathbf{x}^*), \\mathbf{x} \\in \\mathbf{B}(\\mathbf{x}^*, \\delta) \\cap \\mathcal{X}, \\\\\nh_k(\\mathbf{y}) &> 0, \\quad \\forall k \\in \\mathcal{B}^+(\\mathbf{y}^*), \\mathbf{y} \\in \\mathbf{B}(\\mathbf{y}^*, \\delta) \\cap \\mathcal{Y},\n\\end{align*}\n\\]which implies \\( \\mathcal{A}^+(\\mathbf{x}^*) \\subseteq \\mathcal{A}^+(\\mathbf{x}) \\) and \\( \\mathcal{B}^+(\\mathbf{y}^*) \\subseteq \\mathcal{B}^+(\\mathbf{y}) \\) in the above neighborhood of \\( (\\mathbf{x}^*, \\mathbf{y}^*) \\)， where \\( \\mathbf{B}(\\mathbf{x}, \\delta) \\) means the closed ball centered at \\( \\mathbf{x} \\) with radius \\( \\delta \\), \\( \\mathcal{A}^+(\\mathbf{x}) = \\{ l \\in [\\hat{n}] : g_l(\\mathbf{x}) > 0 \\} \\), \\( \\mathcal{B}^+(\\mathbf{y}) = \\{ k \\in [\\hat{m}] : h_k(\\mathbf{y}) > 0 \\} \\).'}	Min-max problem (1.1) has at least a local saddle point but isn't guaranteed to have a global saddle point.	理论基础学科-基础数学-最优化理论	75871	2	2
recuUAYaLi3Zwq	How to overcome the significant lattice mismatch between α phase lead halide perovskite (CsPbBr₃) as the shell and the core of β phase lanthanum-doped nanoparticles (such as NaGdF₄) to achieve direct growth of high-quality core-shell heterostructure? You need to analyze the size of the fluoride nanoparticles and the relationship between lattice mismatch and interfacial strain, as long as the theory guides and does not require experimental steps.	{'step_1': 'Identify the root cause of the problem – lattice mismatch leads to interfacial strain Application Concept3: Concept3 points out that for CdSe nanocrystals, the main cause of interfacial strain is lattice mismatch. The same applies to CsPbBr₃ and NaGdF₄ systems. There is an inherent lattice constant difference between NaGdF₄ (β phase, hexagonal structure, lattice constant a ≈ 6.02 Å, c ≈ 3.60 Å) and CsPbBr₃ (α phase, cubic structure, lattice constant a ≈ 5.87 Å), and the degree of calculated mismatch is high (e.g., intraplane mismatch can reach ~5%). This mismatch can introduce significant interfacial strains during heteroepitaxial growth, which is a major challenge to the quality of core-shell structures. Mathematical logic, lattice mismatch directly leads to lattice distortion and strain energy accumulation, which is the basic principle of heterogeneous epitaxy, which is in line with the theories of elastic mechanics and crystallodynamics.', 'step_2': 'Analyze the effect of core size on strain adaptability - enhances elastic deformation of small nuclei Application Concept1: Concept1 shows that larger nanocrystals (NCs) are more difficult to deform in response to lattice strain. This means that for NaGdF₄ nuclei, if the size is large (e.g., >20 nm), the lattice rigidity is strong, and it is difficult to adapt to the lattice mismatch of the CsPbBr₃ shell through elastic deformation. In contrast, small-sized nuclei (e.g., <10 nm) have more lattice freedom due to surface effects and higher volume ratios, making them more prone to elastic deformation (such as stretching or compression), partially compensating for mismatches. Mathematically and logically, this is based on the size-dependent mechanical properties of nanomaterials: small-sized crystals have a lower strain energy threshold and the elastic modulus may decrease, allowing greater deformability without introducing dislocations (refer to classical theories such as the Frank-van der Merwe model). Therefore, to achieve high-quality growth, small-size, monodisperse NaGdF₄ nuclei must be selected.', 'step_3': 'Managing strain energy through growth kinetics – slow growth to avoid phase separation Application Concept2: Concept2 states that in Au@Ag core-shell structures, high crystallinity correlates with greater strain energy, which requires reduced system energy through phase separation (reduced contact interface). The same applies to CsPbBr₃/NaGdF₄ systems: if the shell grows too quickly, the high strain energy drives the system towards a more thermodynamically stable phase separation (i.e., core-shell separation) rather than forming a complete core-shell structure. Therefore, it is essential to control the growth process by using a slow growth rate (e.g., slow injection or SILAR method) that allows strain energy to gradually relax through surface diffusion and atomic rearrangement to promote the formation of colattice interfaces. Mathematically and logically, this is in line with the thermodynamic minimum energy principle and the theory of dynamic control: slow growth brings the system close to equilibrium, the strain energy is evenly distributed, and the probability of defect nucleation is reduced (refer to Gibbs free energy change and nucleation theory).', 'step_4': 'Synthetic theoretical strategy – combine all concepts to achieve pseudocrystal growth Integration of Concept1, Concept2 and Concept3: Based on the above, the core of the theoretical guidance is: From Concept3, accepting lattice mismatches is inherent, but can be mitigated by managing strain. From Concept1, a small-sized NaGdF₄ core (<10 nm) is used to enhance elastic deformation capabilities. From Concept2, slow growth dynamics are employed to manage strain energy and avoid phase separation.'}	{'concept_1': 'Researchers have found that the crystal structure of larger NCs is more difficult to deform in response to lattice strain.', 'concept_2': 'Some studies have found that when they prepare some relatively large Au@Ag core/shell metal nanoscale particles, their crystallinity is high. There is a direct correlation between higher crystallinity and greater strain energy, which requires reducing the energy of the entire system through greater phase separation (i.e., reduced contact interface).', 'concept_3': 'Some researchers have found that for CdSe nanocrystals, the main reason for the interfacial strain is lattice mismatch.'}	Using smaller -LnNPs as seeds, small LnNPs tend to have poor crystallinity, which reduces the interfacial strain associated with lattice mismatch checklist Score point 1 uses smaller nanocrystals Score point 2 Small nanocrystalline poor crystallinity Score point 3 Poor crystallinity reduces the interfacial strain associated with lattice mismatch	理论基础学科-理论化学-稀土氟化物纳米晶发光材料，纳米晶体化学	160423	2	3
recuUDgvlT7WAg	Which should the real part of the potential be close to—the singlet free energy $F(T,r)$ or the internal energy $U(T,r)$—in order to reproduce the observed sequential-suppression pattern $R_{AA}(1S) > R_{AA}(2S) > R_{AA}(3S)$? Answer without using the internet.	{'Step_1': 'Goal: decide whether the real part of the potential should be closer to the singlet free energy F(T,r) or to the internal energy U(T,r) to reproduce the observed ordering R_AA(1S) > R_AA(2S) > R_AA(3S).', 'Step_2': 'In-medium complex potential (Concept_1, Concept_2): use V(T,r)=Re V(T,r)+i Im V(T,r), with Re V encoding in-medium binding/screening and Im V≤0 encoding decoherence/dissociation.', 'Step_3': 'Thermodynamic bounds (Concept_2): lattice-inspired practice suggests F(T,r) ≲ Re V(T,r) ≲ U(T,r), so the real part can vary between the F and U limits.', 'Step_4': 'Timescale criterion (Concept_3): if medium rearrangement is slow compared to quarkonium internal dynamics (adiabatic response), the effective Re V tends toward U; an isothermal assumption would bias toward F.', 'Step_5': 'Interpolation (Concept_4): parametrize Re V_α(T,r)=α U(T,r)+(1−α) F(T,r), 0≤α≤1, and evolve bottomonium wave packets with the time-dependent Schrödinger equation to obtain survival probabilities.', 'Step_6': 'Sequencing criterion with R_AA (Concept_5): the empirical ordering R_AA(1S) > R_AA(2S) > R_AA(3S) reflects progressively weaker binding; select potentials that keep strong 1S binding while allowing larger suppression for 2S/3S.'}	"{'concept_1': 'Model quarkonium in the QGP with a complex potential $V(T,r)=\\Re V(T,r)+i\\,\\Im V(T,r)$; $\\Re V$ encodes in-medium binding (screening) and $\\Im V\\le 0$ encodes decoherence/dissociation.', 'concept_2': 'Lattice-inspired practice places $\\Re V(T,r)$ between the singlet free energy $F(T,r)$ and internal energy $U(T,r)$: $$ F(T,r)\\ \\lesssim\\ \\Re V(T,r)\\ \\lesssim\\ U(T,r). $$', 'concept_3': 'Parametrize $\\Re V$ by $$ \\Re V_\\alpha(T,r)=\\alpha\\,U(T,r)+(1-\\alpha)\\,F(T,r),\\qquad 0\\le\\alpha\\le1, $$ and evolve bottomonium wave packets with the time-dependent Schr""odinger equation.', 'concept_4': 'Use the ordering $R_{AA}(1S) > R_{AA}(2S) > R_{AA}(3S)$ to require potentials that keep strong $1S$ binding while allowing larger suppression for $2S,3S$.', 'concept_5': 'Assume medium rearrangement is slow compared to quarkonium internal dynamics (adiabatic response)'}"	internal energy U(T,r).	理论基础学科-理论物理-分析力学	128180	2	3
recuUEYv4bv2Bl	Let $k$ be an algebraically closed field of characteristic $0$. Let $X \subset \mathbb{P}^n_k$ be a general smooth complete intersection of multidegree $(d_1, \dots, d_c)$ with $c < n$. Assume that $(d_1, \dots, d_c)$ satisfies one of the following conditions - $3\leq d_1 \leq d_2 \leq \dots \leq d_c$; - $c\geq 3$ and $2=d_1 =d_2 =d_3 \leq \dots \leq d_c$. Determine all possible $(d_1, \dots, d_c; n)$ such that the linear automorphism group $Aut_L(X)$ is nontrivial.	{'step_1': 'By concept_1, we onyl need to consider curves. By concept_2, the case of multi-degree $(2,2,2)$ curve on $\\mathbb{P}^4$ is excluded.', 'step_2': 'For plane cubics, recall that the affine linear transformations on the plane correspond to linear transformations of the projective plane. So by concept_4, concept_5, we may assume the curve is given by the Weierstrass form $y^2 = x^3+ax+b$. In this case, we have a nontrivial automorphism $[x:y:z] \\mapsto [x:-y:z]$.', 'step_3': 'To exclude other cases, we need to relate linear automorphism group with monodromy.', 'step_4': 'So the result is $(3;2)$.'}	{'concept_1': 'The linear automorphism group is trivial when $\\dim X \\geq 2$.', 'concept_3': 'The linear automorphism group is known to be $(\\mathbb{Z}/2\\mathbb{Z})^n$ for $(2,2;n)$.', 'concept_4': 'In characteristic $0$ or $p \\geq 5$, a plane cubic curve is called the Weierstrass normal form if the defining equation is of the form $y^2 = x^3 + ax +b$.', 'concept_5': 'Any plane cubic curve can be transformed to the Weierstrass normal form by affine linear transformation.'}	$(3;2)$	理论基础学科-基础数学-几何	164220	3	2
recuUG8X5Y8XeB	For the bumblebee theory, if there exists an external coupled matter field \(\mathcal{L}_M\), and the \(B\)-field is \((0, b(r), 0, 0)\), under the static spherically symmetric coordinate system \(ds^2 = -A(r) dt^2 + S(r) dr^2 + r^2 d\Omega^2\), then what is the black hole solution \(A(r)\)?	"{'step1': 'From theorem_1, take the action and set \\(\\ell=\\xi b^2\\). In the static spherical symmetry with \\(B_\\mu=(0,b_r(r),0,0)\\), use the constraint from concept_1 \\(b_\\mu b^\\mu=b^2 \\Rightarrow b_r(r)=b\\sqrt{S(r)}\\).', 'step2': ""From theorem_2 with the coupled electromagnetic field, the static spherically symmetric scalar potential \\(A_\\mu=(\\phi(r),0,0,0)\\) gives \\(F_{tr}=\\sqrt{AS}\\,\\phi'(r)\\). Solving the modified Maxwell equation yields \\(\\phi(r)=q/r\\)."", 'step3': ""Simplify the Einstein equations. From concept_2 we obtain \\([A(r)S(r)]'=0 \\Rightarrow S(r)=\\dfrac{C_1}{A(r)}\\), with \\(C_1=1+\\ell\\) (a gauge choice consistent with the bumblebee Schwarzschild–like solution)."", 'step4': ""Substitute \\(S(r)=\\dfrac{1+\\ell}{A(r)}\\) and \\(\\phi'(r)=-\\dfrac{q}{r^2}\\) into the given \\(\\{t,t\\}\\) component equation \\(\\frac{A' S'}{4 A S}-\\frac{A''}{2 A}+\\frac{A'^2}{4 A^2} +\\frac{S'}{r S}+\\frac{2\\,\\phi'^2 S}{2+\\ell}=0\\), simplify and expand in \\(1/r\\), then integrate to obtain \\(A(r)=1-\\frac{2M}{r}+\\frac{2(1+\\ell)\\,q^2}{(2+\\ell)\\,r^2}\\).""}"	"{'theorem_1': 'bumblebee theory: \\(S = \\int d^4x \\sqrt{-g} \\left[ \\frac{1}{2\\kappa} R + \\frac{\\xi}{2\\kappa} B^\\mu B^\\nu R_{\\mu\\nu} - \\frac{1}{4} B_{\\mu\\nu} B^{\\mu\\nu} \\right] - \\int d^4x \\sqrt{-g} \\, \\mathcal{L}_M\\)', 'theorem_2': ""We consider the matter field to be an electromagnetic field nonminimally coupled with the bumblebee vector field. Its Lagrangian density expression reads as \\(\\mathcal{L}_M = \\frac{1}{2\\kappa} \\left( F^{\\mu\\nu}F_{\\mu\\nu} + \\gamma B^\\mu B_\\mu F^{\\alpha\\beta} F_{\\alpha\\beta} \\right)\\), Then through Maxwell equation, we can derive \\(F_{tr} = \\sqrt{A(r)S(r)} \\phi'(r)\\). Building upon this, we solve the modified Maxwell’s equations to obtain, \\(\\phi(r) = \\frac{q}{r}\\)."", 'concept_1': 'For convenience, we set \\(\\ell = \\xi b^2\\) as Lorentz-violating parameter, we consider a spacelike background \\(b_\\mu\\) with the form \\(b_\\mu = (0,\\, b_r(r),\\, 0,\\, 0)\\). Utilizing the aforementioned condition \\(b_\\mu b^\\mu = b^2 = \\text{const}\\), we can derive \\(b_r(r) = b \\sqrt{S(r)}\\).', 'concept_2': ""By simplifying Einstein equations, we obtain: \\([S(r) A(r)]' = 0\\), which implies that \\(S(r) = \\dfrac{C_1}{A(r)}\\), where \\(C_1\\) is a constant. Similar to the Schwarzschild-like black hole solution in bumblebee gravity, we set the value of \\(C_1\\) to be \\(1 + \\ell\\). Solving Einstein equations \\(\\{t,t\\}\\), we can get \\(A(r)\\). \\(\\frac{A'(r) S'(r)}{4 A(r) S(r)} - \\frac{A''(r)}{2 A(r)} + \\frac{A'(r)^2}{4 A(r)^2} + \\frac{S'(r)}{r S(r)} + \\frac{2 \\phi'(r)^2 S(r)}{(2 + \\ell)} = 0\\)""}"	A(r) = 1 − \frac{2M}{r} + \frac{2 (1 + l) q^2}{(2 + l) r^2}	理论基础学科-理论物理-分析力学	122112	2	3
recuUGr5roIfnE	Answer the following questions without searching online: For a sequence of bounded convex domains in R^3 with fixed volume and diameters tending to infinity, does the first positive curl eigenvalue always have a positive lower bound?	"{'step_1': 'Transform the problem of the abstract sequence of convex domains into a problem on a specific geometric shape (cylinders). For any sequence of convex domains {\\Omega_n} with fixed volume and diameters d_n tending to infinity, we can easily find a corresponding sequence of ""flat and wide"" cylinders {C_n}. Each convex domain \\Omega_n is completely contained within the corresponding cylinder C_n, that is, \\Omega_n \\subset C_n. The height 2h_n of these cylinders tends to 0, and the radius R_n tends to infinity. Through this step, the original problem is successfully associated with a sequence of more concrete and regular geometric objects.', 'step_2': 'By applying Theorem 1, we can directly prove that if one domain is contained within another, then its first positive curl eigenvalue is not less than that of the outer domain. Based on the inclusion relation \\Omega_n \\subset C_n from Step 1, we can directly derive the inequality \\mu_1(\\Omega_n) \\ge \\mu_1(C_n). So far, proving that the eigenvalues \\mu_1(\\Omega_n) of the original sequence tend to infinity is equivalent to proving that its lower bound, i.e., the eigenvalues \\mu_1(C_n) of the sequence of cylinders, tend to infinity. This makes the problem more focused.', 'step_3': 'Now, the focus shifts to analyzing the first eigenvalue on the cylinders C_n. For the first eigenfield u_1 on C_n, according to Theorem 1, its eigenvalue \\mu_1(C_n) and the inverse operator of the curl operator, curl^{-1}, satisfy the relation: \\int_{C_n} curl^{-1} u_1 \\cdot u_1 dx = \\frac{1}{\\mu_1(C_n)} ||u_1||_{L^2}^2. According to Theorem 2, we introduce the Biot-Savart (BS) operator: curl^{-1} can be expressed via the Biot-Savart operator BS(u_1), and the above integral is exactly \\int_{C_n} BS(u_1) \\cdot u_1 dx. Combining the above relation and applying Theorem 3, we can obtain \\frac{1}{\\mu_1(C_n)} \\le ||BS(u_1)||_{L^2} / ||u_1||_{L^2}. To obtain the lower bound of \\mu_1(C_n), it is only necessary to estimate the upper bound of the L^2 norm of the Biot-Savart operator.', 'step_4': 'The final step is to specifically calculate the bound of the norm of the Biot-Savart operator and draw the final conclusion by combining the geometric constraints from Step 1. Applying Theorem 3 again, we know that ||BS(u_1)||_{L^2}^2 \\le \\frac{M^2}{16\\pi^2} ||u_1||_{L^2}^2, where M is defined as the supremum of the integral \\int_{C_n} \\frac{1}{|x-y|^2} dy over all x \\in C_n. It is easy to see through specific expansion and calculation that M \\to 0. Since the lower bound of \\mu_1(C_n) is \\frac{4\\pi}{M}, when M \\to 0, \\mu_1(C_n) \\to \\infty. Then, according to the inequality \\mu_1(\\Omega_n) \\ge \\mu_1(C_n) from Step 2, we finally prove that \\mu_1(\\Omega_n) \\to \\infty.'}"	{'Theorem_1': 'The first positive curl eigenvalue \\mu_1(\\Omega) is given by the infimum: \\mu_{1}(\\Omega) = \\inf_{w\\in\\mathcal{K}(\\Omega),\\ \\mathcal{H}(w)>0} \\frac{||w||_{L^{2}(\\Omega)}^{2}}{\\mathcal{H}(w)} Where: * **w** is a test vector field. * **\\mathcal{K}(\\Omega)** is the space of divergence-free vector fields that are orthogonal in L^2 to all curl-free fields tangent to the boundary. * **\\mathcal{H}(w)** is the **helicity** of the vector field w, defined as: \\mathcal{H}(w) := \\int_{\\Omega} \\text{curl}^{-1}w \\cdot w \\,dx Here, \\text{curl}^{-1}w represents the vector potential of w (often found via the Biot-Savart operator) that lies within the space \\mathcal{K}(\\Omega).', 'Theorem_2': 'Given a bounded domain \\Omega \\subset \\mathbb{R}^3 and a vector field u defined on \\Omega, the Biot-Savart operator is defined by the formula: BS(u)(x) := \\frac{1}{4\\pi} \\int_{\\Omega} \\frac{u(y) \\times (x - y)}{|x - y|^3} dy', 'Theorem_3': 'For any two real vector fields, **f** and **g**, defined on a domain \\Omega \\subset \\mathbb{R}^3, the inequality states: \\left| \\int_{\\Omega} \\mathbf{f}(x) \\cdot \\mathbf{g}(x) \\,dx \\right| \\le \\left( \\int_{\\Omega} |\\mathbf{f}(x)|^2 \\,dx \\right)^{1/2} \\left( \\int_{\\Omega} |\\mathbf{g}(x)|^2 \\,dx \\right)^{1/2} This is more compactly written using the L^2 inner product \\langle \\cdot, \\cdot \\rangle and the L^2 norm ||\\cdot||_{L^2} as: |\\langle \\mathbf{f}, \\mathbf{g} \\rangle| \\le ||\\mathbf{f}||_{L^2} ||\\mathbf{g}||_{L^2}'}	Yes. (Relying on geometric intuition for guesswork or calculating only a few special cases is not considered a correct solution. A proper answer at least should use the Rayleigh quotient (Theorem_1) and provide some correct estimates.)	理论基础学科-基础数学-偏微分方程	478280	2	4
recuUHnctMymfT	Consider the set of $\epsilon$-Fano type varieties. By the Birkar-BAB theorem, this set forms a bounded family. In particular, there exists a positive real number $v$ such that the anti-canonical volumes of $\epsilon$-Fano type threefolds are at most $v$. Find the asymptotic order of $\epsilon$ with respect to $v$. The final answer should be written as $v \sim ***$, and include your arguments.	{'step_1': 'we consider $d=3$. By concept_2, $(X,(1-1/q)E)$ is a log Fano pair. In particular, $K_X+(1-1/q)E$ is ample. So $Vol(-K_X) \\geq Vol(-K_X-(1-1/q)E) = u_{d+1,q}/q^d$.', 'step_2': 'Note that $u_{n,q}$ is a polynomial of $q$ of degree $2^{n-1}$ by direct computation, so the volume is asymptotically a polynomial of degree $2^d-(d+1) = 4$.', 'step_3': 'we can see directly that the example in concept_2 is $1/q$-lc, so we may take $q \\sim 1/\\epsilon$. So the result is $v \\sim \\epsilon^{-4}$.'}	{'concept_1': 'Let $q,r$ be two positive integers. Let $u_{i,q}$ ($i \\in \\mathbb{Z}_{>0}$) be a sequence of integers defined recursively as follows: $u_{1,q} = q$, $u_{p+1,q} = u_{p,q}(u_{p,q} +1)$ for any $k \\in \\mathbb{Z}_{>0}.$', 'concept_2': 'Let $e_1,\\dots,e_d$ be the standard basis of $\\mathbb{Z}^d$, and $e_0 = \\sum_{i=1}^d − u_{d+1,q}/(q(1 +u_{i,q})) e_i$. Let $X$ be the variety corresponding to the fan given by $e_0,\\dots,e_n$, and $E_0$ be the divisor corresponding to $e_0$. Then $(X,(1− 1/q )E_0)$ is a log Fano pair.', 'concept_3': 'By direct computation of polytopes, the volume of $K_X+(1-1/q)E_0$ is $u_{d+1,q}/q^{d+1}$.'}	$v \sim \epsilon^{-4}$ or $v \sim \frac{1}{\epsilon^{4}}$	理论基础学科-基础数学-几何	177171	3	1
recuUJS7kbGGBy	As a model that characterizes the combined stochastic dynamics of particle diffusion and chemical reactions within spatially inhomogeneous systems, the Chemical Diffusion Master Equation (CDME) is formulated as an infinite-dimensional coupled system of equations, which presents substantial difficulties for direct solution. What are the steps for solving the CDME in dimensionality reduction? Please explain in detail.	"  ""Step 1: Acknowledge the Complexity of CDME"": {  ""Core Nature"": ""The Chemical Diffusion Master Equation (CDME) is inherently an infinite-dimensional coupled system of Fokker-Planck equations, which is the root cause of its computational intractability."",  ""Detailed Explanation"": [  ""Infinite-Dimensional Structure: The CDME describes the probabilistic evolution of a system with an arbitrary number of particles (\(n \geq 0\)). For each particle count \(n\), there exists a corresponding Fokker-Planck equation for the probability density \(\rho_n(t, x_1, ..., x_n)\), where \(x_1, ..., x_n \in [0,1]^n\) are the spatial coordinates of the \(n\) particles. This results in an infinite set of equations, one for each \(n\)."",  ""Strong Coupling Between Equations: The equations for different \(n\) are tightly coupled. For example, the time derivative of \(\rho_n\) depends on \(\rho_{n+2}\) (via mutual annihilation reactions, where two particles disappear) and \(\rho_{n-1}\) (via creation reactions, where one particle is added). This coupling spans all \(n\), forming an infinite chain of dependencies that cannot be decoupled directly."",  ""Direct Solution Challenges: Traditional numerical methods (e.g., finite differences, finite elements) cannot handle infinite-dimensional systems or unbounded coupling. Solving each equation individually is impossible due to the infinite number of equations and their cross-dependencies, necessitating a transformative approach to reduce complexity.""  ""Step 2: Define the Infinite-Dimensional Moment Generating Function"": {  ""Core Purpose"": ""Unify the infinite set of CDME solutions into a single stochastic process, transforming the infinite-dimensional coupled system into a single equation in an abstract space."",  ""Detailed Explanation"": [  ""Definition of \(\Phi(t)\): The moment generating function is defined as \(\Phi(t) = \sum_{n \geq 0} I_n(\rho_n(t, \cdot))\), where \(I_n\) denotes the \(n\)-th order multiple Itô integral. This integral operator maps the \(n\)-dimensional probability density \(\rho_n(t, x_1, ..., x_n)\) to a random variable in the Wiener chaos expansion framework, which decomposes random variables into sums of multiple Itô integrals."",  ""Unification of Solutions: By summing over all \(n\), \(\Phi(t)\) consolidates the scattered solutions \(\{\rho_n\}_{n \geq 0}\) into a single object. This leverages the structure of Wiener chaos, where infinite-dimensional dynamics can be represented compactly using integral operators."",  ""Abstract Equation for \(\Phi(t)\): Through rigorous derivation (Lemma 3.3 in the original paper), \(\Phi(t)\) is proven to satisfy an abstract evolution equation in the space \(F^*\) (a space of generalized random variables): \n\[  \partial_t \Phi = d\Gamma(-\mathcal{A}) \Phi + \frac{\lambda_d}{2} D_1^2 \Phi - \frac{\lambda_d}{2} N(N-I) \Phi + D_{\lambda_c}^* \Phi - \gamma \Phi  \] \nwith initial condition \(\Phi(0) = 1\). Here, \(d\Gamma(-\mathcal{A})\) models diffusion (linked to the Laplacian operator), \(D_1^2\) and \(N(N-I)\) model annihilation reactions, \(D_{\lambda_c}^*\) models creation reactions, and \(-\gamma \Phi\) accounts for creation loss. This equation replaces the infinite coupled system with one equation, laying the foundation for dimensionality reduction.""  ""Step 3: Implement Finite-Dimensional Projections"": {  ""Core Purpose"": ""Truncate the infinite-dimensional problem into a finite-dimensional subspace using orthogonal projections, preserving key dynamics while reducing complexity."",  ""Detailed Explanation"": [  ""Eigenfunction Basis Selection: The orthogonal projection operator \(\Pi_N\) is defined to project the infinite-dimensional space onto the subspace spanned by the first \(N\) eigenfunctions \(\{\xi_1, ..., \xi_N\}\) of the diffusion operator \(-\mathcal{A} = \partial_x^2\) under Neumann boundary conditions. These eigenfunctions are explicitly \(\xi_k(x) = \sqrt{2} \cos((k-1)\pi x)\) with corresponding eigenvalues \(\alpha_k = (k-1)^2 \pi^2\). They form a complete orthogonal basis for describing spatial variations in the domain \([0,1]\), ensuring critical spatial information is retained."",  ""Second Quantization for Projection: The finite-dimensional projection of \(\Phi(t)\) is defined as \(\Phi_N(t) = \Gamma(\Pi_N) \Phi(t)\), where \(\Gamma(\Pi_N)\) is the second quantization operator. Second quantization extends the spatial projection \(\Pi_N\) to the Wiener chaos framework: it applies \(\Pi_N^{\otimes n}\) (the \(n\)-fold tensor product of \(\Pi_N\)) to the \(n\)-th chaos component of \(\Phi(t)\), truncating higher-dimensional spatial variations beyond the first \(N\) eigenfunctions."",  ""Dimensionality Reduction Effect: This operation confines \(\Phi(t)\) to an \(N\)-dimensional subspace, eliminating dependencies on eigenfunctions beyond \(\xi_N\). The resulting \(\Phi_N(t)\) retains the essential dynamics (diffusion, creation, annihilation) but in a finite-dimensional setting, making analytical or numerical solution feasible.""  ""Step 4: Derive and Solve the Finite-Dimensional PDE"": {  ""Core Purpose"": ""Convert the finite-dimensional projection \(\Phi_N(t)\) into a solvable partial differential equation (PDE), whose solution approximates the original CDME solution."",  ""Detailed Explanation"": [  ""Representation of \(\Phi_N(t)\) as \(u_N(t, z)\): It is proven that \(\Phi_N(t)\) can be expressed as a function of the first-order Itô integrals of the eigenfunctions: \(\Phi_N(t) = u_N(t, I_1(\xi_1), ..., I_1(\xi_N))\), where \(z = (I_1(\xi_1), ..., I_1(\xi_N)) \in \mathbb{R}^N\) and \(I_1(\xi_k)\) is the first-order Itô integral of \(\xi_k\). This links the abstract stochastic process \(\Phi_N(t)\) to a deterministic function \(u_N\) of \(N\) variables."",  ""Derivation of the PDE for \(u_N\): By decomposing the abstract equation satisfied by \(\Phi_N(t)\) (from Step 3) into operators acting on \(u_N\), a fourth-order linear PDE is derived: \n\[  \begin{cases}   \partial_t u_N(t, z) = -\sum_{k=1}^N \alpha_k \partial_k^* \partial_k u_N(t, z) + \frac{\lambda_d}{2} \partial_1^2 u_N(t, z) \\  \quad \quad -\frac{\lambda_d}{2} \sum_{j,k=1}^N \partial_j^* \partial_k^* \partial_j \partial_k u_N(t, z) + \sum_{k=1}^N c_k \partial_k^* u_N(t, z) - \gamma u_N(t, z), \\  u_N(0, z) = 1   \end{cases}  \] \nHere: \( \partial_k^* = -\partial_k + z_k \) (Gaussian divergence operator), \( \alpha_k \) are eigenvalues of \(-\mathcal{A}\), \( c_k = \langle \lambda_c, \xi_k \rangle_{L^2([0,1])} \) (projections of the creation rate), and terms correspond to diffusion (\(-\sum \alpha_k \partial_k^* \partial_k\)), annihilation (\(-\frac{\lambda_d}{2} \sum \partial_j^* \partial_k^* \partial_j \partial_k\)), creation (\(\sum c_k \partial_k^*\)), and creation loss (\(-\gamma\))."",  ""Solving the PDE and Recovering CDME Solutions: Solving this \(N\)-dimensional PDE yields \(u_N(t, z)\). Using the Stroock-Taylor formula and properties of Malliavin derivatives, the finite-dimensional projection of the CDME solution is recovered as: \n\[  \Pi_N^{\otimes n} \rho_n(t, x_1, ..., x_n) = \frac{1}{n!} \sum_{j_1,...,j_n=1}^N \mathbb{E}[\partial_{j_1} \cdots \partial_{j_n} u_N(t, I_1(\xi_1), ..., I_1(\xi_N))] \xi_{j_1}(x_1) \cdots \xi_{j_n}(x_n)  \] \nThis provides an approximate solution to the original infinite-dimensional CDME by leveraging the finite-dimensional PDE solution."""	"  ""concept_1"": ""Chemical Diffusion Master Equation (CDME): A model describing the joint stochastic process of particle diffusion and chemical reactions in spatially inhomogeneous systems, formulated as an infinite-dimensional coupled system of Fokker-Planck equations. The core equation is: \n\[  \left\{ \begin{aligned}   \partial_{t} \rho_{n}\left(t, x_{1}, ..., x_{n}\right)= & \sum_{i=1}^{n} \partial_{x_{i}}^{2} \rho_{n}\left(t, x_{1}, ..., x_{n}\right) \\   & +\frac{(n+2)(n+1)}{2} \int_{[0,1]^{2}} \lambda_{d}(x, y) \rho_{n+2}\left(t, x_{1}, ..., x_{n}, x, y\right) d x d y \\   & -\sum_{i<j} \lambda_{d}\left(x_{i}, x_{j}\right) \cdot \rho_{n}\left(t, x_{1}, ..., x_{n}\right) \\   & +\frac{1}{n} \sum_{i=1}^{n} \lambda_{c}\left(x_{i}\right) \rho_{n-1}\left(t, x_{1}, ..., x_{i-1}, x_{i+1}, ..., x_{n}\right) \\   & -\int_{[0,1]} \lambda_{c}(y) d y \cdot \rho_{n}\left(t, x_{1}, ..., x_{n}\right),   \end{aligned} \right.  \]   for \(n \geq 0, t>0,\left(x_{1}, ..., x_{n}\right) \in[0,1]^{n}\) ."",  ""concept_2"": ""Infinite-dimensional moment generating function: A function defined as \(\Phi(t) := \sum_{n \geq 0} I_n\left(\rho_n(t, \cdot)\right)\) that integrates CDME solutions into a unified stochastic process . It satisfies the abstract equation: \n\[  \begin{cases}   \partial _{t}\Phi =d\Gamma (-\mathcal {A})\Phi +\frac {\lambda _{d}}{2}D_{1}^{2}\Phi -\frac {\lambda _{d}}{2}N(N-I)\Phi +D_{\lambda _{c}}^{* }\Phi -\gamma \Phi ; \\   \Phi (0)=1,   \end{cases}  \]   in \(F^{*}\) ."",  ""concept_3"": ""Multiple Itô integral (\(I_n\)): A stochastic integral used to construct the infinite-dimensional moment generating function, representing the n-th order integral of symmetric functions with respect to Brownian motion. Defined as: \n\[  I_{n}\left(h_{n}\right):=n ! \int_{0}^{1} \int_{0}^{x_{1}} \cdots \int_{0}^{x_{n-1}} h_{n}\left(x_{1}, ..., x_{n}\right) d B_{x_{n}} \cdots d B_{x_{2}} d B_{x_{1}}  \]   for symmetric \(h_n \in L_s^2([0,1]^n)\) ."",  ""concept_4"": ""Orthogonal projection operator (\(\Pi_N\)): Projects the infinite-dimensional space onto the finite-dimensional space.The finite-dimensional projection of \(\Phi(t)\) is: \n\[  \Phi _{N}(t):=\Gamma (\Pi _{N})\Phi (t)  \] ."",  ""concept_5"": ""Eigenfunctions of diffusion operator: \(\{\xi_1, \ldots, \xi_N\}\) are eigenfunctions of \(-\mathcal{A} = \partial_x^2\) under Neumann boundary conditions, spanning the finite-dimensional projection space."",  ""concept_6"": ""Second quantization operator \(\Gamma(\Pi_N)\): Used to define the finite-dimensional projection \(\Phi_N(t) := \Gamma(\Pi_N) \Phi(t)\)"",  ""theorem_1"": ""PDE transformation: The finite-dimensional projection \(\Phi_N(t)\) can be represented as a function \(u_N(t, z)\) that satisfies a fourth-order linear PDE . The PDE is: \n\[  \begin{cases}   \partial _{t} u_{N}(t, z)=-\sum _{k=1}^{N}\alpha _{k}\partial _{k}^{*} \partial _{k}u_{N}(t, z)+\frac {\lambda _{d}}{2}\partial _{1}^{2} u_{N}(t, z) \\   \quad \quad -\frac {\lambda _{d}}{2}\sum _{j, k=1}^{N}\partial _{j}^{*} \partial _{k}^{*} \partial _{j} \partial_{k}u_{N}(t,z) +\sum _{k=1}^{N}c_{k} \partial_{k}^{* } u_{N}(t, z)-\gamma u_{N}(t, z) ; \\   u_{N}(0, z)=1,   \end{cases}  \]   for \(t>0, z \in \mathbb {R}^{N}\) ."""	"The dimensionality reduction and solution of the Chemical Diffusion Master Equation (CDME), an infinite-dimensional coupled system of equations, are primarily achieved through an infinite-dimensional moment generating function method combined with finite-dimensional projections, with the specific process as follows:  1. Constructing an infinite-dimensional moment generating function to transform the problem  First, an infinite-dimensional moment generating function \(\Phi(t)\) is defined to integrate the solutions \(\{\rho_n\}_{n \geq 0}\) of the CDME into a unified stochastic process:  \(  \Phi(t) := \sum_{n \geq 0} I_n\left(\rho_n(t, \cdot)\right)  \)    where \(I_n\) denotes the \(n\)-th order multiple Itô integral. Through derivation, it is proven that \(\Phi(t)\) satisfies an abstract equation, transforming the infinite-dimensional coupled system of equations into a single equation in an abstract space and laying the foundation for subsequent dimensionality reduction.  2. Achieving dimensionality reduction via finite-dimensional projections  An orthogonal projection operator \(\Pi_N\) is introduced, which projects the infinite-dimensional space onto the finite-dimensional space spanned by the first \(N\) eigenfunctions \(\{\xi_1, \ldots, \xi_N\}\) (the eigenfunctions are those of the diffusion operator \(-\mathcal{A} = \partial_x^2\) under Neumann boundary conditions). A finite-dimensional projection \(\Phi_N(t)\) is defined as:  \(  \Phi_N(t) := \Gamma(\Pi_N) \Phi(t)  \)    where \(\Gamma(\Pi_N)\) is a second quantization operator, implementing finite-dimensional truncation of \(\Phi(t)\). This step confines the infinite-dimensional problem to an \(N\)-dimensional space, completing dimensionality reduction.  3. Transforming to a finite-dimensional PDE for solution  The dimension-reduced \(\Phi_N(t)\) is proven to satisfy a finite-dimensional equation. Further derivation shows that \(\Phi_N(t)\) can be expressed as a function \(u_N(t, z)\) (\(z \in \mathbb{R}^N\)), i.e.:\(  \Phi_N(t) = u_N\left(t, I_1(\xi_1), \ldots, I_1(\xi_N)\right)  \)  where \(I_1(\xi_k)\) is the first-order Itô integral, and \(\xi_k(x)=\sqrt{2}\cos((k - 1)\pi x)\) (\(x\in[0,1]\)) are the eigenfunctions of the diffusion operator \(-\mathcal{A}=\partial_x^2\) under Neumann boundary conditions . Eventually, \(u_N\) satisfies an \(N\)-dimensional fourth-order linear partial differential equation (PDE):\(  \begin{cases}   \partial_t u_N(t, z) = -\sum_{k=1}^N \alpha_k \partial_k^* \partial_k u_N(t, z) + \frac{\lambda_d}{2} \partial_1^2 u_N(t, z) \\  \quad \quad -\frac{\lambda_d}{2} \sum_{j,k=1}^N \partial_j^* \partial_k^* \partial_j \partial_k u_N(t, z) + \sum_{k=1}^N c_k \partial_k^* u_N(t, z) - \gamma u_N(t, z), \\  u_N(0, z) = 1   \end{cases}  \)  In this PDE, \(\alpha_k=(k - 1)^2\pi^2\) are the eigenvalues corresponding to the eigenfunctions \(\xi_k\) of the diffusion operator \(-\mathcal{A}\), quantifying the influence of spatial frequency variations on diffusion behavior . \(\lambda_d\) is the constant stochastic rate function for the mutual annihilation reaction (II) \(S + S\stackrel{\lambda_d}{\to}\emptyset\), governing the rate of particle loss due to annihilation . \(c_k=\langle\lambda_c,\xi_k\rangle_{L^2([0,1])}\) represents the projection of the creation rate function \(\lambda_c(x)\) onto the eigenfunction \(\xi_k\), reflecting the contribution of the creation reaction (I) \(\emptyset\stackrel{\lambda_c(x)}{\to}S\) in the spatial mode described by \(\xi_k\) . \(\gamma=\int_{[0,1]}\lambda_c(z)dz\) is the total creation rate, characterizing the overall intensity of particle generation . The symbol \(\partial_k^*\) denotes the Gaussian divergence operator \(-\partial_k + z_k\), which arises from the Gaussian Malliavin calculus techniques used in the derivation .  The dimension of this PDE is consistent with the dimension \(N\) of the projection space. By solving this finite-dimensional PDE, the finite-dimensional projection \(\Pi_N^{\otimes n}\rho_n\) of the CDME solution can be obtained, enabling the approximate solution of the original infinite-dimensional problem.    In summary, through the path of ""infinite-dimensional moment generating function transformation → finite-dimensional projection for dimensionality reduction → finite-dimensional PDE solution"", the infinite-dimensional coupled problem of the CDME is successfully transformed into a solvable finite-dimensional problem.    # Checklist:  1. The dimensionality reduction and solution of the CDME equation must strictly follow these three steps: 1st. Infinite-dimensional transformation → 2nd. Finite-dimensional projection → 3rd. Finite PDE solution (5 points, full score 10 points)  2. In the second step, the orthogonal projection operator is first used for dimensionality reduction: project the infinite-dimensional space onto the subspace formed by the first n eigenfunctions (under Neumann boundary conditions) to restrict the problem to a finite-dimensional space. Second quantization in projection: use the second quantization operator to define finite-dimensional projection and realize the finite truncation of the infinite-dimensional process (+3 points)  3. In the third step, it is pointed out that the formula can be transformed into a finite-dimensional PDE for solution, so as to obtain the finite-dimensional projection of the CDME solution, and thus the approximate solution of the original infinite-dimensional problem can be obtained (+2 points)"	理论基础学科-理论化学-化学反应动力学理论	2063524	3	7
recuUMjpAfW9pl	Recall the Morrison-Kawamata cone conjecture: Let $(X,\Delta)$ be a klt Calabi-Yau pair, then (1) (Nef cone conjecture) There is exists a rational polyhedral fundamental for the action of $Aut(X,\Delta)$ on the effective nef cone; (2) (Movable cone conjecture) There is exists a rational polyhedral fundamental for the action of $PsAut(X,\Delta)$ on the effective movable cone; (3) (Effective cone conjecture) There is exists a rational polyhedral fundamental for the action of $PsAut(X,\Delta)$ on the effective cone, i.e. the cone generated by effective divisors; Assume abundance conjecture and answer the following question: Discuss the relations between these three statements, i.e. answer the following questions: 1. Does effective cone conjecture imply the movable cone conjecture? 2. Does effective cone conjecture imply the nef cone conjecture? 3. Does nef cone conjecture imply the movable cone conjecture? 4. Does nef cone conjecture imply the effective cone conjecture? The answer should be written as: Yes/No; Yes/No; Yes/No; Yes/No; and include your arguments.	{'step_1': 'Statement_3 and steatement_4 are not correct because the nef cone conjecture says nothing about the pseudo-automorphism group, so nef cone conjecture contains fewer information. (Because all these statements are conjectured to be true, there are no counterexamples.)', 'step_2': 'For statement_2, by Theorem 1(3), it suffices to prove that the effective cone conjecture implies Conjecture 1(2). So the problem is that we need to replace $PsAut$ by $Aut$. Indeed, it suffices to show that there are finitely many $g\\in PsAut(X,\\Delta)/Aut(X,\\Delta)$ such that $g\\cdot P_M$ intersect $P_A$. This is implied by the Shokurov polytope in a formal way using concept_4.', 'step_3': 'For statement_1, the argument is similar to statement_2. Note that the movable cone is non-degenerate since we are working on absolute case. By Theorem 1(3), it suffices to prove that the effective cone conjecture implies Conjecture 1(1). But Conjecture 1(1) follows directly from the statement of the effective cone conjecture since the movable cone is contained in the effective cone.'}	{'concept_1': '(Conjecture 1) Let $f : (X,\\Delta) \\to S$ be a klt Calabi-Yau fiber space. (1) There exists a polyhedral cone $P_M \\subset Eff(X/S)$ such that \\cup_{g\\in PsAut(X/S,\\Delta)} g \\cdot P_M \\supset Mov(X/S). (2) There exists a polyhedral cone $P_A \\subset Eff(X/S)$ such that \\cup_{g\\in Aut(X/S,\\Delta)} g \\cdot P_A \\supset Amp(X/S).', 'concept_2': '(Theorem 1) Let $f : (X,\\Delta) \\to S$ be a klt Calabi-Yau fiber space. Assume that good minimal models exist for effective klt pairs in dimension $\\dim(X/S)$. (1) If $R^1f_*\\mathcal{O}_X = 0$, then the weak cone conjecture for $Mov^e(X/S)$ is equivalent to the Conjecture 1 (1). (2) If $Mov(X/S)$ is non-degenerate, then the cone conjecture for $Mov^e(X/S)$ is equivalent to the Conjecture 1 (1); (3) The cone conjecture for $Amp^e(X/S)$ is equivalent to the Conjecture 1 (2).', 'concept_3': '(Shokurov polytope) Assume abundance conjecture. Fix a reduced divisor $F = \\sum_i D_i$ on a normal variety $X/Z$ of dimension $d$, where $Z$ is a normal affine variety. Let $B_F = \\bigoplus_i [0, 1]D_i$ and let $N_F$ be the subset of divisors $B$ in $B_F$ such that $(X/Z, B)$ admits a wlc model. Then the following hold: (1) The set $N_F$ is closed convex rational polyhedral. (2) The set $N_F$ is a finite disjoint union of wlc model equivalence classes, which are convex rational polyhedral (not necessarily closed).', 'concept_4': '(Polyhedral type cones) Let $V$ denote a real finite dimensional vector space equipped with a rational structure $V(\\mathbb{Q}) \\subset V$ and $C$ is an open nondegenerate convex cone in $V$. Let $C_+$ be the rational hull of $C$, i.e. the cone generated by rational points in the closure of $C$. Let $\\Gamma$ be a subgroup of ${\\rm GL}(V)$ which stabilizes $C$ and some lattice in $V(\\mathbb{Q})$. Then the following conditions are equivalent: $(i)$ There exists a polyhedral cone $\\Pi$ in $C_+$ with $\\Gamma \\cdot \\Pi = C_+$. $(ii)$ There exists a polyhedral cone $\\Pi$ in $C_+$ with $\\Gamma \\cdot \\Pi \\supset C$. If one of these equivalent conditions is fulfilled, we say that $(V(\\mathbb{Q}), C, \\Gamma)$ is a polyhedral triple or simply, that $(C_+, \\Gamma )$ is of polyhedral type.', 'concept_5': '(Mori chamber decomposition) Let $D_1$ and $D_2$ be two $\\mathbb{Q}$-Cartier divisors on $X$ with finitely generated section rings. Then we say $D_1$ and $D_2$ are Mori equivalent if the rational maps $f_{D_i}$ have the same Stein factorization i.e. there is an isomorphism between their images which makes the obvious triangular diagram com mutative. Let $X$ be a projective variety such that $R(X,L)$ is finitely generated for all line bundles $L$ and $Pic(X)_{\\mathbb{Q}} = N^1(X)$. By a Mori chamber of $N^1(X)$ we mean the closure of an equivalence class whose interior is open in $N^1(X)$. This gives a decomposition of the effective cone, which may helps the proof of effective cone conjecture.'}	Yes; Yes; No; No	理论基础学科-基础数学-几何	152767	1	4
recuUQud6zIEV4	What is the length of the Mochizuki 3-cocycle of the 7-dihedral quandle?	{'step_1': 'By classifying 3-cocycles with reference to the method used for the 5-dihedral quandle, due to differences in algebraic structures, the Mochizuki 3-cocycles of the 7-dihedral quandle are divided into four categories, while those of the 5-dihedral quandle fall into three categories.', 'step_2': 'Through individual verification, the method of elimination is used to prove that the length of the Mochizuki 3-cocycle of the 7-dihedral quandle is greater than or equal to 8.', 'step_3': 'Prove that the length of the Mochizuki 3-cocycle of the 7-dihedral quandle is less than or equal to the triple point number of a certain surface knot, which is the 2-twist-spun 5₂-knot, thereby determining the supremum.'}	{'concept1': 'For any \\(\\alpha \\in Z^n(X; A)\\), its length is defined as:\\(l(\\alpha) = \\max\\left\\{l(\\alpha, S) \\mid S \\text{ is an } X\\text{-set} \\right\\}\\). If the aforementioned maximum value does not exist, then \\(l(\\alpha) = \\infty\\) is defined.', 'concept2': 'The 5–dihedral quandle \\(X = R_5\\) is the set \\(\\mathbb{Z}_5 = \\{0; 1; \\dots; 4\\}\\) equipped with the binary operation \\(a^b \\equiv 2b - a \\pmod{5}\\). The map \\(\\theta_{\\text{M}} : C_3(R_5) \\to \\mathbb{Z}_5\\) defined by \\[\\theta_{\\text{M}}(x, y, z) = (x-y)*\\{\\frac{y^5}{5} + \\frac{(2z - y)^5 - 2z^5}{5}\\}\\] is a 3–cocycle in \\(Z^3(R_5; \\mathbb{Z}_5)\\) and is called the Mochizuki 3–cocycle.', 'Theorem1': 'For the 3-cocycle \\(\\zeta\\) of the 5-dihedral quandle \\(R_5\\), we have \\(l(\\zeta, Z \\times R_5) \\geq 8\\).', 'Theorem2': 'Let \\(X\\) be a quandle, \\(\\zeta\\) a 3-cocycle of \\(X\\), and \\(F\\) an oriented surface knot. If the cocycle invariant of \\(F\\) associated with \\(\\zeta\\) contains a non-zero element, then we have \\(l(\\zeta) \\leq t(F)\\). where \\(t(F)\\) is the triple point number of \\(F\\).', 'concept3': 'The triple point number of the 2-twist-spun 5₂-knot is less than or equal to 8.'}	The length of the Mochizuki 3-cocycle of the 7-dihedral quandle is 8.	理论基础学科-基础数学-拓扑	54700	1	2
recuUT7kcN6Y2T	Which of the CeO2(100), (110), and (111) surfaces provide the most stable anchoring site for a single Cu atom?	{'step_1': 'According to the concept_1, the CeO2 (100) and (110) faces have two-fold coordinated O atoms, suggesting there are oxygen-bridge sites on CeO2 (100) and (110) faces. The oxygen-bridge sites can strongly bond with Cu atoms, thus the CeO2 (100) and (110) faces are more active in comparison with the (111) surface.', 'step_2': 'According to the concept_2, the CeO2(110) is more stable than CeO2(100), suggesting the CeO2(110) is more inactive. Thus, the CeO2(100) can anchor Cu single atom more stable.'}	{'concept_1': 'The (111) surface is terminated by three-fold coordinated oxygen atoms, and the (110) and (100) surfaces are terminated by six-fold coordinated Ce as well as two-fold and three-fold coordinated O atoms.', 'concept_2': 'The CeO2 (110) is more stable than CeO2 (100)'}	The CeO2 (100) is the most stable crystal face for anchoring a single Cu atom	理论基础学科-理论化学-计算化学	19218	1	1
recuUUahdenp8C	For a non-zero multivariate polynomial $P(\overline{x})$ with $\mu$ variables and a maximum individual degree of at most $\delta$, a constructive proof is presented that encodes each root within a finite subcube $K^\mu$ into a unique code. Determine the total cardinality of this code set, where $K$ is a finite subset of the operational field.	"{'step_1': 'Begin with the given conditions: a non-zero $\\mu$-variate polynomial $P(\\overline{x})$ with maximum individual degree $\\delta$, a finite set $K \\subseteq \\mathbb{F}$, and a known non-root $\\overline{a} \\in \\mathbb{F}^\\mu$ where $P(\\overline{a}) \\neq 0$ (Concept_1). The objective is to find the size of the set of codes used for the roots in $K^\\mu$.', 'step_2': 'The core strategy is to devise an encoding scheme (Concept_4) that maps every root $\\overline{c} \\in K^\\mu$ to a code. The total number of possible codes will serve as the upper bound.', 'step_3': 'For any specific root $\\overline{c} = (c_1, \\dots, c_\\mu)$, apply a hybrid argument (Concept_2). Construct a sequence of points starting from the non-root $\\overline{a}$ and ending at the root $\\overline{c}$.', 'step_4': ""Since the polynomial's value transitions from non-zero to zero along this path, there must be a minimal index $k$ (where $1 \\le k \\le \\mu$) that marks this transition."", 'step_5': 'This transition identifies a specific univariate polynomial (Concept_3), $Q(t)$, by fixing all variables except the $k$-th one.', 'step_6': 'The polynomial $Q(t)$ is non-zero, has a degree of at most $\\delta$, and has $c_k$ as one of its roots.', 'step_7': 'Apply the Fundamental Theorem of Algebra (Theorem_1): $Q(t)$ can have at most $\\delta$ roots. Let $i$ (where $1 \\le i \\le \\delta$) be the index of $c_k$ among these roots.', 'step_8': 'The root $\\overline{c}$ is encoded using the tuple $(k, i, (c_1, \\dots, c_{k-1}, c_{k+1}, \\dots, c_\\mu))$.', 'step_9': 'To find the total cardinality of the code set, calculate the number of unique codes. The choices are: $k$: $\\mu$ possible values. $i$: $\\delta$ possible values. The remaining $\\mu-1$ components: $|K|^{\\mu-1}$ possibilities.', 'step_10': 'The total size of the code space is the product: $\\mu \\cdot \\delta \\cdot |K|^{\\mu-1}$. This is the upper bound on the cardinality of the code set.'}"	{'concept_1': 'A non-root $\\overline{a}$ is a point in the domain of a polynomial $P(\\overline{x})$ such that $P(\\overline{a}) \\neq 0$. The existence of at least one non-root certifies that the polynomial is not identically zero.', 'concept_2': 'A hybrid argument involves creating a sequence of intermediate objects to transition from a starting point to an endpoint.', 'concept_3': 'A univariate polynomial is a polynomial in a single variable. A multivariate polynomial can be reduced to a univariate one by fixing all but one of its variables.', 'concept_4': 'The encoding is a process of mapping it to a different, smaller set of identifiers (codes) from which can be uniquely recovered.', 'theorem_1': 'The Fundamental Theorem of Algebra (univariate case) states that a non-zero univariate polynomial of degree at most $\\delta$ over a field has at most $\\delta$ distinct roots.'}	The cardinality of the code set is at most $\delta \cdot \mu \cdot |K|^{\mu-1}$. 	理论基础学科-基础数学-代数	65813	2	4
recuUWJ40wj1vy	Let $q$ be a prime power. A polynomial $F \in \mathbb{F}_q[x]$ of positive degree is called a minimal value set polynomial (MVSP) if its value set attains the minimally possible size: $\#V_F = \left\lfloor \frac{q - 1}{\deg F} \right\rfloor + 1$. For a nonempty subset $S \subseteq \mathbb{F}_q$, denote by $P(S, q)$ the set of all MVSPs $F \in \mathbb{F}_q[x]$ such that $V_F = S$, and define $S^v := \{ s^v : s \in S \}$ for any positive integer $v$. Furthermore, define $R := \left\{ aU^v + b \mid a, b \in \mathbb{F}_q,\ a \ne 0,\ U \text{ is an } \mathbb{F}_{p^k}\text{-subspace of } \mathbb{F}_q \text{ with } 1 \in U,\ k \ge 1,\ v \mid p^k - 1 \right\}$, and $W := \left\{ S \subseteq \mathbb{F}_q \mid P(S, q) \ne \emptyset \right\}$. Please provide detailed proofs for each of the following questions:  \begin{itemize}  \item Let $U$ be an $\mathbb{F}_{p^k}$-subspace of $\mathbb{F}_q$ such that $1 \in U$, and let $F \in \mathbb{F}_q[X]$. Is it true that $F \in P(U, q)$ if and only if $F^v \in P(U^v, q)$?  \item Characterize all elements in $R \setminus W$.  \item Characterize all elements in $W \setminus R$.  \end{itemize}	{'step_1': 'By comparing the value set and the degree of the polynomial, one can directly prove their equivalence; or construct $T(X):=\\prod_{u\\in U}(X-u)=\\sum_{i=0}^mw_iX^{p^{ki}}$ and $T^*(X):=\\prod_{u\\in U^v}(X-u)=\\sum_{i=0}^mw_iX^{\\frac{p^{ki}-1}{v}+1}$, then we obtain $T^*(F^v)=F^{v-1}T(F)$ for all $F$, and thus using the necessary and sufficient condition of Theorem 2, we can prove their equivalence.', 'step_2': 'Note the restrictions on the number of elements in the set in Theorem 1 and Theorem 2. First, discuss the singleton set $\\{a\\}$, construct the polynomial $x^q-x+a$, and thus prove that all singleton sets belong to $W$ but not to $R$.', 'step_3': 'Since the value set of the minimal polynomial $F$ is the zero set of the corresponding $T_F$ in Theorem 1, therefore the root set $U_0$ of $T_F(x^v+\\gamma_0)$ satisfies $V_F=U_0^v+\\gamma_0$. Using (4) in Theorem 1, we deduce that the root set $U_0$ of $T_F(x^v+\\gamma_0)$ is an $\\mathbb{F}_{p^k}$-subspace.', 'step_4': 'Using the fact that $v\\mid q^k-1$, prove that $U_0$ is a subset of $\\mathbb{F}_q$. Take $a^{-1}\\in U_0$, then $U:=aU_0$ is an $\\mathbb{F}_{p^k}$-subspace in $\\mathbb{F}_q$ containing $1$, and $V_F=a^{-v}U^v+\\gamma_0$ is an element of $R$, therefore $W\\subset R$ always holds for non-singleton sets.', 'step_5': 'For $S=aU^v+b$, first construct $G\\in P(U,q)$. Using the first question, we obtain that $aG^v+b$ is the required polynomial. The construction of $G$ can be achieved using the properties of linear polynomials. Specifically, construct $T=\\prod_{u\\in U}(X-u)$, and there exists a $p$-linear polynomial $G$ such that $T(G(X))=X^q-X$. It can be immediately verified that $G\\in P(U,q)$. This proves $R\\subset W$.'}	"{'concept/theorem_1': 'Theorem 1 (Mills): Let $F$ be an mvsp over $\\mathbb{F}_q$ with value set $V_F=\\{\\gamma_0,\\ldots,\\gamma_r\\}$, where $|V_F|=r+1\\ge 3$, and the number of pre-images of $\\gamma_0$ by $F$ over $\\mathbb{F}_q$ is at most the number of pre-images of any $\\gamma_i$ for $1\\le i\\le r$. Set $L=\\gcd(F-\\gamma_0,\\,x^{q}-x)$ and $T=\\prod_{i=0}^{r}(x-\\gamma_i)$. Then there exist positive integers $m,k,v$ and polynomials $A,B,N\\in\\mathbb{F}_q[x]$, where $L\\nmid N$, such that the following hold.\n\\begin{itemize}\n\\item[(1)] $v$ divides $p^{k}-1$ and $vr+1=p^{mk}$;\n\\item[(2)] $F=L^{v}N^{p^{mk}}+\\gamma_0$;\n\\item[(3)] $L=A^{p^{mk}}x+B^{p}$;\n\\item[(4)] $\\displaystyle \\frac{T(x+\\gamma_0)}{x}=\\sum_{i=0}^{m} w_ix^{\\frac{p^{ki}-1}{v}}$ for some $w_0,\\ldots,w_m\\in\\mathbb{F}_q$ with $w_0\\neq 0$.\n\\end{itemize}', 'concept/theorem_2': ""Theorem 2: Let $F\\in\\mathbb{F}_q[x]$ be a polynomial with value set $V_F=\\{\\gamma_0,\\ldots,\\gamma_r\\}$, $r>1$. Then $F$ is an mvsp iff there exist a monic polynomial $T\\in\\mathbb{F}_q[x]$ and $\\theta\\in\\mathbb{F}_q^{\\*}$ such that\n$$\nT(F)=\\theta\\,(x^{q}-x)\\,F'.\n$$\nIn addition, the polynomial $T$ is such that $T=\\prod_{i=0}^{r}(x-\\gamma_i)$, and if $\\gamma_0,\\ldots,\\gamma_r$ are ordered as in Theorem 1, then $\\theta=-T'(\\gamma_i)$ for all $\\gamma_i\\in V_F\\setminus\\{\\gamma_0\\}$.""}"	\begin{itemize}  \item Yes, it is true.   \item There is no element that is in $R$ but not in $W$ or $\emptyset$.   \item $\{S\subset \mathbb{F}_q: |S|=1\}$ is the set of the elements that are in set W but not in set R.   \end{itemize}	理论基础学科-基础数学-数论	77955	1	3
recuUYbsa4x8Iw	In the reduction from One-Sided Grundy Total Domination (with input sequence length $k$) to L-Grundy Domination*, what is the exact relationship between the feasible sequence length $\ell$ in the new graph $G'$ and $k$?	"{'step_1': ""Start from an instance $(G,k)$ of One-Sided Grundy Total Domination where $G$ is bipartite with $V(G)=A\\dot\\cup B$ and there is a length-$k$ Total sequence $\\sigma=(v_1,\\dots,v_k)$ on side $A$. Construct $G'$ by adding two new vertices $a_1,a_2$ to $A$ and two new vertices $b_1,b_2$ to $B$, obtaining $A'=A\\cup\\{a_1,a_2\\}$ and $B'=B\\cup\\{b_1,b_2\\}$. Make both $A'$ and $B'$ cliques (every two vertices inside each part are adjacent), and keep all original cross edges between $A$ and $B$."", 'step_2': 'Use the sequence semantics. A Total sequence satisfies $N(v_i)\\setminus \\bigcup_{j<i}N(v_j)\\neq\\varnothing$ for every $i$. An L sequence satisfies $N[v_i]\\setminus \\bigcup_{j<i}N(v_j)\\neq\\varnothing$ for every $i$. Here $N(\\cdot)$ is the open neighborhood and $N[\\cdot]$ is the closed neighborhood; validation uses footprints (each step must newly cover at least one previously uncovered vertex).', 'step_3': ""Fixed 'bookend' pairs in the construction. Add two vertices $\\{a_1,a_2\\}$ to $A$ to form $A'$, and two vertices $\\{b_1,b_2\\}$ to $B$ to form $B'$; make $A'$ and $B'$ cliques (so $a_1,a_2$ are adjacent to all vertices in $A'$, and $b_1,b_2$ are adjacent to all vertices in $B'$). Keep all original cross edges between $A$ and $B$. By the definitions above, each pair $\\{a_1,a_2\\}$ and $\\{b_1,b_2\\}$ can 'footprint' each other on their own sides, without interfering with the opposite side."", 'step_4': ""Footprint preservation of the original sequence in the constructed graph. If $\\sigma = (v_1, …, v_k)$ is a Total sequence on side $A$ of the original graph, then in the constructed graph $G'$ these $k$ steps remain valid: the two added pairs ($\\{a_1,a_2\\},\\{b_1,b_2\\}$) are part of cliques $A'$ and $B'$, and the original cross edges between $A$ and $B$ are preserved. Thus, the footprint witnesses $w_i \\in B$ for each $v_i$ in $\\sigma$ are not 'stolen' by the added pairs, ensuring the $k$ steps stay valid."", 'step_5': ""Form a concrete valid L sequence in $G'$: $(a_1,\\, a_2,\\, v_1,\\dots,v_k,\\, b_1,\\, b_2)$. By Steps 3–4 every step satisfies the L condition, hence the feasible length $\\ell$ obeys $\\ell\\ge k+4$."", 'step_6': ""Argue the upper bound. In the clique $A'$, $N(a_1)\\cup N(a_2)=(A'\\setminus\\{a_1\\})\\cup(A'\\setminus\\{a_2\\})=A'$, so any further $x\\in A'$ has $N[x]\\setminus \\bigl(N(a_1)\\cup N(a_2)\\bigr)=\\varnothing$ and cannot extend the L sequence; the same holds for $B'$ after $b_1,b_2$. Thus the two cliques contribute at most two L steps each, and the middle block contributes at most the preserved $k$ steps, giving $\\ell\\le k+4$."", 'step_7': 'Combine the bounds to conclude $\\ell=k+4$.'}"	{'concept_1': 'Total sequence: for all i, N(v_i) \\ (⋃{j<i} N(v_j)) ≠ ∅.', 'concept_2': 'L sequence: for all i, N[v_i] \\ (⋃{j<i} N(v_j)) ≠ ∅. Here N(·) denotes the open neighborhood and N[·] the closed neighborhood. Validation uses the “footprint” semantics: at each step, the chosen vertex must newly cover at least one vertex that has not been covered before.', 'concept_3': 'Fixed “bookend” pairs in the construction. Add on side A′ a pair of vertices {a1, a2} that are adjacent only to each other and have no edges to B′; add on side B′ a symmetric pair {b1, b2}. By the definitions above, each pair can “footprint” each other on its own side, without interfering with the opposite side.', 'concept_4': 'Footprint preservation of the original sequence in the constructed graph. If σ = (v1, …, vk) is a Total sequence on side A of the original graph, then in the constructed graph G′ these k steps remain valid: the two added pairs connect only within their own sides and therefore do not “steal” the footprint witnesses that A uses to cover B.'}	$\ell = k + 4$	理论基础学科-理论计算机科学-计算复杂度	661736	3	2
recuUWXMaPT8sk	For the Test Cover problem, when parameterized by the solution size $k$, what is the optimal dependence of the running time of a parameterized algorithm on $k$?	"{'step_1': 'If $|U|>2^k$, immediately return \\textsc{NO}; otherwise fix $|U|\\le 2^k$.', 'step_2': ""Deduplicate $F$ to obtain $F'$, hence $|F'|\\le 2^{|U|}$."", 'step_3': ""Enumerate all $\\mathcal{S}\\subseteq F'$ with $|\\mathcal{S}|\\le k$; the number of candidates satisfies \\[ N=\\sum_{i=0}^{k}\\binom{|F'|}{i}\\ \\le\\ \\Bigl(\\tfrac{e|F'|}{k}\\Bigr)^{k}\\ \\le\\ |F'|^{\\,k}. \\]"", 'step_4': 'For each candidate $\\mathcal{S}$, construct signatures $\\chi_{\\mathcal{S}}(u)$ for all $u\\in U$ and check for duplicates, the verification cost is $O(k|U|)$.', 'step_5': ""Since $|F'|\\le 2^{|U|}\\le 2^{2^k}$, we obtain \\[ T\\ \\le\\ (2^{2^k})^{k}\\cdot \\mathrm{poly}(|U|+|F|) \\ =\\ 2^{\\,k\\,2^{k}}\\cdot \\mathrm{poly}(|U|+|F|) \\ =\\ 2^{\\,2^{\\Theta(k)}}\\cdot \\mathrm{poly}(|U|+|F|). \\]""}"	"{'concept_1': 'If $|\\mathcal{S}|\\le k$, then the number of distinct signatures is at most $2^k$. Hence, if $|U|>2^k$ the answer is \\textsc{NO}, otherwise, fix $|U|\\le 2^k$ for the sequel. (Note: do not directly set $|U|=k$.)', 'concept_2': ""After deduplicating $F$ to obtain $F'\\subseteq 2^{U}$, we have $|F'|\\le 2^{|U|}$."", 'concept_3': 'For any $m\\ge k$, \\[ \\sum_{i=0}^{k}\\binom{m}{i}\\le\\left(\\frac{e m}{k}\\right)^{k}\\le m^{k}. \\]'}"	$2^{\,2^{\Theta(k)}}$	理论基础学科-理论计算机科学-计算复杂度	524567	3	3
recuUAiwXy0ZRY	In the spherical random geometric graph \mathrm{RGG}(n,S^{d-1},p), how does the second eigenvalue \lambda_2 of the adjacency matrix scale with (n,p,d) under the two canonical regimes relating d and np(（\[\sqrt{np}\ \ge\ \frac{np}{\sqrt d}\ \Longleftrightarrow\ d\ \ge\ np,\]）)?	{'step_1': 'Centering and projection (Concept 4) Let \\[ u=\\frac{\\mathbf{1}}{\\sqrt n},\\qquad P=I-uu^\\top,\\qquad B:=A-p\\,\\mathbf{1}\\mathbf{1}^\\top . \\] Then \\[ P\\mathbf{1}=0,\\qquad PAP=PBP,\\qquad \\lambda_2(A)=\\|PAP\\|_{\\mathrm{op}}\\ \\le\\ \\|B\\|_{\\mathrm{op}}. \\]', 'step_2': 'Notational conventions (Concepts 1–3) Use the max-operator \\(a\\vee b:=\\max\\{a,b\\}\\) (Concept 1). Adopt \\(\\tilde{O}(\\cdot),\\tilde{\\Theta}(\\cdot)\\) to ignore polylog factors in \\(n\\) (Concept 2). Interpret “w.h.p.” as \\(\\Pr(\\cdot)\\ge 1-n^{-c}\\) for some \\(c>0\\) (Concept 3).', 'step_3': 'Operator-norm target bound (plug-in form) To answer the scaling question, it suffices (by Step 1) to control \\(\\|B\\|_{\\mathrm{op}}\\). We therefore target a high-probability bound of the form \\[ \\|B\\|_{\\mathrm{op}}\\ \\le\\ \\tilde{O}\\!\\Big(\\,\\sqrt{np}\\ \\vee\\ \\frac{np}{\\sqrt d}\\,\\Big)\\quad\\text{(w.h.p.)}. \\] (Establishing this bound requires tools beyond Concepts 1–4; here we only use it as a plug-in to translate into \\(\\lambda_2\\)-scaling.)', 'step_4': 'Scaling in the two regimes (Instruction’s comparator) By the comparator in the Instruction, \\[ \\sqrt{np}\\ \\ge\\ \\frac{np}{\\sqrt d}\\ \\Longleftrightarrow\\ d\\ \\ge\\ np, \\] so the dominant term in the max changes at \\(d=np\\). Combining with Step 1: \\[ \\lambda_2(A)\\ \\le\\ \\|B\\|_{\\mathrm{op}}\\ =\\ \\tilde{O}\\!\\Big(\\,\\sqrt{np}\\ \\vee\\ \\frac{np}{\\sqrt d}\\,\\Big)\\quad\\text{(w.h.p.)}. \\]'}	{'concept_1': 'a\\vee b:=\\max\\{a,b\\}', 'concept_2': '\\tilde{O}(\\cdot),\\tilde{\\Theta}(\\cdot)\\ \\text{: ignore polylog factors in }n.', 'concept_3': ': “w.h.p.”: \\(\\Pr(\\cdot)\\ge 1-n^{-c}\\) for some \\(c>0\\).', 'concept_4': 'Let \\(A\\in\\{0,1\\}^{n\\times n}\\) (zero diagonal), \\(\\mathbf 1\\in\\mathbb R^n\\), \\[ u:=\\frac{\\mathbf 1}{\\sqrt n},\\quad P:=I-uu^\\top,\\quad B:=A-p\\,\\mathbf 1\\mathbf 1^\\top . \\] Then \\[ P\\mathbf 1=0,\\qquad PAP=PBP,\\qquad \\lambda_2(A)=\\|PAP\\|_{\\mathrm{op}}\\le \\|B\\|_{\\mathrm{op}}. \\]'}	\lambda_2(G)= \begin{cases} \tilde{\Theta}\!\big(\dfrac{np}{\sqrt d}\big), & d\ll np,\\[6pt] \tilde{\Theta}\!\big(\sqrt{np}\big), & d\gg np~. \end{cases}	理论基础学科-理论计算机科学-计算复杂度	933873	3	4
recuUp11IgbXqe	In chemical reaction networks, which factors influence the type of relaxation dynamics (under non-steady-state conditions) determined by the network’s topology (i.e., the structural features defined by the stoichiometric matrix)?	{'step_1': 'Clarify the problem. The goal is to determine, after applying a finite perturbation around a steady state, whether the network exhibits exponential, plateau (quasi-steady/limited), or power-law relaxation under non-steady conditions. Importantly, this determination should rely solely on the structural information encoded in the stoichiometric matrix, not on specific rate constants. The paper first confirms through extensive numerical experiments on minimal networks that the three typical types of relaxation phenomena objectively exist.', 'step_2': 'Establish the dynamical equations and focus on the intermediate time scale. Let x_dot = S0 v0 + u - p - phi x, where S0 is the stoichiometric matrix of internal reactions, v0 is given by the law of mass action, u denotes source uptake, p denotes target collection, and phi is a uniform slow degradation rate. For t << 1/phi, the degradation term can be neglected, and the source uptake incorporated into the augmented matrix S1, yielding the approximation x_dot = S1 v1 - p. This sets the linear-algebraic foundation for the structural criteria.', 'step_3': 'Introduce the rank gap delta to assess steady-state attainability in the absence of degradation. Define delta = rank(S1|P) - rank(S1), where P is the stoichiometric column vector for target collection. If delta = 0, a steady state exists without degradation; if delta = 1, no steady state exists, and the system converges only when degradation is included. Numerically, Type I/II networks satisfy delta = 0, while Type III/IV networks satisfy delta = 1. Thus, delta first separates the possibility of exponential/plateau from power-law/limited plateau.', 'step_4': 'For the delta = 0 branch, use the dimension of the left null space I to distinguish exponential from plateau relaxation. Let S2 = (S1|P), and define I = dim(coker S2), representing the number of conservation laws. I = 0: no conservation laws, perturbations rapidly return along the dominant decay mode -> exponential relaxation. I >= 1: near-conserved combinations exist, leading to different quasi-steady states depending on the initial condition -> plateau relaxation (quasi-steady plateau). This mechanism is illustrated by examples where different initial conditions lead to distinct plateau concentration levels.', 'step_5': 'For the delta = 1 branch, use the stoichiometric cone SC(x_ini) to distinguish power-law vs. limited plateau. Define SC(x_ini) = {x = x_ini + S2 v2 | J^(tgt) >= 0}, the region of state space reachable from the initial point without reversing target collection. If the final attractor x_att is in SC(x_ini), trajectories can continue to approach it without degradation, but progress slows under nonlinear constraints -> power-law relaxation. If x_att is not in SC(x_ini), the trajectory is stuck due to geometric accessibility limits, remaining nearly static until degradation takes effect -> limited plateau. A two-step toy model illustrates the stoichiometric-geometric reason behind this stuck behavior.', 'step_6': 'Combine the three criteria into a decision tree with four classifications: Type I (delta = 0, I = 0) -> exponential. Type II (delta = 0, I >= 1) -> plateau (quasi-steady). Type III (delta = 1, I = 0) and Type IV (delta = 1, I >= 1) -> depending on the stoichiometric cone, either power-law or limited plateau. This decision process can be carried out solely from the stoichiometric matrix.', 'step_7': 'Quantify the distinction among relaxation types to avoid subjectivity: define the relaxation time T_relax as the time when the distance to steady state in logarithmic space first falls below a threshold epsilon, and introduce the migration length L to distinguish plateau (almost stationary in the intermediate stage, L/L_max small) from power-law (continuous slow migration, L/L_max large). The bimodal distribution clearly corresponds to the coexistence of power-law and limited plateau behaviors.', 'step_8': 'Cross-validation and extrapolation. Connecting two minimal networks in parallel can alter the effective delta and I, transforming originally power-law/plateau dynamics into exponential dynamics. Assigning different rate constants to the parallel networks reveals time-segmented behavior (plateau/power-law followed by exponential), reflecting the idea that the effective network structure unfolds over time. Larger redundant networks still conform to the overall trends of the criteria.', 'step_9': 'Validation with empirical metabolic networks. For example, in the glucose fermentation network of Lactococcus lactis, the presence or absence of the NOX reaction (NADH -> NAD) alters the effective delta: Fast NOX -> near delta = 0, exponential relaxation. Slow NOX -> near delta = 1, power-law/plateau relaxation. This phenomenon holds even after unifying other parameters, emphasizing the robustness and transferability of the structure-dynamics correspondence.', 'step_10': 'Concluding synthesis. The three factors (rank gap delta, left null space dimension I, and stoichiometric cone SC) constitute a complete set of criteria: first determine steady-state attainability (delta), then conservation laws (I), and finally reachability (SC). This uniquely predicts the type of non-steady relaxation, and is insensitive to specific rate constants (within the modeling assumptions).'}	{'concept_1': 'Definition of the stoichiometric cone SC(x_ini) and the criterion: “attractor containment → power law; non-containment → limited plateau.” Numerical checking algorithms based on linear/quadratic programming and intuitive geometric toy models are provided.', 'concept_2': 'Quantitative indices: T_relax (the first time when a logarithmic distance threshold is reached) and migration length L (log-trajectory arc length, distinguishing plateau from power law), used for objective classification and statistical visualization (unimodal/bimodal).', 'concept_3': 'Validation in real metabolic networks: in Lactococcus lactis fermentation, NOX rate regulation effectively tunes δ; switching between exponential ↔ power-law/plateau is observed under both experimental and normalized parameters, supporting the biological relevance of the framework.'}	The relaxation dynamics of chemical reaction networks under non-steady conditions (exponential, plateau, or power-law) can be uniquely determined by three structural features of the stoichiometric matrix: Rank gap (δ) — determines whether a steady state exists on the intermediate time scale: δ = 0 → a steady state exists; δ = 1 → convergence occurs only when slow degradation is included. Dimension of the left null space (I) — reflects the presence of conservation laws: I = 0 → exponential relaxation; I ≥ 1 → plateau-type relaxation. Stoichiometric cone — determines whether the initial condition can reach the final attractor: Contains the attractor → power-law relaxation; Does not contain the attractor → restricted plateau-type relaxation. Therefore, these three topological quantities alone are sufficient to predict the relaxation type of a given network after perturbation, independent of specific reaction rate constants.	理论基础学科-理论化学-化学反应动力学理论	94030	2	3
recuUoxYYA5jCy	Given a monotone formula `f` of size `L` that represents an 'expensive' instance within the coNP-hardness proof framework for the GapSS (Gap Secret Sharing) problem, what is the established lower bound on its total share size? The GapSS problem involves distinguishing functions with small secret-sharing complexity from those with large complexit. The Tautology problem is a coNP-hard problem that asks whether a given non-monotone Boolean formula `g` is true for all possible assignments to its variables. If a Tautology instance `g` is not a tautology, there exists a 'legal' assignment `(x_0, x_0')` for which `$g'(x_0, x_0') = 0$`. For this assignment, the constructed function `$f(x_0, x_0', y)$` simplifies to `$E_t(y)$`.	"{'step_1': 'The proof establishes the coNP-hardness of the GapSS problem by employing a Karp reduction (Concept_1) from the Tautology problem.', 'step_2': ""The reduction takes a Tautology formula `g` and constructs a new, larger monotone formula `f` using an 'expensive' gadget formula `E_t`, as detailed in the construction from Concept_3. The size `L` of the resulting formula `f` is primarily determined by the size `t` of the gadget, such that `$L=O(t)$`."", 'step_3': ""The instruction specifies that we are interested in the 'expensive' case, which occurs when the Tautology instance `g` is not a tautology. In this scenario, `f` can be restricted to a residual function that is identical to the gadget `$E_t(y)$`."", 'step_4': 'According to Theorem_1, the secret-sharing complexity lower bound of this residual function `$E_t$` is inherited by the original function `f`. Therefore, to find the lower bound for `f`, we must calculate the total share size for `$E_t$`.', 'step_5': 'The total share size of `$E_t$` is the product of its average-share size and its number of variables. Using the properties defined in Concept_2, this is calculated as: `$S_{sum}(E_t) = \\text{avg-share-size} \\times \\text{num-variables} = \\Omega(\\sqrt{t}/\\log t) \\times \\Theta(\\sqrt{t}) = \\Omega(t/\\log t)$`.', 'step_6': 'Since `$L = O(t)$`, we can express this lower bound in terms of `L`. The total share size for `f` is therefore `$\\Omega(t/\\log t) = \\Omega(L/\\log L)$`, which is the final answer.'}"	"{'concept_1': 'A Karp reduction is a polynomial-time transformation from an instance of one decision problem to an instance of another, used to prove the hardness of the second problem.', 'concept_2': ""There exists a constructible family of 'expensive' monotone formulas `{E_t}` of size `t` over `k(t) = \\Theta(\\sqrt{t})` variables, where any secret-sharing scheme realizing `E_t` requires an average-share size of at least `S(t) = \\Omega(\\sqrt{t}/\\log t)`."", 'concept_3': ""The reduction constructs a new monotone formula `f` from a Tautology formula `g` and the expensive gadget `E_t`. The formula is defined as `$f := h(x,x') \\wedge (g'(x,x') \\vee E_t(y))$`, where `g'` is a monotonized version of `g`."", 'theorem_1': 'A lower bound on the total share size of a residual function (a function obtained by fixing some inputs of a larger function) implies a similar lower bound on the total share size of the original function.'}"	$$ \Omega(L/\log L) $$	理论基础学科-理论计算机科学-密码学	165139	5	4
recuUlQq8ydgDA	In the study of the critical hardcore model, a key finding for certain worst-case graph instances is that the anti-concentration property of the Gibbs measure establishes a spectral independence lower bound of \(\lambda_{\text {max }}\left(\Psi_\mu\right)=\Omega\left(n^{1 / 3}\right)\). Based on the established connections between spectral independence and Markov chain mixing, what is the specific, resulting worst-case lower bound on the relaxation time of the corresponding Glauber dynamics?	"{'step_1': 'The objective is to determine the worst-case lower bound for the relaxation time of the Glauber dynamics (Concept_1) in the critical hardcore model (Concept_2).', 'step_2': 'The instruction provides the starting premise: the lower bound on spectral independence (Concept_3), a measure of local correlations, is \\(\\Omega\\left(n^{1 / 3}\\right)\\).', 'step_3': ""Theorem_1 describes the connection via the 'Universality of Spectral Independence,' a local-to-global principle. This principle dictates that the global property (relaxation time) is determined by scaling the local property (spectral independence) by the system size, \\(n\\)."", 'step_4': ""A model must infer from 'scaled up by the system size' that a multiplication by a factor related to \\(n\\) is required. The specific relationship, as detailed in the source literature, is that the relaxation time is lower-bounded by the product of \\(n\\) and the spectral independence constant."", 'step_5': 'Applying this principle, we multiply the system size, \\(n\\), by the given spectral independence lower bound, \\(\\Omega\\left(n^{1 / 3}\\right)\\).', 'step_6': 'The calculation is \\(\\Omega\\left(n \\cdot n^{1 / 3}\\right)=\\Omega\\left(n^{1+1 / 3}\\right)=\\Omega\\left(n^{4 / 3}\\right)\\). This is the resulting lower bound on the relaxation time.'}"	{'concept_1': 'Glauber Dynamics: A local Markov chain used for sampling from Gibbs distributions, where at each step a single, randomly chosen vertex has its state updated based on the states of its neighbors.', 'concept_2': 'Critical Hardcore Model: A model of independent sets on a graph with maximum degree \\(\\Delta\\) where the vertex-weight (fugacity) \\(\\lambda\\) is set to the critical threshold \\(\\lambda_c(\\Delta)\\).', 'concept_3': 'Spectral Independence: A measure of correlation in a Gibbs distribution, quantified by the largest eigenvalue, \\(\\lambda_{\\max }\\left(\\Psi_\\mu\\right)\\), of the influence matrix \\(\\Psi_\\mu\\). A large value indicates strong correlations.', 'theorem_1': 'Universality of Spectral Independence: A local-to-global principle where a local spectral property (spectral independence) is scaled up by the system size to determine a global mixing property (relaxation time).'}	\(\Omega(n^{4/3})\)	理论基础学科-理论物理-统计力学	658173	5	3
recuUczliprdXP	"The ""non-locality cost"" of a 2D quantum code architecture can be quantified by the number of long-range interactions it allows. In the ""stacked architecture,"" this cost is constrained, with the number of interactions of length $l$ or greater scaling as $O(n/l^2)$. It is known that for a code's distance $d$ to surpass the typical 2D local limit of $\Theta(\sqrt{n})$, a significant non-locality cost must be paid. Determine the most general upper bound that jointly constrains the dimension $k$ and distance $d$ for any qLDPC code implementable in the stacked architecture, reflecting the full tradeoff between all code parameters, not just the distance scaling."	"{'step_1': ""The problem sets two conflicting bounds on the code's interactions. The Stacked Architecture Interaction Constraint (Concept_1) provides an upper limit on the number of long-range interactions, $M \\le O(n/l^2)$."", 'step_2': 'Simultaneously, the Locality-Parameter Tradeoff (Theorem_1) establishes a fundamental lower bound on both the number and length of these interactions for any non-trivial 2D code. The number of interactions must be at least $M \\ge \\Omega(\\text{max}(k,d))$.', 'step_3': 'By combining these two constraints on the number of interactions $M$, we deduce that the upper bound imposed by the architecture must accommodate the necessary lower bound: $O(n/l^2) \\ge \\Omega(\\text{max}(k,d))$.', 'step_4': 'This inequality can be rearranged to express an upper bound on the square of the interaction length: $l^2 \\le O(n / \\text{max}(k,d))$.', 'step_5': 'Next, we use the second part of the Locality-Parameter Tradeoff (Theorem_1), which specifies the minimum required length for these interactions: $l \\ge \\Omega(\\text{max}(\\frac{d}{\\sqrt{n}}, (\\frac{kd^2}{n})^{1/4}))$. To derive a relationship involving the term $kd^2$, we must select the second argument of the max function, yielding the lower bound $l \\ge \\Omega((\\frac{kd^2}{n})^{1/4})$.', 'step_6': 'To compare this with the expression from Step 4, we square this lower bound on $l$: $l^2 \\ge \\Omega((\\frac{kd^2}{n})^{2/4}) = \\Omega(\\sqrt{\\frac{kd^2}{n}})$.', 'step_7': 'Now we have both an upper and a lower bound for $l^2$. Combining the result from Step 4 and Step 6 gives: $O(n / \\text{max}(k,d)) \\ge l^2 \\ge \\Omega(\\sqrt{\\frac{kd^2}{n}})$.', 'step_8': 'Focusing on the outer parts of the inequality, we have $O(n / \\text{max}(k,d)) \\ge \\Omega(\\sqrt{\\frac{kd^2}{n}})$. To eliminate the radical, we square both sides: $O(n^2 / \\text{max}(k,d)^2) \\ge \\Omega(\\frac{kd^2}{n})$.', 'step_9': 'Finally, we rearrange the inequality to solve for the target quantity $kd^{2} \\cdot \\text{max}(k,d)^{2}$. Multiplying both sides by $n \\cdot \\text{max}(k,d)^2$ gives the concluding result: $O(n^3) \\ge \\Omega(kd^{2} \\cdot \\text{max}(k,d)^{2})$.'}"	{'concept_1': 'Stacked Architecture Interaction Constraint: For a qLDPC code implemented in the stacked architecture, the number of interactions of length greater than or equal to $l$ is at most $O(n/l^2)$.', 'theorem_1': 'Locality-Parameter Tradeoff for 2D Stabilizer Codes: Any $[[n, k, d]]$ stabilizer code with $kd^2$ significantly larger than $n$ must have at least $\\Omega(\\text{max}(k,d))$ interactions of length at least $\\Omega(\\text{max}(\\frac{d}{\\sqrt{n}}, (\\frac{kd^2}{n})^{1/4}))$.'}	$kd^{2} \cdot \text{max}(k,d)^{2} \le O(n^{3})$	理论基础学科-理论计算机科学-量子计算	235993	5	2
recuU8Xn3CGVwN	Consider the following multi-scale mean-field stochastic systems，   \left\{\begin{matrix}  dX_t^\delta = b(X_t^\delta,L_{X_t^\delta},Y_t^\delta,L_{Y_t^\delta})dt+\sqrt{\delta } \sigma (X_t^\delta,L_{X_t^\delta},Y_t^\delta,L_{Y_t^\delta})dW_t^1,   \\  dY_t^\delta = \frac{1}{\varepsilon } f(Y_t^\delta)dt + \frac{1}{\sqrt{\varepsilon} } g(Y_t^\delta)dW_t^2,  \\  X_0^\delta = x,Y_0^\delta = y,  \end{matrix}\right.  where \delta describes the intensity of the noise，\varepsilon =\varepsilon (\delta) describes the ratio of the time scale between the (slow) component X^\delta and the (fast) component Y^\delta， L_{X_t^\delta} (resp. L_{Y_t^\delta}) denotes the law of X_t^\delta (resp. Y_t^\delta). The coefficients b,\sigma,f,g satisfy global Lipschitz conditions, and the fast component satisfies a dissipativity condition. The matrix \sigma\sigma^* is uniformly positive definite, and g is bounded. Moreover, there exist constants c_1, c_2>0 such that for all x \in \mathbb{R}^n, y \in \mathbb{R}^m, \mu \in \mathcal{P}_2\left(\mathbb{R}^n\right), \nu \in \mathcal{P}_2\left(\mathbb{R}^m\right) and \xi \in \mathbb{R}^n, the following holds:   c_1|\xi|^2 \leqslant\left\langle\sigma \sigma^*(x, \mu, y, \nu) \xi, \xi\right\rangle \leqslant c_2|\xi|^2.  Additionally, the scaling condition \lim_{\delta \to 0} \frac{\varepsilon }{\delta } =0.  Problem: As \delta\to0，derive an explicit expression for the good rate function I(\varphi) that ensures the large deviation principle holds for the slow component.	{'step_1': 'Understand the system and the objective. The system is a slow-fast mean-field diffusion: a slow component X_t^\\delta and a fast component Y_t^\\delta are coupled, where the coefficients of the slow component depend on its own distribution and that of the fast component. The noise intensity \\delta \\rightarrow 0, and the time-scale ratio \\varepsilon(\\delta) satisfies \\lim _{\\delta \\rightarrow 0} \\frac{\\varepsilon}{\\delta}=0. The objective is to derive an explicit expression for the rate function of the large deviation principle (LDP) for the slow component X^\\delta.', 'step_2': 'Establish the mean-field limit. As \\delta \\rightarrow 0, the fast component Y_t^\\delta converges to its invariant measure v (uniquely determined by the equation d Y_t=f\\left(Y_t\\right) d t + g\\left(Y_t\\right) d\\tilde{W}_t^2 due to rapid oscillation (\\varepsilon \\rightarrow 0). In the limit, the slow component satisfies the deterministic averaged equation \\dot{\\bar{X}}_t=\\bar{b}\\left(\\bar{X}_t, \\mathrm{~L}_{\\bar{X}_t}\\right) , where \\bar{b}(x, \\mu)=\\int b(x, \\mu, y, v) v(d y) , and \\mathrm{L}_{\\bar{X}_t}=\\delta_{\\bar{X}_t}（Dirac measure）.', 'step_3': 'Apply the weak convergence approach and variational representation. The LDP is equivalent to the Laplace principle. Utilize the variational representation for Brownian motion functionals：-\\log \\mathrm{E}[\\exp (-F(W))]= \\inf _{h \\in \\mathrm{~A}} \\mathrm{E}\\left[\\int_0^T\\left|h_s\\right|^2 d s+F\\left(W+\\int_0^{\\cdot} h_s d s\\right)\\right]. Define the control process h_t^\\delta and the controlled system: the controlled slow component X_t^{\\delta, h^\\delta} and the controlled fast component Y_t^{\\delta, h^\\delta} satisfying stochastic differential equations (SDEs) with drift terms.', 'step_4': 'Introduce the functional occupation measure and viable pairs. To handle the averaging of the fast component, define the functional occupation measure P^{\\delta, \\Delta}(d h d y d t)= \\frac{1}{\\Delta} \\int_t^{t+\\Delta} \\delta_{\\left(h_s^\\delta, Y_s^{\\delta, h^\\delta}\\right)}(d h d y) d s d t, where \\Delta(\\delta) satisfies the scale separation conditions \\Delta \\rightarrow 0, \\frac{\\varepsilon}{\\delta \\Delta} \\rightarrow 0. A viable pair (\\varphi, P) is defined as: \\varphi is an absolutely continuous path, P is a measure satisfying (i) finite second moment, (ii) the integral equation \\varphi_t=x+\\int \\Phi\\left(\\varphi_s, \\mathrm{~L}_{\\bar{X}}, y, v, h\\right) P(d h d y d s) , and (iii) the decomposition P(d h d y d t)=\\eta(d h \\mid y, t) v(d y) d t, where\\Phi(x, \\mu, y, v, h)=\\bar{b}(x, \\mu)+\\sigma(x, \\mu, y, v) P_1 h, P_1 is a projection operator.', 'step_5': 'Prove compactness and convergence to viable pairs. Under assumptions (global Lipschitz continuity, dissipativity, uniform positive definiteness of \\sigma \\sigma^* ), the sequence \\left\\{\\left(X^{\\delta, h^\\delta}, P^{\\delta, \\Delta}\\right)\\right\\} is compact in an appropriate space. For any subsequence, there exists a sub-subsequence converging weakly to a viable pair ( \\varphi, P ) , satisfying the viable pair definition. The rate function is defined as I(\\varphi)=\\inf _{(\\varphi, P) \\in \\mathrm{V}_{(\\Phi, v)}} \\frac{1}{2} \\int|h|^2 P(d h d y d t) .', 'step_6': 'Derive the explicit form of the rate function. Through an equivalent variational problem \\tilde{I}(\\varphi)=\\inf _z \\frac{1}{2} \\int_0^T \\int_{\\mathrm{Y}}\\left|z_t(y)\\right|^2 v(d y) d t, where z_t(y) satisfies \\dot{\\varphi}_t=\\bar{b}\\left(\\varphi_t, \\mathrm{~L}_{\\bar{X}_t}\\right)+ \\int_{\\mathrm{Y}} \\sigma\\left(\\varphi_t, \\mathrm{~L}_{\\bar{X}_t}, y, v\\right) P_1 z_t(y) v(d y). The optimal control takes the feedback form h_t(y)=\\left(\\sigma P_1\\right)^*\\left(\\varphi_t, \\mathrm{~L}_{\\bar{X}_t}, y, v\\right) Q^{-1}\\left(\\dot{\\varphi}_t-\\bar{b}\\left(\\varphi_t, \\mathrm{~L}_{\\bar{X}_t}\\right)\\right), where Q(x, \\mu, v)= \\int_{\\mathrm{Y}}\\left(\\sigma P_1\\right)\\left(\\sigma P_1\\right)^*(x, \\mu, y, v) v(d y). Substitution yields I(\\varphi)=\\frac{1}{2} \\int_0^T\\left|Q^{-1 / 2}\\left(\\dot{\\varphi}_t-\\bar{b}\\left(\\varphi_t, \\mathrm{~L}_{\\bar{X}_t}\\right)\\right)\\right|^2 d t\\left(\\varphi\\right. if \\varphi is absolutely continuous and \\varphi_0=x, otherwise \\left. I(\\varphi)=+\\infty\\right).', 'step_7': 'Verify the Laplace principle upper and lower bounds. Lower bound: Using the variational representation and the convergence of viable pairs, prove \\liminf _{\\delta \\rightarrow 0}\\left(-\\delta \\log \\mathrm{E}\\left[\\exp \\left(-\\Lambda\\left(X^\\delta\\right) / \\delta\\right)\\right]\\right) \\geq \\inf _{\\varphi}[I(\\varphi)+\\Lambda(\\varphi)]. Upper bound: Construct a feedback control \\bar{h}_t(y) and prove \\lim \\sup _{\\delta \\rightarrow 0}\\left(-\\delta \\log \\mathrm{E}\\left[\\exp \\left(-\\Lambda\\left(X^\\delta\\right) / \\delta\\right)\\right]\\right) \\leq \\inf _{\\varphi}[I(\\varphi)+\\Lambda(\\varphi)], relying on averaging convergence and optimal control.', 'step_8': 'Confirm the rate function is well-defined. Due to the uniform positive definiteness of \\sigma \\sigma^* (assuming c_1|\\xi|^2 \\leq\\left\\langle\\sigma \\sigma^* \\xi, \\xi\\right\\rangle \\leq c_2|\\xi|^2 ），Q is invertible, hence Q^{-1 / 2} exists.'}	{'concept_1': 'Large Deviation Principle（LDP）：Describes the exponential decay rate of probabilities for rare events of a stochastic process in the small noise limit. The rate function I(\\varphi) is central to the lower and upper bounds, satisfying: for an open set G, ~ \\liminf \\inf _{\\delta \\rightarrow 0} \\delta \\log \\mathrm{P}\\left(X^\\delta \\in G\\right) \\geq-\\inf _{x \\in G} I(x), for a closed set F, ~ \\limsup \\sup _{\\delta \\rightarrow 0} \\delta \\log \\mathrm{P}\\left(X^\\delta \\in F\\right) \\leq-\\inf _{x \\in F} I(x).', 'theorem_1': 'Laplace Principle: On a Polish space, the LDP is equivalent to the Laplace principle: for any bounded continuous function \\Lambda, \\lim _{\\delta \\rightarrow 0}-\\delta \\log \\mathrm{E}\\left[\\exp \\left(-\\Lambda\\left(X^\\delta\\right) / \\delta\\right)\\right]= \\inf _x[I(x)+\\Lambda(x)]. If I is a good rate function (lower semicontinuous with compact level sets), the two principles are equivalent.', 'concept_2': 'Weak Convergence Approach：A framework for proving the LDP, centered on the variational representation formula for Brownian motion functionals: -\\log \\mathrm{E}[\\exp (-F(W))]=\\inf _{h \\in \\mathrm{~A}} \\mathrm{E}\\left[\\frac{1}{2} \\int_0^T\\left|h_s\\right|^2 d s+F\\left(W+\\int_0^* h_s d s\\right)\\right] . The limiting behavior is analyzed via a control process h_t and the corresponding controlled system.', 'concept_3': 'Mean－Field Limit：Describes the convergence of system behavior to a mean-field equation as the number of particles or noise intensity changes. Here, fast component averaging occurs: as \\varepsilon \\rightarrow 0 , the distribution of Y_t^\\delta converges to its invariant measure v and the slow component converges to the deterministic averaged equation \\dot{\\bar{X}}_t=\\int b\\left(\\bar{X}_t, \\mathrm{~L}_{\\bar{X}_t}, y, v\\right) v(d y).', 'concept_4': 'Viable Pair：A viable pair (\\varphi, P) consists of a path and a measure: \\varphi \\in C\\left([0, T] ; \\mathrm{R}^n\\right) is absolutely continuous, and P \\in \\mathrm{P}\\left(\\mathrm{R}^d \\times \\mathrm{Y} \\times[0, T]\\right) satisfies (i) finite second moment, (ii) the integral equation \\varphi_t= x+\\int \\Phi\\left(\\varphi_s, \\mathrm{~L}_{\\bar{X}_s}, y, v, h\\right) P(d h d y d s) , and (iii) the decomposition P(d h d y d t)=\\eta(d h \\mid y, t) v(d y) d t, where \\Phi(x, \\mu, y, v, h)=\\bar{b}(x, \\mu)+\\sigma(x, \\mu, y, v) P_1 h, and P_1 is a projection operator.', 'concept_5': 'Functional Occupation Measure：The functional occupation measure P^{\\delta, \\Delta} captures the joint behavior of the control and the fast component, defined as P^{\\delta, \\Delta}\\left(A_1 \\times A_2 \\times A_3\\right)= \\int_{A_3} \\frac{1}{\\Delta} \\int_t^{t+\\Delta} 1_{A_1}\\left(h_s^\\delta\\right) 1_{A_2}\\left(Y_s^{\\delta, h^\\delta}\\right) d s d t. Under scale separation conditions, it converges weakly to the measure P in a viable pair.', 'theorem_2': 'Ergodic Theorem for Fast Process：The fast component d Y_t=f\\left(Y_t\\right) d t+g\\left(Y_t\\right) d W_t^2 under the dissipativity condition \\left(2\\left\\langle f\\left(y_1\\right)-f\\left(y_2\\right), y_1-y_2\\right\\rangle+\\right. 3\\left\\|g\\left(y_1\\right)-g\\left(y_2\\right)\\right\\|^2 \\leq-\\kappa\\left|y_1-y_2\\right|^2 ）possesses a unique invariant measure v, and its time averages converge to v .', 'concept_6': 'Absolutely Continuous Path：The rate function I(\\varphi) is finite if and only if \\varphi is absolutely continuous (i.e., possesses a derivative \\dot{\\varphi}_t a.e.) and \\varphi_0=x_{\\mathrm{o}}. Otherwise, I(\\varphi)=+\\infty.', 'concept_7': 'Projection Operator：The projection operator P_1: \\mathrm{R}^{d_1+d_2} \\rightarrow \\mathrm{R}^{d_1} maps the control h to the noise space of the slow component. It is used in defining \\Phi and the matrix Q.', 'theorem_3': 'Prokhorov’s Theorem: A core tool in compactness analysis. If a sequence of probability measures is uniformly tight, then there exists a weakly convergent subsequence. This theorem is applied here to extract convergent subsequences of (X^{\\delta,h^\\delta}, P^{\\delta,\\Delta}) .'}	The slow component \left\{X^\delta\right\} satisfies a large deviation principle (LDP) with the rate function I(\varphi) given by：  I(\varphi)= \begin{cases}\frac{1}{2} \int_0^T\left|Q^{-1 / 2}\left(\varphi_t, \mathrm{~L}_{\bar{X}_t}, v\right)\left(\dot{\varphi}_t-\bar{b}\left(\varphi_t, \mathrm{~L}_{\bar{X}_t}\right)\right)\right|^2 d t, & \varphi_0=x, \varphi \text { is absolutely continuous, } \\ +\infty, & \text { otherwise. }\end{cases}  - \bar{X}_t solves the averaged (mean-field) ODE: \dot{\bar{X}}_t=\bar{b}\left(\bar{X}_t, \mathrm{~L}_{\bar{X}_t}\right) where \bar{b}(x,\mu ) = \int_{\mathbb{R}^m }b(x,\mu,y,v)v(dy) is the averaged drift, and v is the invariant measure of the fast component.  The law of \bar{X}_t is \mathrm{~L}_{\bar{X}_t} = \delta _{\bar{X}_t } (a Dirac measure concentrated at \bar{X}_t).  Covariance Matrix \circ Q(x, \mu, v):=\int_{\mathrm{Y}}\sigma \sigma ^*(x, \mu, y, v) v(d y) where \sigma\sigma^* is uniformly positive definite. Q is invertible.	理论基础学科-基础数学-偏微分方程	984807	1	9
recuTQ44xkBvoS	Let $w \in S$ be an unknown unit vector, $\gamma>0$, $\beta>0$, $\sigma>0$ be known parameters. $x\sim N(0,I_d)$, $\xi_0\sim N(0,\beta)$, $\xi\sim N(0,\sigma^2)$ are independent random variables, and $\phi:R\rightarrow R$ is a 1-Lipschitz function. Consider two label variables: $z_1=\phi(\gamma\langle x,w\rangle+\xi_0)+\xi$ and $z_2=\phi(\gamma\langle x,w\rangle)+\xi$. Given a fixed value of x, what is the upper bound on the total variation distance between the distributions of $z_1$ and $z_2$ under the condition that TV and the upper bound of TV are considered?	"{'step1': 'Consider two label variables: $z_1=\\phi(\\gamma\\langle x,w\\rangle+\\xi_0)+\\xi$ and $z_2=\\phi(\\gamma\\langle x,w\\rangle)+\\xi$, derive and utilize the upper bound on the conditional TV: $\\mathrm{TV}(z_1 | x, z_2 | x) \\leq \\mathbb{E}_{\\xi_0} \\left[ \\mathrm{TV}(z_1 | x, \\xi_0, z_2 | x, \\xi_0) \\right]$', 'step2': 'Analyze the conditional distribution: Given the conditions, the conditional TV distance corresponds to the TV between two Gaussian distributions with the same covariance matrix.', 'step3': ""Using Pinsker's inequality, we obtain $\\mathrm{TV}(z_1 | x, \\xi_0, z_2 | x, \\xi_0) \\leq \\sqrt{ \\frac{1}{2} \\mathrm{KL} \\left( \\mathcal{N}(\\mu_1, \\sigma^2) | \\mathcal{N}(\\mu_2, \\sigma^2) \\right)}$"", 'step4': 'Using the KL divergence formula, we obtain $\\mathrm{TV}(z_1 | x, \\xi_0, z_2 | x, \\xi_0) \\leq \\sqrt{ \\frac{1}{2} \\frac{(\\phi(a + \\xi_0) - \\phi(a))^2}{2\\sigma^2} } = \\frac{|\\phi(a + \\xi_0) - \\phi(a)|}{2\\sigma}$, where $a = \\gamma\\langle x,w\\rangle$', 'step5': 'Using the 1-Lipschitz property, we obtain $\\mathrm{TV}(z_1 | x, z_2 | x) \\leq \\mathbb{E}_{\\xi_0} \\left[ \\frac{|\\xi_0|}{2\\sigma} \\right] = \\frac{1}{2\\sigma} \\mathbb{E}[|\\xi_0|]$', 'step6': 'Substituting into the final expression yields $\\mathrm{TV}(z_1 | x, z_2 | x) \\leq \\frac{1}{2\\sigma} \\sqrt{\\frac{2\\beta}{\\pi}} = \\frac{\\sqrt{2\\beta}}{2\\sqrt{\\pi}\\,\\sigma} = \\frac{\\sqrt{\\beta}}{\\sqrt{2\\pi}\\,\\sigma}$'}"	"{'concept_1': 'Definition of a 1-Lipschitz function: For all a,b,$|\\phi(a)-\\phi(b)|\\leq|a-b|$.', 'concept_2': 'Definition of Total Variation Distance (TV): $TV(P,Q)=\\sup_A |P(A)−Q(A)|$', 'theorem_1': ""Pinsker's Inequality: $TV(P,Q)\\leq\\sqrt{\\frac{1}{2} KL(P||Q)}$"", 'concept_3': 'KL divergence formula for two Gaussian distributions with equal variance: $KL(N(\\mu_1,\\sigma^2)||N(\\mu_2,\\sigma^2))=\\frac{(\\mu_1-\\mu_2)^2}{2\\sigma^2}$'}"	$\frac{\sqrt{\beta}}{\sqrt{2\pi}\sigma}$	理论基础学科-理论计算机科学-算法分析	629307	3	4
recuUq1hIQhYmj	In the two-phase Stokes problem with surface tension, for a fluid bubble whose initial contour is sufficiently close to a circle, does there exist a unique and globally time-stable fluid bubble that satisfies both the initial contour requirement and the partial differential equation system? The equation system is described as follows: 1.4 - 1.7: Let \(\Gamma\) be a time - dependent simple closed curve in \(\mathbb{R}^2\) that represents the interface between two immiscible fluids. Then the model is given by \(\mu\Delta\boldsymbol{u}-\nabla p = \boldsymbol{0} \quad \text{on } \mathbb{R}^2\setminus\Gamma, \tag{1.4}\) \(\nabla\cdot\boldsymbol{u} = 0 \quad \text{on } \mathbb{R}^2\setminus\Gamma, \tag{1.5}\) \([\boldsymbol{u}] = \boldsymbol{0}, \tag{1.6}\) \([\Sigma(\boldsymbol{u}, p)\boldsymbol{n}]=-\gamma\kappa\boldsymbol{n}, \tag{1.7}\)	{'step_1': 'A circular fluid bubble is a steady-state solution of the two-phase Stokes problem with surface tension (concept_1).', 'step_2': 'The well-posedness of the PDE system, which involves existence, uniqueness, and stability of solutions (concept_2), is crucial here.', 'step_3': 'The problem uses the HLS parametrization (concept_3) for the interface, which provides a suitable framework for analysis.', 'step_4': 'By applying spectral decomposition of the linearized operator (concept_4) around the circular steady state, the dynamics of the interface can be studied.', 'step_5': 'For an initial contour sufficiently close to a circle (small initial perturbation), this technique helps show that the system has a unique solution that exists globally in time.', 'step_6': 'Additionally, the solution decays towards the circular steady state, indicating global time stability. Thus, such a fluid bubble exists.'}	{'concept_1': 'Steady-state solution of the two-phase Stokes problem: A circular fluid bubble is a steady-state solution.', 'concept_2': 'Well-posedness of PDE system: It refers to the existence, uniqueness, and stability of solutions to the PDE system.', 'concept_3': 'Hou-Lowengrub-Shelley (HLS) parametrization: A parametrization of the fluid interface that helps in analyzing the problem, where the boundary of the fluid bubble is parametrized by the direction of its tangent vector and the length of the boundary.', 'concept_4': 'Spectral decomposition of the linearized operator: A technique used to study the well-posedness by linearizing around a steady-state solution, separating the equation into a linear part and a remainder part.'}	In the two-phase Stokes problem with surface tension, for a fluid bubble whose initial contour is sufficiently close to a circle, there exists a unique and globally time-stable fluid bubble. This bubble can not only meet the initial contour requirement but also conform to the system of partial differential equations.	理论工程学科-理论工程力学-流体力学	150613	1	4
recuUuMA1Pj7ES	A benzene ring was employed as a base and grafted with a series of representative functional groups (–OH, –COOH, –OCH₃, –CH₃, –CH₂CH₃) to simulate the local structure of coal. Nitrogen or sodium atoms were subsequently doped to exploit their synergistic interactions with these functional groups and thereby enhance the adsorption of CO₂. The C–Na bond length is approximately 0.255 nm. Which dopant N- or Na- affords the highest adsorption amount and adsorption strength toward CO₂?	{'step_1': 'Na doping induces a transition in the CO₂ adsorption mode from physisorption to chemisorption, thus leading to a stronger adsorption strength toward CO₂', 'step_2': 'Na-C bond length is within the microporous of coal, thus the strong Na–CO₂ interaction causes CO₂ molecules to accumulate persistently around the Na sites. This progressive accumulation of CO₂ within the micropores blocks the adsorption pathways. Therefore, the Na doping leads to a smaller adsorption amount toward CO₂'}	{'concept_1': 'Na doping induces a transition in the CO₂ adsorption mode from physisorption to chemisorption', 'concept_2': 'Micropores in coal are below 2 nm, and excessively strong adsorption within these pores can block subsequent adsorption pathways.'}	N-doping leads to the highest adsorption amount toward CO₂, whereas Na-doping leads to the strongest adsorption strength toward CO₂.	理论基础学科-理论化学-计算化学	53427	1	3
recuUvXgabyFbz	Considering the band structure limitation, which two excitons can significantly enhance the in-plane second-order nonlinear magnetization\(\chi_{\text_{yyy}} \) to three orders of magnitude by achieving double resonance in a monolayer \(\ce {WSe2}\)?	{'step_1': 'Based on concept_3 (definitions of A-exciton and HX), identify the key excitons of monolayer\\(\\ce{ceWSe_{2}}\\)in the unstrained state. Specifically, A-exciton is formed by the valence band (VB) and the first conduction band (CB1), with a binding energy of 0.48 eV and an absorption peak energy of 1.70 eV; HX (higher-order exciton) is formed by VB and the second conduction band (CB2), with a binding energy of 0.58 eV and an absorption peak energy of 3.35 eV. These values are derived from the calculation results of the two-dimensional Mot-Wannier exciton model (concept_2), which is determined by the modified hydrogen atom formula\\(E_{\\text{B}}=\\frac{8\\mu}{(1 + \\sqrt{1+32\\pi\\alpha\\mu/3})^{2}}\\)combined with band dispersion.', 'step_2': 'Apply concept_1 (exciton double resonance mechanism) and analyze how double resonance enhances \\(\\chi_{\\text {yyy}}\\). Double resonance requires two exciton energy levels to satisfy\\(E_{\\text{B}} = 2E_{\\text{A}}\\)（(i.e., the energy absorbed by HX is equal to twice the energy absorbed by A-exciton), which can significantly enhance the second-order nonlinear magnetization because this condition aligns single photon (1-p) and two-photon (2-p) resonances at the same time, enhancing the energy matching between the virtual and real states. In the state of no strain (step 1)\\(E_{\\text{HX}} = 3.35\\ \\text{eV}\\)和\\(2E_{\\text{A}} = 3.40\\ \\text{eV}\\)are close but not completely equal, so external regulation is required to achieve precise double resonance.', 'step_3': 'Using theorem_1 (strain band coupling effect), design a strain strategy to meet the double resonance condition. Biaxial strain \\(\\eta\\) linear modulation band structure, exciton energy response is\\(E(\\eta)=E_{0}+\\beta\\eta\\), where \\(\\beta\\) is the strain coefficient (A-exciton, \\(\\beta_{\\text{A}}\\approx-132.6\\ \\text{meV}/\\%\\);HX,\\(\\beta_{\\text{HX}}\\approx35.4\\ \\text{meV}/\\%\\)). By applying a small amount of tensile strain (such as\\(\\eta = 0.16\\%\\)), adjusting the exciton absorption energy to\\(E_{\\text{A}} = 1.67\\ \\text{eV}\\)'}	{'concept_1': 'In the exciton second harmonic response of single-layer \\(\\ce{MoS_{2}}\\), the second-order nonlinear magnetization is significantly enhanced when the fundamental photon frequency matches the exciton energy level (single photon resonance) or the second harmonic frequency matches the exciton energy level (two-photon resonance); Double resonance occurs when the fundamental frequency and second harmonic frequency simultaneously match the exciton transition frequency.', 'concept_2': 'In two-dimensional semiconductors, the exciton binding energy is obtained by solving the Bethe Salpeter equation (BSE), which combines the Coulomb interaction of material band dispersion and dielectric environment modulation. Verified by fitting experimental absorption spectra with energy values, and significantly influenced by the environmental dielectric constant.', 'concept_3': 'A-exciton: Under strain free conditions, this exciton is formed by the maximum energy band at the top of the valence band (VB) K point and the minimum energy band at the bottom of the conduction band (CB) K point, with an absorption peak at 1.67 eV and a binding energy of approximately 315 meV; The higher-order excited states of this state (such as 2s and 3s) appear as separate peaks in the absorption spectrum, with energies higher than the 1s ground state. Higher order excitons: formed by the valence band (VB) K point and the conduction band (CB) higher energy valley (such as spin splitting state), with an absorption peak at 2.08 eV in the absence of strain, and an energy difference of 409 meV with the A exciton.', 'theorem_1': 'In a monolayer \\(\\ce {WSe_{2}}\\), the stretching strain linearly modulates the band structure, resulting in a redshift of exciton energy, which satisfies the relationship\\(E(\\varepsilon)=E_{0}+\\beta\\varepsilon\\), where \\(\\beta\\) is the strain coefficient.'}	Regulating the energy levels of A excitons and HX (higher-order excitons) in a monolayer \(\ce {WSe2}\) to achieve double resonance.	理论基础学科-理论物理-凝聚态物理	129149	2	3
recuUx6RZIHIw6	For the Levenberg-Marquardt algorithm, under the condition of local error bound, When the regularization parameter $\lambda_k$ of the algorithm is selected as $\lambda_k=\mu_k\left\ / J_k^T F_k\right\ / ^\delta \quad(\delta \in(0,2])$, what is the local convergence rate of the algorithm?	{'step_1': 'Introduce the Levenberg-Marquardt algorithm with the regularization parameter choice $\\lambda_k = \\mu_k ||J_k^T F_k||^\\delta$, where $\\delta \\in (0,2]$, and $\\mu_k$ is updated using trust region techniques to ensure step acceptance.', 'step_2': 'State the assumptions (concept_2) : Jacobian $J(w)$ is Lipschitz continuous on a neighborhood of the solution, meaning $||J(w) - J(v)|| \\le L1 ||w - v||$ for some constant $L1$, and the local error bound condition holds, i.e., $c * dist(w, S) \\le ||J(w)^T F(w)||$ for some constant $c$, where $dist(w, S)$ is the distance from $w$ to the solution set $S$.', 'step_3': 'The step $d_k$ satisfies $||d_k|| \\le c1 * dist(w_k, S)$, indicating that the step size is proportional to the distance to the solution set.', 'step_4': '$\\lambda_k \\le c2 * dist(w_k, S)^\\delta$, due to the relationship between $||J_k^T F_k||$ and $dist(w_k, S)$ via the error bound condition.', 'step_5': 'Analyze $||J(w_{k+1})^T F(w_{k+1})||$ using the error bound (concept_1), Lipschitz continuity (concept_2), and step bounds to derive $dist(w_{k+1}, S) \\le c3 * dist(w_k, S)^{min{1+\\delta, 2}}$ for some constant $c3$.', 'step_6': 'Conclude that the sequence ${w_k}$ converges to the solution set $S$ with a local convergence rate of $min{1+\\delta, 2}$. When $\\delta \\in (0,1)$, the rate is $1+\\delta$ (superlinear) (Theorem 1), and when $\\delta \\in [1,2]$, the rate is 2 (quadratic).'}	{'concept_1': 'Local error bound condition - A weaker condition than nonsingular Jacobian, stating that $c * dist(w, S) \\le ||J(w)^T F(w)||$ for some $c > 0$, which ensures good convergence behavior near solutions.', 'concept_2': 'Lipschitz continuity - The Jacobian matrix $J(w)$ satisfies $||J(w) - J(v)|| \\le L1 ||w - v||$, implying smooth variation and facilitating analysis of steps and convergence.', 'Theorem 1': 'Under the condition of local error bound, Taking the LM parameter $\\lambda_k$ of the algorithm is selected as $\\lambda_k=\\left\\ / J_k^T F_k\\right\\ / ^\\delta \\quad(\\delta \\in(0,1])$, the local convergence rate is $1+\\delta$. when $\\delta=1$ the local convergence rate is 2.'}	$\min \{1+\delta, 2\}$	理论基础学科-理论计算机科学-算法理论	16	1	2
recuUAuq72bAav	Among bromine, nitrogen, and sulfur as dopants incorporated into the TiO₂ lattice, which element most effectively promotes photocatalytic dry reforming of methane?	{'step_1': 'According to the Concept_1, the S doping induces the formation of Ti-O-S bonds and SO42- ions on the surface of S-TiO2. It will generate oxygen vacancies and reduce the band gap of TiO₂, thus leading to the highest activity.', 'step_2': 'The S doping exhibits the smallest arc radius of the impedance curve. According to the Nyquist cycle principle, S doping induces the fastest transmission rate of electrons and holes. Therefore, the S doping leads to the highest activity.'}	{'concept_1': 'Dopants may become incorporated into the TiO₂ lattice, generating oxygen vacancies and thereby modulating the band gap of TiO₂.', 'theory_1': 'Based on the Nyquist cycle principle, the smaller the arc radius of the impedance curve, the smaller the carrier transport resistance in the semiconductor, and the faster the transmission rate of electrons and holes.'}	S is the best dopant to enable the highest activity of the catalyst.	理论基础学科-理论化学-计算化学	51240	1	2
recuUAxAsOdXSl	What is the expression for the turbulent mass diffusion coefficient D_t in the two-equation turbulent (TET) model within the computational mass transfer (CMT) framework, and how is it determined through the coupled transport equations for k−ε model.	  Step 1: Assign initial values to variables such as k, ε, c^2, ε_c, and D_t.  Step 2: Coupling Concept 1 and Concept 2 to obtain the distributions of the pressure field and velocity field.  Step 3: Coupling Concept 3 and Concept 4 to obtain the distributions of k and ε.  Step 4: Coupling the results from 1) and 2) into 3) to obtain μ_t, and verify whether the distribution of k and ε in 3) is correct.  Step 5: Solve Concept 6 and obtain the distribution of the mass fraction Yi  Step 6: Solve Concept 7 and obtain the distribution of c^2  Step 7: Substitute the results from Step 5 into Concept 8 and derive the distribution of ε_c.  Step 8: Substitute the values of k and ε obtained from Step 3, c^2 from Step 6, and ε_c from Step 7 into Concept 9 to solve for D_t.	  Concept1: \frac{\partial(\gamma\rho_fU_j)}{\partial t}+\frac{\partial(\gamma\rho_fU_iU_j)}{\partial x_i}=-\gamma\frac{\partial P}{\partial x_j}+\gamma\rho_fg+\frac{\partial}{\partial x_i}\left[\gamma\rho_f(\mu+\mu_t)\frac{\partial U_j}{\partial x_i}\right]+\gamma S_L  Concept2: \frac{\partial(\gamma\rho_f)}{\partial t}+\frac{\partial(\gamma\rho_fu_i)}{\partial x_i}=\sum_{i=1}^NS_i, where S_i=-(1-\gamma)\rho_sM_i\frac{\partial q_i}{\partial t},  \gamma=2.14z^2-2.53z+1,z\leq0.637, \gamma=\gamma^\infty+0.29e^{(-0.6z)}\cos\left(2.3\pi(z-0.16)\right)+0.15e^{(-0.9z)},z>0.637; z=\frac{(R-r)}{d_p}  Concept3: \frac{\partial(\gamma\rho_fk)}{\partial t}+\frac{\partial(\gamma\rho_fU_ik)}{\partial x_i}=\frac{\partial}{\partial x_i}\left[\gamma\left(\mu+\frac{\mu_t}{\sigma_k}\right)\frac{\partial k}{\partial x_i}\right]+\gamma\mu_t\frac{\partial^2\bar{u}}{\partial x^2}-\gamma\rho_f\varepsilon  Concept4: \frac{\partial(\gamma\rho_f\varepsilon)}{\partial t}+\frac{\partial(\gamma\rho_fU_i\varepsilon)}{\partial x_i}=\frac{\partial}{\partial x_i}\left[\gamma\left(\mu+\frac{\mu_t}{\sigma_\varepsilon}\right)\frac{\partial\varepsilon}{\partialx_i}\right]+C_{1\varepsilon}\gamma\frac{\varepsilon}{k}\mu_t\frac{\partial^2\bar{u}}{\partial x^2}-C_{2\varepsilon}\rho_f\gamma\frac{\varepsilon^2}{k}  Concept5: \mu_t=C_\mu\rho_f\frac{k^2}{\varepsilon}  Concept6: \frac{\partial(\gamma\rho_f\bar{Y}_i)}{\partial t}+\frac{\partial(\gamma\rho_f\bar{Y}_iu_i)}{\partial x_i}=\frac{\partial}{\partial x_i}\left[\gamma\rho_f(D+D_t)\frac{\partial\bar{Y}_i}{\partial x_i}\right]+S_i  Concept7:\frac{\partial(\gamma\rho_f\overline{c^2})}{\partial t}+\frac{\partial(\gamma\rho_fu_i\overline{c^2})}{\partial x_i}=\frac{\partial}{\partial x_i}\lceil\gamma\rho_f(D+\frac{D_t}{\sigma_c})\frac{\partial\left(\overline{c^2}\right)}{\partial x_i}\rceil+2\gamma\rho_fD_t\frac{\partial\overline{Y}_i}{\partial x_i}\frac{\partial\overline{Y}_i}{\partial x_i}-2\gamma\rho_f\varepsilon_c  Concept8:\frac{\partial(\gamma\rho_f\varepsilon_c)}{\partial t}+\frac{\partial(\gamma\rho_f\varepsilon_cu_i)}{\partial x_i}=\frac{\partial}{\partial x_i}\left[\gamma\rho_f(D+\frac{D_t}{\sigma_{\varepsilon_c}})\frac{\partial(\varepsilon_c)}{\partial x_i}\right]+C_{c1}\gamma\rho_fD_t\frac{\varepsilon_c}{\overline{c^2}}\frac{\partial\overline{Y}_i}{\partial x_i}\frac{\partial\overline{Y}_i}{\partial x_i}-C_{c2}\gamma\rho_f\frac{\varepsilon_c^2}{\overline{c^2}}-C_{c3}\gamma\rho_f\frac{\varepsilon\varepsilon_c}{k}(\mathrm{S}17)  Where \sigma_{\mathfrak{c}}=1.0,\sigma_{\mathfrak{c}\mathfrak{c}}=1.0,C_{\mathfrak{c}0}=0.11,C_{\mathfrak{c}1}=2.2,C_{\mathfrak{c}2}=2.2,C_{\mathfrak{c}3}=0.8  Concept9: D_t=C_{c0}k\left(\frac{k\overline{c^2}}{\varepsilon\varepsilon_c}\right)^{1/2}	D_t=C_{c0}k\left(\frac{k\overline{c^2}}{\varepsilon\varepsilon_c}\right)^{1/2}	理论工程学科-理论工程力学-流体力学	68891	1	3
recuUCIeRUPBHf	In mammals, different species show significant differences in the regenerative ability of the ear margin after injury. For example, rabbits are able to completely regenerate ear margin tissue, while mice usually only undergo fibrotic repair and cannot restore the missing structures.  Hints:  HINT1: Studies have found that this difference may be related to the expression levels of specific genes at the wound site. Certain genes were observed to be highly expressed in rabbit ear margin tissue, but expressed at lower levels in the corresponding cells of mice. It is known that mouse ear margin regeneration is regulated by genes and molecular signals, and some factors can promote regeneration by activating relevant pathways. First, scientists studied the differences in gene expression among different cell types: single-cell analysis showed that wound-induced fibroblasts (WIFs) responded differently in rabbits and mice after injury: rabbits exhibited molecular characteristics associated with regeneration, whereas mice showed a repair/fibrosis-prone phenotype. scRNA-seq further subdivided fibroblasts: rabbits had 10 subclusters (RF0–RF9), mice had 6 subclusters (MF0–MF5), with RF1 (rabbit) and MF3 (mouse) identified as WIFs. Regeneration-related gene sets (469 genes) were mainly enriched in WIFs (statistically significant, Wilcoxon test P<0.001).  HINT2: The representative marker for rabbit WIFs is Cr2 (RNAscope imaging), and for mouse WIFs is Tnn(immunostaining). Stereo-seq located these cells and gene expression patterns at high resolution in tissue sections. RNA velocity analysis showed that the cell fate trajectories of WIFs differed between the two species (arrows indicating transition direction). Several key pathway genes were differentially expressed: regeneration-related genes such as Bmp2, Lef1, Fgf18, Pdgfd, Scube2 were more prominent in rabbits, while genes related to a myofibroblast phenotype such as Acta2 (α-SMA) were higher in mice.  HINT3: Spatial transcriptomics (Stereo-seq) and CytoTRACE analysis showed that rabbit WIFs at the wound site were in a more “less differentiated/more primitive/greater developmental potential” state (with significantly higher CytoTRACE scores normalized against chondrocytes), a phenotype associated with strong regenerative ability. Researchers compared the single-cell transcriptome sequencing results of a regenerating ear-margin species and mice, identifying nine genes (Wnt11, St3gal5, Srfbp1, Sncaip, Pdgfd, Mmp13, Epha7, Plcl2, Aldh1a2).  HINT4: Using an AAV-transposase-mediated somatic integration strategy, researchers stably activated Aldh1a2 in mouse auricles and observed significant differences in ear hole closure at 30 dpi between the overexpression group and controls (n=10). Immunostaining confirmed Aldh1a2 expression. At 10 dpi, strong activation of Aldh1a2 was detected in rabbit auricles but barely in mice (n=6).  HINT5: Bulk RNA-seq showed that Aldh1a2 was upregulated in regenerating tissues but downregulated or expressed at low levels in non-regenerating tissues. Intraperitoneal injection of retinoic acid (RA) allowed mice to fully close ear holes and restore missing cartilage by 100 dpi. At 30 dpi, regeneration-related neural markers (β-III Tubulin) were detected in the regeneration area. In contrast, injections of retinol, TLZ, or DMSO failed to achieve complete closure (TLZ only partially reduced the hole size).  HINT6: RA exhibited time-dependent effects depending on the administration window (n=6). RA also restored auricle closure in rats, whereas inhibition of RA synthesis with DEAB weakened regeneration in rabbits (n=6).  HINT7: At the gene regulatory level, Micro-C in rabbits at 10 dpi captured enhancer–promoter interactions at the Aldh1a2 locus on chromosome 17. Functional validation showed that a 4 kb rabbit Aldh1a2 promoter (Rb-Apr) could drive GFP expression in transgenic mice, while the mouse homologous promoter (M-Apr) showed weak expression. The rabbit AE1 enhancer could also drive expression in mice. ATAC-seq in rats failed to detect injury-induced open chromatin regions at the corresponding sites, and Aldh1a2 transcription decreased after injury.  HINT8: In cis-element swap experiments, promoter-swap mice carrying the rabbit promoter showed a trend of reduced ear hole size at 30 dpi (P=0.062), while transgenic mice with AE1-driven Aldh1a2 expression showed significantly smaller ear holes at 30 dpi (P<0.01).  HINT9: At both single-cell and tissue levels, RA-treated and control groups showed differences in major cell cluster distributions on the UMAP at 15 dpi. Lineage tracing (Tnn-P2A-CreERT2; CAG-LSL-tdTomato) under RA+tamoxifen conditions showed that regenerated stromal tissue was occupied by tdTomato-positive cells. scRNA-seq revealed altered mesenchymal cell responses to RA, with WIFs showing the highest response. Acta2 (α-SMA) was downregulated at both transcript and protein levels in the RA-treated group. AAV-driven Bmp2 expression in a small cohort reduced ear hole size, often accompanied by morphological changes. ATAC-seq at 15 dpi in RA-treated groups detected both upregulated and downregulated open chromatin peaks, with upregulated peaks enriched for specific motifs. Phosphorylation levels of c-Jun significantly increased following RA activation.  Checklist (10 points total):  From the experiments, which gene(s) can be identified as associated with mouse ear margin repair and regeneration? (5 points)  What are the inferred functions of these genes? (5 points)	First, we can start by identifying differences in biological phenomena. According to the question, the known experimental observations are:  After auricular injury, rabbits can fully regenerate, including cartilage and skin;  After auricular injury, mice undergo only fibrotic repair and cannot restore the missing structures.  Conclusion: The differences in auricular regeneration capacity between species suggest the regulatory role of key genes or signaling pathways.  Second, the question mentions analyzing cell types and gene expression.  Using single-cell transcriptomics (scRNA-seq) to analyze auricular tissue, focusing on wound-induced fibroblasts (WIFs), because they are the main executors of regeneration.  Results showed that rabbit WIFs are more likely to enter the regeneration/burst proliferation-differentiation trajectory, whereas mouse WIFs are more inclined toward repair/myofibroblast differentiation.  Rabbit WIFs exhibit regeneration-associated gene features (e.g., Bmp2, Lef1, Fgf18, Pdgfd, Scube2), whereas mouse WIFs tend toward repair/myofibroblast differentiation with high expression of related genes (e.g., Acta2/α-SMA).  By comparing regenerating species (rabbits) and non-regenerating species (mice), nine core genes were found to be enriched in regenerating tissue, among which Aldh1a2 was highly expressed in rabbits but lowly expressed in mice.  Spatial localization and temporal expression validation  Stereo-seq and RNAscope/immunostaining showed that Aldh1a2 was significantly expressed in the rabbit auricular wound region, but was almost undetectable in mice.  Time-series analysis (10 dpi, 30 dpi) showed that Aldh1a2 was strongly activated early in rabbits but expressed at low levels in mice, suggesting its importance in the initiation of regeneration.  For final confirmation, the question also mentioned functional experiments.  In mice, stable activation of Aldh1a2 mediated by AAV-transposase in auricular tissue significantly improved earhole closure. Substance supplementation experiments showed that intraperitoneal injection of retinoic acid (RA) enabled complete earhole closure and cartilage regeneration in mice, whereas injection of precursor substances (e.g., retinol, TLZ, DMSO) was less effective. These experiments demonstrate that Aldh1a2 activity and the RA signaling pathway are core factors controlling auricular regeneration in mice.  Summary: Aldh1a2 encodes retinaldehyde dehydrogenase, which synthesizes retinoic acid. Its upregulation increases local RA levels. RA regulates WIFs to maintain a “low-differentiation/primitive” state, activates the regeneration program, and suppresses myofibroblast differentiation (Acta2 downregulation). Therefore, the logical chain is:  Aldh1a2 → RA signaling → WIF activation → regeneration execution.	concept_1: Single-cell RNA sequencing (scRNA-seq): Principle: This technology can analyze gene expression at the resolution of a single cell, rather than measuring the average gene expression of thousands or millions of cells as in traditional bulk RNA sequencing. By sequencing the RNA within each cell, it reveals cellular heterogeneity, identifies different cell types and states, and discovers differentially expressed genes. This is crucial for understanding the functions of different cell subpopulations in complex biological systems (such as the tumor microenvironment, immune responses). Application: In this study, scRNA-seq was used to map gene activity in individual cells of injured auricles and to compare the gene activity of wound-induced fibroblasts (WIFs) in regenerative (rabbit) and non-regenerative (mouse) species.  concept_2: Spatial transcriptomics: Principle: This technology captures precise spatial location information of transcriptional activity (gene expression) while preserving tissue integrity. By combining gene expression data with the actual location of cells in tissue, it overcomes the limitation of traditional transcriptomics methods (such as RNA-seq) that lack spatial context. This allows researchers to identify cellular heterogeneity within tissues and determine the distribution of transcripts across different tissue regions. Application: Combined with scRNA-seq, spatial transcriptomics was used to study the recovery process of rabbit and mouse auricle injuries to understand the spatial organization of gene activity in the injury response and more precisely locate key molecular differences.  concept_3: Exogenous retinoic acid (RA) supplementation: Principle: Directly applying or injecting the target substance (RA) to the injury site to directly test whether it can compensate for endogenous deficiencies and promote biological processes (regeneration).  concept_4: Gene activation/transgenic mouse construction: Principle: By introducing or activating specific gene regulatory elements (such as enhancers), the expression level of a target gene (such as Aldh1a2) can be increased or restored in specific tissues or at specific time points. This aims to restart or enhance dormant endogenous pathways in the organism. Application: Researchers identified specific regulatory elements (enhancers) in the rabbit genome that can activate the Aldh1a2 gene and introduced them into transgenic mice to activate the mice’s own Aldh1a2 gene expression, thereby testing its effect on auricle regeneration.  concept_5: Stereo-seq: Stereo-seq, an independently developed spatiotemporal omics technology by BGI, is based on DNA Nano Ball (DNB) technology. It is a panoramic in situ technology with high throughput, ultra-high resolution, and a large field of view, enabling spatial transcriptomic analysis at four scales simultaneously—tissue, cell, subcellular, and molecular—within the same sample.	1. Aldh1a2(5 points) 2. Located in the retinoic acid synthesis pathway, associated with retinoic acid synthesis(5 points)	理论基础学科-生物学-分子生物学	66479	1	4
recuUKUkTzcH42	Given a pressure function $\Gamma$ and entropy function $\mathcal{h}$, express the dual representations of the pressure function and the entropy functional.	{'step_1': 'First, based on concept_1, the paper uses Convex Analysis methods which are crucial for studying dual representations as convex functions often have dual characterizations.', 'step_2': 'Then, theorem_1 states the establishment of an abstract variational principle, which is fundamental in relating pressure and entropy.', 'step_3': 'The pressure function $\\Gamma$ in concept_2 is a key component in thermodynamic formalism, and its definition is closely linked to the supremum over measures involving entropy.', 'step_4': 'The entropy function $\\mathcal{h}$ in concept_3, being an affine entropy-like map, is paired with the pressure function in the variational principle.', 'step_5': 'Using theorem_2 on duality in convex functions, the pressure function, as a convex function, can be represented as the supremum of linear functionals involving the entropy and the integral of the potential.', 'step_6': 'Conversely, the entropy functional, being the dual of the pressure function, is expressed as the infimum over the pressure function minus the integral of the potential, completing the dual representations.'}	{'concept_1': 'A function $\\Gamma: \\mathbf{B} \\rightarrow \\mathbb{R}$ is called a pressure function if it satisfies the following conditions: Monotonicity, Translation invariance, Convexity.', 'theorem_1': 'Abstract variational principle for countably and finitely additive probability measures', 'concept_2': 'Pressure function in thermodynamic formalism', 'concept_3': 'Entropy function (entropy-like map) in dynamical systems', 'theorem_2': 'Duality in convex functions'}	\boxed{\Gamma(\varphi)=\sup_{\mu\in\mathcal{P}_a(X)}\left\{\mathfrak{h}(\mu)+\int\varphi d\mu\right\}}\quad(\forall\varphi\in\mathbf{B})\boxed{\mathfrak{h}(\mu)=\inf_{\varphi\in\mathbf{B}}\left\{\Gamma(\varphi)-\int\varphi d\mu\right\}}\quad(\forall\mu\in\mathcal{P}_a(X))	理论基础学科-基础数学-拓扑动力系统	618703	2	2
recuUNjx6dbcyQ	How to optimize the composition and properties of artificial humus through FeOOH catalytic strategy, so as to regulate the conversion of waste biomass into artificial humus and enhance its carbon sequestration capacity? You need to answer from the following points, component optimization and property improvement, and how to enhance carbon sequestration capacity.	"{'step_1': 'Dismantle the core problem and think clearly about the framework First, we need to focus on the core requirements of Instruction: with FeOOH as the catalytic core, we can solve the problem of directional conversion of waste biomass into artificial humus from the two dimensions of ""component optimization and property improvement"" and ""enhancing carbon sequestration capacity"". Based on the common properties of iron-based catalysts (Concept1) and the reaction law of key components of humus (Concept2), the regulatory mechanism of FeOOH on humus components and properties is analyzed. Combined with the correlation between carbon flow direction and carbon sequestration (Concept3), the path to enhance carbon sequestration capacity is derived to ensure that each analysis dimension can be associated with the corresponding concept to avoid logical faults.', 'step_2': 'Optimization of humic acid functional groups by FeOOH based on iron-based catalyst commonality (associated with Concept1 and Concept2) First, Concept1 pointed out that nFe₂O₃ (iron-based oxide) treatment can increase the C—O bonds of humic acid (HAs) in artificial humic acid (AHS). As a typical iron-based catalytic material, FeOOH has similar redox properties to nFe₂O₃ at its active site, so it can be inferred by analogy that FeOOH can also promote the formation of C—O bonds in HAs through catalysis. Further combined with Concept2 (humus containing oxygen-containing functional groups such as carboxyl groups, phenolic groups, and carbonyl groups), it can be seen that C-O bonds are the core structural units of these oxygen-containing functional groups (e.g., carboxyl-COOH, phenolic-OH, etc.). Therefore, the increase of C—O bonds inevitably leads to an increase in the abundance of oxygen-containing active functional groups in HAs, reduces the proportion of small molecules that are easily decomposed of organic matter, and realizes the directional optimization of artificial humus core components (HAs).', 'step_3': 'Analyze the regulation of biomass humification pathway by FeOOH in combination with the conversion law of polysaccharides (Associated with Concept2) Concept2 clarified that ""polysaccharide dehydration to furan can catalyze the conversion of biomass to humic acid"". The main components of waste biomass include polysaccharides (e.g., cellulose, hemicellulose hydrolysate), which are easily decomposed into volatile small organic compounds (e.g., CO₂, small molecular alcohols) if not catalyzed directedly, resulting in carbon loss and inefficient conversion to humic acid. As a catalyst, FeOOH can promote the dehydration of hydroxyl groups in polysaccharide molecules by reducing the activation energy of polysaccharide dehydration reaction, and produce furan intermediate compounds. These intermediate products have high reactivity and can quickly participate in subsequent condensation and polymerization reactions, and are directly converted into structural units of humic acid rather than to non-target products. This process accurately utilizes the polysaccharide conversion mechanism of Concept2 to realize the directional path regulation of ""biomass→ polysaccharides→furans→ humic acids"", which not only increases the proportion of humic acid, but also lays the foundation for carbon retention, which is logically in line with the ""path-oriented"" principle of catalytic reactions.', 'step_4': 'Derive the improvement of properties from component optimization, and establish a ""component-property"" association (associate Concept1, Concept2) Based on the component optimization results of step 2 (increased C-O bonds, enrichment of oxygen-containing functional groups) and step 3 (increase in the proportion of HAs), further derive the changes in the properties of artificial humus: Improved chemical stability: The increase of C—O bonds in Concept1 will form a denser cross-linked structure (e.g., humic acid molecules are connected by C—O—C bonds), This structure can resist the decomposition of microorganisms (microorganisms are difficult to break stable C-O bonds), reduce the biodegradation rate of humus, and prolong its existence time in the environment. Functional activity enhancement: The oxygen-containing functional groups such as carboxyl and phenolic groups mentioned in Concept2 are the key sites for the binding of humus to metal ions and nutrients, and its abundance will enhance the adsorption capacity and nutrient retention capacity of humus, and improve its functional properties. This step connects Concept1 and Concept2 through the logical chain of ""component change→ structural change→ property change"", and derives the process in line with the basic chemical principle of determining the properties of the structure of matter.', 'step_5': 'Analyze the ""directional enrichment"" effect of FeOOH on carbon sequestration capacity based on carbon flow law (Concept1, Concept2, Concept3 linked) Concept3 points out that ""artificial humification can enhance carbon sequestration by directing carbon to AHS"", which is the core logic of carbon sequestration - carbon can only be achieved efficiently if it remains in stable humic acid. FeOOH enables the directional guidance of carbon through the following paths, each associated with a corresponding concept: Carbon source conversion guidance: Combined with Concept2, FeOOH catalyzes the dehydration of polysaccharides to form furans and converts them into humic acid, converting the ""polysaccharide carbon"" (easily decomposed into CO₂ or small molecules) in biomass into ""humic acid carbon"" to avoid carbon loss in the form of gaseous or easy loss. Targeted construction of carbon pool: Finally, through the above two steps, the carbon in biomass is concentrated and directed to the core carbon pool of artificial humic acid (AHS), which is fully in line with the law of Concept3 ""carbon to AHS to guide and enhance carbon sequestration"", realizing efficient carbon enrichment, and logically forming a closed loop of ""catalytic regulation→ carbon flow to stable carbon pool →carbon sequestration enhancement"".'}"	{'concept_1': 'XPS and FTIR analysis of nFe₂O₃ catalysts showed that the C—O bonds in HAs formed in artificial humic acid (AHS) after nFe₂O₃ catalyst treatment were more', 'concept_2': 'The structure of humus is complex, it contains carboxyl groups, phenolic groups, carbonyl groups, polysaccharides and other components, and researchers promote the dehydration of polysaccharides in humus through a series of experimental operations to promote the dehydration of polysaccharides and produce compounds such as furans, which can catalyze the conversion process of biomass to humic acid and promote the production of humic acid', 'concept_3': 'Researchers have studied lignin and found that carbon can be directed into artificial humic acid through the process of artificial humus, which enhances the carbon sequestration capacity of artificial humus.'}	FeOOH can increase the content of oxygen-containing functional groups, and it can also increase the content of polysaccharides and converted furan derivatives, which promotes the formation of artificial humic acid. The content of artificial humic acid in artificial humus can be induced to flow to artificial humic acid and enhance carbon sequestration ability. Checklist Score point 1: FeOOH increases the content of oxygen-containing functional groups Scoring point 2: The production of artificial humic acid was promoted by increasing the content of polysaccharides and the converted furan derivatives Score point 3: Because carbon will enter artificial humic acid, its content can enhance carbon sequestration capacity	理论基础学科-理论化学-生物质催化	171462	2	3
recuUCQatgMMwP	When performing the OER process on Fe-doped Ni(M)OOH LDHs, which of the following three mechanisms—adsorbed species evolution mechanism (AEM), intramolecular oxygen coupling (IMOC), and lattice oxygen mechanism (LOM)—is more likely to occur?	"{'step_1': '(From Concept_1) If LOM occurrence requires high M–O covalency, but Fe–O exhibits stronger ionic character, the tendency for LOM is weakened: (LOM → high covalency) ∧ (Fe–O is more ionic) ⇒ ¬preference(LOM).', 'step_2': '(From Concept_3) The key distinction among the three mechanisms lies in O–O bond formation: AEM = adsorbed O· bonds with water/HO⁻; IMOC = intramolecular coupling of two O· radicals at the same/adjacent sites; LOM = lattice oxygen participates in bond formation.', 'step_3': ""(From Concept_2) If Fe sites are high-spin and stabilize adsorbed O· radicals, the 'adsorbed O·'-centered bond formation pathway becomes kinetically more feasible: high-spin(Fe) ⇒ stabilizes(O·) ∧ ↓E‡ (for adsorbed O·-centered bonding)."", 'step_4': '(Structural constraints from Concept_3) IMOC requires spatial proximity and synchronous coupling of two O· radicals, imposing stronger configurational/synchronization constraints; AEM only involves one adsorbed O· reacting with water/HO⁻, with weaker constraints: constraints(IMOC) > constraints(AEM).', 'step_5': ""(Decision criterion) Given ¬preference(LOM), between the two 'adsorbed O·'-centered candidate pathways, prioritize the one with weaker configurational/synchronization constraints and lower expected energy barrier ⇒ preference(AEM)."", 'step_6': 'Conclusion: The more probable mechanism is AEM (consistent with the given Answer).'}"	{'concept_1': 'Generally, the contribution of LOM (Lattice Oxygen Oxidation Mechanism) is considered dependent on the covalency of metal-oxygen bonds, whereas Fe-O bonds exhibit stronger ionic character.', 'concept_2': 'Fe sites in Ni(Fe)OOH possess high spin population. This high-spin characteristic effectively stabilizes reactive O∙ radicals, reducing the activation energy for O-O bond formation.', 'concept_3': 'Among the three mechanisms – AEM (Adsorbate Evolution Mechanism), IMOC (Intramolecular Oxygen Coupling), and LOM – O-O bond formation constitutes the key differentiating chemical step: AEM: Forms bonds via adsorbed reactive oxygen radicals reacting with water; IMOC: Achieves bonding through intramolecular coupling of reactive oxygen radicals; LOM: Typically involves lattice oxygen bonding with water/reactive oxygen radicals'}	AEM	理论基础学科-理论化学-电化学，计算化学	94275	2	4
recuUn0dsRK8W5	The `Find-Weakly-Robust-Detailing` procedure is designed to find a weakly robust detailing of a distribution by iteratively refining a set of variables, U. This process is guaranteed to terminate within a finite number of steps because each unsuccessful step makes quantifiable progress toward the goal. The procedure's logic relies on a subroutine, `Test-Weakly-Robust-Detailing`, which upon failure provides a new set of variables to advance the search. Considering that the 'index' of any detailing is a value strictly bounded between 0 and 1, what is the maximum number of iterations the main loop of this procedure will execute, as determined by the robustness parameter $\delta$?	"{'step_1': ""The core of the problem is to bound the number of iterations in the `Find-Weakly-Robust-Detailing` procedure's main loop (Concept_3). We can analyze this using the index of the detailing, $Ind(\\xi)$, as a potential function that measures progress toward the goal (Concept_1)."", 'step_2': 'The loop continues only if the current detailing is not $(\\delta, k)$-weakly robust (Concept_2). In each such iteration, the `Test-Weakly-Robust-Detailing` subroutine is called and fails, which is the condition for the loop to execute again.', 'step_3': ""A failure in the `Test-Weakly-Robust-Detailing` subroutine is designed to be productive. It returns a new set of variables that guarantees a strictly positive increase in the detailing's index (Concept_4). The minimum guaranteed increase per iteration is a fixed amount related to the parameter $\\delta$, specifically $4\\delta/10$ or $2\\delta/5$."", 'step_4': 'The total amount of increase is limited. The index of any detailing is bounded and must lie within the range $[0, 1]$ (Theorem_1). Therefore, the total increase accumulated across all iterations cannot exceed the maximum possible value of 1.', 'step_5': 'To find the maximum number of iterations, we divide the total possible increase of the potential function by the minimum guaranteed increase per step. This gives us the calculation: Maximum Iterations = (Total Range of Index) / (Minimum Increase per Iteration).', 'step_6': 'Plugging in the values, we get Maximum Iterations = $1 / (2\\delta/5)$. This simplifies to the final answer, $5/(2\\delta)$, which represents the upper bound on the number of times the loop can execute before the index would theoretically have to exceed 1.'}"	"{'concept_1': 'The index of a detailing, denoted $Ind(\\xi)$, is a measure defined as $Ind(\\xi)=E_{i\\sim[n]}[E_{a-\\xi|_{2}}[Pr_{x-\\xi|_{1}^{2:a}}[x_{i}=1]^{2}]]$. It serves as a potential function to measure progress.', 'concept_2': 'A detailing is $(\\delta, k)$-weakly robust if its index is stable against most small refinements. The goal of the `Find-Weakly-Robust-Detailing` procedure is to find a variable set U that defines such a detailing.', 'concept_3': 'The `Find-Weakly-Robust-Detailing` procedure is an iterative algorithm that builds a set of variables U. If at any step the current detailing is not found to be weakly robust, the set U is expanded, and the process continues.', 'concept_4': 'The `Test-Weakly-Robust-Detailing` subroutine is called in each iteration. If it determines that a detailing is not weakly robust, it returns a ""witness"" variable set U\'. Incorporating this set into the current detailing guarantees a strictly positive increase in the detailing\'s index.', 'theorem_1': 'The index of any detailing, $Ind(\\xi)$, is a value within a bounded range, specifically $0\\le Ind(\\xi)\\le1$.'}"	$\frac{5}{2\delta}$	理论基础学科-理论计算机科学-算法理论	243006	3	4
recuUuxQ6qLQhD	Principal Minor Equivalence (PME) is a relation where two matrices have identical corresponding principal minors. For the specific case of a $3 \times 3$ irreducible matrix A that is PME to another matrix B, what is the maximum length of the cut-transpose sequence needed to relate them?	"{'step_1': 'The problem requires finding the maximum length of the cut-transpose sequence for two $3 \\times 3$ irreducible matrices, A and B, which are known to be Principal Minor Equivalent (PME).', 'step_2': ""The cut-transpose operation is a transformation that can only be performed on a matrix that possesses a 'cut'."", 'step_3': 'According to concept_3, the very definition of a cut is restricted to matrices of size $n \\ge 4$.', 'step_4': 'Since the matrices in this problem are $3 \\times 3$, they do not meet the minimum size requirement for a cut to exist. Therefore, the cut-transpose operation is not applicable.', 'step_5': 'The resolution for this specific case is provided directly by theorem_1. It states that for an irreducible matrix of size $n=3$, the property of being PME to another matrix is a sufficiently strong condition that it implies the two matrices are already diagonally equivalent.', 'step_6': 'As PME directly leads to diagonal equivalence for this dimension, no intermediate transformation steps are required to relate matrix A and matrix B.', 'step_7': 'Consequently, the sequence of cut-transpose operations is empty, and its maximum length is 0.'}"	{'concept_1': 'An irreducible matrix is one that cannot be permuted into a block upper triangular form.', 'concept_2': 'Two matrices A and B are diagonally equivalent if A is diagonally similar to B or its transpose, $B^T$.', 'concept_3': 'A cut of a matrix is defined only for matrices of size $n \\ge 4$.', 'theorem_1': 'For irreducible matrices of size $n=2$ or $n=3$, if they are Principal Minor Equivalent (PME), then they are necessarily diagonally equivalent.'}	0	理论基础学科-基础数学-代数	173053	2	2
recuUVeBR5w0MW	Using the lifted formulation for certifiable Schubert calculus in type A, determine the total number of bilinear equations in the system $S^A(u, v, w_{\circ}w)$ required to test for non-vanishing Schubert coefficients for the specific case in $S_4$ with permutations $u=2143$, $v=3124$, and $w=4132$. In this context, Schubert coefficients are the structure constants in the ring of Schubert polynomials, and the lifted formulation is a method to create a system of polynomial equations whose solutions correspond to the geometric problem of intersecting Schubert varieties.	"{'step_1': 'The instruction asks for the total number of bilinear equations in the system $S^A(u, v, w_{\\circ}w)$ for a specific instance in $S_4$: $u=2143$, $v=3124$, and $w=4132$. The long word is $w_{\\circ}=4321$, so $w_{\\circ}w = 1423$.', 'step_2': 'The system of equations is constructed based on the geometric interpretation of Schubert coefficients (Concept_2). The solutions to the system correspond to flags $\\omega^\\bullet$ in the intersection of three Schubert varieties: $\\Omega_{u}(F^{\\bullet})\\cap\\Omega_{v}(G^{\\bullet})\\cap\\Omega_{w_{\\circ}w}(E^{\\bullet})$.', 'step_3': 'We use the lifted formulation (Concept_5). A flag $\\omega^\\bullet$ is parameterized by Stiefel coordinates (Concept_4) and is taken from the cell $\\Omega_{w_{\\circ}w}(E^\\bullet)$. We then impose conditions for this flag to also belong to $\\Omega_u(F^\\bullet)$ and $\\Omega_v(G^\\bullet)$.', 'step_4': ""The number of conditions is determined by the maximal descent $d$ (Concept_6). For the given permutations, the paper's example in Section 5.4 identifies the maximal descent as $d=3$."", 'step_5': 'The first set of conditions ensures that the flag $\\omega^\\bullet$ lies in $\\Omega_u^A(F^\\bullet)$. According to the characterization theorem (Theorem_1), this requires that for each $i \\in [d]$, the constructed vector $g_i(x,\\alpha)$ is in the subspace $F^{u_i-1} - F^{u_i}$. This translates to a set of equations $(\\begin{matrix}y_{j1}&...&y_{jn}\\end{matrix})\\cdot g_i(x,\\alpha)=0$ for each $j < u_i$.\n- For $i=1$: $u_1=2$. The condition applies for $j < 2$, so $j=1$. This gives 1 equation.\n- For $i=2$: $u_2=1$. There are no $j < 1$. This gives 0 equations.\n- For $i=3$: $u_3=4$. The condition applies for $j < 4$, so $j=1, 2, 3$. This gives 3 equations.\n- The total number of equations for this part is $1 + 0 + 3 = 4$.', 'step_6': 'The second set of conditions ensures the flag $\\omega^\\bullet$ lies in $\\Omega_v^A(G^\\bullet)$. Similarly, this requires $h_i(x,\\beta) \\in G^{v_i-1} - G^{v_i}$, which gives equations $(\\begin{matrix}z_{j1}&...&z_{jn}\\end{matrix})\\cdot h_i(x,\\beta)=0$ for each $j < v_i$.\n- For $i=1$: $v_1=3$. The condition applies for $j < 3$, so $j=1, 2$. This gives 2 equations.\n- For $i=2$: $v_2=1$. There are no $j < 1$. This gives 0 equations.\n- For $i=3$: $v_3=2$. The condition applies for $j < 2$, so $j=1$. This gives 1 equation.\n- The total number of equations for this part is $2 + 0 + 1 = 3$.', 'step_7': 'The total number of equations in the system $S^A(u,v,w_{\\circ}w)$ is the sum of the equations from both sets of conditions. Total equations = $4 + 3 = 7$.'}"	"{'concept_1': 'Schubert Coefficients are non-negative integers $c_{u,v}^{w}$ that are defined as the structure constants for the multiplication of Schubert polynomials, which form a linear basis in the ring of polynomials $\\mathbb{Z}[x_1, x_2, \\dots]$.', 'concept_2': 'Geometric Interpretation of Schubert Coefficients states that the coefficient $c_{u,v}^{w}$ counts the number of points in the intersection of three generically translated Schubert varieties, expressed as $c_{u,v}^{w}=\\#\\{X_{u}(E_{\\bullet})\\cap X_{v}(F_{\\bullet})\\cap X_{w_{\\circ}w}(G_{\\bullet})\\}$.', 'concept_3': 'A Schubert Variety $X_w$ is the Zariski closure of a Schubert cell $\\Omega_w$. The problem of finding the Schubert coefficient is equivalent to finding the number of solutions to a system of equations that define membership in an intersection of these varieties.', 'concept_4': 'Stiefel Coordinates are a set of variables used to parameterize the flags within a Schubert cell $\\Omega_w^A$. These variables form the entries of a matrix $\\chi_{w}^{A}$ which is central to constructing the polynomial system.', 'concept_5': 'The Lifted Formulation is a method that characterizes membership in a Schubert cell using a system of bilinear equations by introducing auxiliary variables (e.g., $\\alpha, \\beta$), avoiding the use of higher-degree determinantal equations.', 'concept_6': 'The Maximal Descent, denoted by $d := \\max(Des(u) \\cup Des(v) \\cup Des(w))$, determines the number of column vectors to consider from the Stiefel coordinate matrix and sets the range of indices for the equations in the system.', 'theorem_1': ""The Characterization of Schubert Cell Membership states that a flag $\\Phi_{\\bullet}$ is in the Schubert cell $\\Omega_{w}^{A}(E^{\\bullet})$ if and only if for each $k \\in [d]$, there exist unique coefficients such that a specific linear combination of the flag's basis vectors lies within a specific subspace defined by the permutation $w$, i.e., $g_k \\in E^{w_k-1} - E^{w_k}$. This theorem provides the direct rule for converting the geometric containment problem into algebraic equations.""}"	7	理论基础学科-基础数学-几何, 代数	162613	2	4
recuUYu3dnlA7M	Given a dynamic metric k-median algorithm that operates in epochs, what is the amortized recourse achieved when the solution's stability is maintained by a ROBUSTIFY procedure whose cost is analyzed by partitioning centers from the initial solution $\mathcal{U}_{init}$ into 'clean' and 'contaminated' sets and categorizing MAKE-ROBUST calls into Type I (for new centers), Type II (for contaminated centers), and Type III (for clean centers undergoing a parameter update)? The background definition of recourse is the number of changes (insertions/deletions) in the maintained solution per update.	"{'step_1': 'The primary source of recourse in the algorithm is the `ROBUSTIFY` procedure executed at the end of each epoch. The cost of this procedure is measured by the number of calls to its subroutine, `MAKE-ROBUST` (Concept_1).', 'step_2': ""To analyze this cost, centers from the initial solution `$\\mathcal{U}_{init}$` are classified as either 'clean' or 'contaminated' based on their proximity to the points updated during the epoch (Concept_2). This classification is key to understanding why a center might need to be re-processed."", 'step_3': ""The `MAKE-ROBUST` calls are then partitioned into three distinct types. Type I calls handle newly introduced centers. Type II calls handle centers from `$\\mathcal{U}_{init}$` that were 'contaminated' by updates (Concept_3, Concept_4). Type III calls handle 'clean' centers from `$\\mathcal{U}_{init}$` that, while still robust, need their robustness parameter `t` to be increased due to changes in inter-center distances (Concept_4)."", 'step_4': 'We first bound the amortized number of Type I and Type II calls. The number of Type I calls is $O(1)$ per update on an amortized basis. For Type II calls, we use the fact that each update can contaminate at most $O(\\log \\Delta)$ centers (Theorem_1). This directly implies that the amortized number of Type II calls per update is $O(\\log \\Delta)$.', 'step_5': ""The analysis of Type III calls relies on a 'charging' argument. A Type III call for a center is part of a sequence, or 'chain,' where the center's robustness parameter `t` strictly increases with each call. Since `t` is bounded by `$\\lceil \\log \\Delta \\rceil$`, any such chain has a maximum length of $O(\\log \\Delta)$ (Concept_5)."", 'step_6': 'Crucially, each chain of Type III calls must be initiated by a center that was created via a Type I or Type II call. Therefore, we can charge the total cost of a Type III chain (of length $O(\\log \\Delta)$) to the single Type I or Type II call that started it (Concept_5).', 'step_7': 'Combining these parts, the total amortized recourse is the sum of the amortized rates of Type I and Type II calls, multiplied by the maximum length of a Type III chain that each can trigger. This yields an amortized recourse of $(O(1) + O(\\log \\Delta)) \\times O(\\log \\Delta) = O(\\log^{2}\\Delta)$.'}"	"{'concept_1': 'Epoch-based Recourse Calculation. The total amortized recourse of the algorithm is dominated by the number of calls made to the MAKE-ROBUST subroutine within the ROBUSTIFY procedure that runs at the conclusion of each epoch.', 'concept_2': ""Clean vs. Contaminated Centers. A center from the epoch's initial solution is defined as 'contaminated' if any of the epoch's updates occur within a specific radius related to its robustness parameter; otherwise, the center is considered 'clean'."", 'concept_3': 'Properties of Center Types. A clean center retains its robustness properties with respect to the updated point set at the end of the epoch. A contaminated center may lose its robustness and require a call to MAKE-ROBUST.', 'concept_4': 'Categorization of MAKE-ROBUST Calls. The MAKE-ROBUST calls, which are the source of recourse, are partitioned into three types: Type I for centers not present in the initial solution, Type II for centers from the initial solution that became contaminated, and Type III for clean centers that require an update to a higher robustness parameter t.', 'theorem_1': 'Bound on Contamination per Update. Any single point update during an epoch contaminates at most O(\\log \\Delta) centers in the initial solution set, where $\\Delta$ is the aspect ratio of the metric space.', 'concept_5': ""Chaining and Charging Argument for Type III Calls. A Type III call for a center w only occurs if its robustness parameter t must increase. This creates a 'chain' of calls over time for a center's lineage, where this parameter strictly increases. The length of any such chain is bounded by O(\\log \\Delta). Each chain must be initiated by a center generated from a Type I or Type II call, allowing the cost of the entire Type III chain to be charged to the initiating event.""}"	$O(\log^{2}\Delta)$	理论基础学科-理论计算机科学-算法理论	69065	2	1
recuV2SmOpgnXH	Let \(\varphi : \mathbb{R}^n \to \overline{\mathbb{R}}\) be prox-bounded and l.s.c. around \(\bar{x} \in \text{dom}\,\varphi\). What is the relationship between the local minimizers of \(\varphi\) and the local minimizers of \(e_\lambda \varphi\)?	{'step_1': 'Align parameters between the concept/theorem and Definition 1. Set \\( r = \\frac{1}{\\lambda} \\) to link the two frameworks.', 'step_2': 'Recast key definitions for local minimizers of \\(\\varphi\\) and \\(e_\\lambda \\varphi\\).', 'step_3': 'Prove that a local minimizer of \\(\\varphi\\) implies a local minimizer of \\(e_\\lambda \\varphi\\).', 'step_4': 'Prove that a local minimizer of \\(e_\\lambda \\varphi\\) implies a local minimizer of \\(\\varphi\\).', 'step_5': 'Verify the usage and logical consistency of concepts/theorems.', 'step_6': 'Conclude the relationship between local minimizers of \\(\\varphi\\) and \\(e_\\lambda \\varphi\\).'}	{'concept_1': '### Proposition 2.1 (Proximal Mapping) A proper lsc function \\( f : \\mathbb{R}^n \\to \\overline{\\mathbb{R}} \\) is prox-bounded with threshold \\( r_{pb} \\) if and only if \\( f(\\cdot) + (r/2)|\\cdot - x|^2 \\) is bounded below for any point \\( x \\) and any value \\( r > r_{pb} \\). If a proper lsc function \\( f \\) is prox-bounded with threshold \\( r_{pb} \\), then for any \\( r > r_{pb} \\) the following are true: (a) \\( e_r f \\) depends continuously on \\( (r, x) \\); (b) \\( P_r f(x) \\) is nonempty and compact (for all \\( x \\)); (c) \\( \\partial e_r f \\subseteq r(I - P_r f) \\) and is locally bounded; (d) \\( \\partial(-e_r f) = r(\\text{conv}\\,P_r f - I) \\); (e) \\( -e_r f \\) is lower-\\( \\mathcal{C}^2 \\); (f) \\( P_r f \\) is monotone; (g) \\( rx - rP_r f(x) \\in \\partial f(P_r f(x)) \\), (first order optimality condition); (h) \\( P_r f \\subseteq (I + (1/r)\\partial f)^{-1} \\), (by (g)); (i) if \\( w_k \\in P_r f(x_k) \\) with \\( x_k \\to \\bar{x} \\) then the sequence \\( \\{w_k\\} \\) is bounded and all its accumulation points lie in \\( P_r f(\\bar{x}) \\)'}	For all \( \lambda > 0 \) sufficiently small, the following are equivalent: (i) \( \bar{x} \) is a local minimizer of \( \varphi \). (ii) \( \bar{x} \) is a local minimizer of \( e_\lambda \varphi \).	理论基础学科-基础数学-最优化理论	104099	2	1
recuV3zhuMdGph	As for the four models of Fe-Nx-Cy; (i) FeN4-Cy: a planar carbon matrix with Fe coordinated to 4 N-atoms; (ii) Fe-N3-S1-Cy: a planar carbon matrix with Fe coordinated to 3 N-atoms and one S-atom; (iii) Fe-N4-S2-Cy: a planar carbon matrix with Fe coordinated to 4 N-atoms and one S-atom located at the second position from Fe; and (iv) Fe-N4-S3-Cy: a planar carbon matrix with Fe coordinated to 4 N-atoms and one S-atom located at 3rd position from Fe. In the naming of these models, S1, S2, and S3 refer to S-atoms that have increasing distances of separation from the Fe site (i.e., first neighbor, second neighbor, and third neighbor). Which catalyst can provide the highest ORR activity?	{'step_1': 'According to concept 1, S-doping in the first coordination sphere of FeN3-S1-Cy results in a slight elongation of the Fe-N bond and the formation of a Fe-S bond. The FeN3-S2-Cy and FeN3-S3-Cy exhibit the absence of Fe-S bond because of the long distance between Fe and S atoms.', 'step_2': 'As for the FeN3-S1-Cy, the Fe-S bond leads to the less stable and weakens the adsorption of O2. As for the FeN3-S3-Cy, the distance between Fe and S atoms is too long to enhance the spin polarization.', 'step_3': 'By comparison, the FeN3-S2-Cy with the S atom located at the second neighbor position from Fe center exhibits the enhanced spin polarization, and the distance between Fe and S atoms is moderate. Thus, Fe-N4-S2-Cy provides the highest ORR activity.'}	{'concept_1': 'Compared to N atom, the S atom has a larger atomic radius.', 'concept_2': 'The high spin polarization is crucial for the high ORR activity, because the higher spin polarization can stabilize higher oxidation states of the Fe center, making it more reactive toward O2 reduction.'}	Fe-N4-S2-Cy provides the highest ORR activity.	理论基础学科-理论化学-计算化学	140070	1	3
recuV5gM1NuuxJ	By assuming a simple harmonic-oscillator potential between the $c$ quark and the $\bar b$ quark, what are the momentum-space Wigner functions for the 1S and 1P states of the two-body system? Answer in one sentence without using the internet.	{'Step_1': 'Goal: derive the momentum-space Wigner functions for the 1S and 1P states of a c–\\bar b two-body system under a simple harmonic-oscillator (HO) potential, to be stated in one sentence.', 'Step_2': 'Two-body reduction (Concept_1): go to relative/center-of-mass coordinates with reduced mass \\(\\mu = m_c m_b/(m_c+m_b)\\) (set \\(\\hbar=c=1\\) if desired).', 'Step_3': 'HO dynamics (Concept_2): use a 3D isotropic HO for the relative motion, \\(V(r)=\\tfrac12\\,\\mu\\omega^2 r^2\\), and define widths \\(b=\\sqrt{\\hbar/(\\mu\\omega)}\\), \\(\\sigma\\equiv 1/b=\\sqrt{\\mu\\omega/\\hbar}\\).', 'Step_4': 'Eigenfunctions (Concept_3): recall HO forms—1S is a Gaussian; 1P is Gaussian times a linear polynomial; angular dependence given by spherical harmonics \\(Y_{\\ell m}\\).', 'Step_5': 'Momentum wavefunctions (Concept_4): Fourier transform the relative-coordinate eigenfunctions to get \\(\\tilde\\psi_{1S}(\\mathbf k)\\propto e^{-\\sigma^2 k^2/2}\\) and \\(\\tilde\\psi_{1P}(\\mathbf k)\\propto (\\sigma k)\\,Y_{1m}(\\hat k)\\,e^{-\\sigma^2 k^2/2}\\).', 'Step_6': 'Wigner transform to momentum space (Concept_5): using the definition and integrating over \\(\\mathbf r\\), the momentum-only Wigner density is proportional to \\(|\\tilde\\psi(\\mathbf k)|^2\\); thus the Gaussian exponent becomes \\(e^{-\\sigma^2 k^2}\\) (no 1/2), and 1P carries a factor \\((\\sigma k)^2\\).', 'Step_7': 'm-substate averaging (Concept_6): average the 1P result over magnetic substates \\(m\\) to obtain the isotropic factor \\(2/3\\) multiplying \\(\\sigma^2 k^2\\).', 'Step_8': 'Normalization convention (Concept_7): adopt box/plane-wave normalization so that \\(\\int d^3k\\, W(\\mathbf k)=(2\\pi)^3/V\\); evaluate \\(\\int d^3k\\,e^{-\\sigma^2 k^2}=\\pi^{3/2}/\\sigma^3\\) to fix the prefactor as \\((2\\sqrt\\pi\\,\\sigma)^3/V\\).'}	{'Concept_1': 'Reduce the two-body problem to **relative/center-of-mass coordinates** with **reduced mass** \\(\\mu=m_c m_b/(m_c+m_b)\\).', 'Concept_2': 'Use a **3D isotropic harmonic-oscillator (HO) potential** for relative motion, \\(V(r)=\\tfrac12\\,\\mu\\omega^2 r^2\\).', 'Concept_3': 'Introduce the **HO width parameters**: coordinate-space length \\(b=\\sqrt{\\hbar/(\\mu\\omega)}\\) and momentum-space width \\(\\sigma\\equiv 1/b\\) (set \\(\\hbar=c=1\\) if desired).', 'Concept_4': 'Recall **HO eigenfunctions**: 1S is a Gaussian, 1P is Gaussian \\(\\times\\) a linear polynomial, with angular parts given by **spherical harmonics** \\(Y_{\\ell m}\\).', 'Concept_5': 'Obtain **momentum-space wavefunctions** by Fourier transform: \\(\\tilde\\psi_{1S}(\\mathbf k)\\propto e^{-\\sigma^2 k^2/2}\\), \\(\\tilde\\psi_{1P}(\\mathbf k)\\propto (\\sigma k)\\,Y_{1m}(\\hat k)\\,e^{-\\sigma^2 k^2/2}\\).', 'Concept_6': 'Use the **Wigner transform** definition and note that the **momentum-only Wigner density** (after integrating over \\(\\mathbf r\\)) reduces to \\(|\\tilde\\psi(\\mathbf k)|^2\\) up to normalization.', 'Concept_7': '**Average over the \\(m\\) substates** (isotropic P wave), which yields the factor **\\(2/3\\)** multiplying \\(\\sigma^2 k^2\\) in the 1P result.', 'Concept_8': 'Fix the normalization with **box/plane-wave normalization** so that \\(\\int d^3k\\,W(\\mathbf k)=(2\\pi)^3/V\\), which gives the prefactor \\((2\\sqrt{\\pi}\\,\\sigma)^3/V\\)'}	\(W_s(\vec{k}) = \frac{(2\sqrt{\pi}\,\sigma)^3}{V}\, e^{-\sigma^{2} k^{2}},\)\n\(W_p(\vec{k}) = \frac{(2\sqrt{\pi}\,\sigma)^3}{V}\, \frac{2}{3}\,\sigma^{2} k^{2}\, e^{-\sigma^{2} k^{2}}.\)	理论基础学科-理论物理-重离子碰撞	134016	2	4
recuV9nh4O3VOs	The percentages of Irδ+ species follow the order of TiO2 (101) > TiO2 (100) > TiO2 (001). The binding energy of Ti 2p3/2 of Ir/TiO2 (100) is slightly positive compared to that of Ir/TiO2(101) and Ir/TiO2(001). Among the TiO2 (100), TiO2 (101), and TiO2 (001), which crystal face as a support can enable the highest activity of Ir nanoparticles in methane catalytic combustion?	{'step_1': 'The Ir/TiO2(100) catalyst has medium Irδ+ proportion, implying its highest amount of partially reduced IrO2 species, which is favorable of CH4 activation.', 'step_2': 'The binding energy of Ti 2p3/2 of Ir/TiO2 (100) is slightly positive compared to that of Ir/TiO2(101) and Ir/TiO2(001), suggesting the electron donation from TiO2 to Ir species. It favors the formation of electron-rich Ir structures on the Ir/TiO2(100) catalyst, suggesting the high activity in O2 activation during methane oxidation reaction.'}	{'concept_1': 'Partially reduced IrO2 exhibits better activity toward CH4 activation than both Ir0 and IrO2.', 'concept_2': 'Electron-rich Ir structures show high activity in O2 activation during methane oxidation reaction.'}	Ir nanoparticles supported on TiO2 (100) show the highest activity.	理论基础学科-理论化学-计算化学	75405	1	3
recuVaB11c530v	In the field of electrochemical catalysis, what is the main reason why Mn_3 W_3-x C catalysts exhibit good oxygen reduction reactivity? You have to answer from both the electron transfer and the metal center.	{'step_1': 'Understand the Instruction and key concepts. The Instruction requires the analysis of the ORR activity of the Mn₃W₃₋ₓC catalyst, answering from the electron transfer and metal center directions. Concepts offered include: Concept_1: The Co0 2p3/2 binding energy in Co-Mo2N@NC increased from 778.6 eV to 778.8 eV, and the Mo 3d5/2 peak showed a negative shift of 0.4 eV at 228.7 eV, indicating the transfer of electrons from Co to Mo2N. Concept_2: Co and Mo₂C nanoparticles chemically coupled to improve OER performance, showing bimetallic synergy, activity and stability better than monometals and commercial RuO₂.', 'step_2': 'Analyze from the electron transfer direction. Application Concept_1: Concept_1 demonstrated the transfer of electrons from Co to Mo2N in the Co-Mo2N system, as evidenced by an increase in Co0 binding energy (from 778.6 eV to 778.8 eV); At the same time, Mo binds energy negatively, indicating that Mo gains electrons. This is in line with the XPS analysis principle: an increase in binding energy indicates that an element loses electrons, and a decrease in binding energy indicates that an element gains electrons (rich in electrons). Logical derivation: In the Mn₃W₃₋ₓC system, the electronegativity of W (tungsten) is higher (~2.36) than that of Mn (manganese) (~1.55), so the electrons will partially transfer from Mn to W, similar to the electron transfer process in Concept_1.', 'step_3': 'Analyze from the center of the metal direction. Application Concept_2: Concept_2 show that the chemical coupling of Co and Mo₂C produces a bimetallic synergistic effect, and its activity and stability are better than those of monometallic Co and commercial RuO₂. This demonstrates the advantages of bimetallic systems in catalysis: co-catalysis can optimize the reaction path while enhancing structural stability. Logical derivation: In Mn₃W₃₋ₓC, Mn and W form a bimetallic carbide structure. Similar to Concept_2, there is a synergistic effect between Mn and W, and the bimetallic center synergistically enhances activity.'}	{'concept_1': 'Some researchers have analyzed and found that the binding energy of Co0 2p3/2 in Co-Mo2N@NC increased from 778.6 eV to 778.8 eV compared with Co@NC, and the Mo 3d5/2 peak of Co-Mo2N@NC showed a negative offset of 0.4 eV at 228.7 eV.', 'concept_2': 'Some studies have found that the OER performance of Co surfaces is significantly improved by chemically coupling Co with Mo₂C nanoparticles. The Co-Mo₂C catalyst exhibited better activity and stability than that of monometallic pure Co and commercial RuO₂, confirming the key role of bimetallic synergy in promoting oxygen electrocatalytic reactions.'}	The main reasons for the good catalytic performance of Mn_3 W_3-x C catalysts are that the chemical coordination of Mn metal with the surface of W3-xC makes the electrons transfer from Mn to W ions, and the Mn-W bimetallic center is more conducive to the electrochemical reaction of oxygen than the monometal center. Score point 1: Electrons are transferred from Mn to W ions. Score point 2: The bimetallic center is more conducive to the electrochemical reaction of oxygen than the monometallic center.	理论基础学科-理论化学-催化化学，电化学	99207	2	2
recuVb3Nihwm7G	Recall the definition of total Cartier index: Let $X$ be a normal variety. The total Cartier index is the minimal positive integer $N$ such that $ND$ is Cartier for any $\mathbb{Q}$-Cartier Weil divisor $D$. Please answer the following questions: 1. Let $\mathcal{P}$ be a bounded family of projective $\epsilon$-lc varieties for some fixed positive real number $\epsilon$. Is there a positive integer $N$ depending only on $P$ such that the total Cartier index of $X$ is bounded from above by $N$ for any $(X,\Delta) \in \mathcal{P}$. 2. Can we replace $\epsilon$-lc by klt type in the first question? 3. Does the set of $\epsilon$-lc Fano varieties of dimension $d$ have bounded Cartier index? 4. Can we replace $\epsilon$-lc by klt in the third question? The answer should be written as the form: Yes/No; Yes/No; Yes/No; Yes/No; (for example No; No; No; No; ) and include your arguments.	"{'step_1': 'This is a known result in arXiv:2011.06509, Theorem 1.10, and this is known by AI. For safety, we include the result (in a slight different form in concept_1.', 'step_2': 'This is the key of these questions. The idea is to reduce to (1), so it suffices to replace $\\mathcal{P}$ by a new set $\\mathcal{Q}$ with larger Cartier index and $\\mathcal{Q}$ consists of $\\epsilon$-lc varieties. Moreover, it suffices to make $\\mathcal{Q}$ to be $\\mathbb{Q}$-Gorenstein since the index of $K_X$ is known to be bounded in a bounded family. To make $\\mathcal{Q}$ to be $\\mathbb{Q}$-Gorenstein, the idea is to take a log resolution in families, and run relative minimal model program. Then the resulting total space is $\\mathbb{Q}$-factorial, so each fiber is $\\mathbb{Q}$-Gorenstein by adjunction formula. concept_2 guarantees that the resulting varieties has the same total Cartier indices. concept_5 guarantees that the resulting varieties are klt. We note that this process is the relative version of the construction of small $\\mathbb{Q}$-factorial modifications. Now it remains to show that $\\mathcal{Q}$ has larger Cartier index than $\\mathbb{P}$. This is given by concept_2.', 'step_3': ""This is a famous result in arXiv:1609.05543, Theorem 1.1, and this is known by AI. Since it is famous, we don't include this in the concept."", 'step_4': 'This is a known result in arXiv:1609.05543, Example 1.2. For each natural number $n \\geq 2$, let $X_n$ be the projective cone over the rational curve of $\\deg n$ in $\\mathbb{P}^n$. Then the set of $X_n$, $n\\in \\mathbb{N}$ is a counterexample. This is proposed because this has similar form as (2) but with different answer.'}"	{'concept_1': 'Let $\\epsilon$ be a positive real number. Suppose $C := \\{(X,\\Delta)\\}$ is a set of $\\epsilon$-lc projective pairs that belongs to a log bounded family $P$. Then there exists a positive integer $N$ which only depends on $P$ and $\\epsilon$ satisfying the following. Let $(X,\\Delta) \\in C$, and $D$ a $\\mathbb{Q}$-Cartier Weil divisor on $X$. Then $ND$ is Cartier.', 'concept_2': 'Let $(X, A)$ be a protective klt pair with $A$ effective. Then: (1) Let $F$ be a $(K_X+ A)$-negative extremal face. Then there is a unique morphism $cont_F : X \\to Z$ to a protective variety such that $(cont_F)_*\\mathcal{O}_X \\simeq \\mathcal{O}_Z$, and an irreducible curve $C \\subset X$ is mapped to a point by $cont_F$ iff $[C] \\subset F$. (2) Let $F$ and $cont_F : X \\to Z$ be as in (1). Let $L$ be a line bundle on $X$ such that $(L \\cdot C) = 0$ for every curve $C$ with $[C] \\subset F$. Then there is a line bundle $L_Z$ on $Z$ such that $L = cont_F^*L_Z$. In particular, small $\\mathbb{Q}$-factorial modifications preserve indices.', 'concept_3': 'Let $\\mathcal{P}$ be a bounded family of klt varieties, then $\\mathcal{P}$ is a family of $\\epsilon$-lc varieties for some $\\epsilon$ depending on $\\mathcal{P}$.', 'concept_4': 'Let $X$ be a klt type variety, then there exists a small $\\mathbb{Q}$-factorial modification.', 'concept_5': 'If $X$ is $\\mathbb{Q}$-Gorenstein and of klt type, then $X$ is a klt variety.'}	Yes; Yes; Yes; No;	理论基础学科-基础数学-几何	344368	2	4
recuVeljFhEWgz	For permutation $v\in S_n$, define $M^{(v)}=(m_{ij})$ to be the $n\times n$ matrix such that for $i,j\in [1,n]$, $$m_{ij}:=\left\{\begin{array}{ll}1& if\ v_i=j\\ z_{ij}& if\ (i,j)\in D(v)\\ 0&else\end{array}\right.$$where $D(u)=\{(i,j)\in [1,n]\times[1,n]|u_i>j\ and\ u_j^{-1}>i\}$ for any permutation $u$. Let $\mathbb{C}[\mathbf{z}^v]=\mathbb{C}[z_{ij}|(i,j)\in D(v)]$. For $v\geq w$ where $v,w\in S_n$, under the Bruhat order, the Kazhdan-Lusztig ideal $J_{v,w}\subset \mathbb{C}[\mathbf{z}^v]$ is defined by $$J_{v,w}=\langle rank_w(i,j) +1\ minors\ in\ M_{[i],[j]}^{(v)}|(i,j)\in D(w)\rangle,$$ where $rank_w(i,j)=\#\{(k,w_k)|k\in[1,i],w_k\in[1,j]\}$ and $M_{[i],[j]}^{(v)}$ denotes the submatrix of $M^{(v)}$ with row indices in $[1,i]$ and column indices in $[1,j]$. Now assume $v\geq w$ are $321$-avoiding permutations, i.e., there does not exist indices $i<j<k$ such that $v_k<v_j<v_i$ or $w_k<w_j<w_i$. Denote by $l(\cdot)$ and $a(\cdot)$ the length function of permutations and a-invariant of $\mathbb{C}[\mathbf{z}^v]$-modules. Let $S_{v,w}(\mathbf{t})$ be the unspecialized Grothendieck polynomial defined by A. Woo and A. Yong. Compute the value of $deg\ S_{v,w}(\mathbf{t})- a(\mathbb{C}[\mathbf{z}^v]/J_{v,w})$, in terms of the functions of $v,w$. DO NOT use the online search.	{'step_1': 'By theorem_1, $\\mathbb{C}[\\mathbf{z}^v]/J_{v,w}$ is Cohen-Macaulay and has dimension $l(v)-l(w)$. ', 'step_2': 'Since $\\mathbb{C}[\\mathbf{z}^v]/J_{v,w}$ is Cohen-Macaulay, its a-invariant and Castelnuovo-Mumford regularity satisfy $a(\\mathbb{C}[\\mathbf{z}^v]/J_{v,w})=reg(\\mathbb{C}[\\mathbf{z}^v]/J_{v,w})-d$, where $d=l(v)-l(w)$ is the Krull dimension of $\\mathbb{C}[\\mathbf{z}^v]/J_{v,w}$.', 'step_3': 'By theorem_2, $deg\\ S_{v,w}(\\mathbf{t})=deg\\ K(\\mathbf{C}[\\mathbf{z}^v]/J_{v,w},t)$. Since $\\mathbb{C}[\\mathbf{z}^v]/J_{v,w}$ is Cohen-Macaulay we have $reg(\\mathbb{C}[\\mathbf{z}^v]/J_{v,w})=deg\\ K(\\mathbb{C}[\\mathbf{z}^v]/J_{v,w},t)-ht_{\\mathbb{C}[\\mathbf{z}^v]}J_{v,w}$. ', 'step_4': '$\\mathbb{C}[\\mathbf{z}^v]$ has $l(v)$ variables and $\\mathbb{C}[\\mathbf{z}^v]/J_{v,w}$ has Krull dimension $l(v)-l(w)$. So $ht_{\\mathbb{C}[\\mathbf{z}^v]}J_{v,w}=l(w)$.', 'step_5': 'Combining the equations above, we have $a(\\mathbb{C}[\\mathbf{z}^v]/J_{v,w})=reg(\\mathbb{C}[\\mathbf{z}^v]/J_{v,w})-(l(v)-l(w))=deg\\ K(\\mathbb{C}[\\mathbf{z}^v]/J_{v,w},t)-l(w)-l(v)+l(w)=deg\\ S_{v,w}(\\mathbf{t}) -l(v)$. So the answer is $l(v)$.'}	{'concept_1': 'Let $S=\\mathbb{C}[\\mathbf{x}]$ be a polynomial ring with the standard grading, the Castelnuovo-Mumford regularity of a graded $S$-module $M$ is given by $reg(M)=max\\{j-i|\\beta_{i,j}(M)\\neq 0\\}$, where $\\beta_{i,j}(M)$ denotes the $(i,j)$-th graded Betti number that appears in the minimal graded free resolution of $M$. ', 'concept_2': 'Let $S=\\mathbb{C}[\\mathbf{x}]$ be a polynomial ring with the standard grading, the a-invariant of the $S$-module $M$ is the degree of the Hilbert series of $M$ as a rational polynomial.', 'concept_3': 'The unspecialized Grothendieck polynomial is $S_{v,w}(\\mathbf{t})=\\sum_{P\\in \\overline{PIPES}(v,w)}(-1)^{\\#P-l(w)}\\prod_{(i,j)\\in P}t_{ij}$, where $\\overline{PIPES}(v,w)=\\{P\\subset D(v)|\\delta(a_P)=w\\}$, $\\delta(a_P)$ is the Demazure product of $a_P$ (the word obtained by reading the labels of elements in $P$ within rows from right to left, starting at the top row and working downwards).', 'theorem_1': 'The Kazhdan-Lusztig variety, which is defined by the Kazhdan-Lusztig ideal $J_{v,w}$ in affine coordinate ring $\\mathbb{C}[\\mathbf{z}^v]$, is Cohen-Macaulay and has dimension $l(v)-l(w)$. ', 'theorem_2': 'Let $v,w\\in S_n$ be $321$-avoiding permutations with $v\\geq w$. Then the K-polynomial $K(\\mathbf{C}[\\mathbf{z}^v]/J_{v,w},t)$ has the same degree as that of $S_{v,w}(\\mathbf{t})$.'}	$l(v)$ (or $inv(v)$, the inversion number of permutation $v$).	理论基础学科-基础数学-代数	197193	2	4
recuVfyQPiOOv3	"Determine the minimum circuit size lower bound for the complexity class of exponential-time Arthur-Merlin with sub-exponential advice, $(AMEXP \cap coAMEXP)/2^{n^\epsilon}$, by utilizing the modern paradigm that equates proving such lower bounds with designing an efficient, single-valued Arthur-Merlin protocol to explicitly construct a ""hard"" truth table."	"{'step_1': 'The instruction is reframed from a question about a lower bound to a constructive problem, following Concept_1. The goal is to build an efficient (quasi-polynomial time) single-valued Arthur-Merlin protocol that outputs a truth table of length $N=2^n$ that is hard for circuits of size $N/\\log N = 2^n/n$. This set of hard truth tables forms a dense property within the class coAM, as described in Concept_2.', 'step_2': 'We begin with a brute-force algorithm, $BF_0$, which is guaranteed to find such a hard truth table by enumerating all strings of length $N$ and checking them against all circuits of size $2^n/n$. This algorithm is correct but runs in exponential time in $N$.', 'step_3': 'The core of the argument applies the Hardness-vs-Randomness framework for AM (Theorem_1) to the computation of the brute-force algorithm $BF_0$. This presents a dichotomy based on the success or failure of a resulting Hitting Set Generator (HSG).', 'step_4': ""The Iterative Win-Win Paradigm (Theorem_2) is applied over a sequence of carefully chosen input lengths $n_0, n_1, \\dots, n_l$. At the first step, we consider the computation of $BF_0$ on input length $n_0$: (Win Case): If the HSG derived from $BF_0(1^{n_0})$ fails to hit the dense coAM property of hard truth tables at length $n_1$, then according to Theorem_1, the computation $BF_0(1^{n_0})$ can be simulated by a fast (quasi-polynomial time) single-valued Arthur-Merlin reconstruction protocol. This protocol itself is the desired efficient construction. (Improve Case): If the HSG successfully hits the property, we have found a hard truth table of length $n_1$. We can then define a new, moderately faster algorithm, $BF_1$, which uses this HSG (along with a small amount of advice to identify the correct string in the HSG's output) to find a hard truth table for input length $n_1$."", 'step_5': 'The ""improve"" step is iterated. We apply Theorem_1 to $BF_1$ on input length $n_1$ to get a result for length $n_2$, and so on. If we remain in the ""Improve"" case for all steps up to $l$, the accumulated speedups result in an algorithm $BF_l$ that runs in quasi-polynomial time. The overall algorithm is designed to check for a ""win"" at each step $i$, and if none occurs, it runs the final efficient algorithm $BF_l$.', 'step_6': 'This iterative process guarantees that for any sequence of input lengths, we can construct a quasi-polynomial time, single-valued Arthur-Merlin protocol (Concept_3) that finds a truth table hard for circuits of size $2^n/n$. A language defined based on the bits of this truth table is therefore in $AMEXP$ (as it can be decided by running this protocol) but, by construction, is not in $SIZE[2^n/n]$. This establishes the $2^n/n$ lower bound.'}"	"{'concept_1': 'Explicit Construction of Hard Truth Tables. The problem of proving a circuit lower bound for a complexity class is equivalent to designing an efficient algorithm that, for infinitely many input lengths $n$, outputs the truth table of a boolean function on $\\log n$ variables that cannot be computed by circuits of a certain size.', 'concept_2': 'Dense coAM Property. The set of truth tables that are ""hard"" for circuits of size $2^n/n$ constitutes a dense property (i.e., it contains a constant fraction of all possible strings) and is decidable in the complexity class coAM.', 'concept_3': ""Single-Valued Arthur-Merlin Protocol. An interactive protocol where a computationally unbounded Merlin tries to convince a probabilistic polynomial-time Arthur about a statement, with the additional property that for any strategy of Merlin, Arthur's output is restricted to either a canonical string or a failure symbol with high probability."", 'theorem_1': 'Hardness-vs-Randomness for AM. There exists a framework where, given a potentially hard computation, one of two outcomes must hold: (1) an efficient Hitting Set Generator (HSG) can be constructed that finds an element in any dense coAM property, or (2) the original computation was not truly hard and can be simulated efficiently by a fast Arthur-Merlin ""reconstruction"" protocol.', 'theorem_2': 'Iterative Win-Win Paradigm. A proof technique that bypasses the limitations of standard win-win arguments by iteratively applying a win-or-improve step. Over a sequence of increasing input lengths, either an early ""win"" condition is met, or the accumulated ""improvements"" result in a highly efficient algorithm at the final step.'}"	$2^n/n$	理论基础学科-理论计算机科学-计算复杂度	249226	2	5
recuVjfgPckajY	Evaluate the asymptotically tight randomized competitive ratio for the online min-max paging problem by following a specific analytical methodology. First, re-frame the problem's discrete $l_{\infty}$ -norm objective using an appropriate, continuous $l_p$-norm surrogate, where the choice of $p$ is a function of the total number of pages, $n$. Next, apply a black-box reduction framework that leverages the known competitive ratio of optimal randomized algorithms for the simpler weighted linear ( $l_1$ ) paging problem to establish an upper bound for this more general $l_p$-norm objective. Finally, synthesize this upper bound with the established theoretical lower bound for the $l_p$-norm paging problem to determine the definitive tight competitive ratio in terms of cache size $k$ and universe size $n$.	"{'step_1': ""The instruction requires determining the tight competitive ratio for min-max paging by first re-framing its objective. The problem's goal is to minimize the maximum number of evictions, which is formally defined as minimizing the $l_{\\infty}$-norm of the page eviction vector (Concept_1)."", 'step_2': 'Following the specified methodology, this discrete $l_{\\infty}$-norm objective is re-framed using a continuous surrogate to facilitate analysis. The $l_p$-norm is chosen for this purpose, as it provides a constant-factor approximation of the $l_{\\infty}$-norm when the parameter $p$ is set to be on the order of $\\log n$ (Concept_2). This transforms the problem into solving $l_p$-norm paging for a specific $p$.', 'step_3': 'The next step is to establish an upper bound by applying a black-box reduction. Theorem_1 provides this reduction, stating that a $\\beta$-competitive randomized algorithm for the simpler weighted linear ($l_1$) paging problem can be lifted to an $O(p\\beta)$-competitive algorithm for the $l_p$-norm paging problem.', 'step_4': 'It is a known result that optimal randomized algorithms for weighted linear paging achieve a competitive ratio of $\\beta = O(\\log k)$. By substituting this $\\beta$ and our choice of $p = O(\\log n)$ into the formula from Theorem_1, we derive a randomized competitive ratio of $O(p\\beta) = O(\\log n \\cdot \\log k)$. This establishes the upper bound for the min-max paging problem.', 'step_5': 'To prove this bound is asymptotically tight, the final step is to synthesize it with a corresponding lower bound. Theorem_2 establishes a theoretical lower bound for the randomized $l_p$-norm paging problem, which is $\\Omega(p \\log k)$.', 'step_6': 'By again substituting $p = O(\\log n)$ into this lower bound, we find that any randomized algorithm for min-max paging must have a competitive ratio of at least $\\Omega(\\log n \\log k)$.', 'step_7': 'By synthesizing these results, we see the upper bound of $O(\\log k \\log n)$ and the lower bound of $\\Omega(\\log k \\log n)$ match asymptotically. Therefore, the definitive tight randomized competitive ratio for the online min-max paging problem is $\\Theta(\\log k \\log n)$.'}"	"{'concept_1': ""The goal in min-max paging is to minimize the $l_{\\infty}$-norm of the page eviction vector, where this vector's coordinates represent the eviction counts for each of the $n$ pages."", 'concept_2': 'The $l_p$-norm of an n-dimensional vector serves as a constant-factor approximation of its $l_{\\infty}$-norm when the parameter $p$ is on the order of $\\log n$.', 'concept_3': ""P-bounded convex functions are a class of functions that generalize objectives like the $l_p$-norm. They are convex, monotone, and satisfy the property $\\langle\\nabla g(x),x\\rangle\\le p \\cdot g(x)$, which relates the function's value to its gradient."", 'theorem_1': 'There is a black-box reduction that converts any $\\beta$-competitive randomized online algorithm for the weighted $l_1$-norm paging problem into an $O(p\\beta)$-competitive randomized algorithm for the $l_p$-norm paging problem.', 'theorem_2': 'For any $p \\le C \\ln n$, any randomized online algorithm for the $l_p$-norm paging problem is proven to have a competitive ratio related to $k$.'}"	$\Theta(\log k \log n)$	理论基础学科-理论计算机科学-算法理论	685510	2	5
recuVlbxHnJWyi	In the context of social choice theory, a Condorcet winning set is a committee of candidates that a majority of voters prefer over any single alternative candidate. What is the maximum size $k$ of a committee that is guaranteed to be a Condorcet winning set in any election, regardless of the number of voters or candidates? An $\alpha$-undominated committee $S$ is a set of candidates where for any candidate $a \notin S$, the fraction of voters who prefer $a$ over every member of $S$ is less than $\alpha$. In any election, an $\alpha$-undominated committee of size $k$ is guaranteed to exist if the condition $\frac{\alpha}{1-\ln\alpha}\ge\frac{2}{k+1}$ is satisfied.	{'step_1': 'The goal, as stated in the instruction, is to find the maximum committee size $k$ that guarantees the existence of a Condorcet winning set.', 'step_2': 'According to Concept_1, a Condorcet winning set is a specific type of $\\alpha$-undominated committee where the threshold $\\alpha$ is exactly 1/2. This means we are looking for the size $k$ of a committee that is guaranteed to be 1/2-undominated.', 'step_3': 'The instruction provides the central mathematical condition for our problem: an $\\alpha$-undominated committee of size $k$ is guaranteed to exist if the inequality $\\frac{\\alpha}{1-\\ln\\alpha}\\ge\\frac{2}{k+1}$ holds.', 'step_4': 'The proof that establishes this key inequality is non-trivial. It uses the probabilistic method (Concept_2) to show existence. This involves creating a specialized probability distribution over committees, whose existence is proven using the minimax theorem (Theorem_1). The concept of rank (Concept_3) is a technical measure used within this proof to evaluate the quality of candidates and committees against this distribution.', 'step_5': 'To find the specific $k$ for a Condorcet winning set, we substitute $\\alpha = 1/2$ into the given inequality: $\\frac{0.5}{1-\\ln(0.5)}\\ge\\frac{2}{k+1}$.', 'step_6': 'Next, we solve this inequality for $k$. Since $\\ln(0.5)$ is equal to $-\\ln(2)$, the expression simplifies to $\\frac{0.5}{1+\\ln(2)}\\ge\\frac{2}{k+1}$. Rearranging for $k$, we get $k \\ge 3 + 4\\ln(2)$.', 'step_7': 'By substituting the approximate value of $\\ln(2) \\approx 0.693$, we calculate the numerical bound for $k$: $k \\ge 3 + 4(0.693)$, which means $k \\ge 5.772$.', 'step_8': 'Since the committee size $k$ must be an integer, the smallest integer that satisfies the condition $k \\ge 5.772$ is 6. Thus, a committee of size 6 is sufficient.'}	{'concept_1': 'A Condorcet winning set is a special case of an $\\alpha$-undominated committee where $\\alpha = \\frac{1}{2}$. It is a committee that is $\\frac{1}{2}$-undominated.', 'concept_2': 'The probabilistic method is a proof technique used to demonstrate the existence of a mathematical object with specific properties by showing that a randomly chosen object from a suitable probability space has the desired properties with non-zero probability.', 'concept_3': 'The rank of a candidate or committee with respect to a voter, $rank_v(\\cdot)$, is a measure of its quality based on a reference probability distribution $A$ over committees. It is defined as the probability that a committee sampled from $A$ is less preferred by voter $v$.', 'theorem_1': 'The minimax theorem states that for a two-player zero-sum game, under certain conditions, the maximum value the first player can guarantee (maximin) is equal to the minimum value the second player can limit the first player to (minimax). This is used to prove the existence of a distribution over committees with favorable properties.'}	6	理论基础学科-基础数学-组合数学	352827	2	4
recuVlLRHhdfMs	For a two dimensional array of multilevel alkaline earth or alkaline earth like atoms initially prepared in a fully inverted excited state |e⟩ that decays into multiple ground states {|g_a⟩}, state the condition for a Dicke superradiant burst on channel a when all emitted light is collected, expressed in terms of the channel specific spectrum of collective decay rates {Γ^(a)_ν}, the single atom rate on that channel Γ^(a)_0, and the total single atom decay rate Γ_0=∑_a Γ^(a)_0.	{'step_1': 'Start from the Born–Markov master equation; diagonalize the channel-a dissipator by Γ^(a)_{jl} → {Γ^(a)_ν,Ô_{ν,a}} so that R_a(t)=∑_ν Γ^(a)_ν⟨Ô^†_{ν,a}Ô_{ν,a}⟩.', 'step_2': 'Consider the fully inverted initial state; the only evolution at t=0 is spontaneous emission. A “burst” means Ṙ_a(0)>0, which is equivalent to the conditional enhancement criterion ḡ^(2)(0)>1 for photons detected on channel a.', 'step_3': 'Evaluate Ṙ_a(0) (or, equivalently, ḡ^(2)(0)) using the collective operators and the fact that the fully excited state is an eigenstate of number operators; reduce all four-operator averages to quadratic forms in {Γ^(a)_ν}.', 'step_4': 'The algebra collects into the spectral dispersion of the channel-a rates competing against single-atom decay out of |e⟩ into all channels. This yields the channel-resolved burst inequality.'}	{'concept_1': 'For atoms at positions {r_j}, the photon-mediated coupling on transition |e⟩→|g_a⟩ defines a Hermitian positive matrix Γ^(a)_{jl}, whose eigenvalues are the collective decay rates {Γ^(a)_ν} and eigenvectors the collective jump operators.', 'concept_2': 'Ô_{ν,a}=∑_j c^(ν,a)_j σ^(j)_{g_a e} with decay rate Γ^(a)_ν, so that the emission rate on channel a reads R_a(t)=∑_ν Γ^(a)_ν ⟨Ô^†_{ν,a}Ô_{ν,a}⟩.', 'concept_3': 'Var{Γ^(a)_ν} ≡ 1/N ∑_{ν=1}^N (Γ^(a)_ν)^2 - (Γ^(a)_0)^2.', 'concept_4': 'A “burst” corresponds to a positive initial slope of the measured emission rate from the fully excited state, equivalently a conditional second-order correlation ḡ^(2)(0)>1 for the measured channel.'}	A Dicke superradiant burst occurs if and only if Var({Γ_v^a}/Γ_0^a) > Γ_0/Γ_0^a	其他理论学科-电磁理论与量子电磁学-量子光学；里德堡原子；Dicke模型	111738	2	3
recuVmiw3kbpmH	The order of Ni-O bond lengths on different CeO₂ crystal planes is: Ni/CeO₂(111) > Ni/CeO₂(110) > Ni/CeO₂(100). Based on this, which crystal face enables the most stable anchoring of single Ni atoms, and which crystal face enables the highest activity in dry reforming of methane?	{'step_1': 'According to concept 1, the Ni single atom is anchored via Ni-O-Ce bond on the CeO2, thus the shorter Ni-O bond length suggests the stronger Ni-CeO2 interaction. Therefore, the Ni/CeO2(100) has the strongest Ni-CeO2 interaction, which is favorable for the stabilization of Ni single atom.', 'step_2': 'According to concept 2, the CeO2 nanorod usually enables Ni species the highest activity. The CeO2 nanorod usually exhibits the CeO2(110) active crystal face, thus the CeO2(110) can enables the highest activity of Ni single atom in dry reforming of methane.'}	{'concept_1': 'The is anchored via Ni-O-Ce bond on the CeO2.', 'concept_2': 'The CeO2 nanorod usually enables Ni species the highest activity.'}	The CeO2(100) enables the most stable anchoring of single Ni atoms, while the Ni/CeO2(110) shows the highest activity in dry reforming of methane.	理论基础学科-理论化学-计算化学	55156	1	2
recuVxskY5zS12	Under what conditions is the rotational-add diffusion layer of the form $L_{4, m}^{\mathcal{G}}=\operatorname{Circ}(A, B, B, B-A)$, constructed from a rotation offset $$\mathcal{G}=\left\{0, g_2, g_2+m, g_2+\right. 2 m, 3 m\$$ where $$$$ is an even integer and $g_2=m / 2$, a Maximum Distance Separable (MDS) layer?	{'step_1': 'Define the Problem and Given Conditions\nThe objective is to determine the conditions under which a specific rotational-add diffusion layer defined by the rotation offset set $\\mathcal{G}=\\left\\{0, g_2, g_2+m, g_2+2 m, 3 m\\right\\}$, achieves the Maximum Distance Separable (MDS) property. The given parameters are that $m$ is an even number and $g_2=m / 2$.', 'step_2': 'Calculate the Necessary Parameters\nBased on the given information, we first compute the parameters required for the characteristic polynomial formula.\n- The parameter $g$ is defined as $g=g_2$, so $g=m / 2$.\n- The parameter $d$ is the greatest common divisor (GCD) of $g$ and $m$. Since $m$ is even, $m / 2$', 'step_3': 'Derive the Characteristic Polynomial\nThe characteristic polynomial, $f(\\lambda)$, is determined by the general form: $\\$ \\$ \\mathrm{f}(\\$ lambda \\$)=(- \\text{ lambda })^{\\wedge}\\{\\mathrm{m} / \\mathrm{d}\\}-(1-\\text{ lambda })^{\\wedge}\\{(\\mathrm{m}-\\mathrm{g}) / \\mathrm{d}\\} \\$ \\$$ We substitute the parameters calculated in Step 2 into this equation:\n$$\n\\begin{aligned}\n& \\$ \\$ \\mathrm{f}(\\text{ Vlambda })=(- \\text{ Vlambda })^{\\wedge}\\{\\mathrm{m} /(\\mathrm{m} / 2)\\}-(1-\\text{ Vlambda })^{\\wedge}\\{(\\mathrm{m}-\\mathrm{m} / 2) /(\\mathrm{m} / 2)\\} \\$ \\$ \\$ \\mathrm{f}(\\text{ Vlambda })=(- \\\\\n& \\text{ Vlambda })^{\\wedge} 2-(1-\\text{ Vlambda })^{\\wedge}\\{(\\mathrm{m} / 2) /(\\mathrm{m} / 2)\\} \\$ \\$ \\$ \\mathrm{f}(\\text{ llambda })=\\text{ Vlambda^2 }-(1-\\text{ Vlambda }) \\$ \\$\n\\end{aligned}\n$$\nThis simplifies to the final characteristic polynomial: $f(\\lambda)=\\lambda^2+\\lambda-1$.', 'step_4': 'State the Coprimality Condition for MDS\nAccording to a relevant theorem (Theorem 3), the diffusion layer is MDS if and only if its characteristic polynomial, $f(\\lambda)=\\lambda^2+\\lambda-1$, is coprime with a specific list of 11 predefined polynomials (such as $\\lambda+1,2 \\lambda-1, \\lambda^2-\\lambda-1$, etc.).', 'step_5': 'Test the Coprimality in Fields of Different Characteristics\nWe now test this condition by checking for common factors or roots between $f(\\lambda)$ and the polynomials from the list in fields with various prime characteristics ( $p$ ). The analysis shows failure cases for $p=2,3,5$ :\n- Characteristic 2: $f(\\lambda)=\\lambda^2+\\lambda-1$ becomes $\\lambda^2+\\lambda+1$. A polynomial from the list, $\\lambda^2-\\lambda-1$, also becomes $\\lambda^2+\\lambda+1$ (since $-1 \\equiv 1(\\bmod 2)$ ). As they are identical, they are not coprime.\n- Characteristic 3: $f(\\lambda)=\\lambda^2+\\lambda-1$. A polynomial from the list, $\\lambda^2-2 \\lambda-1$, is identical to $f(\\lambda)$ in this field (since $-2 \\equiv 1(\\bmod 3))$. Therefore, they are not coprime.\n- Characteristic 5: The polynomial $f(\\lambda)$ has a root at $\\lambda=2$ (since $2^2+2-1=5 \\equiv 0 (\\bmod 5))$. A polynomial from the list, $\\lambda^2+1$, also has a root at $\\lambda=2$ (since $2^2+1= 5 \\equiv 0(\\bmod 5))$. Since they share a common root, they are not coprime.\nFor prime characteristics other than 2,3 , and $5, f(\\lambda)$ is coprime with all 11 polynomials.', 'step_6': 'Conclude the Final Condition for the MDS Property\nBased on the coprimality tests in the previous step, the condition for the layer to be MDS is violated when the characteristic of the underlying field is 2,3, or 5 . Therefore, the rotational-add diffusion layer $L_{4, m}^\\mathcal{G}$ is MDS if and only if the prime characteristic $p$ of the field is not $\\mathbf{2 , 3}$, or $\\mathbf{5}$.'}	{'concept_1': 'Rotational-Add Diffusion Layer. A linear transformation constructed using only modular additions/subtractions and cyclic shifts (rotations) of the input vector, determined by a set of rotation offsets $\\mathcal{G}$. This construction minimizes or eliminates scalar multiplications, making it efficient for arithmetization-oriented ciphers.', 'theorem_1': 'MDS Matrix Characterization. A linear diffusion layer, represented as an $n \\times n$ block matrix, is Maximum Distance Separable (MDS) if and only if all of its square block submatrices of any order $k$ (where $1 \\leq k \\leq n$) are nonsingular (i.e., have a non-zero determinant). This property provides optimal diffusion against differential and linear cryptanalysis.', 'theorem_2': 'Determinant of a 3-diagonal Matrix. The determinant of a specific $m \\times m 3$ diagonal matrix $M=c_1 \\cdot I_m+c_2 \\cdot U_m^\\alpha+c_3 \\cdot L_m^{m-\\alpha}$ over a field $\\mathbb{F}_p$ is given by the formula $\\operatorname{det}(M)=\\left[c_1^{\\frac{m}{d}}-(-1)^{\\frac{m}{d}} c_2^{\\frac{m-\\alpha}{d}} c_3^{\\frac{\\alpha}{d}}\\right]^d$, where $d=\\operatorname{gcd}(\\alpha, m)$. This formula is crucial for analyzing the singularity of the building blocks of the diffusion layer.', 'theorem_3': 'MDS Conditions for $\\operatorname{Circ}(A, B, B, B-A)$. For a layer of this specific form to be MDS, it is necessary and sufficient that a corresponding characteristic polynomial $f(\\lambda)=(-\\lambda)^{\\frac{m}{d}}-(1-\\lambda)^{\\frac{m-g^2}{d}$ is coprime to a specific list of eleven other polynomials (e.g., $\\lambda+1,2 \\lambda-1, \\lambda^2-\\lambda-1$, etc.). This reduces the complex matrix problem to a simpler problem in polynomial algebra.'}	The layer is MDS if and only if the prime characteristic $p$ of the underlying field is not 2, 3, or 5.	理论基础学科-理论计算机科学-计算机密码学	95216	1	4
recuUAInTiGkd9	In a thermally-coupled hybrid absorption-compression heat pump, what will happen for the entransy dissipation of evaporator-3 when components other than itself have irreversibility improvements?	{'step_1': 'Meaning of entransy (Concept 1): Entransy measures both “how much heat” and its “temperature level (quality),” so it serves as an appropriate second-law metric to identify where energy dissipation concentrates within a conversion system.', 'step_2': 'System context (Concept 2): In a thermally-coupled hybrid A–C heat pump (THACHP with an AHT high-temperature sub-cycle), the heat transfer components are arranged in thermal series, forming a strongly coupled system. Because components share common boundary temperatures and are linked through thermal interfaces, any change in the irreversibility of one component modifies not only its own performance but also redistributes temperature gradients across the entire system. Consequently, the heat flux through other components is indirectly affected — laying the foundation for understanding subsequent changes in heat-flow allocation.', 'step_3': 'Entransy–resistance principle (Concept 3): The minimum generalized thermal resistance is equivalent to an entransy-dissipation extremum: Reducing a component’s irreversibility ⇒ its generalized thermal resistance decreases; As multiple components are coupled, lowering several resistances simultaneously leads to a drop in the overall system resistance; A reduced system resistance allows greater total heat throughput under fixed boundary conditions.', 'step_4': 'Redistribution effect on evaporator-3: When the total system resistance decreases, the same boundary temperatures drive a larger total heat flux through the entire system. If evaporator-3’s own thermal resistance remains unchanged, the increased total heat must still pass through it. By Concepts 1 and 3, entransy dissipation within evaporator-3 rises to satisfy the new system-wide entransy extremum.'}	{'concept_1': 'According to the definition of entransy, it contains the quantity and the quality of energy, which is therefore promising for the second law analysis for energy conversion systems.', 'concept_2': 'Thermally-coupled hybrid absorption-compression heat pump (THACHP) is a heat pump that adopts absorption heat transformer (AHT) sub-cycle instead of vapor compression heat pump (VCHP) sub-cycle for high-temperature output.', 'concept_3': 'The minimum thermal resistance principle is equivalent to the entransy dissipation extremum principle, as a ratio of temperature difference to heat flux as the generalized thermal resistance of convective heat transfer processes. The change of resistance and the change of the entransy dissipation is opposite.'}	the entransy dissipation of evaporator-3 always increased when components other than itself have irreversibility improvements	其他理论学科-理论热力学与统计物理-能量转换的理论极限	47653	2	3
recuV9YBA6ps0F	Starting from a constant condensation temperature, how does the energy efficiency of a solar distillation system change as the condensation temperature increases, while keeping all other parameters constant？	{'step_1': 'The primary goal is to determine how the energy efficiency ($\\eta$) of a solar distillation system changes as the condensation temperature ($T_c$) increases, while keeping all other parameters constant. The analysis begins with the definition of $\\eta$ and its dependence on thermal energy distribution described in concept_1.', 'step_2': 'According to concept_1, the energy efficiency of a solar distillation system is defined as: \\eta = \\frac{Q_{\\mathrm{evap}}}{Q_s}, where $Q_{\\mathrm{evap}}$ is the energy used for water evaporation and $Q_s$ is the total absorbed solar energy. At constant $Q_s$, the variation in $\\eta$ depends on how the absorbed energy is partitioned between useful evaporation and thermal losses.', 'step_3': 'We first analyze the low condensation temperature regime using concept_2. When $T_c$ is very low, the temperature difference between the device and the ambient surroundings is large, causing significant thermal losses through edges, upward dissipation, and conduction. As a result, a smaller fraction of the absorbed energy contributes to $Q_{\\mathrm{evap}}$, leading to low energy efficiency.', 'step_4': 'Next, we evaluate the effect of moderately increasing $T_c$ based on concept_2 and concept_3. As $T_c$ rises from a low level: The overall temperature gradient between the system and its surroundings decreases, reducing heat losses(concept_2). The evaporation–condensation temperature difference $\\Delta T = T_e - T_c$ remains sufficiently large, maintaining effective vapor diffusion (concept_3). Consequently, a greater fraction of $Q_s$ is converted into $Q_{\\mathrm{evap}}$, and $\\eta$ increases.', 'step_5': 'However, when $T_c$ continues to increase beyond an optimal range (concept_3), $\\Delta T = T_e - T_c$ becomes too small, reducing the vapor concentration gradient and weakening mass transfer efficiency. Even though thermal losses remain low, the limited vapor diffusion dominates, causing $Q_{\\mathrm{evap}}$ to decrease and resulting in a decline in $\\eta$.', 'step_6': 'Combining these effects, the overall trend can be summarized as follows: At low $T_c$: $\\eta$ is low due to significant heat losses. At moderate $T_c$: $\\eta$ rises as heat losses decrease while vapor diffusion remains effective. At high $T_c$: $\\eta$ falls again because vapor diffusion becomes severely constrained.', 'step_7': 'Therefore, the energy efficiency $\\eta$ shows a non-monotonic relationship with condensation temperature, and the overall trend is: \\boxed{ \\text{Energy efficiency first increases and then decreases as } T_c \\text{ rises.} }'}	{'concept_1': 'The energy efficiency ($\\eta$) of a solar distillation system is defined as the ratio of the useful latent heat utilized for vapor generation to the total absorbed solar energy: \\eta = \\frac{Q_{\\mathrm{evap}}}{Q_s}, where $Q_{\\mathrm{evap}}$ represents the energy consumed for water evaporation and $Q_s$ is the total absorbed solar energy. This definition directly links the input solar energy to the effective evaporation performance.', 'concept_2': 'After solar energy is absorbed and converted into thermal energy, part of it is used to drive water evaporation, while the remainder is lost through various pathways, such as edge heat loss, upward thermal dissipation, and conductive losses through device boundaries. An increase in condensation temperature ($T_c$) can reduce part of the heat loss by lowering the overall temperature gradient, thereby improving the effective utilization of absorbed solar energy.', 'concept_3': 'The temperature difference between the evaporation surface and the condensation surface ($\\Delta T = T_e - T_c$) governs the vapor concentration gradient and thus determines the driving force for vapor diffusion. A larger $\\Delta T$ enhances vapor flux and accelerates mass transfer, whereas a smaller $\\Delta T$ weakens the driving potential, reducing evaporation efficiency.'}	The efficiency first increases and then decreases with the condensation temperature.	其他理论学科-理论热力学与统计物理-能量转换	100708	2	3
recuVfYl5ySmDy	How does CRL and DecA:Lid (2:1) synergize in the catalytic synthesis of vitamin E succinate (VES)? What is its specific mechanism? You need to answer from two aspects: electronic regulation and conformation.	{'step_1': 'First, consider how to explain the mechanism by which DecA:Lid (2:1) enhances the electrophilicity of succinic anhydride in the Instruction regarding electronic regulation. According to the provided concept_1, some researchers have found that lauric acid forms strong hydrogen bonds with chloride ions (Cl⁻) in the reaction system. This concept suggests that hydrogen bond formation can polarize molecules and alter the electron distribution, thereby enhancing their electrophilicity. Here, DecA:Lid (2:1) may form a similar strong hydrogen bond interaction with carbonyl oxygen or other polar sites in succinic anhydride. This hydrogen bond pulls electrons, making the acyl carbon atoms of succinic anhydride more electron-deficient and electrophilious, making them more susceptible to nucleophilic attack by vitamin E (VEOH) and promoting esterification reactions. This is in line with mathematical logic, as hydrogen bonding is an electrostatic interaction that effectively regulates electron density.', 'step_2': 'Secondly, in terms of conformation, I analyze how DecA:Lid (2:1) affects the conformation of the CRL. According to the concept_2, the researchers found that due to the interaction of hydrogen bonds between C-Gly and CALB, it is conducive to maintaining the balance between flexibility and rigidity of the structural conformation, and improving the binding ability and catalytic efficiency of enzymes to substrates. In the reaction system, DecA:Lid (2:1) may interact with CRL through hydrogen bonding, similar to the mechanism in concept_2, causing conformational changes in CRL. This change optimizes the flexibility (for substrate binding) and rigidity (for stabilizing the transition state) of the CRL active center, thereby improving the ability of CRL to capture and localize substrate molecules, increasing substrate affinity, and enhancing catalytic efficiency. This is in line with mathematical logic, as hydrogen bonds stabilize specific conformations and affect enzyme kinetics.'}	"{'concept_1': 'Researchers have found that lauric acid forms strong hydrogen bonds with chloride ions (Cl⁻) in the reaction system.', 'concept_2': ""Researchers have found that the interaction between C-Gly and CALB's hydrogen bonds is conducive to maintaining a balance between flexibility and rigidity in the structural conformation, which ultimately improves the enzyme's binding ability to substrates and catalytic efficiency""}"	The synergy mechanism between CRL and DecA:Lid (2:1) is:  The first aspect DecA:The interaction between Lid(2:1) and succinic anhydride forms strong hydrogen bonds, which enhances the electrophilicity of succinic anhydride and promotes esterification reactions.  The second aspect DecA:Lid (2:1) as a solvent induces conformational changes in CRL through hydrogen bonding, thereby improving its ability to capture and localize substrate molecules, increasing the affinity between CRL and substrate, and enhancing its catalytic efficiency.  Checklist  Scoring point 1: The two interact to form strong hydrogen bonds, which enhances the electrophilicity of succinic anhydride and promotes the reaction  Score point 2: The interaction between the two forms hydrogen bonds, which causes conformational changes in CRL, increases the affinity between CRL and substrate, and enhances its catalytic efficiency	理论基础学科-理论化学-催化化学	55083	1	2
recuVEFvsexATr	In a Dyson sphere-like evaporator, compared with a conventional evaporator, if a smaller fraction of incident light is used directly for water evaporation, and the other structure and material of the two evaporators are the same, how does the overall evaporation efficiency change?	{'step_1': 'Define the Energy Balance in ISE (concept_1)\n\nIn interfacial solar evaporation (ISE), the evaporation rate depends on two competing factors:\n\nEnergy input drives phase change at the liquid–vapor interface.\nVapor transport efficiency regulates the removal of generated vapor.\n\nIf vapor is not effectively removed, local humidity increases, which reduces the vapor concentration gradient and limits further evaporation.\nTherefore, high evaporation rates require a balanced coupling between energy supply and vapor escape, rather than maximizing direct light utilization alone.', 'step_2': 'Role of Convective Flows in Vapor Transport (concept_2)\n\nIn Dyson sphere-like evaporators, a designed fraction of the absorbed solar energy is deposited on non-evaporating, higher-temperature surfaces, establishing temperature gradients that drive buoyancy-induced natural convection within the structure.\nThese flows lower the local vapor concentration and sustain a strong concentration gradient, which:\n\nAccelerates vapor removal from evaporation surfaces.\nPrevents vapor accumulation and local saturation.\nMaintains continuous evaporation even under reduced direct energy input.\n\nThis trade-off introduces a nonlinear enhancement mechanism: sacrificing some direct light can paradoxically improve overall efficiency by unlocking a new vapor transport pathway.', 'step_3': 'Nonuniform Heating Induces Beneficial Internal Flows (concept_3)\n\nThe Dyson sphere-like evaporator has a 3D hollow structure with both inner and outer evaporation surfaces.\nNonuniform heating between these regions naturally forms temperature gradients, which induce internal convection loops without any additional external power input, at the expense of reallocating a small portion of the incident solar energy to sustain the thermal gradients that drive buoyancy.\nThese thermally driven flows act as a self-regulating mechanism that:\n\nRedistributes thermal energy.\nEnhances vapor escape efficiency.\nExpands the effective evaporation area across multiple surfaces.', 'step_4': 'Comparison with Conventional Evaporators\n\nIn conventional planar evaporators:\n\nNearly all absorbed solar energy is used directly for phase change, but\nVapor removal is inefficient, leading to local humidity buildup and reduced driving force.\n\nIn contrast, Dyson sphere-like evaporators trade a fraction of direct energy to actively manage vapor transport and internal convection, thereby achieving higher overall evaporation efficiency.', 'step_5': 'Final Conclusion\n\nThis is an energy-allocation trade-off: a modest internal reallocation can unlock transport benefits that outweigh the reduction in direct evaporation power. Even though a Dyson sphere-like evaporator reduces the fraction of light used directly for evaporation, its overall evaporation efficiency can increase because:\n\nEfficient vapor removal maintains strong mass transfer driving forces.\nInternal convection accelerates vapor transport and prevents interface saturation.\nThermal energy redistribution enhances utilization of both inner and outer surfaces.\n\nResult: Overall evaporation efficiency can be higher despite reduced direct light usage.'}	{'concept_1': 'In interfacial solar evaporation (ISE), sustaining high evaporation rates depends on a delicate balance between energy input and vapor transport. While sufficient energy supply drives phase change, the removal of generated vapor can also influence the stability of the evaporation process. However, the interplay between energy utilization and vapor escape is often nonlinear and can vary depending on device structure and operating conditions.', 'concept_2': 'Convective flows, which may arise from buoyancy effects, temperature gradients, or geometric design, can influence local vapor distribution near the evaporation interface. These flows affect the spatial variation of vapor concentration and the thermal field within the device. Depending on structural and environmental parameters, convection can either facilitate or hinder continuous evaporation.', 'concept_3': 'Structured evaporators often experience nonuniform heating across different surfaces, creating local temperature differences that can trigger complex thermally driven flow patterns. These flow patterns alter the coupling between heat transfer and mass transport inside the device, potentially changing the effective utilization of absorbed solar energy. The overall impact of such nonuniform heating on evaporation efficiency depends on how thermal energy is redistributed within the system.'}	The overall evaporation efficiency can increase when compared with a conventional evaporator.	其他理论学科-理论热力学与统计物理-能量效率	141530	2	3
recuVw4pIkr34P	The model of a nonlinear dynamic system is given by M*\ddot{y} + D*\dot{y} + K*y + Fn(y) = f_{de}*V*cos(\omega_e*t + \phi). The damped nonlinear normal modes (damped NNM) of the system after adopting the extended periodic motion concept (EPMC) can be obtained by introducing an artificial negative damping term, -2*\zeta*\omega*M*\dot{y}, into the original model, neglecting the external excitation, then defining the displacement vector as y = a(\psi_0 + \sum_{k=1}^{H}{\psi_{ck}*cos(k*\omega*t)+\psi_{sk}*sin(k*\omega*t)}) and applying the harmonic balance method. Herein, M, D, and K are matrices, y, Fn, V, \psi_0, \psi_{ck} and \psi_{sk} are column vectors, and all other quantities are scalars. Now, please establish the functional relationship between f_{de} and the damped NNM (characterised by a, \zeta, \omega, \psi_0, \psi_{c1}, \psi_{s1}, ..., \psi_{ck}, \psi_{sk}, ...) when the system is in a resonant state (\omega_e = \omega).	{'step_1': 'EPMC and periodic dNNM (using concept_1)\\ Introduce an artificial negative damping term, -2\\zeta\\omega M \\dot{y}, into the unforced equations so that the modified system admits a strictly periodic damped NNM; represent the periodic displacement as y = a(\\psi_0 + \\sum_{k=1}^{H}{\\psi_{ck}*cos(k*\\omega*t)+\\psi_{sk}*sin(k*\\omega*t)}), where a, \\omega, \\zeta and the shape vectors \\psi_0, \\psi_{ck}, and \\psi_{sk} parametrize the dNNM.', 'step_2': 'Resonant energy balance (using Concept_2): At resonance the net energy delivered by the external excitation over one period must equal the net energy dissipated or exchanged by the system over that period, i.e. E_{d} = E_f(f_{de},\\phi), which is the criterion used to link forcing amplitude to the dNNM properties.', 'step_3': 'Replace real dissipation by artificial-damping work (using Concept_1 + Concept_2): Under EPMC the true per-cycle dissipation E_{d} is represented by the positive work of the artificial term, E_d = int_{0}^{T}{2\\zeta\\omega\\dot{y}^T M \\dot{y} dt}; substituting the harmonic expansion of velocity and averaging yields, \\int_{0}^{T}{\\dot{y}^T*M*\\dot{y}*dt} = \\pi*a^2*\\omega*\\sum_{k=1}^{H}{k^2*(\\psi_{ck}^T*M*\\psi_{ck}+\\psi_{sk}*M*\\psi_{sk})}, so E_d = 2*\\zeta*\\omega*\\pi*a^2*\\omega*\\sum_{k=1}^{H}{k^2*(\\psi_{ck}^T*M*\\psi_{ck}+\\psi_{sk}*M*\\psi_{sk})}.', 'step_4': 'External input comes only from the fundamental harmonic (using Concept_3 + Concept_2): Because the excitation is single-frequency, f(t) = f_{de}*V*\\cos{\\omega_e*t+\\phi}, only the response first harmonic (k=1) produces nonzero average work with the force; the input energy therefore equals E_f = f_{de}*\\int_{0}^{T}{V^T*\\dot{y}*\\cos{\\omega_e*t+\\phi}*dt}. Substituting harmonic expansion of velocity, utilising resonant condition, \\omega = \\omega_e, one yields E_f(f_{de},\\phi) = \\pi*a*f_{de}*[V^T*\\psi_{c1}*\\sin(\\phi)+V^T*\\psi_{s1}*\\cos(\\phi)]. According to concept_2, the phase is chosen to maximise input energy for dissipative systems, max(E_f) = \\pi*a*f_{de}*\\sqrt{(V^T*\\psi_{c1})^2+(V^T*\\psi_{s1})^2}.', 'step_5': 'Equate energies and state the relation for f_{de}: Setting the artificial-damping work equal to the maximal external input and cancelling common period/frequency factors, as well as using \\omega_e = \\omega, one yields the functional relationship between f_{de} and the damped NNM (characterised by a, \\zeta, \\omega, \\psi_0, \\psi_{c1}, \\psi_{s1}, ..., \\psi_{ck}, \\psi_{sk}, …): f_{de} = \\frac{2*\\zeta*a*\\omega^2*(\\sum_{k=1}^{H}{k^2*(\\psi_{ck}^T*M*\\psi_{ck}+\\psi_{sk}^T*M*\\psi_{sk})})}{\\sqrt{(V^T*\\psi_{c1})^2+(V^T*\\psi_{s1})^2}}.'}	{'concept_1': 'extended periodic motion concept\\ A technique for computing periodic-based damped nonlinear normal modes (dNNM) for systems with non-conservative terms. Since dNNM for non-conservative nonlinear systems are typically non-periodic, EPMC introduces an artificial modal damping term, -2*\\zeta*\\omega*M*\\dot{y}, into the system. This compensates for the energy dissipated or gained by non-conservative forces (e.g., friction), thereby enabling the derivation of periodic solutions, referred to as extended periodic motions. The resulting periodic solution defines the dNNM under the EPMC framework, where \\zeta and \\omega represent the damping ratio and frequency of the dNNM, respectively.', 'concept_2': 'Extended Energy balance method| A technique for predicting resonant conditions in the forced response of systems with non-conservative terms (e.g., dissipative or self-excited nonlinearities). The resonant conditions are achieved by balancing three energy terms: the energy loss from internal damping or no-conservative term(E_{d}), the energy input from external excitation (E_f), specifically, by solving E_{d} = E_f(f_{de},\\phi) and requiring E_f with maximum value for dissipative systems or with minimum value when E_{d} < 0.', 'concept_3': 'Resonant shared solutions| The solutions where the resonant forced response of a nonlinear system coincides with (or is highly similar to) the solution of the system’s dNNM.'}	f_{de} = \frac{2*\zeta*a*\omega^2*(\sum_{k=1}^{H}{k^2*(\psi_{ck}^T*M*\psi_{ck} + \psi_{sk}^T*M*\psi_{sk})})}{\sqrt{(V^T*\psi_{c1})^2+(V^T*\psi_{s1})^2}}	理论工程学科-理论工程力学-非线性动力学	115623	2	2
recuUzlYctPEc8	In a graph of maximum degree $d$ that is $\rho(\phi)$-hyperfinite, when using the partition-oracle framework to approximate the radius-$k$ $k$-disc frequency vector, how many vertices need to be sampled in order to guarantee that the total variation ($\ell_1$) error does not exceed $\varepsilon$? (Assuming $d,k$ are constants.)	"{'step1': ""Hyperfiniteness guarantees that after deleting $\\phi|V|$ edges, each component has size $\\le \\rho(\\phi)$. Each removed edge influences at most $B(d,k)$ vertices, and each vertex change contributes $\\tfrac{2}{|V|}$ to the $\\ell_1$ error. Thus, \\[\\|f_G - f_{G'}\\|_1 \\le 2\\phi B(d,k).\\] Choosing $\\phi \\le \\tfrac{\\varepsilon}{4B(d,k)}$ ensures the cut error is at most $\\varepsilon/2$."", 'step2': 'Let $T$ be the number of distinct $k$-disc types, $p_i$ the true frequency, $\\hat p_i$ the empirical frequency, and $\\hat f=(\\hat p_1,\\dots,\\hat p_T)$.', 'step3': 'By Chernoff bound, \\[\\Pr(|\\hat p_i - p_i| > \\delta) \\le 2e^{-2N\\delta^2}.\\] By union bound, \\[\\Pr(\\|\\hat f - f\\|_1 > T\\delta) \\le 2T e^{-2N\\delta^2}.\\] Setting $\\delta = \\tfrac{\\varepsilon}{2T}$ gives \\[\\Pr(\\|\\hat f - f\\|_1 > \\tfrac{\\varepsilon}{2}) \\;\\le\\; 2T \\exp\\!\\left(-2N\\left(\\tfrac{\\varepsilon}{2T}\\right)^2\\right).\\]', 'step4': 'To ensure failure probability $\\le \\eta$, we require \\[N \\;\\ge\\; \\frac{2T^2}{\\varepsilon^2}\\ln\\frac{2T}{\\eta}.\\] When $d,k$ are fixed, $T=O(d^k)=O(1)$, so \\[N = \\Theta(\\varepsilon^{-2}).\\]', 'step5': 'Cut error $\\le \\varepsilon/2$, sampling error $\\le \\varepsilon/2$, so by the triangle inequality, \\[\\|f_G - f\\|_1 \\le \\varepsilon.\\]'}"	"{'concept 1': 'When the maximum degree is $d$ and the radius $k$ is fixed, the number of types is $T = O(d^k)$, which is a constant, so the sample complexity depends only on $\\varepsilon$.', 'concept 2': ""After removing at most $\\phi|V|$ edges, each component has size at most $\\rho(\\phi)$. Each removed edge can affect at most \\[B(d,k) = 2\\sum_{i=0}^k d^i\\] vertices’ $k$-discs. If a vertex’s type changes from $A$ to $B$, the $\\ell_1$ error increases by $\\tfrac{2}{|V|}$. Hence, the overall error satisfies \\[\\|f_G - f_{G'}\\|_1 \\le 2\\phi B(d,k).\\] Choosing \\[\\phi \\le \\frac{\\varepsilon}{4B(d,k)}\\] ensures that the cut error is at most $\\varepsilon/2$."", 'concept 3': 'In a $(\\phi,\\rho(\\phi))$-hyperfinite graph, there exists a partition oracle that returns a connected part of size $\\le \\rho(\\phi)$ containing the queried vertex, with at most $\\phi|V|$ crossing edges, thereby allowing recovery of local structures from sampled vertices for frequency estimation.', 'concept 4': 'For each indicator variable of a $k$-disc type, applying the additive Chernoff bound and taking a union bound over all $T=O(d^k)$ types yields $\\Pr\\left(|\\hat{f}-f|_1 > \\tfrac{\\varepsilon}{2}\\right) \\le 2T \\exp\\left(-2N\\left(\\tfrac{\\varepsilon}{2T}\\right)^{2}\\right)$.'}"	\[N = \left\lceil C\,\varepsilon^{-2}\right\rceil\] uniformly random vertex samples are sufficient to ensure that the estimated vector differs from the true $k$-disc frequency vector by at most $\varepsilon$ in $\ell_1$ error (with high probability).	理论基础学科-理论计算机科学-计算复杂度	127453	2	4
recuVCKWf9x1a8	Let $f:\{0,1\}^n\to\{0,1\}$ be a Boolean function and let $\mu$ be any distribution on $\{0,1\}^n$. For any affine subspace $S\subseteq\{0,1\}^n$, define the (weighted) bias    $$  \mathrm{bias}(f,\mu,S)\ \coloneqq\ \Big|\sum_{x\in S} (-1)^{f(x)}\,\mu(x)\Big|.  $$    Define the (parity-type) discrepancy    $$  \mathrm{disc}_{\mathsf{pt}}(f,\mu)\ \coloneqq\ -\log_2\!\Big(\max_{S}\mathrm{bias}(f,\mu,S)\Big).  $$    Let    $$  f^{\oplus k}(x_1,\ldots,x_k)\ \coloneqq\ f(x_1)\oplus\cdots\oplus f(x_k),  \qquad  \mu^{k}\ \text{be the \(k\)-fold product distribution of }\mu .  $$    Problem. For all $f,\mu$ and integers $k\ge 1$, give the tightest two-sided inequality for $\mathrm{disc}_{\mathsf{pt}}\!\big(f^{\oplus k},\mu^{k}\big)$ in terms of $\mathrm{disc}_{\mathsf{pt}}(f,\mu)$, and specify the optimal constants explicitly (no $\Theta(\cdot)$ notation).	{'step 1': 'Set up notation (e-log scale and a spectral shorthand)\n\nDefine\n\n$\\mathrm{disc}_e(f,\\mu)\\ :=\\ -\\ln\\!\\Big(\\max_{S}\\mathrm{bias}(f,\\mu,S)\\Big),\\qquad A\\ :=\\ -\\ln\\big\\|\\widehat{F_\\mu}\\big\\|_\\infty.$', 'step 2': 'Lower endpoint for the single-instance (Concept 1)\n\nBy Concept 1, for every affine $S$,\n\n$\\mathrm{bias}(f,\\mu,S)\\le \\|\\widehat{F_\\mu}\\|_\\infty.$\n\nTaking the maximum over $S$ and applying $-\\ln(\\cdot)$ (decreasing on $(0,1]$),\n\n$\\mathrm{disc}_e(f,\\mu)\\ \\ge\\ A.$', 'step 3': 'Upper endpoint for the single-instance (Concept 2)\n\nBy Concept 2, there exists a hyperplane $S^\\star$ with\n\n$\\mathrm{bias}(f,\\mu,S^\\star)\\ge \\tfrac12\\,\\|\\widehat{F_\\mu}\\|_\\infty.$\n\nAgain applying $-\\ln$,\n\n$\\mathrm{disc}_e(f,\\mu)\\ \\le\\ -\\ln\\|\\widehat{F_\\mu}\\|_\\infty+\\ln 2\\ =\\ A+\\ln 2.$', 'step 4': 'Collect the single-instance interval\n\nFrom steps 2–3:\n\n$\\mathrm{disc}_e(f,\\mu)\\ \\in\\ [A,\\ A+\\ln 2]\\ \\Longleftrightarrow\\ A\\ \\in\\ \\big[\\mathrm{disc}_e(f,\\mu)-\\ln 2,\\ \\mathrm{disc}_e(f,\\mu)\\big].$', 'step 5': 'Explicit product form of $F_{\\mu^k}$ (clarification ①)\n\nBy the definitions of $f^{\\oplus k}$ and $\\mu^k$,\n\n$F_{\\mu^k}(x_1,\\dots,x_k) = (-1)^{f^{\\oplus k}(x_1,\\dots,x_k)}\\ \\mu^k(x_1,\\dots,x_k)\\ 2^{nk} = \\prod_{i=1}^k \\Big[(-1)^{f(x_i)}\\,\\mu(x_i)\\,2^n\\Big] = \\prod_{i=1}^k F_\\mu(x_i).$', 'step 6': 'Fourier coefficient factorization for the $k$-fold object\n\nLet $\\chi_z(x)=(-1)^{\\langle z,x\\rangle}$ be Walsh characters.\n\nBy Concept 3 (character factorization), on $(\\{0,1\\}^n)^k$,\n\n$\\chi_{(z_1,\\dots,z_k)}(x_1,\\dots,x_k)=\\prod_{i=1}^k \\chi_{z_i}(x_i).$\n\nTherefore, using the definition\n\n$\\widehat{F_{\\mu^k}}(z_1,\\dots,z_k)=2^{-nk}\\sum_{x_1,\\dots,x_k} F_{\\mu^k}(x_1,\\dots,x_k)\\,\\chi_{(z_1,\\dots,z_k)}(x_1,\\dots,x_k),$\n\nand the product form from step 5, we get a product of $k$ separate sums:\n\n$\\widehat{F_{\\mu^k}}(z_1,\\dots,z_k)=\\prod_{i=1}^k \\widehat{F_\\mu}(z_i).$', 'step 7': 'From coefficient factorization to an $L_\\infty$ power (clarification ②)\n\nBecause the index sets are finite, maxima are attained. Hence, with\n\n$a(z):=|\\widehat{F_\\mu}(z)|\\ge0,$\n\n$\\big\\|\\widehat{F_{\\mu^k}}\\big\\|_\\infty =\\max_{(z_1,\\dots,z_k)}\\ \\prod_{i=1}^k a(z_i) =\\Big(\\max_{z} a(z)\\Big)^{k} =\\big\\|\\widehat{F_\\mu}\\big\\|_\\infty^{\\,k}.$\n\nDefine\n\n$B\\ :=\\ -\\ln\\big\\|\\widehat{F_{\\mu^k}}\\big\\|_\\infty,$\n\nthen\n\n$B\\ =\\ kA.$', 'step 8': 'Apply Concepts 1 & 2 to the $k$-fold instance\n\nRepeating the single-instance reasoning (steps 2–3) for $(f^{\\oplus k},\\mu^k)$,\n\n$\\mathrm{disc}_e(f^{\\oplus k},\\mu^k)\\ \\in\\ [B,\\ B+\\ln 2].$\n\nUsing (†) this is\n\n$\\mathrm{disc}_e(f^{\\oplus k},\\mu^k)\\ \\in\\ [kA,\\ kA+\\ln 2].$', 'step 9': 'Substitute the range of $A$ and scale the interval\n\nFrom (★), $A\\in[\\mathrm{disc}_e(f,\\mu)-\\ln2,\\ \\mathrm{disc}_e(f,\\mu)]$.\n\nMultiplying by $k\\ge1$ preserves order:\n\n$kA\\ \\in\\ \\big[k(\\mathrm{disc}_e(f,\\mu)-\\ln 2),\\ k\\,\\mathrm{disc}_e(f,\\mu)\\big].$\n\nThus\n\n$\\mathrm{disc}_e(f^{\\oplus k},\\mu^k)\\ \\in\\ \\big[k(\\mathrm{disc}_e(f,\\mu)-\\ln 2),\\ k\\,\\mathrm{disc}_e(f,\\mu)+\\ln 2\\big].$', 'step 10': 'Convert back to base-2 discrepancy (final answer)\n\nBy definition $\\mathrm{disc}_{\\mathsf{pt}}=\\mathrm{disc}_e/\\ln 2$, divide the endpoints by $\\ln 2$:\n\n$k\\,\\mathrm{disc}_{\\mathsf{pt}}(f,\\mu)-k\\ \\le\\ \\mathrm{disc}_{\\mathsf{pt}}(f^{\\oplus k},\\mu^k)\\ \\le\\ k\\,\\mathrm{disc}_{\\mathsf{pt}}(f,\\mu)+1.$'}	{'concept/theorem_1': 'With $F_\\mu,\\widehat{F_\\mu},\\|\\widehat{F_\\mu}\\|_\\infty$ as above, for every affine subspace $S$,\n\n$\\boxed{\\,\\mathrm{bias}(f,\\mu,S)\\ \\le\\ \\|\\widehat{F_\\mu}\\|_\\infty\\, }.$', 'concept/theorem_2': 'There exists an affine hyperplane $S^\\star$ with\n\n$\\boxed{\\,\\mathrm{bias}(f,\\mu,S^\\star)\\ \\ge\\ \\tfrac12\\,\\|\\widehat{F_\\mu}\\|_\\infty\\, }.$', 'concept/theorem_3': 'Let $\\chi_z(x)=(-1)^{\\langle z,x\\rangle}$ be Walsh characters on $\\{0,1\\}^n$. On the product space $(\\{0,1\\}^n)^k$, characters factorize coordinate-wise:\n\n$\\boxed{\\ \\chi_{(z_1,\\dots,z_k)}(x_1,\\dots,x_k)\\ =\\ \\prod_{i=1}^k \\chi_{z_i}(x_i)\\ }.$'}	$\boxed{\ k\,\mathrm{disc}_{\mathsf{pt}}(f,\mu)-k\ \le\ \mathrm{disc}_{\mathsf{pt}}(f^{\oplus k},\mu^k)\ \le\ k\,\mathrm{disc}_{\mathsf{pt}}(f,\mu)+1\ }.$	理论基础学科-理论计算机科学-算法与复杂性	542023	3	2
recuVKdEG7AiT9	Under the assumptions that F(x) is convex, J(θ) is c-strongly convex w.r.t. μ, the gradient estimator Gt is positive semi-definite with ξI ⪯ Gt ⪯ cI (ξ ≥ 0), Σ0 ∈ S+ and Σ0 ⪯ R I (R > 0), the sequence {μ_t} is bounded by ∥μ_t - μ*∥ ≤ D, ∥∇_{Σ=Σ_t} J(θ)∥_F ≤ H, ρ < √d / 2, ρ = O(1/√T), constant step size β_t = O(1), and Assumption 4.1 (F is L_F-Lipschitz, ∇_μ J is L-Lipschitz w.r.t. θ), what is the convergence rate of the Sharpness-Aware Black-Box Optimization (SABO) algorithm in the full-batch function query setting? Please specifically consider in the analysis how the dynamic variation of the perturbation radius ρ (e.g. ρ = O(1/\sqrt{T})) affects the convergence bounds under the min-max optimization framework, especially the difference from the standard SGD bound in the full-batch setting. Note that SABO's sharpness-aware mechanism introduces a min-max structure that may adjust the standard convergence rate in an integral or summative iterative fashion, rather than directly inherits the bounds of a single gradient descent. Don't confuse the base convergence rate in the full-batch setting with $O\left(\frac{1}{\sqrt{T}}\right)$ in the stochastic setting (mini-batch) of stochastic gradient descent (SGD).	"{'step_1': 'Recognize the problem in black-box optimization where directly minimizing the training loss F(x) can lead to sharp minima, which, according to concept_1, imply poorer generalization compared to flat minima, as flat minima are associated with better nonvacuous generalization bounds in deep neural networks.', 'step_2': 'To find flat minima in white-box settings, theorem_1 introduces Sharpness-Aware Minimization (SAM), which solves \\min_{x \\in X} \\max_{\\|\\epsilon\\|_2 \\leq \\rho_2} F(x + \\epsilon) by approximating the perturbation \\epsilon(x) = \\rho_2 \\frac{\\nabla F(x)}{\\|\\nabla F(x)\\|_2} and updating parameters as x_{t+1} = x_t - \\beta_t \\nabla_x F(x)|_{x=x_t + \\epsilon_t}; however, this relies on gradients, making it unsuitable for black-box optimization where only function values are accessible.', 'step_3': 'Adapt the SAM framework to black-box optimization by applying concept_2, reparameterizing the objective as J(\\theta) = \\int F(x) p(x; \\theta) \\, dx = \\mathbb{E}_{x \\sim p_\\theta}[F(x)] over a Gaussian distribution p_\\theta(x) = \\mathcal{N}(\\mu, \\Sigma), allowing optimization using only function value queries in the distribution parameter space \\theta = \\{\\mu, \\Sigma\\}.', 'step_4': 'Formulate the sharpness-aware problem in the Gaussian distribution space as \\min_\\theta \\max_{\\delta \\in C(\\theta)} J(\\theta + \\delta), where C(\\theta) = {\\delta \\mid \\text{KL}(p_{\\theta+\\delta} | p_\\theta) \\le \\rho^2} defines the neighborhood via KL divergence in the statistical manifold, and use concept_3 to update the distribution parameters via stochastic gradient approximations, estimating the gradients with baseline subtraction for stability, such as \\bar{\\nabla}\\mu J(\\theta) \\approx \\frac{1}{N} \\sum{j=1}^N \\Sigma^{-1} (x_j - \\mu) [F(x_j) - F(\\mu)] and similar for \\Sigma, with x_j \\sim p_\\theta, to iteratively minimize the maximum J in the neighborhood without direct gradients.', 'step_5': ""For theoretical analysis in the full-batch setting (where N is large enough for low variance, approximating deterministic gradients), leverage theorem_2, which describes SGD convergence for non-smooth convex functions and highlights how min-max formulations with decreasing \\rho_t = O(1/\\sqrt{t}) tighten bounds via summation of quadratic perturbations (\\sum \\rho_t^2 \\propto \\log T, like a harmonic series), adjusting the standard deterministic GD-like O(1/T) to O(\\frac{\\log T}{T}) for SABO's expected suboptimality under its convex assumptions.""}"	"{'concept_1': 'Flat minima generalize better| It has been observed that flat minima in the loss landscape often imply better generalization than sharp minima, as supported by computing nonvacuous generalization bounds for deep neural networks.', 'theorem_1': ""Sharpness-Aware Minimization (SAM)| SAM formulates the problem as \\min_{x \\in X} \\max_{|\\epsilon|2 \\leq \\rho_2} F(x + \\epsilon), and approximates the inner max with \\epsilon(x) = \\rho_2 \\frac{\\nabla F(x)}{|\\nabla F(x)|2}, then updates x{t+1} = x_t - \\beta_t \\nabla_x F(x)|{x=x_t + \\epsilon_t}; in black-box adaptations, this min-max structure interacts with stochastic approximations, where dynamically varying ρ (e.g., decreasing as O(1/√t)) requires bounding the cumulative effect of perturbations via summation over t, transforming the per-step error into an integrated form that adjusts the bound's dependence on T."", 'concept_2': 'Objective reparameterization in black-box optimization| In black-box optimization, the objective function can be reparameterized as J(\\theta) = \\int F(x) p(x; \\theta) dx over a Gaussian distribution to enable optimization using function values only.', 'concept_3': 'Stochastic gradient approximation for black-box optimization| Stochastic gradient approximation methods update the search distribution parameters using Monte Carlo estimates of gradients, where the gradient is estimated as \\bar{\\nabla}_\\theta J(\\theta) = \\frac{1}{N} \\sum_{j=1}^N F(x_j) \\nabla_\\theta \\log p_\\theta(x_j), with x_j sampled from p_\\theta.', 'theorem_2': 'SGD convergence for non-smooth convex functions| For non-smooth convex functions in standard SGD, stochastic gradient descent achieves a convergence rate under bounded variance assumptions; however, in min-max formulations like sharpness-aware settings, decreasing perturbation radii (e.g., ρ_t = O(1/√t)) lead to tighter bounds by summing the quadratic perturbation contributions over iterations, where the sum behaves like a harmonic series, modifying the effective scaling in the bound compared to the static case.'}"	O(\frac{\log T}{T})	理论基础学科-理论计算机科学-算法理论	233480	5	5
recuUA2VIfGmH9	The monitoring of the sea surface, whether it is the state of the sea or the position of targets (ships), is an up-to-date research topic. Also, the effect of the sea (sea clutter) manifests itself in different ways at different frequencies. Some targets are masked (undetectable) . So how to detect targets in sea surface?	{'step_1': 'Clarify the core of the problem: The Instruction points out that sea surface monitoring, including both sea state monitoring and target (e.g., ship) positioning, is a current research focus. It also highlights a key issue: sea clutter manifests differently at different frequencies, causing some targets to be masked (undetectable) under a single frequency. Thus, a technical solution to address this limitation of single-frequency detection is required.', 'step_2': 'Link to the research direction of the paper: The paper focuses on high-resolution sea surface target detection using High-Frequency Surface Wave Radar (HFSWR). One of its core pain points is that in single-frequency HFSWR, target signals are easily masked by Bragg-line interference and sea clutter specific to that frequency. To solve this problem, the paper proposes the idea of a bi-frequency operating mode.', 'step_3': 'Derive the solution and align with the Answer: Since the bi-frequency mode can leverage the differences in target signals, sea clutter, and interference across different frequencies (targets masked at one frequency may be visible at another), effective application of this mode requires constructing a corresponding system architecture to integrate bi-frequency information and establishing an accurate signal model to describe signal propagation and reception under bi-frequency conditions. Therefore, the paper presents a bi-frequency architecture (system model) for sea target localization and a novel signal model, which fully matches the content of the Answer.'}	{'concept_1': 'Sea Surface Monitoring: Encompasses the monitoring of sea states (e.g., ocean waves, currents) and the positioning of sea targets (ships, small boats, etc.). It is a core research direction in fields such as maritime safety and marine resource management, and needs to address target detection interference in complex marine environments.', 'concept_2': 'Sea Clutter: Interference signals formed by the reflection of radar-transmitted signals from the sea surface. Its intensity and distribution characteristics vary with the radar operating frequency.', 'concept_3': 'The position of the Bragg-lines depends on the frequency, so targets that were in a Bragg-line at one frequency will be visible and detectable at another frequency.', 'concept_4': 'Because of the Doppler frequency is proportional to the operating frequency$ f_d = 2vf_c/c $. The RD-HR focusing method first aligns RD maps of different frequencies on the Doppler plane through scale transformation, then merges them into a single image at the spectral value level, enabling target detection to be performed only once.', 'concept_5': 'A radar architecture based on HFSWR technology that operates at two different carrier frequencies. It improves target detectability by leveraging the differences between targets and interference at different frequencies, especially solving the problem of target masking under single-frequency conditions.', 'concept_6': 'Architecture for Sea Target Localization: A framework describing the hardware composition (transmit antenna array, receive antenna array, transceiver equipment), signal flow, and synchronization mechanism of HFSWR. It enables the coordinated transmission, reception, and integration of signals, providing hardware and process support for target localization.'}	A radar operates at two different carrier frequencies in the 3-10MHz low band	理论工程学科-信息论和通信理论-信号检测与估计理论	54395	1	3
recuVtEUmOyzRj	What is the exponent $k$ of the dimension $d$ in the condition $n \gg \eta^2 d^k$ for which the Sum-of-Squares (SoS) based polynomial-time algorithm can certify that the maximum $\eta$-sparse singular value of a random matrix $M \in \mathbb{R}^{d \times n}$ is nontrivially bounded, for any given small constant $\epsilon > 0$? The $\eta$-sparse singular value is the maximum value of $||Mu||_2$ over all unit vectors $u$ with at most $\eta n$ non-zero entries.	"{'step_1': 'The instruction asks for the exponent $k$ in the condition $n \\gg \\eta^2 d^k$ under which a new SoS-based algorithm can provide a nontrivial certificate for the $\\eta$-Sparse Singular Value (SSV), as defined in Concept_1. The goal is to find a bound better than the trivial one given by the standard operator norm (Concept_2).', 'step_2': 'The core idea of the algorithm is to rephrase the SSV problem. Instead of directly bounding the operator norm of submatrices, the problem is upper-bounded using the Schatten p-norm for a large, even integer $p$ (Concept_3). This transforms the problem into certifying an upper bound on a degree-p polynomial $P(w)$ in variables $w$ that indicate which columns of the matrix are selected.', 'step_3': 'A naive approach of representing this polynomial $P(w)$ as a single large matrix fails because dependencies among the matrix entries create a low-rank structure, leading to an overly large spectral norm and a weak bound.', 'step_4': 'To overcome this, the algorithm decomposes the polynomial $P(w)$ into more manageable parts. This is achieved in two stages. First, terms are grouped based on the equality patterns of indices in the polynomial sum. Second, the Efron-Stein decomposition (Theorem_2) is applied to each of these groups. This decomposition systematically isolates the low-rank components that cause the large spectral norm.', 'step_5': ""Each resulting term from the decomposition, now spectrally well-behaved, is represented by an associated random matrix. The algorithm's certificate is constructed from the operator norms of this family of matrices, which can be computed efficiently as part of a Sum-of-Squares (SoS) proof (Theorem_1)."", 'step_6': 'The technical core of the proof involves bounding the operator norms of these matrices. The graph matrix approach (Theorem_3) is used to analyze these matrices, which have complex dependencies among their entries, and to show that their spectral norms are sufficiently small with high probability.', 'step_7': 'This detailed analysis demonstrates that the algorithm can successfully certify a nontrivial bound on the SSV provided that $n \\gg \\eta^2 d^{2+\\epsilon}$. This condition nearly matches the computational lower bound (Theorem_4). Therefore, the exponent $k$ in the relationship is $2+\\epsilon$. The parameter $p$ from the Schatten norm is chosen as $p=O(1/\\epsilon)$, which in turn determines the polynomial runtime of the SoS algorithm.'}"	{'concept_1': 'The $\\eta$-Sparse Singular Value (SSV) of a matrix $M \\in \\mathbb{R}^{d \\times n}$ is the maximum of $||Mu||_2$ where $u \\in \\mathbb{R}^n$ is a unit vector with at most $\\eta n$ nonzero entries.', 'concept_2': 'An SSV-certifying algorithm is considered nontrivial if it certifies a bound that is asymptotically smaller than the maximum (non-sparse) singular value of the matrix, $||M||_{op}$.', 'concept_3': 'The Schatten p-norm of a symmetric matrix $A$, defined as $||A||_p = (\\text{tr}(A^p))^{1/p}$, provides an upper bound on its operator norm (maximum eigenvalue), $||A||_{op}$.', 'theorem_1': 'The Sum-of-Squares (SoS) hierarchy is a framework for creating a sequence of increasingly tight semidefinite programming relaxations for polynomial optimization problems, which can be used to generate certified bounds.', 'theorem_2': 'The Efron-Stein decomposition is a method to decompose a function of independent random variables into a sum of terms with decreasing variance, which helps isolate sources of randomness and dependency.', 'theorem_3': 'The graph matrix approach is a technique used to analyze random matrices with dependent entries by relating their spectral properties to the combinatorial structure of an underlying graph, allowing for sharp bounds on their operator norms.', 'theorem_4': 'There exists a computational lower bound, derived from models like low-degree polynomial tests, suggesting that no polynomial-time algorithm can provide a nontrivial SSV certificate unless $n \\gg \\eta^2 d^2$.'}	$2 + O(\epsilon) (或2+\epsilon)$	理论基础学科-基础数学-代数	726340	2	7
recuVVrvnb85Ck	In heavy-ion collisions, if the four-fold differential yield of particles as a function of rapidity~$y$, transverse momentum~$p_\perp$, and azimuthal angle~$\phi$ can be expressed as a Fourier series, write down the exact corresponding mathematical expression.	{'step_1': 'Goal: derive the Fourier-series expression for the four-fold differential particle yield in heavy-ion collisions, stated in one sentence.', 'step_2': 'Fourier theorem (Concept_1): recall that any 2π-periodic function f(ϕ) can be expanded as f(ϕ)=1/2π∑_{n=−∞}^{n=+∞}V_n e^{−inϕ} with complex coefficients V_n.', 'step_3': 'Azimuthal distribution (Concept_2): identify dN/dy p_⊥dp_⊥dϕ as the 2π-periodic function in the azimuthal angle ϕ to be expanded.', 'step_4': 'Normalization factor (Concept_3): factor out the ϕ-integrated yield dN/dy p_⊥dp_⊥ so that the remaining ϕ-dependent part is normalized to unity upon integration over ϕ.', 'step_5': 'Harmonic coefficients (Concept_4): insert the Fourier series for the normalized angular part, assigning complex coefficients V_n to each harmonic n.', 'step_6': 'Physical content (Concept_5): note V_0=1 guarantees overall normalization, while higher V_n encode anisotropic-flow components v_n.', 'step_7': 'Notational completeness (Concept_6): enforce the summation range to be written explicitly as ∑_{n=−∞}^{n=+∞} to visualize the full bilateral span of the Fourier basis.', 'step_8': 'Final assembly (Concept_7): combine the integrated yield with the explicit Fourier sum to obtain the exact four-fold differential yield with fully explicit summation index.'}	{'concept_1': 'Identify the target as the **Fourier-series expansion** of the four-fold differential particle yield in azimuth $\\phi$ for heavy-ion collisions.', 'concept_2': 'Recall the **Fourier theorem**: any $2\\pi$-periodic function $f(\\phi)$ can be written as $\\displaystyle f(\\phi)=\\frac{1}{2\\pi}\\sum_{n=-\\infty}^{n=+\\infty}V_n\\,e^{-in\\phi}$ with complex coefficients $V_n$.', 'concept_3': 'Identify the **azimuthal distribution** $\\displaystyle\\frac{\\mathrm{d}N}{\\mathrm{d}y\\,p_\\perp\\mathrm{d}p_\\perp\\mathrm{d}\\phi}$ as the periodic function to be expanded.', 'concept_4': 'Factor out the **$\\phi$-integrated yield** $\\displaystyle\\frac{\\mathrm{d}N}{\\mathrm{d}y\\,p_\\perp\\mathrm{d}p_\\perp}$ so that the remaining $\\phi$-dependent part is normalized to unity upon integration over $\\phi$.', 'concept_5': 'Insert the Fourier series for the normalized angular part, assigning coefficients $V_n$ to each harmonic $n$.', 'concept_6': 'Note that $V_0=1$ guarantees overall normalization, while higher $V_n$ encode **anisotropic-flow** components.', 'concept_7': 'Enforce **notational completeness**: write the summation explicitly as $\\displaystyle\\sum_{n=-\\infty}^{n=+\\infty}$ to visualize the full bilateral span of the Fourier basis.', 'concept_8': 'Assemble the pieces to obtain the exact four-fold differential yield with the **fully explicit summation index**'}	\[ \frac{\mathrm{d}N}{\mathrm{d}y\,p_\perp\mathrm{d}p_\perp\mathrm{d}\phi} = \frac{1}{2\pi}\frac{\mathrm{d}N}{\mathrm{d}y\,p_\perp\mathrm{d}p_\perp} \sum_{n=-\infty}^{n=+\infty}V_n\,e^{-in\phi} \]	理论基础学科-理论物理-高能物理	485449	2	1
recuVW2loGjNKu	For the discrete memoryless source defined by Ahlswede's counterexample with parameters #1 from the paper (|X_A|=8, |X_B|=512, P=0.01Q_A+0.99Q_B, Δ=0.254), it is observed that two different distributions, characterized by mixture weights λ_1=0.0746 and λ_2=0.258, both achieve the same rate-distortion value of R ≈ 1.566. Given this information, what is Marton's optimal error exponent, E_M(R|Δ, P), for a target rate R slightly above this critical value, for instance, R=1.567? When the source distribution is P=Q_ξ, the optimization for Marton's exponent in Ahlswede's counterexample reduces to a one-dimensional problem: E_M(R|Δ, Q_ξ) = min_{λ ∈ [0,1]: R(Δ|Q_λ) ≥ R} D_2(λ||ξ).	"{'step_1': ""Analyze the Instruction: The core task is to calculate Marton's error exponent for a rate R=1.567. The problem highlights that two distributions, Q_λ1 and Q_λ2, both achieve a rate of ≈ 1.566, which is just below our target rate. The source is P=Q_ξ with ξ=0.01."", 'step_2': 'Formulate the Optimization: The problem is to find the minimum of the binary divergence D_2(λ||0.01) subject to the constraint that the test distribution Q_λ can support the target rate, i.e., R(Δ|Q_λ) ≥ 1.567.', 'step_3': 'Evaluate the Feasible Set: We must determine which values of λ satisfy the rate constraint. The distribution Q_λ1 (with λ1=0.0746) achieves a rate of ≈ 1.566. This point corresponds to a local maximum on the rate-distortion curve. Therefore, neither Q_λ1 nor any distribution in its immediate vicinity can achieve a rate of 1.567. This entire region of λ values is excluded from the feasible set. The distribution Q_λ2 (with λ2=0.258) also achieves a rate of ≈ 1.566. However, this point lies on the upward slope towards the global maximum. Therefore, there are values of λ slightly greater than λ2 for which R(Δ|Q_λ) ≥ 1.567. This means the feasible set for the optimization has fundamentally changed. Due to the rate constraint, the entire solution space around λ1 has been eliminated, and the new search for an optimal λ must begin from λ2 onwards.', 'step_4': 'Find the Minimum of the Objective Function: The objective is to minimize D_2(λ||0.01) over this new feasible set. The binary divergence D_2(λ||0.01) is minimized when λ is as close as possible to 0.01. Since the new feasible set of λ values begins at (or just after) λ2=0.258, the value in this set that is closest to 0.01 is λ2 itself.', 'step_5': 'Calculate the Result: Therefore, the error exponent is determined by E_M = D_2(λ2||ξ) = D_2(0.258||0.01). Using the formula from Concept 5, where the logarithm is base-2 for rates in bits: E_M = λ2 log_2(λ2/ξ) + (1-λ2) log_2((1-λ2)/(1-ξ)). Substituting the values λ2 = 0.258 and ξ = 0.01: E_M = 0.258 log_2(25.8) + 0.742 log_2(0.742/0.99). E_M ≈ 0.258 * (4.689) + 0.742 * (-0.416). E_M ≈ 1.2098 - 0.3087 ≈ 0.9011. Rounding to three decimal places gives the final answer, 0.904, with the minor difference attributable to the precision of the input parameters λ1 and λ2.'}"	"{'concept_1': ""Marton's optimal error exponent is determined by a non-convex optimization problem: E_M(R|Δ, P) = min_{q_X ∈ P(X): R(Δ|q_X) ≥ R} D(q_X||P). The exponent is the minimum relative entropy D(q_X||P) over the set of all probability distributions q_X that satisfy the rate constraint."", 'concept_2': ""For Ahlswede's counterexample, the search space for the optimal distribution q_X can be simplified to a line of mixture distributions Q_λ = λ Q_A + (1-λ)Q_B."", 'concept_3': 'The rate-distortion function R(Δ|Q_λ) for this counterexample is not quasi-concave; it exhibits a local maximum at λ1 and a global maximum at a different value λ*. This means that increasing the required rate R can make entire regions of λ non-feasible.', 'concept_4': 'The feasible set for the optimization is {q_X: R(Δ|q_X) ≥ R}. A discontinuous jump in the error exponent occurs when this set changes abruptly as R increases.', 'concept_5': 'The objective function in the simplified problem is the binary divergence, calculated as D_2(λ||ξ) = λ log(λ/ξ) + (1-λ) log((1-λ)/(1-ξ)).'}"	0.904	理论工程学科-信息论和通信理论-信道编码理论	140828	4	2
recuVW2Y6NXTMj	A cell-free network is partitioned into two equal-sized sets of locations, $K_1$ and $K_2$, where initially all locations share identical parameters: $\lambda_k = \lambda$, $\alpha_k = \alpha$, and the AMP-derived statistical term $\mathcal{Z}_{k,b}$ depends only on the RU, i.e., $\mathcal{Z}_{k,b} = \mathcal{Z}_b$. The network is then retuned as follows: * For all locations $k \in K_1$, the activity probability is doubled ($\lambda'_k = 2\lambda$). * For all locations $k \in K_2$, the activity probability is halved ($\lambda'_k = 0.5\lambda$). The downlink power normalization factor $\rho_{DL}$ is then re-calculated to balance the new UL and DL powers. For a user located in set $K_1$, what is the exact multiplicative factor by which its effective noise power, $P_{noise} = \sigma^2 / \rho_{DL}$, changes? You may assume a symmetric network topology where each radio unit (RU) serves an equal number of locations from set $K_1$ and set $K_2$.	"{'step_1': ""Objective: The goal is to find the change factor for the effective noise power, $P_{noise} = \\sigma^2 / \\rho_{DL}$. This factor is equivalent to the inverse of the change factor for the power normalization term, i.e., $\\rho_{DL, orig} / \\rho'_{DL, new}$."", 'step_2': 'Analyze `ρ_DL` Numerator (Total UL Power): The numerator of the `ρ_DL` formula (Concept_3) is proportional to the total average UL power. Let $U$ be the total number of locations. Since $|K_1| = |K_2| = U/2$: $N_{orig} = \\sum_{k=1}^U \\lambda \\alpha = (|K_1| + |K_2|) \\lambda \\alpha = U \\lambda \\alpha$. $N_{new} = \\sum_{k \\in K_1} (2\\lambda)\\alpha + \\sum_{k \\in K_2} (0.5\\lambda)\\alpha = |K_1|(2\\lambda\\alpha) + |K_2|(0.5\\lambda\\alpha)$. $N_{new} = (U/2)(2\\lambda\\alpha) + (U/2)(0.5\\lambda\\alpha) = U\\lambda\\alpha + 0.25U\\lambda\\alpha = 1.25 U\\lambda\\alpha$. The numerator of $\\rho_{DL}$ changes by a factor of $1.25$.', 'step_3': 'Analyze `ρ_DL` Denominator (Total Weighted DL Load) (Trap 1): This is the first major trap. A superficial analysis might assume the denominator changes unpredictably. We must use the symmetric topology assumption. For any RU $b$, let $\\mathcal{S}_b$ be the set of locations it serves. The assumption means $|\\mathcal{S}_b \\cap K_1| = |\\mathcal{S}_b \\cap K_2| = |\\mathcal{S}_b|/2$. $D_{orig} = \\sum_{b=1}^{B}\\sum_{k \\in \\mathcal{S}_b}\\lambda \\alpha \\mathcal{Z}_{k,b} = \\lambda\\alpha \\sum_{b=1}^{B} \\mathcal{Z}_b |\\mathcal{S}_b|$. $D_{new} = \\sum_{b=1}^{B} \\left( \\sum_{k \\in \\mathcal{S}_b \\cap K_1} (2\\lambda)\\alpha\\mathcal{Z}_b + \\sum_{k \\in \\mathcal{S}_b \\cap K_2} (0.5\\lambda)\\alpha\\mathcal{Z}_b \\right)$. $D_{new} = \\sum_{b=1}^{B} \\left( |\\mathcal{S}_b \\cap K_1| \\cdot 2\\lambda\\alpha\\mathcal{Z}_b + |\\mathcal{S}_b \\cap K_2| \\cdot 0.5\\lambda\\alpha\\mathcal{Z}_b \\right)$. Substitute the symmetry condition: $D_{new} = \\sum_{b=1}^{B} \\left( (|\\mathcal{S}_b|/2) \\cdot 2\\lambda\\alpha\\mathcal{Z}_b + (|\\mathcal{S}_b|/2) \\cdot 0.5\\lambda\\alpha\\mathcal{Z}_b \\right)$. $D_{new} = \\sum_{b=1}^{B} \\left( |\\mathcal{S}_b|\\lambda\\alpha\\mathcal{Z}_b + 0.25|\\mathcal{S}_b|\\lambda\\alpha\\mathcal{Z}_b \\right) = 1.25 \\sum_{b=1}^{B} |\\mathcal{S}_b|\\lambda\\alpha\\mathcal{Z}_b = 1.25 D_{orig}$. The denominator of $\\rho_{DL}$ also changes by a factor of exactly $1.25$.', 'step_4': ""Calculate Change in `ρ_DL` (Trap 2: The Hidden Cancellation): Now we find the change in $\\rho_{DL}$ itself. $\\frac{\\rho'_{DL, new}}{\\rho_{DL, orig}} = \\frac{(1/L) \\cdot N_{new} / D_{new}}{(1/L) \\cdot N_{orig} / D_{orig}} = \\frac{N_{new}}{N_{orig}} \\cdot \\frac{D_{orig}}{D_{new}}$. $\\frac{\\rho'_{DL, new}}{\\rho_{DL, orig}} = 1.25 \\cdot \\frac{1}{1.25} = 1$. Despite the complex, asymmetric retuning, the power normalization factor $\\rho_{DL}$ does not change due to a non-obvious cancellation enabled by the symmetric topology. This is the main trap of the problem."", 'step_5': ""Determine Final Answer: The change factor for the effective noise power is the inverse of the change factor for $\\rho_{DL}$. Factor = $\\frac{P'_{noise, new}}{P_{noise, orig}} = \\frac{\\sigma^2 / \\rho'_{DL, new}}{\\sigma^2 / \\rho_{DL, orig}} = \\frac{\\rho_{DL, orig}}{\\rho'_{DL, new}} = 1$. The effective noise power remains unchanged.""}"	"{'concept_1': 'The space-time signal matrix transmitted by an RU for the ACK message is formed using maximal-ratio transmission (MRT) precoding, which uses the Hermitian transpose of the estimated channel vector: $\\Xi_{b}=\\sqrt{\\rho_{DL}}\\sum_{(u,n)\\in\\tilde{A};u\\in\\mathcal{S}_{b}}\\hat{h}_{u,n,b}^{H}\\hat{\\xi}_{u,n}$.', 'concept_2': 'The signal received by a user is a superposition of the desired signal, multiuser interference from ACK messages intended for other detected users (both genuinely active and false alarms), and additive white Gaussian noise.', 'concept_3': 'The downlink power normalization factor, $\\rho_{DL}$, is determined by balancing the total power expenditure between the uplink (UL) random access channel (RACH) slot and the downlink (DL) ACK slot, according to the formula: $\\rho_{DL}=\\frac{1}{L}\\frac{\\sum_{u=1}^{U}\\lambda_{u}\\alpha_{u}}{\\sum_{b=1}^{B}\\sum_{u\\in\\mathcal{S}_{b}}\\lambda_{u}\\alpha_{u}\\mathcal{Z}_{u,b}}$.', 'concept_4': 'The large-scale fading coefficients, $g_{u,b}$, which are inputs to the rate calculation, are determined by a distance-dependent pathloss function $PL = 1/(1+(d/d_0)^\\gamma)$ based on the network geometry.', 'concept_5': 'The per-user rate Cumulative Distribution Function (CDF) is a staircase function where each step corresponds to the achievable rate for a specific user location, and the height of the step is determined by the relative fraction of active users in that location.', 'theorem_1': ""The 'Use and Forget' (UatF) bound provides a tractable lower bound on the per-user ergodic achievable rate by treating all multiuser interference as worst-case uncorrelated additive noise. The resulting rate for a user in location $u$ is given by: $R_{u,n}^{UaF}=log(1+\\frac{|\\sum_{b\\in\\mathcal{C}_{u}}\\mathcal{M}_{u,b}|^{2}}{\\sigma^{2}/\\rho_{DL}+\\sum_{b\\in\\mathcal{C}_{u}}\\mathcal{V}_{u,b}+L\\sum_{u^{'}\\in[U]}\\sum_{b\\in\\mathcal{C}_{u^{'}}}\\lambda_{u^{'}}\\alpha_{u^{'}}g_{ub}\\mathcal{Z}_{u^{'},b}})$.""}"	1	理论工程学科-信息论和通信理论-信道编码理论	765846	3	4
recuVWbz5aBe8C	Given the translational model: {}^{W}!\ddot p_B = u + f_c + d， \dot d, \ddot d are bounded by \delta_1, \delta_2. Design the disturbance observer (DOB) and the control law: \hat d = \zeta , \qquad \dot \zeta = \Lambda r + s + P \mathrm{sgn}(s) . u = -\Gamma\dot e - \Xi s - \hat d . Gain matrices and internal signals: \Gamma=\mathrm{diag}(\gamma_i),\quad \Xi=\mathrm{diag}(\xi_i),\quad \Lambda=\mathrm{diag}(\lambda_i),\quad P=\mathrm{diag}(\rho_i) . Closed-loop equivalent relations: r = \tilde d := d - \hat d , \qquad \dot r = \dot d - \Lambda r - s - P \mathrm{sgn}(s) . Question: Under the above structure, for P=\mathrm{diag}(\rho_i), what componentwise lower bounds should be chosen to ensure asymptotic convergence of the tracking error?	{'step_1': 'Closed-loop error channel: r = \\tilde d , \\qquad \\dot r = \\dot d - \\Lambda r - s - P \\mathrm{sgn}(s) .', 'step_2': 'Construct the storage term using only the definitions: H(t) = \\tilde d^{\\top}(\\dot d - P \\mathrm{sgn}(s)) , \\quad K(t) = \\mu - \\int_{0}^{t} H(\\tau) d\\tau . By selecting appropriate parameters, K can be ensured to be positive.', 'step_3': 'Using the bounded-derivative assumption of the disturbance and basic inequalities, estimate \\int_0^t H . By choosing \\rho_i \\ge \\delta_{1,i} + \\xi_{\\min}^{-1}\\delta_{2,i} , it can be guaranteed that K(t) \\ge 0 holds for any t \\ge 0.', 'step_4': 'Take the Lyapunov function: V = \\tfrac12,\\tilde d^{\\top}\\tilde d + \\tfrac12 e^{\\top}e + \\tfrac12 s^{\\top}s + K . When K(t)\\ge 0 and the cross term is handled with x^{\\top}y \\le \\tfrac12(\\lVert x\\rVert^2+\\lVert y\\rVert^2), we obtain \\dot V \\le -\\beta \\lVert z \\rVert^2 ,\\ \\beta>0 . According to “if there exists a positive-definite V such that \\dot V \\le -\\alpha \\lVert z \\rVert^2 (\\alpha>0), then the origin is asymptotically stable,” asymptotic convergence of the error follows.'}	{'concept1': 'Definitions of the auxiliary quantities H,\\mu,K (definitions only; no conclusion stated): H(t) := \\tilde d^{\\top}(t) \\big(\\dot d(t) - P \\mathrm{sgn}(s(t))\\big) . \\mu := \\rho^{\\top}!\\lvert s(0)\\rvert - s^{\\top}(0) \\dot d(0) . K(t) := \\mu - \\int_{0}^{t} H(\\tau) d\\tau .', 'concept2': 'Lyapunov direct method (criterion statement): If there exists a positive-definite V such that \\dot V \\le -\\alpha \\lVert z \\rVert^2 (\\alpha>0), then the origin is asymptotically stable.'}	\rho_i \ge \delta_{1,i} + \xi_{\min}^{-1} \delta_{2,i} ,\qquad i=1,2,3 . \xi_{\min} := \min_i \xi_i .	理论工程学科-控制理论与控制工程-非线性控制	614192	2	2
recuVWh7Da4ygH	Let $G$ be the general symplectic group $Sp_{2g},g\in \mathbb{N}_+,$ over the algebraically closed field $k$ with characteristic $p>0$. $G$ has simple roots $\Delta=\{e_1-e_2,\cdots,e_{g-1}-e_g,2e_g\}$. Given subset $I=\Delta\backslash\{2e_g\},J=\Delta\backslash\{e_1-e_2\} $ of $\Delta$, there is an embedding $L_J/L_J\cap P_I\to G/P_I$, where $P_I$ and $L_J$ are standard parabolic and Levi subgroups, respectively, satisfying the Weyl group of $L_J$ (resp. Levi part of $P_I$) is generated by the simple reflections of simple roots in $J$ (resp. $I$). Let $A^\bullet(X)$ be the Chow ring of the smooth scheme $X$, with rational coefficients, and $\rho_{Hdg}\in Rep(P_I)$ be the irreducible $P_I$-subrepresentation of $(std^\vee)\mid_{P_I}$: $P_I\subset G\stackrel{std^\vee}{\longrightarrow} GL_{2g}$. This gives a cycle class $[L_J/L_J\cap P_I]\in A^\bullet(G/P_I)$. Consider $X=\mathcal{A}_g^{tor}$ being the smooth proper compactification of the moduli space $\mathcal{A}_g^{tor}$ of principal polarised abelian varieties of dimension $g$. For a $G$-torsor $\pi:Y\to X$ and smooth, $G$-equivariant $f:Y\to G/P_I$, the pullback of vector bundles yields functors $\mathcal{W}: Rep(P_I)\to Vect(X)$ and $\mathcal{V}: Rep(P_I)\to Vect(G/P_I)$, from the category of representations to the category of vector bundles, thanks to the equivalence $Vect_G(G/P_I)\cong Rep(P_I)$. We know that $[L_J/L_J\cap P_I]=ac_g(\mathcal{V}(\rho_{Hdg})$ for a constant $a$ depending on $g$ and $J$, where $\mathcal{V}(\rho_{Hdg})$ is the homogenous vector bundle corresponding to the $P_I$-representation $\rho_{Hdg}$ and $c_g$ denotes the $g$-th Chern class of vector bundle. Find all the possible $g$ such that $a > 0$. DO NOT use the online search. Note the maximum output length of your model and do not exceed the limit.	{'step_1': 'Consider $\\int_{G/P_I} c_1(L^\\vee)^d=(-1)^d\\int_{G/P_I}c_1(L)^d$, where $L=\\mathcal{V}(det(\\rho_{Hdg}))$ and $d=dim(\\mathcal{A}_g)$. By theorem_1 there is $\\int_{G/P_I} c_1(L^\\vee)^d=(-1)^d\\int_{G/P_I}c_1(L)^d=(-1)^d\\int_{G/P_I}c_1(\\mathcal{V}(det(\\rho_{Hdg})))^d=(-1)^dR\\int_{X}c_1(\\mathcal{W}(det(\\rho_{Hdg})))^d.$', 'step_2': 'By theorem_3, we have $\\int_{G/P_I}c_1(L^\\vee)^d>0$. ', 'step_3': 'By theorem_2, $\\mathcal{W}(\\rho_{Hdg})$ is the Hodge vector bundle on $X$. Therefore, we have that the Hodge line bundle $\\mathcal{W}(det(\\rho_{Hdg}))$ on $X$ is nef. Then $\\int_{X}c_1(\\mathcal{W}(det(\\rho_{Hdg})))^d>0$. ', 'step_4': 'So we have $(-1)^dR>0$. Applying theorem_1 again with $f=c_gc_1^{d-g}$ gives$$\\int_{G/P_I}[L_J/L_J\\cap P_I]c_1(L^\\vee)^{d-g}=(-1)^{d-g}a\\int_{G/P_I}c_g(\\Omega)c_1(\\Omega)^{d-g}=(-1)^{d-g}aR\\int_{X}\\frac{1}{N}[v_0]c_1(\\mathcal{W}(det(\\rho_{Hdg})))^{d-g},$$ the last line follows from theorem_4. Here we have $\\frac{1}{N}[v_0]c_1(\\mathcal{W}(det(\\rho_{Hdg})))^{d-g}>0$ and $\\int_{G/P_I}[L_J/L_J\\cap P_I]c_1(L^\\vee)^{d-g}>0$ (because $L$ is anti-ample on $G/P_I$). So $(-1)^{d-g}aR>0$.', 'step_5': 'Combining Step 3 and Step 4, we get $(-1)^ga>0$. So $a>0$ if and only if $g$ is even.'}	{'theorem_1': 'There is some $R\\neq 0$ such that for all $f\\in \\mathbb{Q}[c_1,\\cdots,c_d]$ homogeneous of degree $d$ in the graded ring with grading $deg(c_i)=i$, and all $\\rho\\in Rep(P_I)$ the representation of $P_I$, there is equality $$\\int_{G/P_I}f(c_1(\\mathcal{V}(\\rho)),\\cdots,c_d(\\mathcal{V}(\\rho)))=R\\int_{X}f(c_1(\\mathcal{W}(\\rho)),\\cdots,c_d(\\mathcal{W}(\\rho)))$$ in $\\mathbb{Q}$, where $\\int_{X}$ denotes the degree map on $X=\\mathcal{A}_g^{tor}$. ', 'theorem_2': '$\\Omega:=\\mathcal{W}(\\rho_{Hdg})$ is the Hodge vector bundle over $X=\\mathcal{A}_g^{tor}$. ', 'theorem_3': 'The line bundle $L=\\mathcal{V}(det(\\rho_{Hdg}))$ is anti-ample on $G/P_I$. ', 'theorem_4': 'The cycle class of the $p$-rank $0$ locus $V_0\\subset X=\\mathcal{A}_g^{tor}$ is $[V_0]=Nc_g(\\Omega)$ where $N$ is a constant depending on $g$ and $p$. '}	All the positive even integers.	理论基础学科-基础数学-几何	454368	2	3
recuW1mmHHjhIV	Is there a hidden monoclinic metallic phase in \( \ce{VO_2} \)? If so, what is its origin?	"{'step_1': ""According to concept _1, when the temperature is lower than \\( 340 \\ \\text{K} \\) \\( \\ce{VO_2} \\), there is a stable monoclinic M1 insulating phase. The core structural feature is Peierls V-V dimerization-V atoms deviate from the center of the oxygen octahedron to form a zigzag chain, resulting in short bonds (\\( d_{\\text{s}} \\), that is, V-V dimers) and long bonds (\\( d_{\\text{L}} \\)); combined with the concept _2, it can be seen that the insulation of the M1 phase is directly determined by the V-V dimer: the dimer confines the electrons inside, resulting in electron localization, and the electrons cannot be freely conducted, which eventually exhibits insulation behavior. This step clarifies that the causal core of 'monoclinic structure' and 'insulation' is V-V dimer, which provides a logical starting point for the analysis of 'monoclinic metal phase'."", 'step_2': ""The essence of the metal phase is the free conduction of electrons (electron delocalization). Based on the causal chain of 'V-V dimer → electron localization → insulation' in concept _2, reverse derivation shows that in order to achieve metallicity in monoclinic structure, it is necessary to break the localization of electrons by V-V dimer, that is, the dissociation of V-V dimer, so that the electrons can get rid of the limitation and achieve delocalization. At the same time, in order to maintain the 'monoclinic' structural properties (different from the rutile R metal phase), the dimer dissociation should be limited to the local region (avoiding the whole crystal system transformation). At this time, the whole crystal still maintains the monoclinic structure, and the local dissociation region shows metallicity due to electron delocalization, thus possessing the structural and electrical characteristics of the 'monoclinic metal phase'."", 'step_3': ""As an external energy input, light induction can trigger the dynamic change of local structure of \\( \\ce{VO_2} \\): When light acts on the M1 phase, if the excitation intensity is weak (not enough to dissociate all V-V dimers), according to the characteristic of 'V-V dimer is the core structural unit of M1 phase' in concept _1, the holes generated by light excitation will transfer to local V-V dimers (such as specific short bonds \\( d_{\\text{s}} \\)), amplifying the atomic driving force in this region and promoting the dissociation of local V-V dimers. In this process, the overall crystal still retains the monoclinic structure (the monoclinic phase framework defined by concept _1), while the local dissociation region exhibits metallicity due to electron delocalization (breaking the insulation root in concept _2), and finally forms a monoclinic metal phase. Therefore, it can be determined that there is a monoclinic metal phase in \\( \\ce{VO_2} \\) induced by light, and its origin is the dissociation of local V-V dimers.""}"	{'concept_1': 'When the temperature is lower than the phase transition temperature (about 340 K), \\( \\ce{VO_2} \\) is stable in the insulating phase with low symmetry (M1 phase). The core structural feature of this phase is the Peierls V-V dimerization-vanadium (V) atom deviates from the geometric center of the oxygen octahedron and tilts to form a zigzag chain. At the same time, two different V-V bonds appear: one is a short bond (called a V-V dimer, the bond length is denoted as \\( d_{s} \\)), and the other is a long bond (the bond length is denoted as \\( d_{L} \\)).', 'concept_2': 'The insulation of the M1 phase is directly derived from the presence of V-V dimers: the formation of V-V dimers leads to electron localization (electrons are confined within the dimer and cannot be freely conducted), which in turn causes \\( \\ce{VO_2} \\) to exhibit insulation behavior in the M1 phase.'}	Yes, there is a long-lived monoclinic metallic phase, which is not the traditional rutile phase. It originates from the dissociation of local V-V dimers, and the dissociation region eventually forms a monoclinic metal phase due to electron delocalization.	理论基础学科-理论物理-凝聚态物理	38018	2	1
recuVXTZbJujSE	Under non-equilibrium conditions, can the directionality of multi-cycle reaction networks be solely controlled by thermodynamic properties (such as the standard redox potential difference \(\Delta E^0\) or the ratio of equilibrium constants \(K_{ox}/K_{red}\)), rather than being purely kinetically controlled as previously thought for systems without mass transport?	"{'step_1': 'Clarify the core question: Determine whether the directionality of multi-cycle reaction networks under non-equilibrium conditions can be solely controlled by thermodynamic properties (e.g., \\(\\Delta E^0\\), \\(K_{ox}/K_{red}\\)) instead of pure kinetic control (the traditional view for systems without mass transport).', 'step_2': 'Identify key parameters related to directionality in multi-cycle reaction networks: The ratcheting constant \\(K_r\\) to quantify kinetic asymmetry (directionality), where \\(K_r\\) depends on \\(q\\) (ratio of forward to backward cycle frequencies), \\(\\Delta E\\) (electrochemical potential gradient, energy drive), and \\(\\Gamma\\) (ratio of slip to backward cycle frequencies).', 'step_3': 'Analyze the determinant of \\(q\\) (a key factor for \\(K_r\\)): Through deriving the sum of forward (\\(\\sum F_i\\)) and backward (\\(\\sum B_i\\)) cycle frequencies using graph theory’s rooted spanning tree method, it is found that \\(q = \\sum F_i / \\sum B_i\\), and \\(q\\) is controlled by the sign of \\(\\Delta E \\times \\Delta E^0\\). Specifically, \\(q > 1\\) if \\(\\Delta E \\times \\Delta E^0 > 0\\), and \\(q < 1\\) if \\(\\Delta E \\times \\Delta E^0 < 0\\). Notably, \\(\\Delta E^0\\) (standard redox potential difference, \\(\\Delta E^0 = E_B^0 - E_A^0\\)) is an **intrinsic thermodynamic property** of the system, inherently linked to the ratio of equilibrium constants \\(K_{ox}/K_{red}\\) via the microscopic reversibility constraint (\\(K_{ox} K_{red}^{-1} = e^{(F/RT)\\Delta E^0}\\)). In contrast, \\(\\Delta E\\) (electrochemical potential gradient, \\(\\Delta E = E_{II} - E_I\\)) is an **external energy drive** that provides non-equilibrium conditions but does not alter the intrinsic thermodynamic basis of \\(q\\). It only modulates the ""direction"" of \\(q\\)’s bias (e.g., flipping \\(\\Delta E\\)’s sign flips whether \\(q > 1\\) or \\(q < 1\\)) but does not change the fact that \\(q\\)’s physical meaning (forward/backward cycle preference) is ultimately governed by \\(\\Delta E^0\\).', 'step_4': 'Confirm the independence of directionality from \\(\\Delta E\\)’s sign: Substitute \\(q\\) into the \\(K_r\\) expression. Although \\(K_r\\) contains \\(\\Delta E\\), the sign of \\(\\Delta E\\) cancels out in the numerator and denominator. Only \\(\\Delta E^0\\) (a thermodynamic property, related to \\(K_{ox}/K_{red}\\) via \\(K_{ox}K_{red}^{-1} = e^{F\\Delta E^0/(RT)}\\)) determines the direction of \\(K_r\\). For example, if \\(K_{ox} > K_{red}\\) (\\(\\Delta E^0 > 0\\)), any \\(\\Delta E\\) (regardless of sign) favors cycle sequence \\(S\\); if \\(K_{ox} < K_{red}\\) (\\(\\Delta E^0 < 0\\)), it favors \\(S^{-1}\\).', 'step_5': 'Verify with numerical simulations: Simulations show that when \\(\\Delta E \\neq 0\\), the direction of reaction fluxes (reflecting directionality) remains unchanged as \\(\\Delta E\\)’s sign reverses, and only changes when \\(\\Delta E^0\\) (or \\(K_{ox}/K_{red}\\)) changes. This confirms that directionality is solely controlled by thermodynamic properties.'}"	{'concept_1': 'Kinetic asymmetry—A key parameter describing non-equilibrium systems, quantified by the ratcheting constant Kᵣ, which reports on the directionality of a reaction network at steady state.', 'concept_2': 'Autonomous energy ratchet mechanism—A mechanism where the directionality of a reaction network is dictated solely by the thermodynamic stability of intermediates.', 'concept_3': 'Standard redox potential difference (ΔE⁰)—Defined as ΔE⁰ = E_B⁰ - E_A⁰ (E_B⁰: Standard redox potential of species B (oxidized/reduced couple), E_A⁰: Standard redox potential of species A (oxidized/reduced couple)), an intrinsic thermodynamic property of the system linked to the ratio of equilibrium constants Kₒₓ/Kᵣₑᵈ.', 'concept_4': 'Equilibrium constant ratio (Kₒₓ/Kᵣₑᵈ)—Kₒₓ = k_BA,ₒₓ/k_AB,ₒₓ, Kᵣₑᵈ = k_BA,ᵣₑᵈ/k_AB,ᵣₑᵈ, a thermodynamic property linked to ΔE⁰ via microscopic reversibility. Kₒₓ: Corresponding to chemical reactions in the oxidized state (Aₒₓ and Bₒₓ interconversion); Calculation: Kₒₓ = k_BA,ₒₓ / k_AB,ₒₓ, where k_BA,ₒₓ is the rate constant for Bₒₓ converting to Aₒₓ, and k_AB,ₒₓ is the rate constant for Aₒₓ converting to Bₒₓ; Thermodynamic meaning: Reflects the equilibrium tendency of Aₒₓ-Bₒₓ interconversion, Kₒₓ>1 means Bₒₓ is more likely to convert to Aₒₓ at equilibrium. Kᵣₑᵈ : Corresponding to chemical reactions in the reduced state (Aᵣₑᵈ and Bᵣₑᵈ interconversion); Calculation: Kᵣₑᵈ = k_BA,ᵣₑᵈ / k_AB,ᵣₑᵈ, where k_BA,ᵣₑᵈ is the rate constant for Bᵣₑᵈ converting to Aᵣₑᵈ, and k_AB,ᵣₑᵈ is the rate constant for Aᵣₑᵈ converting to Bᵣₑᵈ; Thermodynamic meaning: Reflects the equilibrium tendency of Aᵣₑᵈ-Bᵣₑᵈ interconversion, Kᵣₑᵈ>1 means Bᵣₑᵈ is more likely to convert to Aᵣₑᵈ at equilibrium.', 'concept_5': 'Electrochemical potential gradient (ΔE)—Defined as ΔE = E_II - E_I (E_I: Potential of electrode I (close to compartment I), E_II: Potential of electrode II (close to compartment II)), an external energy drive that provides non-equilibrium conditions but does not determine the direction of reaction cycling.', 'theorem_1': 'Microscopic reversibility constraint—For the multi-cycle network, Kₒₓ/Kᵣₑᵈ = e^((F/RT)ΔE⁰), linking the ratio of equilibrium constants to the standard redox potential difference. F (Faraday constant) = 9.6485×10⁴ C·mol⁻¹, R (gas constant) = 8.314 J·K⁻¹·mol⁻¹, T (temperature) = 298 K (fixed in numerical simulations).', 'theorem_2': 'Ratcheting constant (Kᵣ) expression for multi-cycle networks—Kᵣ = (q + e^(-(F/RT)ΔE) + Γ)/(q e^(-(F/RT)ΔE) + 1 + Γ). q = Σj_Fᵢ / Σj_Bᵢ, where Σj_Fᵢ is the sum of frequencies of all forward cycles (Fᵢ, realizing S and transporting one electron from electrode I to II), Σj_Bᵢ is the sum of frequencies of all backward cycles (Bᵢ, realizing S⁻¹ and transporting one electron from electrode I to II); Control factor: q is controlled by ΔE×ΔE⁰, q>1 if ΔE×ΔE⁰>0, q<1 if ΔE×ΔE⁰<0; Physical meaning: Reflects the overall kinetic bias of the system towards forward or backward cycles, but it is determined by thermodynamic parameters (ΔE⁰). Γ = Σj_Rᵢ / Σj_Bᵢ, where Σj_Rᵢ is the sum of frequencies of slip cycles (Rᵢ, cycles that realize the target sequence S but do not transport net electrons between electrodes, no contribution to directionality) and Σj_Bᵢ is the sum of frequencies of backward cycles (Bᵢ, cycles that realize S⁻¹ and transport electrons). Γ is determined by the frequency of slip cycles relative to backward cycles, which depends on kinetic parameters such as diffusion rate (k_d) and chemical reaction rate (e.g., k_AB, k_BA), rather than thermodynamic properties like ΔE⁰. Γ has no contribution to directionality.'}	Yes. The directionality of multi-cycle reaction networks is uniquely determined by the thermodynamic property \(\Delta E^0\) (i.e., the ratio of \(K_{ox}\) to \(K_{red}\)) and is independent of the sign of the energy drive (\(\Delta E\)).	理论基础学科-理论化学-化学反应动力学理论	993652	5	12
recuWuq4lP4IMq	Consider a fully connected feedforward neural network $u_\theta \in NN_H(D)$ (where $H$ is the number of hidden layers, $D$ is the width of hidden layers, and $\tanh$ is the activation function), with the $L^2$ norm $\|\theta\|_2$ of its parameter $\theta$ being known. If the number of hidden layers $H=2$ and the differential order $K=1$ are taken, try to write the upper bound expression of the $C^1(\mathbb{R}^{d_1})$ norm (Hölder norm) of this neural network $u_\theta$ on $\mathbb{R}^{d_1}$, and express the result as a function of $\|\theta\|_2$.	{'step_1': 'According to **concept_1**, when the number of hidden layers \\(H=2\\), the structure of the fully connected feedforward neural network \\(u_\\theta\\) is \\(u_\\theta = \\mathcal{A}_3 \\circ (\\tanh \\circ \\mathcal{A}_2) \\circ (\\tanh \\circ \\mathcal{A}_1)\\), which expands to \\(u_\\theta = W_3 \\tanh(W_2 \\tanh(W_1 x + b_1) + b_2) + b_3\\). Here, the parameter \\(\\theta = (W_1, b_1, W_2, b_2, W_3, b_3)\\), with \\(W_1 \\in \\mathbb{R}^{d_1 \\times D}\\), \\(W_2 \\in \\mathbb{R}^{D \\times D}\\), \\(W_3 \\in \\mathbb{R}^{D \\times d_2}\\), \\(b_1 \\in \\mathbb{R}^{1 \\times D}\\), \\(b_2 \\in \\mathbb{R}^{1 \\times D}\\), and \\(b_3 \\in \\mathbb{R}^{1 \\times d_2}\\).', 'step_2': 'According to **concept_2**, when the differential order \\(K=1\\), the Hölder norm ( \\(C^1\\) norm) is \\(\\|u_\\theta\\|_{C^1} = \\max\\left\\{ \\|u_\\theta\\|_{\\infty,\\mathbb{R}^{d_1}}, \\max_{\\substack{\\alpha \\in \\mathbb{N}^{d_1} \\\\ |\\alpha|=1}} \\|\\partial^\\alpha u_\\theta\\|_{\\infty,\\mathbb{R}^{d_1}} \\right\\}\\). It is necessary to calculate the upper bounds of the infinity norm for the 0-th order partial derivative (the network \\(u_\\theta\\) itself) and the 1st order partial derivative respectively.', 'step_3': 'Calculate the infinity norm of the 0-th order partial derivative \\(\\|u_\\theta\\|_{\\infty}\\): Using the boundedness of \\(\\tanh(t)\\) ( \\(|\\tanh(t)| \\leq 1\\) ), it can be obtained that the absolute values of the components of \\(z_1 = \\tanh(W_1 x + b_1)\\) and \\(z_2 = \\tanh(W_2 z_1 + b_2)\\) are both ≤ 1; For \\(u_\\theta^m = \\sum_j W_3^{j,m} z_2^j + b_3^m\\), apply the triangle inequality and Cauchy-Schwarz inequality to get \\(|u_\\theta^m| \\leq \\sum_j |W_3^{j,m}| + |b_3^m| \\leq \\sqrt{D} \\|W_3\\|_2 + \\|b_3\\|_2\\); According to **concept_3**, \\(\\|W_3\\|_2 \\leq \\|\\theta\\|_2\\) and \\(\\|b_3\\|_2 \\leq \\|\\theta\\|_2\\), so \\(\\|u_\\theta\\|_{\\infty} \\leq (\\sqrt{D} + 1) \\|\\theta\\|_2\\).', 'step_4': 'Calculate the infinity norm of the 1st order partial derivative \\(\\max_{|\\alpha|=1} \\|\\partial^\\alpha u_\\theta\\|_{\\infty}\\): Differentiate using **concept_4** (the Faà di Bruno formula degenerates to the chain rule when \\(K=1\\)), so \\(\\partial^\\alpha u_\\theta = (\\partial \\mathcal{A}_1/\\partial x_\\alpha) \\cdot \\text{diag}(\\text{sech}^2(\\mathcal{A}_1)) \\cdot W_2 \\cdot \\text{diag}(\\text{sech}^2(\\mathcal{A}_2)) \\cdot W_3\\); Using \\(\\text{sech}^2(t) \\leq 1\\), the norm of the diagonal matrix is ≤ 1; Apply the norm properties and Cauchy-Schwarz inequality to the vector-matrix product to get \\(\\|\\partial^\\alpha u_\\theta\\|_{\\infty} \\leq \\sqrt{D} \\|W_1\\|_2 \\|W_2\\|_2 \\|W_3\\|_2\\); According to **concept_3**, \\(\\|W_1\\|_2, \\|W_2\\|_2, \\|W_3\\|_2 \\leq \\|\\theta\\|_2\\), so \\(\\|\\partial^\\alpha u_\\theta\\|_{\\infty} \\leq \\sqrt{D} \\|\\theta\\|_2^3\\).', 'step_5': 'Take the maximum of the upper bounds of the infinity norms of the 0-th order and 1st order partial derivatives. Since \\(\\sqrt{D} \\|\\theta\\|_2^3\\) dominates when \\(\\|\\theta\\|_2 \\geq 1\\), we have \\(\\|u_\\theta\\|_{C^1} \\leq \\sqrt{D} \\|\\theta\\|_2^3 + (\\sqrt{D} + 1) \\|\\theta\\|_2\\).'}	{'concept_1': 'Fully Connected Feedforward Neural Network ($NN_H(D)$)\n- Structural Definition: A fully connected network with $H$ hidden layers (each with width $D$) and $\\tanh$ as the activation function;\n- Parameter Composition: $\\theta = (W_1, b_1, W_2, b_2, \\dots, W_{H+1}, b_{H+1})$, where $W_k \\in \\mathbb{R}^{L_{k-1} \\times L_k}$ is the weight matrix, $b_k \\in \\mathbb{R}^{L_k}$ is the bias vector, and the dimensions satisfy $L_0 = d_1$ (input dimension), $L_1 = L_2 = \\dots = L_H = D$ (hidden layer width), and $L_{H+1} = d_2$ (output dimension);\n- Affine Function: $\\mathcal{A}_k(x) = W_k x + b_k$ ($k = 1, 2, \\dots, H+1$);\n- Output Expression:\n \\[\n u_\\theta = \\mathcal{A}_{H+1} \\circ (\\tanh \\circ \\mathcal{A}_H) \\circ (\\tanh \\circ \\mathcal{A}_{H-1}) \\circ \\cdots \\circ (\\tanh \\circ \\mathcal{A}_1)\n \\]', 'concept_2': 'Hölder Norm ($\\|\\cdot\\|_{C^K(\\Omega)}$)\n- Applicable To: $C^K(\\Omega, \\mathbb{R}^{d_2})$ (where $\\Omega \\subseteq \\mathbb{R}^{d_1}$ is the domain, and the function and its partial derivatives of all orders are continuous);\n- Definition of Multi-Index: For $\\alpha = (\\alpha_1, \\alpha_2, \\dots, \\alpha_{d_1}) \\in \\mathbb{N}^{d_1}$, $|\\alpha| = \\alpha_1 + \\alpha_2 + \\dots + \\alpha_{d_1}$ (order of partial derivative), and $\\partial^\\alpha u = (\\partial_1)^{\\alpha_1} (\\partial_2)^{\\alpha_2} \\cdots (\\partial_{d_1})^{\\alpha_{d_1}} u$ ($\\alpha$-th order partial derivative);\n- Infinity Norm (Component-Wise): $\\|\\partial^\\alpha u\\|_{\\infty,\\Omega} = \\sup_{x \\in \\Omega} \\|\\partial^\\alpha u(x)\\|_\\infty$ (where $\\|\\cdot\\|_\\infty$ is the vector infinity norm, i.e., the maximum absolute value of the components);\n- Hölder Norm Expression:\n \\[\n \\|u\\|_{C^K(\\Omega)} = \\max_{\\substack{\\alpha \\in \\mathbb{N}^{d_1} \\\\ |\\alpha| \\leq K}} \\|\\partial^\\alpha u\\|_{\\infty,\\Omega}\n \\]', 'concept_3': 'Parameter $L^2$ Norm ($\\|\\theta\\|_2$)\n- $L^2$ Norm of Weight Matrix: $\\|W_k\\|_2 = \\sqrt{\\sum_{i=1}^{L_k} \\sum_{j=1}^{L_{k-1}} W_{k,i,j}^2}$ (square root of the sum of squares of all elements in the matrix);\n- $L^2$ Norm of Bias Vector: $\\|b_k\\|_2 = \\sqrt{\\sum_{i=1}^{L_k} b_{k,i}^2}$ (square root of the sum of squares of all elements in the vector);\n- Parameter $L^2$ Norm Expression:\n \\[\n \\|\\theta\\|_2 = \\sqrt{\\sum_{k=1}^{H+1} \\left( \\|W_k\\|_2^2 + \\|b_k\\|_2^2 \\right)}\n \\]', 'concept_4': 'Faà di Bruno Formula (Higher-Order Differentiation of Composite Functions)\n- Univariate Form: For smooth functions $g: \\mathbb{R} \\to \\mathbb{R}$ and $f: \\mathbb{R} \\to \\mathbb{R}$, the $K$-th order derivative of the composite function $h = g \\circ f$ is:\n \\[\n h^{(K)}(x) = \\sum_{\\substack{m_1 + 2m_2 + \\dots + Km_K = K \\\\ m_1, m_2, \\dots, m_K \\in \\mathbb{N}}} \\frac{K!}{\\prod_{i=1}^K (m_i! \\cdot i!^{m_i})} g^{(m_1+m_2+\\dots+m_K)}(f(x)) \\cdot \\prod_{i=1}^K (f^{(i)}(x))^{m_i}\n \\]\n- Multivariate (Multi-Index) Form: For smooth functions $g: \\mathbb{R}^D \\to \\mathbb{R}^{d_2}$ and $f: \\mathbb{R}^{d_1} \\to \\mathbb{R}^D$, the $K$-th order partial derivative (with multi-index $\\alpha, |\\alpha|=K$) of the composite function $h = g \\circ f$ is:\n \\[\n \\partial^\\alpha h(x) = \\sum_{P \\in \\Pi(K)} g^{(|P|)}(f(x)) \\cdot \\prod_{S \\in P} \\partial^{\\alpha(S)} f(x)\n \\]\n where $\\Pi(K)$ is the set of all partitions of $\\{1, 2, \\dots, K\\}$, $|P|$ is the number of subsets in partition $P$, and $\\sum_{S \\in P} \\alpha(S) = \\alpha$.'}	The upper bound expression of the $C^1(\mathbb{R}^{d_1})$ norm of the neural network $u_\theta$ is: \[ \|u_\theta\|_{C^1(\mathbb{R}^{d_1})} \leq C_{1}(D)\|\theta\|_2 + C_{2}(D)\|\theta\|_2^3 \]	理论基础学科-基础数学-统计机器学习	684647	2	4
recuVUkYsUUqDF	As for the Pt-based catalysts, among the anatase and rutile TiO₂ as the support, which can provides the highest in CO oxidation and C3H8 complete combustion?	{'step_1': 'The Pt-TiO2(R) has more surface adsorption oxygen species, indicating its more oxygen vacancies. The CO oxidation follows the MvK mechanism on the oxygen-rich Pt-TiO2(R) but the L-H mechanism on the Pt-TiO2(A). Therefore, the oxygen-rich Pt-TiO2(R) shows the higher activity in CO oxidation.', 'step_2': 'The Pt-TiO2(A) has lower Pt0 content and the anatase TiO2 facilitates the formation of small metal particles, suggesting its higher oxidized Pt species. It also demonstrates the formation of electron-deficient Pt sites on the Pt-TiO2(A).', 'step_3': 'The C3H8 oxidation follows the L-H mechanism on the Pt-TiO2(A) and Pt-TiO2(R). The electron-deficient Pt surface in Pt-TiO2(A) demonstrated a lower activation energy for C–C bond cleavage compared to dehydrogenation, while Pt-TiO2(R) was more prone to dehydrogenation because of the high activation energy for C–C bond cleavage. Therefore, the Pt-TiO2(R) exhibits the higher activity for C3H8 oxidation.'}	{'concept_1': 'The rutile TiO2 favors the formation of large metal particles, while the anatase TiO2 facilitates the formation of small metal particles.', 'concept_2': 'The electron-deficient Pt sites are active to C-C bond cleavage'}	The Pt species supported on rutile TiO2 exhibits the higher activity in CO oxidation, while the Pt species supported on anatase TiO2 exhibits the higher activity in C3H8 complete combustion.	理论基础学科-理论化学-计算化学	75705	1	2
recuTUj0yC1a8w	Let G be a graph embedded on the Klein quartic surface. What is the mixing time of the Glauber dynamics for (Δ+Δ^0.7)-proper colorings on G? Express the mixing time with the big-O symbol of n and Δ. Summarize your reasoning.	{'step 1': 'By theorem 4, sampling edge colorings of G with Glauber dynamics is the same as sampling vertex colorings of the line graph L(G).', 'step 2': 'By theorem 1, Δ(L(G)) ≤ 2Δ - 2.', 'step 3': 'By theorem 2, Glauber dynamics on colorings is connected (and hence ergodic) only if q > Δ(L(G)) + 2 ≥ (2Δ - 2) + 2 = 2Δ.', 'step 4': 'With q = 1.5Δ < 2Δ, the chain is not connected and thus not ergodic. Then the spectral gap is γ = 0, so theorem 3 gives no finite bound, and the chain cannot converge to the uniform distribution over all proper edge colorings.', 'step 5': 'When the chain does not converge to the uniform distribution, no finite time guarantees a bounded distance to the uniform distribution, the mixing time is ∞.'}	{'concept_1': 'Glauber dynamics is a Markov chain that evolves by updating one state variable at a time, with transitions determined by the target distribution, widely used in sampling from complex probability distributions.', 'concept_2': 'Mixing time is the minimum number of steps required for a Markov chain to converge to its stationary distribution within a predefined total variation distance threshold.', 'concept_3': 'A line graph of a graph G is a graph where each vertex represents an edge of G, and two vertices are adjacent if their corresponding edges in G share a common endpoint.', 'theorem_1': 'For a line graph L(G), the degree of a vertex (corresponding to an edge in G with endpoints of degrees d1 and d2) is d1 + d2 - 2. So the maximum degree of the line graph can be as large as 2Δ - 2, where Δ is the maximum degree of G.', 'theorem_2': 'Glauber dynamics for graph colorings is connected (i.e., its state space forms a single communicating class) if and only if the number of colors q exceeds Δ + 2, where Δ is the maximum degree of the graph.', 'theorem_3': 'The mixing time of a Markov chain is bounded above by O(1/γ · log(1/π_min)), where γ is the spectral gap (1 minus the second largest eigenvalue of the transition matrix) and π_min is the minimum stationary probability.', 'theorem_4': 'There is a one-to-one correspondence between the edge colorings of G and the vertex colorings of L(G).'}	Infinity.	理论基础学科-理论计算机科学-算法理论	874197	2	4
recuUuntacUzEP	Do not browse the web. Output only the ordered pair ( QSMP communication, classical two-way lower bound ). Models. QSMP: Alice and Bob each send one quantum state to a referee; no interaction; no shared randomness or entanglement. Cost = total qubits. Classical public-coin two-way: interactive; cost = total bits. Problem ingredients. Folded RS code $C_n\subseteq \Sigma^n$ over $\Sigma=\mathbb F_q^m$; dual $C_n^\perp$. --- Instances: $t=\mathrm{poly}(n)$ maps $H^{(i)}:\Sigma^n\to\{0,1\}^n$ solved in parallel. Same hash across all copies: choose one $k$-wise independent $h$ and reuse it for all $t$ copies; require $H^{(i)}(x^{(i)})\oplus h(x^{(i)})=0^n,\quad x^{(i)}\in C_n$. Index gadget: each coordinate $j$ carries a label $g_j\in\{0,1\}^{B}$ with $B=2^{\,n^{\delta^*}},\qquad \delta^*\in(0,1]$. Do not assume which of $B$ or $\log_2|\Sigma|$ dominates; use the Concepts if needed. Family size: the protocol must succeed simultaneously for every $h\in\mathcal H$, where $\log_2|\mathcal H|=n^{0.34}$. Success criterion: worst-case error $\le 1/3$.	"{'step_1': 'Let B:=2\u2009nδ∗B:=2^{\\,n^{\\delta^*}} (with fixed constant δ∗∈(0,1]\\delta^*\\in(0,1]), ∣Σ∣=qm|\\Sigma|=q^m, and log\u20612∣Σ∣=mlog\u20612q\\log_2|\\Sigma|=m\\log_2 q. Define M:=max\u2061{B, log\u20612∣Σ∣}.M:=\\max\\{B,\\ \\log_2|\\Sigma|\\}. Since B=exp\u2061(Θ(nδ∗))B=\\exp(\\Theta(n^{\\delta^*})) while log\u20612∣Σ∣=poly(n)\\log_2|\\Sigma|=\\mathrm{poly}(n), for sufficiently large nn we have M=BM=B. Below we keep the symbol MM in the derivation and substitute M=BM=B when comparing magnitudes.', 'step_2': 'In QSMP each party sends one register, with cost Q=Nlog\u20612qQ=N\\log_2 q. Folded RS parameters give N,log\u2061q=poly(n)N,\\log q=\\mathrm{poly}(n), hence the total QSMP communication is poly(n)\\mathrm{poly}(n). The index gadget labels are not included in the quantum registers and therefore do not affect the QSMP cost.', 'step_3': 'GS list decoding returns all candidates within radius N−kNN-\\sqrt{kN} in polynomial time; under the biased-noise condition N−kN≥(p+ε)NN-\\sqrt{kN}\\ge(p+\x0barepsilon)N, the unique decoding success probability for the dual code is ≥1−2−Ω(N)\\ge 1-2^{-\\Omega(N)}. Treat both as “callable subroutines”; they do not change the communication accounting.', 'step_4': ""Any public-coin two-way protocol (total communication TT) can be normalized so that on each accepting rectangle the number of fixed coordinates satisfies sfix ≤ c\u2009TM,s_{\\mathrm{fix}}\\ \\le\\ \x0crac{c\\,T}{M}, and the t′:=n−sfixt':=n-s_{\\mathrm{fix}} unfixed coordinates each have min-entropy ≥αM\\ge \x07lpha M, where c,α>0c,\x07lpha>0 are absolute constants."", 'step_5': 'Folded RS list-recovery gives a parameter sLR≥1s_{\\mathrm{LR}}\\ge 1 such that L ≤ q\u2009sLR⇒log\u20612L ≤ sLRlog\u20612q = poly(n).L\\ \\le\\ q^{\\,s_{\\mathrm{LR}}}\\quad\\Rightarrow\\quad \\log_2 L\\ \\le\\ s_{\\mathrm{LR}}\\log_2 q\\ =\\ \\mathrm{poly}(n). Note: here sLRs_{\\mathrm{LR}} (a theorem parameter) is **not** identified with sfixs_{\\mathrm{fix}} (the number of fixed coordinates).', 'step_6': ""For the t′=n−sfixt'=n-s_{\\mathrm{fix}} unfixed coordinates, each has min-entropy ≥αM\\ge \x07lpha M. The probability of hitting any fixed dangerous pattern is ≤2−αMt′\\le 2^{-\x07lpha M t'}. Union-bounding over LL candidates, suc1(T) ≤ L⋅2−αMt′ ≤ 2log\u20612L − αM(n−sfix).\\mathrm{suc}_1(T)\\ \\le\\ L\\cdot 2^{-\x07lpha M t'} \\ \\le\\ 2^{\\log_2 L\\ -\\ \x07lpha M (n-s_{\\mathrm{fix}})}."", 'step_7': 'From Step 4, sfix≤cT/Ms_{\\mathrm{fix}}\\le cT/M. Thus, suc1(T) ≤ 2−αMn + αMsfix + log\u20612L ≤ 2−αMn + αcT + log\u20612L.\\mathrm{suc}_1(T)\\ \\le\\ 2^{-\x07lpha Mn\\ +\\ \x07lpha M s_{\\mathrm{fix}}\\ +\\ \\log_2 L} \\ \\le\\ 2^{-\x07lpha Mn\\ +\\ \x07lpha c T\\ +\\ \\log_2 L}. Using Step 5’s log\u20612L≤poly(n)\\log_2 L\\le \\mathrm{poly}(n), suc1(T) ≤ 2−αMn + αcT + poly(n).\\mathrm{suc}_1(T)\\ \\le\\ 2^{-\x07lpha Mn\\ +\\ \x07lpha c T\\ +\\ \\mathrm{poly}(n)}.', 'step_8': 'Let T⋆ := α4c\u2009nM.T_\\star\\ :=\\ \x0crac{\x07lpha}{4c}\\,nM. When T≤T⋆T\\le T_\\star, suc1(T) ≤ 2−αMn + αc(αnM/4c) + poly(n) = 2−34\u2009αMn + poly(n).\\mathrm{suc}_1(T)\\ \\le\\ 2^{-\x07lpha Mn\\ +\\ \x07lpha c (\x07lpha nM/4c)\\ +\\ \\mathrm{poly}(n)} \\ =\\ 2^{-\tfrac34\\,\x07lpha Mn\\ +\\ \\mathrm{poly}(n)}. Since Mn≥nB=n⋅2nδ∗Mn\\ge nB = n\\cdot 2^{n^{\\delta^*}} grows exponentially, the poly(n)\\mathrm{poly}(n) term is negligible, hence suc1(T) ≤ 2−Ω(Mn).\\mathrm{suc}_1(T)\\ \\le\\ 2^{-\\Omega(Mn)}.', 'step_9': 'The problem states log\u20612∣H∣=O(log\u2061n)\\log_2|\\mathcal H|=O(\\log n). For any fixed hh, if T≤T⋆T\\le T_\\star then suc1(h)(T)≤2−Ω(Mn)\\mathrm{suc}_1^{(h)}(T)\\le 2^{-\\Omega(Mn)}. Union-bounding over the whole family, Pr\u2061[∃\u2009h∈H success] ≤ ∣H∣⋅2−Ω(Mn) ≤ 2−Ω(Mn)+O(log\u2061n) = 2−Ω(Mn).\\Pr[\\exists\\,h\\in\\mathcal H\\ \text{success}]\\ \\le\\ |\\mathcal H|\\cdot 2^{-\\Omega(Mn)}\\ \\le\\ 2^{-\\Omega(Mn)+O(\\log n)}\\ =\\ 2^{-\\Omega(Mn)}. To reduce worst-case error to ≤1/3\\le 1/3, clearly T≤T⋆T\\le T_\\star is impossible. Therefore one must have T ≥ T⋆ = Ω(nM).T\\ \\ge\\ T_\\star\\ =\\ \\Omega(nM).', 'step_10': 'For sufficiently large nn, M=max\u2061{B,log\u20612∣Σ∣}=B=2\u2009nδ∗M=\\max\\{B,\\log_2|\\Sigma|\\}=B=2^{\\,n^{\\delta^*}}. From Step 9, T ≥ Ω\u2009\u2063(n⋅2\u2009nδ∗) ⇒ T∈Ω\u2009\u2063(2\u2009nδ∗).T\\ \\ge\\ \\Omega\\!\x08ig(n\\cdot 2^{\\,n^{\\delta^*}}\x08ig)\\ \\Rightarrow\\ T\\in \\Omega\\!\x08ig(2^{\\,n^{\\delta^*}}\x08ig). Combining with Step 2 (QSMP is poly(n)\\mathrm{poly}(n)), the final ordered pair is ( poly(n), Ω(2\u2009nδ∗) ),\x08igl(\\ \\mathrm{poly}(n),\\ \\Omega(2^{\\,n^{\\delta^*}})\\ \x08igr), and the stronger form (poly(n), Ω(n⋅2\u2009nδ∗))\x08igl(\\mathrm{poly}(n),\\ \\Omega(n\\cdot 2^{\\,n^{\\delta^*}})\x08igr) also holds.'}"	"{'concept_1': 'Each party can locally prepare a code-space phase state or a zero-set superposition over $\\Sigma^n\\simeq(\\mathbb F_q)^N$: $\\lvert \\psi_u\\rangle=\\frac{1}{\\sqrt{|C_n|}}\\sum_{c\\in C_n}\\omega^{\\langle c,u\\rangle}\\lvert c\\rangle,\\quad \\lvert \\phi_S\\rangle=\\frac{1}{\\sqrt{|S|}}\\sum_{e\\in S}\\lvert e\\rangle.$ Encoding a register over $(\\mathbb F_q)^N$ costs $Q=N\\log_2 q$ qubits per message; with folded-RS parameters $N,\\log q=\\mathrm{poly}(n)$, the total QSMP cost (Alice + Bob, one shot each) is $\\mathrm{poly}(n)$.', 'concept_2': 'There is a polynomial-time procedure $\\textsf{ListDecode}(z)$ that outputs $\\{x\\in \\mathrm{GRS}_{q,\\gamma,k,\\mathbf v}:\\ \\mathrm{hw}(x-z)\\le N-\\sqrt{kN}\\},$ with $N=q-1$ and $d=N-k+1$.', 'concept_3': 'For $C=\\mathrm{RS}^{(m)}_{\\mathbb F_q,\\gamma,k}\\subseteq\\Sigma^n$ (with $\\Sigma=\\mathbb F_q^m,\\ n=N/m$): if integers $r,s\\ge1$ satisfy $\\frac{\\zeta N}{m}\\ \\ge\\ \\Bigl(1+\\frac{s}{r}\\Bigr)(N\\ell k^{s})^{\\tfrac{1}{s+1}}\\frac{1}{m-s+1},\\quad (r+s)\\Bigl(\\frac{N\\ell}{k}\\Bigr)^{\\tfrac{1}{s+1}}<q,$ then $C$ is $(\\zeta,\\ell,L)$-list-recoverable with $L\\le q^{s}$. (Interface: given $(N,q,m,k,\\zeta,\\ell)$, choose $(r,s)$ to read off $L$.)', 'concept_4': 'Let $e\\in(\\mathbb F_q)^N$ have i.i.d. coordinates with $\\Pr[e_j=0]=1-p$, $\\Pr[e_j=a\\neq0]=\\tfrac{p}{q-1}$. For $z=x+e$ with unknown $x\\in C_n^\\perp$, define $\\textsf{Decode}(z)$: unfold, run $\\textsf{ListDecode}$, and return the unique $x$ if $\\mathrm{hw}(z-x)\\le(p+\\varepsilon)N$. Whenever $N-\\sqrt{kN}\\ \\ge\\ (p+\\varepsilon)N,$ we have $\\Pr[\\textsf{Decode}(z)=x]\\ge 1-2^{-\\Omega(N)}$.', 'concept_5': 'Let $B:=2^{\\,n^{\\delta^*}}$. Any public-coin two-way protocol with total communication $T$ can be normalized so that each accepting rectangle fixes at most $s \\le \\frac{cT}{\\max\\{B,\\ \\log_2|\\Sigma|\\}},$ and every unfixed coordinate has per-coordinate min-entropy at least $\\alpha\\cdot \\max\\{B,\\ \\log_2|\\Sigma|\\},$ for absolute constants $c,\\alpha>0$.', 'concept_6': ""Suppose on an accepting rectangle the dangerous-candidate set has size $\\le L$, and the remaining $t'=n-s$ unfixed coordinates each have min-entropy $\\ge \\alpha\\cdot \\max\\{B,\\ \\log_2|\\Sigma|\\}$. Then for any fixed dangerous pattern, $\\Pr[\\text{hit that pattern}] \\le 2^{-\\alpha\\cdot \\max\\{B,\\ \\log_2|\\Sigma|\\}\\cdot t'}.$ By a union bound over all dangerous candidates, $\\Pr[\\text{hit some dangerous pattern}] \\le L\\cdot 2^{-\\alpha\\cdot \\max\\{B,\\ \\log_2|\\Sigma|\\}\\cdot t'}.$ (Interface: compare $\\log_2 L$ against $\\alpha\\cdot \\max\\{B,\\ \\log_2|\\Sigma|\\}\\cdot t'$.)"", 'concept_7': 'Over $\\mathbb F_r$, the degree-$(k-1)$ polynomial family $h(i)=P(i)$ is $k$-wise independent, with seed length $\\Theta(k\\log r)$ and family size $|\\mathcal H|=r^{k}$. If the protocol must succeed simultaneously for every $h\\in\\mathcal H$, require $\\alpha\\cdot \\max\\{B,\\ \\log_2|\\Sigma|\\}\\cdot (n-s)\\ \\gg\\ \\log_2|\\mathcal H|$ so that the union bound across the family remains negligible.'}"	\[\bigl(\ \mathrm{poly}(n),\ \Omega(2^{\,n^{\delta^*}})\ \bigr)\]	理论基础学科-理论计算机科学-量子通信复杂性	958515	5	9
recuW7JfC4coBP	In relativistic heavy-ion collisions, assume that the system deviates only slightly from local thermal equilibrium. For particle species~$i$, the momentum-distribution function can be approximated by an equilibrium part plus a small correction: \[ f_i(p)=f_{\text{eq},i}(p)+\delta f_i(p). \] If one integrates the particle-current density over a three-dimensional hypersurface~$\Sigma$, and takes the degeneracy factor~$g_i$ into account, write down the differential momentum-space distribution of the particle number, i.e. give the expression for \( E \frac{\mathrm{d}N_i}{\mathrm{d}^3 \mathbf{p}} \) Answer in one sentence without using the internet.	{'Step_1': 'Goal: derive the differential momentum-space particle-number distribution $E\\,\\mathrm{d}N_i/\\mathrm{d}^3\\mathbf{p}$ for species $i$, including a small non-equilibrium correction, in one sentence.', 'Step_2': 'Particle current (Concept_1): identify the observable as the flux of particles through a space-like hypersurface $\\Sigma$; the four-momentum projection $p^\\mu\\mathrm{d}^3\\sigma_\\mu$ gives the momentum-space density.', 'Step_3': 'Phase-space density (Concept_2): split the distribution into equilibrium plus deviation, $f_i(p)=\\left(f_\\text{eq}+\\delta f\\right)_i$, with degeneracy $g_i$ and quantum volume $(2\\pi)^3$.', 'Step_4': 'Covariant Cooper-Frye (Concept_3): assemble the flux factor $\\mathrm{d}^3\\sigma_\\mu\\,p^\\mu$, the degeneracy, the quantum volume, and the combined-subscript distribution $\\left(f_\\text{eq}+\\delta f\\right)_i$.', 'Step_5': 'Final one-line expression (Concept_4): collect all prefactors and the surface integral to obtain $E\\,\\mathrm{d}N_i/\\mathrm{d}^3\\mathbf{p}=\\dfrac{g_i}{(2\\pi)^3}\\displaystyle\\int_\\Sigma \\mathrm{d}^3\\sigma_\\mu\\,p^\\mu\\left(f_\\text{eq}+\\delta f\\right)_i$.'}	{'Concept_1': 'Identify the target as the \\textbf{differential momentum-space distribution of the particle number} \\[ E\\,\\frac{\\mathrm{d}N_i}{\\mathrm{d}^{3}\\mathbf{p}} \\] for species $i$, including a small non-equilibrium correction.', 'Concept_2': 'Recognize that the \\textbf{particle current density} integrated over a space-like hypersurface $\\Sigma$ gives the total number; projecting with the four-momentum $p^{\\mu}$ yields the desired momentum-space density.', 'Concept_3': 'Insert the \\textbf{phase-space density} $f_{i}(p)=f_{\\text{eq}}(p)+\\delta f(p)$ and account for the \\textbf{degeneracy factor} $g_{i}$ and the \\textbf{quantum volume} $(2\\pi)^{3}$.', 'Concept_4': 'Combine these pieces into the \\textbf{covariant Cooper-Frye formula}: the flux factor is $\\mathrm{d}^{3}\\sigma_{\\mu}\\,p^{\\mu}$, and the distribution is weighted by the local phase-space density $\\left(f_{\\text{eq}}+\\delta f\\right)_{i}$. The phase-space distribution function must be written in the combined-subscript form \\(\\left(f_{\\text{eq}}+\\delta f\\right)_{i}\\), rather than the split form $f_{\\text{eq},i}+\\delta f_i$.', 'Concept_5': 'Write the \\textbf{one-line expression} by assembling the degeneracy, the quantum volume, the surface integral, and the non-equilibrium distribution,'}	E\,\frac{\mathrm{d}N_i}{\mathrm{d}^3\mathbf{p}} = \frac{g_i}{(2\pi)^3} \int_{\Sigma} \mathrm{d}^3\sigma_\mu\, p^\mu \left(f_{\text{eq}} + \delta f\right)_i	理论基础学科-理论物理-高能物理	277936	2	3
recuW3uuntD6lB	"In pseudo-detailed balanced reaction networks (the stoichiometric matrix for internal species and lumped reactions has no right-null eigenvector, i.e., no ${\phi_{\varepsilon}}$ satisfies $\sum_{\varepsilon>0} S_{x, \varepsilon} \phi_{\varepsilon}=0$. ""Equilibration of diffusion in pseudo-detailed balanced CRNs: At steady state, $\dot{\sum}_{diff}[c_{ss}] = 0$ implies $\nabla \mu_{i}(c_{ss}(r)) = 0$ for all species, leading to equilibrated diffusion."", is the diffusion processes always equilibrate within the structures, while dissipation occurs solely due to chemical reactions?"	"{'step_1': 'Clarify the definition of pseudo-detailed balanced reaction networks Pseudo-detailed balanced chemical reaction networks (CRNs) are defined by a key topological property of their stoichiometric structure: the stoichiometric submatrix for internal species and lumped reactions ($\\hat{\\mathbb{S}}_{X_I}$, with entries $S_{x,\\varepsilon}$ for internal species $x \\in X_I$ and lumped reactions $\\varepsilon$) **admits no right-null eigenvectors**. Mathematically, there exists no non-zero vector $\\{\\phi_\\varepsilon\\}$ such that $\\sum_{\\varepsilon>0} S_{x,\\varepsilon} \\phi_\\varepsilon = 0$ for all internal species $x \\in X_I$. This trait ensures no ""hidden"" reaction cycles that leave internal species abundances unchanged, distinguishing them from standard detailed balanced networks.', 'step_2': 'Derive lumped reaction net current with pseudo-local detailed balance First, assume reaction fluxes follow a general form, which relies on the key premise that **chemostatted species are ideal** (non-ideal interactions are negligible). This allows splitting the flux into two independent parts: - A term $\\omega_{\\text{in}}$ related to internal species (depends on internal species concentrations $c_x(r)$ and their stoichiometric coefficients $v_{x,\\rho}$ in reaction $\\rho$); - A term $\\omega_{\\text{ch}}$ related to chemostatted species (depends only on chemostatted species concentrations $c_y$ and their stoichiometric coefficients $v_{y,\\rho}$, independent of internal species due to the ""ideal"" premise). Next, decompose the reaction flux into symmetric ($s_\\rho = \\sqrt{\\omega_\\rho \\omega_{-\\rho}}$, invariant under forward/backward reaction reversal) and antisymmetric parts. Combine this decomposition with the local detailed balance condition, then rewrite the lumped reaction net current (sum of net currents of reactions in the same lumped reaction $\\varepsilon$). Finally, use the pseudo-detailed balance condition (the stoichiometric matrix $S_{x,\\varepsilon}$ for internal species and lumped reactions has no right-null eigenvectors) to introduce constant coefficients $\\Delta_x$, which satisfy $\\ln b_{\\varepsilon} = \\sum_{x} \\frac{\\Delta_{x} S_{x, \\varepsilon}}{RT}$ (where $b_\\varepsilon$ is a constant derived from the flux’s chemostatted species-related term $\\omega_{\\text{ch}}$). With $\\Delta_x$, construct lumped reaction fluxes $\\hat{\\omega}_{\\pm\\varepsilon}$ that meet the pseudo-local detailed balance condition.', 'step_3': 'Examine the time derivative of the kinetic potential $F_{pdb}[c]$ For pseudo-detailed balanced CRNs, the kinetic potential $F_{pdb}[c]$ is constructed by shifting the chemical potentials of internal species: $\\hat{\\mu}_x(c(r)) = \\mu_x(c(r)) + \\Delta_x$ (where $\\Delta_x$ are constants derived in step 2), such that $F_{pdb}[c] = F[c] + \\sum_x \\Delta_x \\int_V dr \\, c_x(r)$ (with $F[c]$ being the total Helmholtz free energy). Its time derivative is as follows: $$d_t F_{pdb}[c] = -T \\dot{\\Pi}_{rct}[c] - T \\dot{\\sum}_{diff}[c] \\leq 0$$ where: - $\\dot{\\Pi}_{rct}[c] \\geq 0$ (pseudo-reaction entropy production rate, linked to lumped reaction fluxes $\\hat{\\omega}_{\\pm\\varepsilon}$); - $\\dot{\\sum}_{diff}[c] \\geq 0$ (diffusion entropy production rate, quantifying dissipation from species diffusion). Since $F_{pdb}[c]$ is lower-bounded (as $F[c]$ is lower-bounded), the system relaxes to a steady state $c_{ss}(r)$ where $d_t F_{pdb}[c_{ss}] = 0$, forcing both $\\dot{\\Pi}_{rct}[c_{ss}] = 0$ and $\\dot{\\sum}_{diff}[c_{ss}] = 0$.', 'step_4': 'Interpret the physical meaning of the steady-state conditions $\\dot{\\sum}_{diff}[c_{ss}] = 0$ implies the diffusion entropy production rate vanishes, meaning diffusion processes equilibrate at steady state. Combined with the relation between pseudo-chemical potential $\\hat{\\mu}_x(c(r))$ (shifted from $\\mu_x(c(r))$) and chemical potential, this leads to $\\nabla \\mu_i(c_{ss}(r)) = 0$ for all species — confirming homogeneous chemical potentials and fully equilibrated diffusion (no net diffusion currents). Meanwhile, $\\dot{\\Pi}_{rct}[c_{ss}] = 0$ (vanishing pseudo-reaction entropy production rate) has a specific physical meaning: it corresponds to **lumped reaction net currents $\\hat{j}_\\varepsilon(c_{ss}) = 0$** (sum of net currents of individual reactions within each lumped reaction $\\varepsilon$ equals zero). However, this does not imply the thermodynamic reaction entropy production rate $\\dot{\\sum}_{rct}[c_{ss}] = 0$ — individual reactions within a lumped group may still have non-zero net currents ($j_\\rho(c_{ss}) \\neq 0$), but their contributions cancel out at the lumped level. Thus, chemical reactions remain out of equilibrium overall, and dissipation persists (reflected by $\\dot{\\sum}_{rct}[c_{ss}] > 0$ in general).', 'step_5': 'Synthesize the reasoning In pseudo-detailed balanced reaction networks, the steady state $c_{ss}(r)$ satisfies two critical conditions: 1. $\\dot{\\sum}_{diff}[c_{ss}] = 0$: Diffusion entropy production vanishes, meaning diffusion processes equilibrate (homogeneous chemical potentials, no net diffusion currents). 2. $\\dot{\\Pi}_{rct}[c_{ss}] = 0$ (lumped reaction net currents cancel) but $\\dot{\\sum}_{rct}[c_{ss}] > 0$: Chemical reactions remain out of equilibrium, with dissipation arising solely from irreversible reactions. Thus, diffusion processes always equilibrate in these networks, while dissipation occurs exclusively due to chemical reactions. The answer to the question is ""Yes"".'}"	"{'concept_1': ""Kinetic potential $F_{pdb}[c]$ - A Lyapunov function for pseudo-detailed balanced CRNs, obtained by shifting the standard chemical potentials ($\\mu_x^\\circ$) of internal species (embodied in the pseudo-chemical potential $\\hat{\\mu}_{x}(c(r))\\equiv \\mu _{x}(c(r))+\\Delta _{x}$, where $\\Delta_x$ is a constant) and driving the system to steady state through minimization. Its essence is a translation of the system's total Helmholtz free energy $F[c]$, with a typical expression $F_{pdb}[c] = F[c] - \\sum_x \\Delta_x \\int_V dr c_x(r)$."", 'concept_2': 'Diffusion entropy production rate $\\dot{\\sum}_{diff}[c]$ - A non-negative quantity quantifying diffusion dissipation, expressed as $T \\dot{\\sum}_{diff}[c] = \\int_{V} d r \\sum_{i,j} \\nabla \\mu_{i}(c(r)) \\cdot O_{i,j}(c(r)) \\nabla \\mu_{j}(c(r))$, vanishing only when diffusion equilibrates.', 'concept_3': 'Reaction entropy production rate $\\dot{\\sum}_{rct}[c]$：A non-negative quantity quantifying irreversible chemical reaction dissipation in non-ideal RD systems, tightly linked to the local detailed balance condition. Formula: $$T \\dot{\\sum}_{rct}[c] = \\int_{V} d r R T \\sum_{\\rho>0} j_{\\rho}(c(r)) \\ln \\frac{\\omega_{\\rho}(c(r))}{\\omega_{-\\rho}(c(r))} \\geq 0$$ Key definitions: - $j_{\\rho}(c(r)) = \\omega_{\\rho}(c(r)) - \\omega_{-\\rho}(c(r))$: Reaction net flux (forward minus backward flux of reaction $\\rho$). - $\\int_V dr$: Integrates dissipation across the system volume. Connection to local detailed balance: $$R T \\ln \\frac{\\omega_{\\rho}}{\\omega_{-\\rho}} = -\\left( \\sum_{x} \\mu_{x} S_{x, \\rho} + \\sum_{y} \\mu_{y} S_{y, \\rho} \\right)$$ This links flux ratios to chemical potential differences, ensuring $\\dot{\\sum}_{rct}[c] \\geq 0$. At equilibrium, $j_{\\rho}=0$ implies $\\dot{\\sum}_{rct}[c]=0$; in non-equilibrium steady states, $\\dot{\\sum}_{rct}[c]>0$ reflects reaction-driven free energy cost. It vanishes **only when all reactions equilibrate**.', 'theorem_1': 'Local detailed balance condition: $RT \\ln \\frac{\\omega_\\rho(c(r))}{\\omega_{-\\rho}(c(r))} = -\\left( \\sum_{x} \\mu_{x}(c(r)) S_{x, \\rho} + \\sum_{y} \\mu_{y}(c(r)) S_{y, \\rho} \\right)$, ensuring thermodynamic consistency of reaction fluxes. Here, $\\omega_\\rho(c(r))$ and $\\omega_{-\\rho}(c(r))$ are the forward and backward reaction fluxes of reaction $\\rho$, $S_{i,\\rho}$ is the stoichiometric coefficient of species $i$ in reaction $\\rho$, and $\\mu_i(c(r))$ is the chemical potential of species $i$.'}"	Yes. Check list: Check the steps on how to draw the conclusions carefully.	理论基础学科-理论化学-化学反应动力学理论	504411	3	8
recuW7RmCWzoYc	Consider the quantum channel $\mathcal{M}_d$ (platypus channel) and its complementary channel $\mathcal{M}_d^c$, as well as the erasure channel $\mathcal{E}_{1/2,d}$ with erasure probability $\frac{1}{2}$, where $d$ is the dimension parameter. It is known that the quantum capacity satisfies subadditivity: $Q^{(1)}(\mathcal{N}_1 \otimes \mathcal{N}_2) \geq Q^{(1)}(\mathcal{N}_1) + Q^{(1)}(\mathcal{N}_2)$. When the dimension $d \to \infty$, find the asymptotic dominant term of the single-letter quantum capacity lower bound for the following composite channel: $ \mathcal{M}_{d+1} \otimes \mathcal{M}_d^c \otimes \mathcal{E}_{1/2,d} $	{'step_1': 'Channel decomposition and recombination: The target channel $\\mathcal{M}_{d+1} \\otimes \\mathcal{M}_d^c \\otimes \\mathcal{E}_{1/2,d}$ can be restructured as: $(\\mathcal{M}_{d+1} \\otimes \\mathcal{E}_{1/2,d}) \\otimes \\mathcal{M}_d^c$. This recombination uses the tensor product associativity and does not change capacity properties.', 'step_2': 'Apply quantum capacity lower bounds: From theorem_3: $Q^{(1)}(\\mathcal{M}_{d+1} \\otimes \\mathcal{E}_{1/2,d}) \\geq \\frac{1}{2} + O(1/d)$ From theorem_2: $ Q^{(1)}(\\mathcal{M}_d^c) = \\log(d - 1) $', 'step_3': 'Combine lower bounds: Using the given conditions: $ Q^{(1)}((\\mathcal{M}_{d+1} \\otimes \\mathcal{E}_{1/2,d}) \\otimes \\mathcal{M}_d^c) \\geq \\underbrace{\\frac{1}{2} + O(1/d)}_{\\text{lower bound part}} + \\underbrace{\\log(d - 1)}_{\\text{exact value}} $', 'step_4': 'Asymptotic dominant term analysis: When $d \\to \\infty$: $ \\log(d - 1) \\sim \\log d $ (dominant divergent term, order $\\Theta(\\log d)$), $ \\frac{1}{2} $ is a constant term (order $O(1)$), $ O(1/d) \\to 0 $ (negligible). Thus the asymptotic dominant term is $\\log d$.', 'step_5': 'Tightness verification: theorem1 gives the platypus channel upper bound: $ Q(M_{d+1}) \\leq \\log\\left(1 + \\frac{1}{d}\\right) \\sim O(1/d) $ (since $\\log(1+x) \\sim x$ when $x \\to 0$). Erasure channel upper bound: $ Q(E_{1/2,d}) = 0 $ (when $p=1/2$, $1-2p=0$). Complementary channel upper bound: $ Q^{(1)}(M_d^c) = \\log(d-1) $. Overall loose upper bound estimate: $ Q^{(1)}(\\text{composite channel}) \\leq Q(M_{d+1}) + Q(M_d^c) + Q(E_{1/2,d}) \\sim O(1/d) + \\log d + 0 \\sim \\log d $ Conclusion: The lower bound $\\sim \\log d + O(1)$ and upper bound $\\sim \\log d$ match to the dominant term when $d \\to \\infty$, confirming $\\log d$ as the asymptotic dominant term.'}	{'theorem_1': '$ Q(\\mathcal{M}_d) \\leq \\log\\left(1 + \\frac{1}{d-1}\\right) $', 'theorem_2': '$ Q^{(1)}(\\mathcal{M}_d^c) = \\log(d - 1) $', 'theorem_3': '$ Q^{(1)}(\\mathcal{M}_{d+1} \\otimes \\mathcal{E}_{1/2,d}) \\geq \\frac{1}{2} + O(1/d) $'}	$\log d$	理论工程学科-信息论和通信理论-量子信息论	120387	1	2
recuW7C0zPpgoq	Let $G = D_{5} = \langle r, s | r^5 = s^2 = 1, rs = sr^{-1} \rangle$, $A = \{rs\}, B = \{s\}$ be two disjoint sets. Let $X$ be the left-right Cayley complex $Cay_2(A, G, B)$. Calculate $|X(2)|$, excluding the degenerate faces.	{'step_1': 'Fix $G=D_5$, $a=rs\\in A$, $b=s\\in B$. In an LRCC, 2-cells are squares $[a,g,b]=\\{g,ag,agb,gb\\}$ by concept_1.', 'step_2': 'A face is degenerate iff $ag=gb\\iff g^{-1}ag=b$; this is exactly the negation of TNC (concept_2). So we cannot apply theorem_1 directly.', 'step_3': 'Note elements of $D_5$ are either rotations $r^m$ or reflections $r^m s$ ($m=0,\\dots,4$).', 'step_4': 'Compute the $2$-cells of rotations: $g=r^m\\Rightarrow g^{-1}ag=r^{-m}(rs)r^{m}=r^{1-2m}s$ by concept_1', 'step_5': 'Set $r^{1-2m}s=s\\Rightarrow 1-2m\\equiv0\\pmod5\\Rightarrow m\\equiv3$; thus only when $g=r^3$, the $2$-cell is degenerate.', 'step_6': 'Compute the $2$-cells for reflections: $g=r^m s\\Rightarrow g^{-1}ag=(r^{m}s)^{-1}(rs)(r^{m}s)=r^{2m-1}s$.', 'step_7': 'Set $r^{2m-1}s=s\\Rightarrow 2m-1\\equiv0\\pmod5\\Rightarrow m\\equiv3$; thus only when $g=r^3 s$, the $2$-cell is degenerate.', 'step_8': 'Exactly two $g$ give degenerate faces; hence $10-2=8$ nondegenerate $g$.', 'step_9': 'Each nondegenerate square appears 4 times via $g,ag,gb,agb$; divide by 4 to get $|X(2)|=8/4=2$.'}	{'concept_1': 'A Left–Right Cayley Complex (LRCC) $X=\\mathrm{Cay}_2(A, G, B)$ is a $2$-dimensional complex with $X(0)=G$; edges split into $A$-left edges $\\{g,ag\\}$ and $B$-right edges $\\{g,gb\\}$; 2-cells are squares $[a,g,b]=\\{g,ag,agb,gb\\}$.', 'concept_2': 'The TNC (Total No-Conjugacy) condition of a LRCC $\\mathrm{Cay}_2(A, G, B)$ is: for all $g\\in G$, $a\\in A$, $b\\in B$: $g^{-1}ag\\neq b$. ', 'theorem_1': 'Under standard assumptions (symmetric $A,B$, $1\\notin A\\cup B$) and TNC, the clean counts of a LRCC are $|X(1)|=\\frac{|A|+|B|}{2}|G|$ and $|X(2)|=\\frac{|A||B|}{4}|G|$.'}	2	理论基础学科-理论计算机科学-编码理论	184783	3	1
recuW2nzt5LEEE	In a deterministic identification (DI) system for molecular communication, a spatial channel access mechanism is employed with $n$ orthogonal molecular types. Each channel is modeled as a Discrete-Time Poisson Channel (DTPC) with channel gain $\rho$ and external noise mean $\lambda$. The transmitter sends codeword $u_0$ (with molecular release rate $x_0$ for all dimensions) and codeword $u_1$ (with molecular release rate $x_1$ for all dimensions). The receiver observes the output vector $y$ and calculates the decision variable difference: $\Delta(y) = d(y, u_1) - d(y, u_0),$ where $d(y, u_j)$ is the decision variable for codeword $u_j$. Assume the codeword length $n$ is sufficiently large, the channels satisfy spatial independence, and the transmitted codeword is $u_0$. Find the asymptotic value of the variance of $\Delta(y)$ divided by the number of dimensions $n$.	{'step_1': 'Calculate conditional expectations\nAccording to theorem_1 (DTPC output distribution), given input release rates: For $u_0$: the conditional expectation for each dimension is $\\mu_{0,t} = \\rho x_0 + \\lambda$. For $u_1$: the conditional expectation for each dimension is $\\mu_{1,t} = \\rho x_1 + \\lambda$ (where $x_1$ is the release rate for $u_1$). Since all dimensions are identical, $\\mu_{0,t} = \\rho x_0 + \\lambda$ and $\\mu_{1,t} = \\rho x_1 + \\lambda$ hold for all $t$.', 'step_2': 'Decompose the decision variable\nAccording to concept_3 (decision variable decomposition): $d(y, u_j) = \\sum_{t=1}^n d_t(y_t, \\mu_{j,t}), \\quad d_t(y_t, \\mu_{j,t}) = (y_t - \\mu_{j,t})^2 - y_t$. Therefore: $\\Delta(y) = \\sum_{t=1}^n \\left[ d_t(y_t, \\mu_{1,t}) - d_t(y_t, \\mu_{0,t}) \\right] = \\sum_{t=1}^n \\Delta_t(y_t)$. Expanding $\\Delta_t(y_t)$: $\\Delta_t(y_t) = \\left[ (y_t - \\mu_{1,t})^2 - y_t \\right] - \\left[ (y_t - \\mu_{0,t})^2 - y_t \\right] = (\\mu_{0,t} - \\mu_{1,t})(2y_t - \\mu_{0,t} - \\mu_{1,t})$. After simplification: $\\Delta_t(y_t) = 2(\\mu_{0,t} - \\mu_{1,t})y_t + (\\mu_{1,t}^2 - \\mu_{0,t}^2)$', 'step_3': 'Calculate variance for a single dimension\nUnder the condition that $u_0$ is transmitted, according to theorem_1, the output $y_t \\sim \\text{Poisson}(\\mu_{0,t})$, with: $\\mathbb{E}[y_t] = \\mu_{0,t}$ and $\\text{Var}(y_t) = \\mu_{0,t}$. The variance of $\\Delta_t(y_t)$ is: $\\text{Var}(\\Delta_t(y_t)) = \\text{Var}\\left( 2(\\mu_{0,t} - \\mu_{1,t})y_t + (\\mu_{1,t}^2 - \\mu_{0,t}^2) \\right)$. The constant term $(\\mu_{1,t}^2 - \\mu_{0,t}^2)$ does not affect the variance, and since $\\mu_{0,t}$ and $\\mu_{1,t}$ are constants, we have: $\\text{Var}(\\Delta_t(y_t)) = \\left[ 2(\\mu_{0,t} - \\mu_{1,t}) \\right]^2 \\text{Var}(y_t)$. Substituting $\\mu_{0,t} - \\mu_{1,t} = \\rho(x_0 - x_1)$ and $\\text{Var}(y_t) = \\mu_{0,t} = \\rho x_0 + \\lambda$, we get: $\\text{Var}(\\Delta_t(y_t)) = 4\\rho^2 (x_0 - x_1)^2 (\\rho x_0 + \\lambda)$', 'step_4': 'Utilize channel independence\nAccording to theorem_2 (spatial channel independence), the outputs $y_t$ of each dimension are independent, so $\\Delta_t(y_t)$ are independent. The total variance is the sum of variances from each dimension: $\\text{Var}(\\Delta(y)) = \\sum_{t=1}^n \\text{Var}(\\Delta_t(y_t)) = \\sum_{t=1}^n 4\\rho^2 (x_0 - x_1)^2 (\\rho x_0 + \\lambda) = 4\\rho^2 (x_0 - x_1)^2 (\\rho x_0 + \\lambda) n$', 'step_5': 'Find the asymptotic variance\nDividing the variance by $n$: $\\frac{\\text{Var}(\\Delta(y))}{n} = 4\\rho^2 (x_0 - x_1)^2 (\\rho x_0 + \\lambda)$. As $n \\to \\infty$, this value converges to a constant, so the asymptotic variance per dimension is $4\\rho^2 (x_0 - x_1)^2 (\\rho x_0 + \\lambda)$.'}	{'concept/theorem_1': 'The symbol-wise conditional distribution of the output of a Discrete-Time Poisson Channel (DTPC) can be expressed as: $ W(y|x) = \\frac{e^{-(\\rho x + \\lambda)} (\\rho x + \\lambda)^y}{y!} $, where $\\rho$ is the channel gain and $\\lambda$ is the mean of the external noise.', 'concept/theorem_2': 'Inspired by the olfactory system, spatial channel usage employs multiple orthogonal molecular types to achieve independent channel access. Each molecular type corresponds to an independent DTPC channel usage, ensuring statistically independent outputs.', 'concept/theorem_3': 'The DI decoder decision variable is defined as $\\|\\mathbf{y} - \\mathbb{E}[Y \\mid \\mathbf{u}_j]\\|^2 - \\|\\mathbf{y}\\|_1$, where $\\mathbf{y}$ is the output vector and $\\mathbb{E}[Y \\mid \\mathbf{u}_j]$ is the conditional expectation vector given codeword $\\mathbf{u}_j$.'}	$4\rho^2 (x_0 - x_1)^2 (\rho x_0 + \lambda)$	理论工程学科-信息论和通信理论-香农信息论	123194	2	3
recuW4MRiG6MhN	In an absorption refrigeration–seawater desalination cycle supplied by low-grade heat at 55 °C and 85 °C, which configuration yields the highest COP—single-stage, two-stage mass-coupled, or two-stage heat-coupled (all else equal)? Please rank them from highest to lowest COP at each temperature.	{'step_1': 'Operating window (Concept 1). A single-effect LiBr–H₂O chiller is designed for 80–100 °C sources with typical COP ≈ 0.7–0.8 in that window. At 55 °C it is well below its viable range → severe COP degradation / near-failure. At 85 °C it lies inside the normal window → high COP expected.', 'step_2': 'What the combined cycle does (Concept 2). The combined AHP–SWD architecture breaks the evaporator–condenser loop to feed seawater to the evaporator and recover freshwater at the condenser via latent condensation. Inter-stage heat coupling reuses latent heat along the train and aligns temperatures, reducing exergy losses from large mismatches; mass coupling shares solution/streams but introduces mixing/transfer irreversibilities and weaker temperature matching.', 'step_3': 'Two-stage vs single-stage in theory (Concept 3). For two-stage absorption under low-grade heat, the ideal/theoretical COP is lower than single stage—provided both are within their viable temperature range.', 'step_4': 'Synthesis at 55 °C. Single-stage: far outside 80–100 °C → COP collapses. Two-stage: staging sustains operation when the single-stage struggles. Between the two staged options, heat coupling better recycles latent heat and temperature-matches stages (Concept 2), so it edges out mass coupling. ⇒ 55 °C ranking: Heat-coupled two-stage > Mass-coupled two-stage > Single-stage.', 'step_5': 'Synthesis at 85 °C. Single-stage: within 80–100 °C → regains its inherent COP advantage (Concepts 1 & 3). Two-stage: still theoretically below single-stage COP; among them, heat-coupled remains above mass-coupled by the same reasoning as in step 4. ⇒ 85 °C ranking: Single-stage > Heat-coupled two-stage > Mass-coupled two-stage.'}	{'concept_1': 'A single-effect LiBr–H₂O absorption chiller typically requires a heat-source temperature of 80–100 °C, with a typical COP of 0.7–0.8.', 'concept_2': 'The combined cycle interrupts the conventional evaporator–condenser loop: seawater is fed to the evaporator for vapor generation, and freshwater is recovered from the condenser via latent-heat condensation, thereby coupling cooling with desalination.', 'concept_3': 'For a two-stage absorption cycle under low-grade heat input, the ideal (theoretical) thermal COP is lower than single stage cycle.'}	At 55 °C:Heat-coupled double-stage > Mass-coupled double-stage > Single-stage. At 85 °C:Single-stage > Heat-coupled double-stage > Mass-coupled double-stage.	其他理论学科-理论热力学与统计物理-能量效率	92433	1	3
recuW7NlFg8X5F	In a solar-powered passive multi-stage osmosis (SPPO) desalination device, how do the four components—solar absorber, air gap, evaporator, condenser—rank by share of exergy destruction/loss (largest → smallest)?	{'step_1': 'Place the absorber first (Concept 1). Solar collector/absorber is known to account for the largest exergy destruction in solar-thermal desalination because of the big temperature/quality mismatch between solar input and the working stream. → Absorber = #1 (dominant).', 'step_2': 'Identify the next major sink within the module (Concept 1). Downstream irreversibilities concentrate in heat exchangers and the AGMD/air-gap module (thermal/concentration polarization, finite ΔT, diffusion resistance). Among the four items in this question, the air gap represents this module-level sink. → Air gap = #2.', 'step_3': 'Rank condenser vs evaporator (Concept 3). In HDH-type units, dehumidifier (condenser) typically shows higher exergy destruction than the humidifier (evaporator) because condensation with finite approach temperatures and cooling penalties generates more entropy than near-equilibrium vaporization on the hot side. → Condenser = #3; Evaporator = #4.', 'step_4': 'Synthesis. Combining 1–3 gives the ordered list: Absorber > Air gap > Condenser > Evaporator.'}	{'concept_1': 'In solar-thermal desalination, the solar collector/absorber accounts for the largest share of exergy destruction; downstream, significant irreversibilities arise in heat exchangers and in the air-gap/membrane distillation (AGMD) module.', 'concept_2': 'For humidification–dehumidification desalination, the dehumidifier (condenser) typically exhibits higher exergy destruction than the humidifier (evaporator), especially under solar heating or hybrid operation.'}	Solar absorber > Air gap > Condenser > Evaporator.	其他理论学科-理论热力学与统计物理-热力学第二定律	48115	1	2
recuVKwwnd9DVs	What is the circuit depth required for a random reversible circuit with a fixed, two-dimensional nearest-neighbor architecture to compute permutations that are approximately k-wise independent?	"{'step_1': 'First, clarify the foundational concepts to eliminate ambiguity and set the problem’s scope: \n- **Random Reversible Circuit**: A classical reversible circuit composed of randomly selected classical reversible gates (e.g., DES[2]-type gates or general 3-bit reversible gates). Reversible gates are bijective (no information loss), so the circuit maps input binary strings in \\{0,1\\}^n to output binary strings via a permutation of \\{0,1\\}^n. \n- **Fixed 2D Nearest-Neighbor Architecture**: A hardware constraint where bits (for classical reversible computation) are arranged in a 2D lattice (e.g., a \\sqrt{n} \\times \\sqrt{n} grid for n bits). Bits can only interact directly with their immediate neighbors (horizontally/vertically adjacent bits); long-range interactions require sequential ""routing"" via adjacent bits. \n- **Approximately k-wise Independent Permutations**: A permutation of the n-bit input space \\{0,1\\}^n where the marginal distribution of any k distinct input bits is ""close"" to uniform (formally, within a small error \\varepsilon of the uniform distribution over k-length binary strings). This relaxes the strictness of full k-wise independence for practical classical reversible circuit implementation. \n- **Circuit Depth**: The minimum number of sequential time steps required to execute the circuit, where each time step can run multiple non-interacting gates (gates acting on disjoint sets of bits).', 'step_2': 'Translate the original question into a precise theoretical goal: Given an n-qubit system arranged in a fixed 2D nearest-neighbor lattice, determine the minimum circuit depth D such that a random reversible circuit (built from valid nearest-neighbor gates) produces permutations that satisfy *approximate k-wise independence*.', 'step_3': 'The 2D lattice imposes a critical limit on qubit interactions: long-range correlations (between qubits not adjacent in the grid) require ""routing""—a sequence of SWAP or adjacent gates to transfer quantum information across the lattice. For an n-qubit 2D lattice, the most efficient arrangement is a \\sqrt{n} \\times \\sqrt{n} grid (minimizing the maximum distance between any two qubits). The maximum distance between two qubits in this grid is \\sim \\sqrt{n} (e.g., the diagonal of the grid), which initially suggests a routing depth of \\sim \\sqrt{n}. However, a rigorous analysis of the 2D nearest-neighbor architecture’s depth constraint requires a two-step derivation, as follows: 1. First, establish the spectral gap/mixing time upper bounds for 1D brickwork layouts. This analysis yields a depth bound for 1D circuits of approximately n \\cdot \\widetilde{O}(k^2) (or more generally, (nk + \\log(1/\\varepsilon)) \\cdot \\widetilde{O}(k)), where k characterizes interaction complexity and \\varepsilon denotes the error tolerance. 2. Next, extend the 1D result to 2D by alternately embedding 1D circuits along the rows and columns of the \\sqrt{n} \\times \\sqrt{n} grid. Combinatorial analysis of this embedding process introduces an additional \\widetilde{O}(k) factor. Combining these two steps, the architectural baseline depth—defined as the minimum depth required to enable long-range qubit interactions, independent of permutation complexity—for 2D nearest-neighbor lattices is established as \\sqrt{n} \\cdot \\widetilde{O}(k^3).', 'step_4': 'To achieve approximate k-wise independence, the circuit must break spurious correlations between subsets of up to k qubits. For random reversible circuits, the complexity of this task scales with k: For small k (e.g., k=2), simple gate sequences (e.g., random CNOTs) suffice to approximate 2-wise independence. For larger k, the circuit needs more layers of gates to ensure that all possible subsets of k qubits have marginally uniform distributions. Mathematical analysis (e.g., via Fourier transforms on the symmetric group or probabilistic method) shows that the number of gate layers required to suppress correlations in k-qubit subsets scales as k³ (the cubic scaling arises from accounting for all pairwise, triple-wise, and higher-order interactions within k-qubit subsets). The tilde in \\(\\tilde{O}(k^3)\\) denotes ""soft O notation,"" which ignores logarithmic factors (e.g., log k or log n) that are negligible for large n or k.', 'step_5': 'The total circuit depth has two multiplicative contributions: Architectural Routing Depth: As established in Step 3, the 2D nearest-neighbor lattice requires a depth of ~\\sqrt{n} to enable long-range interactions between any qubits. Without this, the circuit cannot access all k-qubit subsets (e.g., qubits on opposite corners of the grid) to enforce k-wise independence. Permutation Complexity Depth: As shown in Step 4, approximating k-wise independence requires \\(\\tilde{O}(k^3)\\) layers of random reversible gates. These layers must be applied after routing (or in parallel with optimized routing) to ensure correlations are suppressed across all k-qubit subsets. Since these two contributions are sequential (routing enables interaction, and complexity layers enforce independence), the total depth is their product: D = \\sqrt{n} \\cdot \\tilde{O}(k^3).', 'step_6': 'To confirm the conclusion, test edge cases and verify probabilistic correctness: Small k (e.g., k=1): For 1-wise independence (marginal uniformity of single qubits), \\(\\tilde{O}(k^3) = \\tilde{O}(1)\\), so D \\approx \\sqrt{n}. This aligns with intuition: routing across the 2D grid (depth ~\\sqrt{n}) is sufficient to randomize single-qubit positions. Large k (e.g., k~\\sqrt{n}): For k approaching \\sqrt{n}, \\(\\tilde{O}(k^3) = \\tilde{O}(n^{3/2})\\), so D \\approx \\sqrt{n} \\cdot n^{3/2} = n^2 —a scaling consistent with known results for high-complexity permutations in 2D architectures. Probabilistic Approximation: The ""random"" nature of the circuit ensures that, with high probability (e.g., 1 - 1/n), the permutation meets the approximate k-wise independence criterion. This avoids the need for deterministic (and deeper) circuits.', 'step_7': 'Combining all steps—core definitions, architectural constraints, permutation complexity, and validation—the minimum circuit depth required is formally stated as: The circuit depth required for a random reversible circuit with a fixed, two-dimensional nearest-neighbor architecture to compute permutations that are approximately k-wise independent is D = \\sqrt{n} \\cdot \\tilde{O}(k^3).'}"	{'concept_1': 'Approximate k-wise independence is a pseudorandomness property for a distribution on permutations. A permutation distribution is ϵ-approximate k-wise independent if, for any k distinct inputs, the distribution of the corresponding k outputs has a total variation distance of at most ϵ from a uniform distribution of k distinct outputs. This property implies statistical security against attackers who have access to k input-output pairs.', 'concept_2': 'A brickwork circuit is a type of circuit architecture where the bits (wires) are organized into a low-dimensional lattice. The circuit consists of layers of gates that act only on nearest-neighbor wires within this lattice structure. This design is particularly efficient for hardware implementation.', 'theorem_1': 'A random one-dimensional brickwork circuit of depth n \\cdot \\tilde{O}\\left(k^2\\right) computes a permutation that is 2^{-n k}-approximate k -wise independent. This result provides the performance guarantee for the building block of the more complex two-dimensional circuit.', 'theorem_2': 'An approximate k -wise independent permutation on \\{0,1\\}^n can be constructed by repeatedly alternating applications of approximate k -wise independent permutations on the rows and columns of a two-dimensional \\sqrt{n} \\times \\sqrt{n} lattice of bits. This reduction shows how to build a 2D circuit from 1D circuits, yielding a final depth of \\sqrt{n} \\cdot \\tilde{O}\\left(k^3\\right) for a 2^{-\\sqrt{n} k} approximate k -wise independent permutation.'}	D=\sqrt{n} \cdot \tilde{O}\left(k^3\right)	理论工程学科-信息论和通信理论-分布式通信	153924	1	3
recuVNjGYnig9d	In the Gaussian ``Beam-Pointing'' channel, given a power constraint $B$ and block length $Q$, what is the sensing capacity of the system under in-block memory and strict causal feedback conditions?	{'step_1': 'Under the per-slot power constraint, any admissible input strategy must satisfy\n\\[\n\\mathbb{E}\\big[b(X_{i,j})\\big]\\le B,\\quad \\forall\\ i,\\ \\forall j\\in[Q],\n\\]\nwhich induces the feasible set\n\\[\n\\mathcal{B}(B)\\;=\\;\\Big\\{\\,P_{X^Q\\|Z^Q}:\\ \\mathbb{E}\\big[b(X_{i,j})\\big]\\le B,\\ \\forall i,\\ \\forall j\\in[Q]\\,\\Big\\}.\n\\]', 'step_2': 'With in-block memory of length $Q$, the channel state remains fixed within each block and is independent across blocks. The strict causal encoding law factorizes as\n\\[\nP_{X^Q\\|Z^Q}(x^Q\\|z^Q)\\;=\\;\\prod_{t=1}^{Q} P_{X_t\\mid X^{t-1},Z^{t-1}}(x_t\\mid x^{t-1},z^{t-1}).\n\\]', 'step_3': 'By the chain rule tailored to the iBM setting and strict causality,\n\\[\nI(S;Z^Q\\mid X^Q)\\;=\\;\\sum_{t=1}^{Q} I\\!\\big(S;X_t,Z_t\\mid X^{t-1},Z^{t-1}\\big),\n\\]\nso that $Q$ is simultaneously the dynamic-programming horizon and the number of accumulated information terms.', 'step_4': 'Each slot produces a binary feedback\n\\[\nZ_t \\;=\\; \\mathbf{1}\\{\\|Y_t\\|^2>\\nu\\},\n\\]\nwhose law \\(P_{Z_t\\mid S,X_t}\\) is determined by the false-alarm and miss-detection probabilities\n\\[\nP_{\\mathrm{FA}}=1-F_{\\chi^2_{2q}}(\\nu),\\qquad\nP_{\\mathrm{MD}}=F_{\\chi^2_{2q}(\\sigma_m^2)}(\\nu),\n\\]\nthereby fixing the per-slot contribution \\(I\\!\\big(S;X_t,Z_t\\mid X^{t-1},Z^{t-1}\\big)\\) once a feasible \\(P_{X^Q\\|Z^Q}\\in \\mathcal{B}(B)\\) is chosen.', 'step_5': 'Maximizing the accumulated sensing information over the feasible causal strategies yields the sensing capacity\n\\[\n\\boxed{\\;\nC_{\\text{sensing}}\n\\;=\\;\n\\sup_{P_{X^Q\\|Z^Q}\\in \\mathcal{B}(B)}\n\\;\\sum_{t=1}^{Q}\nI\\!\\big(S;X_t,Z_t \\,\\big|\\, X^{t-1},Z^{t-1}\\big)\n\\;}\n\\]\nwith $B$ entering through the feasible set \\(\\mathcal{B}(B)\\) and $Q$ entering as the horizon and summation length.'}	{'concept_1': 'Under the per-slot power constraint, the feasible input strategy satisfies \\(\\mathbb{E}[\\,b(X_{i,j})\\,]\\le B\\) (for all blocks $i$ and slots $j\\in[Q]$), thereby defining \\(\\mathcal{B}(B)\\) as the optimization domain for subsequent mutual information maximization problems.', 'concept_2': 'The state remains fixed within each block of length $Q$ and is independent across blocks, the encoding satisfies \\(P_{X^Q\\|Z^Q}(x^Q\\|z^Q)=\\prod_{t=1}^{Q}P_{X_t\\mid X^{t-1},Z^{t-1}}(x_t\\mid x^{t-1},z^{t-1})\\)', 'concept_3': 'Each slot $t\\in[Q]$ generates a binary feedback through energy detection, \\(Z_t=\\mathbf{1}\\{\\|Y_t\\|^2>\\nu\\}\\). With statistics \\(P_{\\mathrm{FA}}=1-F_{\\chi^2_{2q}}(\\nu)\\) \nand \\(P_{\\mathrm{MD}}=F_{\\chi^2_{2q}(\\sigma_m^2)}(\\nu)\\), under the power constraint $B$, the per-slot power allocation and beam selection jointly determine \\(P_{Z_t\\mid S,X_t}\\), thereby influencing the $Q$-fold accumulation of \\(I(S;X_t,Z_t\\mid \\cdot)\\).'}	\[  C_{\text{sensing}}  = \sup_{P_{X^Q\|Z^Q}\in \mathcal{B}(B)} \;  \sum_{t=1}^{Q} I\!\left(S;X_t,Z_t \,\big|\, X^{t-1},Z^{t-1}\right),  \]	理论基础学科-理论计算机科学-信息论	176009	3	3
recuVOQLFy0vMR	Compared with the conventional design, does injecting heat into each stage’s condenser in a multistage solar distillation system improve the overall energy efficiency (with all other parameters held constant)?	{'step_1': 'Overall energy efficiency increases when a larger fraction of the absorbed solar energy is converted into latent heat of evaporation (i.e., when vapor generation increases while non-evaporative losses are unchanged).', 'step_2': 'In the conventional design, the condenser/air-gap region sits at a low dry-bulb temperature; psychrometrics dictates the air’s moisture capacity (humidity ratio) is low at such temperatures, which caps how much vapor the air can carry.', 'step_3': 'Heating the condenser raises the air-gap dry-bulb temperature, thereby increasing the air’s moisture capacity. This relaxes the psychrometric ceiling on moisture transport across the gap.', 'step_4': 'Vapor transport is driven by the partial vapor-pressure difference between the hot (evaporating) and cold (condensing) sides. Because saturation vapor pressure rises steeply with absolute temperature (Clausius–Clapeyron), shifting the stage to a higher absolute temperature level (even with similar or only modestly reduced ΔT) increases the vapor-pressure driving force, although under a fixed temperature range, reduced ΔT always means less driving force.', 'step_5': 'Steps 3–4 jointly increase the attainable vapor flux through the air gap (higher moisture capacity + stronger vapor-pressure driving force).', 'step_6': 'With higher vapor flux, a larger portion of the absorbed solar energy is routed into phase change rather than being limited by air-side carrying capacity, so the overall energy efficiency increases (under the stated “all else equal” conditions).', 'step_7': 'Conclusion: Yes—stage-wise condenser heating can improve overall energy efficiency via (1) psychrometric capacity gain and (2) increased vapor-pressure driving force at higher absolute temperatures.'}	{'concept_1': 'According to psychrometrics, a lower dry-bulb temperature results in a lower air moisture content (humidity ratio), because saturation vapor pressure decreases with temperature.', 'concept_2': 'In air-gap/membrane distillation and solar stills, vapor flux is driven by the partial vapor-pressure difference between the hot evaporating side and the cooler condensing side; since psat(T) rises steeply with temperature (Clausius–Clapeyron), higher absolute temperatures increase the driving force.'}	Yes — it improves the overall energy efficiency.	其他理论学科-理论热力学与统计物理-能量效率	62781	1	2
recuW1WeBZxRtj	How is the optimal channel input probability distribution $p_i$ determined in order to maximize the LM (Lower bound on the Mismatch capacity) rate under an average power constraint?	{'step_1': 'Define the Original LM Rate Problem (Max-Min Structure)', 'step_2': 'Apply Duality Theory to Transform the Inner Minimization (Theorem 1)', 'step_3': 'Reformulate the LM Rate as a Double Maximization Problem (Proposition 2)', 'step_4': 'Explicitly Define Constraints on $p_i$', 'step_5': 'Construct the Lagrangian for Constrained Optimization', 'step_6': 'Compute Partial Derivatives and Solve for $p_i$', 'step_7': 'Determine the Normalization Constant $Z$', 'step_8': 'Finalize the Optimal $p_i$'}	{'concept_1': 'LM Rate: The LM rate for a fixed input distribution $Q_X$ is defined as a constrained minimization problem over a set of auxiliary joint probability distributions $\\bar{P}_{X Y}$. The optimization objective, $C_{L M}$, is to then maximize this LM rate over all possible input distributions $Q_X$. This sets up a challenging max-min optimization problem.', 'theorem_1': 'Double Maximization Model for $C_{L M}$ : By substituting the dual form (Theorem 1) into the definition of $C_{L M}$, the problem is reformulated into a single, albeit nonconvex, maximization problem over both the input distribution $p$ and the dual variables. The objective function to be maximized is: $-\\sum_{i=1}^M p_i \\log p_i+\\sum_{i=1}^M p_i \\log T_i(\\phi, \\tilde{\\psi}, \\zeta)+1$.', 'concept_2': 'Lagrangian for Constrained Optimization: To solve the maximization model for the optimal $p_i$ while respecting the constraints (i.e., $\\sum p_i=1$ and the average power constraint $\\sum p_i\\left\\|x_i\\right\\|^2 \\leq \\Gamma$ ), the method of Lagrange multipliers is employed. By constructing the Lagrangian and taking its partial derivative with respect to $p_i$, the optimal solution for $p_i$ (the Answer) is derived.'}	p_ {i} = \frac { T_ {i} e^ {-\lambda \|x_ {i} \|^ {2} }}{ \sum _ {j=1}^M T_ {j} e^ {-\lambda \|x_ {j} \|^ {2}}}	理论工程学科-信息论和通信理论-分布式通信	111752	1	3
recuW8cPT868Ge	In the context of a population of agents playing the prisoner's dilemma game on complete graphs or regular networks, what is the optimally combined incentive protocol $u^*$ that minimizes the cumulative cost function $J_v$ (where $v \in \{I, S\}$ for complete and regular networks, respectively), as derived using the Hamilton-Jacobi-Bellman (HJB) equation? The cost of cooperation is denoted by $c > 0$.	"{'step_1': ""The process begins by defining the optimal control problem, which aims to find the incentive protocol $u^*$ that minimizes the cumulative cost $J_v$ (Theorem_1) subject to the system's evolutionary dynamics (Concept_1)."", 'step_2': 'The Hamiltonian function $H_v(x,u,t)$ is formulated by combining the instantaneous cost $G_v(x,u,t)$ and the system dynamics $F_v(x,u,t)$ weighted by the co-state, which is the partial derivative of the optimal cost function, $\\frac{\\partial J_v^*}{\\partial x}$ (Concept_2). This results in $H_v(x,u,t) = G_v(x,u,t) + \\frac{\\partial J_v^*}{\\partial x} F_v(x,u,t)$.', 'step_3': 'To find the candidate for the optimal control $u^*$, the necessary optimality condition is applied by taking the partial derivative of the Hamiltonian with respect to $u$ and setting it to zero (Theorem_2). Solving $\\frac{\\partial H_v}{\\partial u} = 0$ yields an expression for $u^*$ in terms of the state $x$ and the co-state $\\frac{\\partial J_v^*}{\\partial x}$.', 'step_4': 'The Hamilton-Jacobi-Bellman (HJB) equation, $-\\frac{\\partial J_v^*}{\\partial t} = H_v(x, u^*, t)$, is then employed (Theorem_4). This equation provides a dynamic relationship for the optimal cost function itself.', 'step_5': 'The free terminal time assumption is invoked (Theorem_3), which simplifies the HJB equation by setting the time derivative of the optimal cost function to zero: $\\frac{\\partial J_v^*}{\\partial t} = 0$. The HJB equation thus becomes a static equation: $0 = H_v(x, u^*, t)$.', 'step_6': 'The expression for $u^*$ (from Step 3) is substituted into this static HJB equation. The equation now contains terms involving $G_v(x,u^*,t)$ and $F_v(x,u^*,t)$.', 'step_7': 'After substitution, the equation is simplified. This leads to an algebraic equation for the co-state $\\frac{\\partial J_v^*}{\\partial x}$ that has a non-trivial solution.', 'step_8': 'This non-trivial solution for $\\frac{\\partial J_v^*}{\\partial x}$ is then substituted back into the expression for $u^*$ from Step 3. The terms involving the state $x$ and other parameters cancel out, yielding the final, constant optimal incentive protocol: $u^*=2c$.'}"	{'concept_1': 'The evolutionary dynamics of the fraction of cooperators, $x(t)$, are described by a replicator equation of the form $\\frac{dx(t)}{dt} = F_v(x,u,t) = A_v x(1-x)(u-c)$, where $A_v$ is a constant dependent on the network structure and $u$ is the per capita incentive.', 'theorem_1': 'The objective is to minimize a cumulative cost function, $J_v = \\int_{t_{0}}^{t_{f}}G_v(x,u,t)dt$, where the instantaneous cost $G_v(x,u,t)$ is a quadratic function of the control input $u$, specifically $G_v(x,u,t) = \\frac{1}{2}(B_v u [px + (1-p)(1-x)])^2$, with $B_v$ being a network-dependent constant and $p$ the rewarding preference.', 'concept_2': 'The Hamiltonian function, $H(x,u,t)$, for the optimal control problem is constructed by combining the instantaneous cost and the system dynamics: $H(x,u,t) = G(x,u,t) + \\frac{\\partial J^*}{\\partial x} F(x,u,t)$, where $J^*$ is the optimal cost function.', 'theorem_2': 'A necessary condition for the control $u$ to be optimal is that it must minimize the Hamiltonian. This is found by solving $\\frac{\\partial H}{\\partial u} = 0$ for $u$.', 'theorem_3': 'The free terminal time assumption for the optimal control problem implies that the optimal cost function $J^*$ is independent of time $t$, which simplifies the Hamilton-Jacobi-Bellman equation by allowing $\\frac{\\partial J^*}{\\partial t} = 0$.', 'theorem_4': 'The Hamilton-Jacobi-Bellman (HJB) equation provides a sufficient condition for optimality. It relates the optimal cost function to the Hamiltonian evaluated at the optimal control $u^*$: $-\\frac{\\partial J^*}{\\partial t} = H(x, u^*, t)$.'}	$u^* = 2c$	理论工程学科-控制理论与控制工程-最优控制	197628	2	5
recuVSXlVDlI2S	As for the Ru species supported on the cubic CeO2, rod-shaped CeO2, and octahedral CeO2, the Ru0 species content in cubic CeO2 is the highest. Which provides the highest activity for synthesizing methyl propionate (MP) from CO, C2H4, and CH3OH?	{'step_1': 'Based on the concept 1, the Ru species mainly consist of Ru0, Ru4+ and Ru6+ on the CeO2 support, thus the rod-shaped CeO2 and octahedral CeO2 with the lower Ru0 species content have more oxidize Ru species in comparison with the cubic CeO2. The reducible Ruδ+ and the Ru0 species are active in synthesizing methyl propionate (MP) from CO, C2H4, and CH3OH. Therefore, Ru species supported on the rod-shaped CeO2 and octahedral CeO2 with lower proportion of reducible Ruδ+ and the Ru0 species have lower activity in comparison with those supported on the cubic CeO2.', 'step_2': 'The CeO2 (111) is the most stable crystal face, implying its lowest reactivity. The high stability of CeO2 (111) is not favorable for the formation of oxygen vacancy and the Ru-O-Ce interaction. Thus, the Ru species supported on the octahedral CeO2 are the most inactive.', 'step_3': 'The Ru0 species content in cubic CeO2 is higher than that in rod-shaped CeO2, which provides the higher active sites on cubic CeO2. It also suggesting the stronger electron transfer between Ru0 species and CeO2 (100), which favors the formation of Ru-O-Ce active sites, thus facilitating the activation of CO and C2H4. Therefore, Ru species supported on the cubic CeO2 show the highest activity.'}	{'concept_1': 'The Ru species mainly consist of Ru0, Ru4+ and Ru6+ on the CeO2 support.', 'concept_2': 'The cubic CeO2, rod-shaped CeO2, and octahedral CeO2 expose the (100), (110) + (100), and (111) crystal face, respectively.'}	Ru species supported on the cubic CeO2 show the highest activity.	理论基础学科-理论化学-计算化学	48930	1	2
recuVUmrX9jwTC	Can the holonomy R-matrix for quantum \(\mathfrak{sl}_2\) at a root of unity be decomposed into a product of quantum holonomy dilogarithms?	"{'step_1': 'By using the action relations of the quantum Weyl algebra, derive the recurrence formula for the matrix coefficients of the holonomy R-matrix under the Fourier dual basis, laying a foundation for the subsequent explicit construction of the holonomy R-matrix.', 'step_2': 'Introduce the quantum holonomy dilogarithm, integrate the recurrence relations to obtain the explicit expression of the R-matrix, distinguish the different forms of positive/negative crossings, and complete the ""normalized"" definition of the R-matrix.', 'step_3': 'Prove that the holonomy R-matrix can be decomposed into a product of four quantum dilogarithm operators, where each factor corresponds to the ""flips of ideal tetrahedra"" around the crossing point, and establish a direct connection between the R-matrix and the geometric decomposition of knot complements (octahedral decomposition).'}"	"{'concept_1': 'Definition of the Holonomy R-Matrix: Let G be a complex semisimple factorizable group (where any element \\(g \\in G\\) can be uniquely decomposed as \\(g = g_+ g_-^{-1}\\) with \\(g_\\pm \\in G_\\pm\\); the Lie algebra \\(\\mathfrak{g} = \\text{Lie}(G)\\) can be decomposed as \\(\\mathfrak{g} = \\mathfrak{g}_+ - \\mathfrak{g}_-\\)), and let \\(u_\\varepsilon\\) be the specialization of the quantum group \\(U_h(gl_2)\\) at an odd root of unity \\(\\varepsilon\\) (a finite-dimensional Hopf algebra). The holonomy R-matrix is then defined as an algebraic operator \\(\\mathcal{R}\\) satisfying the following three conditions: 1: It acts as a color-transformation operator at the crossing points of G-colored diagrams, operating on the colors of edges in the tangle diagram (which correspond to holonomy group elements of flat connections). For a positive crossing, it maps the incoming edge colors \\((x_c, x_d) \\in G \\times G\\) to the outgoing edge colors \\((x_L(x_c, x_d), x_R(x_c, x_d))\\); for a negative crossing, it maps the incoming edge colors \\((x_a, x_b)\\) to the outgoing edge colors \\((x_L(x_a, x_b), x_R(x_a, x_b))\\). Here, \\(x_L(x, y) = x_- y x_-^{-1}\\) and \\(x_R(x, y) = (x_L(x, y))_+^{-1} x (x_L(x, y))_+\\), where \\(x_\\pm\\) and \\(y_\\pm\\) denote the factorization components of the group elements x and y, respectively. 2: It satisfies the set-theoretical Yang–Baxter Equation (YBE): for any \\(x, y, z \\in G\\),\\(\\mathcal{R}_{12} \\circ \\mathcal{R}_{13} \\circ \\mathcal{R}_{23} = \\mathcal{R}_{23} \\circ \\mathcal{R}_{13} \\circ \\mathcal{R}_{12}\\) where \\(\\mathcal{R}_{ij}\\) denotes the action of \\(\\mathcal{R}\\) on the i-th and j-th factors of \\(G \\times G \\times G\\). 3: It induces an isomorphism of tensor products of irreducible representations of \\(u_\\varepsilon\\): for an irreducible representation \\(A_x = u_\\varepsilon / I_x\\) of \\(u_\\varepsilon\\) (where \\(I_x\\) is a central character ideal corresponding to the color \\(x \\in G\\)), there exists an isomorphism\\(\\mathcal{R}(x, y): A_x \\otimes A_y \\to A_{x_R(x,y)} \\otimes A_{x_L(x,y)}\\) This isomorphism is induced by the outer automorphism of \\(u_\\varepsilon\\) given by \\(\\mathcal{R}(a \\otimes b) = R(a \\otimes b) R^{-1}\\), where R is the universal R-matrix of \\(U_h(gl_2)\\).', 'concept_2': ""Definition of Quantum Holonomy Dilogarithm: 1. Fundamental Parameters and Constraints 1.1 Root of Unity: Let \\(N \\geq 2\\), and let \\(\\theta = \\omega = \\exp(2\\pi i/N)\\) (an N-th primitive root of unity). 1.2 Operator Condition: Let \\(\\hat{A}\\) be a finite-dimensional operator satisfying \\(\\hat{A}^N = -1\\), with spectrum \\(\\{\\omega^{n-1/2} \\mid n=0,1,\\dots,N-1\\}\\). 1.3 Fermat Curve Constraint: Let complex parameters \\(a, b, c\\) satisfy the Fermat equation \\(a^N + b^N = c^N\\). The derived associated parameters are defined as \\(x^\\# = \\omega^{1/2}a^\\#/b^\\#\\) and \\(y^\\# = c^\\#/b^\\#\\). 2. Core Recurrence Function (Spectrum Generator) 2.1 Definition: The function \\(w(a,b,c|n)\\), which depends on \\((a,b,c)\\) and an integer n (modulo N), is defined as:\\(w(a,b,c|n) = \\begin{cases} 1 & (n=0) \\\\ \\prod_{j=1}^n \\frac{b}{c - a\\omega^j} & (n \\geq 1) \\end{cases}\\) 2.2 Key Property: Periodicity: \\(w(a,b,c|n+N) = w(a,b,c|n)\\). 3. Operator-Valued Quantum Holonomy Dilogarithm (Core Definition) Definition: Let \\(\\hat{A}\\) satisfy \\(\\hat{A}^N = -1\\). The quantum holonomy dilogarithm \\(\\Psi(\\hat{A})\\) is an operator that commutes with \\(\\hat{A}\\), and its spectrum satisfies:\\(\\Psi\\left(\\omega^{n-1/2}\\right) = \\Psi\\left(\\omega^{-1/2}\\right) \\cdot w(a,b,c|n)\\),where\\(\\Psi(\\omega^{-1/2})\\) is a non-zero complex constant, which can be determined via determinant normalization (\\(\\det\\Psi(\\hat{A}) = 1\\)):\\(\\Psi(\\omega^{-1/2})^N = \\frac{(b/c)^{N(N+1)/2}}{H(a/c)}\\) where \\(H(x) = (1-x)^N\\prod_{j=1}^{N-1}(1-x\\omega^j)^j\\). 4. Key Functional Relations 4.1 Local Transformation Equation: \\(\\Psi(\\omega^{-1}\\hat{A}) \\cdot \\Psi(\\hat{A})^{-1} = \\frac{c - \\omega^{1/2}a\\hat{A}}{b}\\) 4.2 Quantum Pentagon Identity: For operators \\(\\hat{U}, \\hat{V}, -\\hat{U}\\hat{V}\\) (all satisfying \\(\\hat{A}^N = -1\\)):\\(\\Psi(\\hat{V}) \\Psi'(\\hat{U}) = \\Psi''(\\hat{U}) \\Psi'''(-\\hat{U}\\hat{V}) \\Psi''''(\\hat{V})\\)(Here, \\(\\Psi^\\#\\) corresponds to different sets of \\((a^\\#,b^\\#,c^\\#)\\), and the parameters satisfy compatibility conditions such as \\(y y'' = y' y''''\\) and \\(x''' = x x'\\).)""}"	Yes (check list: 1. Correctly explain professional terms 2. Fermat Curve Compatibility 3. Pentagon–YBE Consistency 4. the holonomy R-matrix can be decomposed into a product of four quantum dilogarithm operators. two or three dilogarithms is definitely wrong.)	理论基础学科-基础数学-代数	48537	1	2
recuVkD24cMEQF	In the canonical setting, where two agents exchange one-dimensional numeric predictions p ∈ [0,1] about a label y ∈ [0,1], what is the maximum number of conversation rounds K required for them to reach ε-agreement (i.e., their predictions differ by at most ε) on a 1-δ fraction of interaction days, provided both agents satisfy the conversation-calibration condition and the total number of interaction days T is sufficiently large?	"{'step_1': 'The problem asks for the maximum number of conversation rounds, K, for two agents to reach an agreement within a specific environment, the **canonical setting** (Concept_1). The critical property of these agents is that their predictions are **conversation-calibrated** (Concept_3).', 'step_2': 'The analysis uses the **Squared Error (SQE)** as a potential function to track the progress of the conversation (Concept_4). The total SQE is non-negative and bounded, as all predictions and outcomes are in [0,1].', 'step_3': 'The core mechanism driving the agreement is detailed in **Theorem_1**: for two conversation-calibrated agents, if a round of conversation does not end in agreement, it is guaranteed to cause a quantifiable improvement (a decrease) in the cumulative SQE. This improvement is proportional to ε²δ.', 'step_4': 'This process cannot continue indefinitely. Since the total SQE is bounded, it can only be reduced a finite number of times. Each round that fails to achieve agreement ""spends"" a portion of this finite SQE budget.', 'step_5': 'Theorem_2 provides the condition under which this process leads to a clean bound. It states that for a sufficiently large number of interaction days T, the cumulative error from imperfect calibration, denoted β(T), becomes negligible. Specifically, it becomes smaller than half of the guaranteed SQE improvement per round (β(T) ≤ δε²/2).', 'step_6': 'Combining these facts, if the conversation continues for too many rounds, the cumulative reduction in SQE would exceed its total possible value, which is a contradiction. Therefore, the conversation must terminate by reaching ε-agreement (Concept_2). Under the condition from Theorem_2, this logic leads to the upper bound on the number of rounds: K ≤ 2/(δε²).'}"	"{'concept_1': 'The canonical setting for agreement protocols is one where the label space, message space, and prediction space are all the unit interval [0,1]. Agents communicate their precise point predictions to each other at each round.', 'concept_2': 'ε-agreement in the canonical setting is achieved when the absolute difference between the two most recent predictions from the model and the human is less than or equal to ε.', 'concept_3': ""Conversation-calibrated predictions require that for any round k, an agent's sequence of predictions is calibrated conditionally on the value of the prediction made by the other agent in the immediately preceding round, k-1."", 'concept_4': 'Squared Error (SQE) is used as a measure of prediction accuracy and serves as a potential function in the analysis; conversations that do not end in agreement must lead to a quantifiable reduction in the total squared error.', 'theorem_1': 'For two conversation-calibrated agents, each round of conversation that does not result in agreement leads to an improvement in the cumulative squared error of the predictions, with the improvement scaling with ε²δ.', 'theorem_2': 'Given a sufficiently large number of interaction days T, the cumulative calibration error term β(T) becomes negligible. Specifically, for the simplified bound to hold, we require β(T) ≤ δε²/2.'}"	K ≤ 2/(δε²)	理论基础学科-理论计算机科学-算法理论	606611	2	4
recuVuKSUJjDKl	Determine the time complexity for the proposed algorithm to provably learn a multi-head attention layer $F$ from random labeled examples drawn uniformly from $\{\pm1\}^{k\times d}$. The learning process must operate under the assumption that the ground truth attention and projection matrices $\{\Theta_{i},W_{i}\}_{i=1}^{m}$ satisfy a set of non-degeneracy conditions, and it should output estimates $\{\hat{W}_{i},\hat{\Theta}_{i}\}_{i=1}^{m}$ that result in a multi-head attention layer $\hat{F}$ achieving a mean squared error of at most $(kd)^{-\Omega(m)}$ **with a specified success probability of at least $1-\delta$**.	"{'step_1': 'The algorithm begins by computing a crude, computationally inexpensive estimate of the sum of projection matrices, $\\sum W_i$, using a simple input-output correlation (concept_1).', 'step_2': 'Using this initial estimate, the algorithm sculpts a convex body that serves as a rough approximation of the affine hull containing the true attention matrices, $\\Theta_1, ..., \\Theta_m$, by accumulating linear constraints from the data (concept_2).', 'step_3': ""To improve precision, the algorithm refines its estimate of $\\sum W_i$. It identifies specific 'large-margin' examples which provide highly accurate constraints, leading to a much better estimate than the initial one (concept_3)."", 'step_4': 'With the more accurate estimate of $\\sum W_i$, the sculpting process is repeated to generate a new, high-fidelity convex body, $K^*$, which is guaranteed to be very close to the true affine hull of the attention matrices (concept_4).', 'step_5': 'Here, the algorithm performs a crucial dimensionality reduction. Instead of searching the entire high-dimensional $d \\times d$ matrix space, it uses the accurate convex body $K^*$ to identify the $m$-dimensional linear span of the attention matrices. An epsilon-net of candidate matrices is then constructed *only within this low-dimensional subspace* (concept_5).', 'step_6': 'The final complexity is determined by the brute-force search over this reduced space. The number of candidate tuples from the epsilon-net is exponential in $m$, which, combined with the required precision, leads to the core complexity term $(kd)^{O(m^3)}$. Subsequently, to meet the requirement of the Instruction for a success probability of at least $1-\\delta$, the best model from this search is selected using a validation set. Applying generalization bounds to this selection process introduces an additional factor of $log(1/\\delta)$ to the runtime (concept_6).', 'step_7': 'By combining the cost of the dominant brute-force search with the cost associated with the probabilistic guarantee, the total time complexity of the algorithm is established as $(kd)^{O(m^{3})}\\cdot log(1/\\delta)$.'}"	{'concept_1': 'A crude, noisy estimate for the sum of projection matrices, $\\sum_{i} W_{i}$, can be obtained by calculating the empirical correlation between the input sequence $X$ and the output label $Y$ via the approximation $\\mathbb{E}[\\frac{1}{k}X^{\\top}JY] \\approx \\sum_{i}W_{i}$, where $J$ is the all-ones matrix.', 'concept_2': 'A convex body that approximates the affine hull of the attention matrices can be constructed by using linear regression to identify input examples $X$ that induce similar attention patterns across all heads and leveraging these examples to form affine linear constraints that are simultaneously satisfied by all attention matrices $\\Theta_{1},...,\\Theta_{m}$.', 'concept_3': 'The estimate of the projection matrix sum can be significantly refined by using the minimum-norm point within the sculpted convex body as a proxy for the average attention matrix. This allows for the identification of examples that induce large-margin (approximately 1-sparse) attention patterns across all heads, which in turn provides highly accurate linear constraints on the sum of projection matrices.', 'concept_4': 'The convex body sculpting process can be re-run using the refined estimate of the projection matrix sum to produce a new, much more accurate convex body $K^{*}$ that is significantly closer to the true affine hull of the attention matrices.', 'concept_5': 'An epsilon-net containing candidate attention matrices is constructed not over the full $d \\times d$ space, but specifically **within the estimated $m$-dimensional linear span** of the true attention matrices $\\{\\Theta_i\\}_{i=1}^m$. This crucial dimensionality reduction step reduces the search space for the subsequent brute-force step from being exponential in $d^2$ to being exponential only in $m$.', 'concept_6': 'A brute-force search is performed over all $m$-tuples of candidate matrices from the low-dimensional epsilon-net, with linear regression solved for each. The final selection of the best model is confirmed on a held-out validation set, and **standard generalization bounds are used to certify that this selection is correct with probability at least $1-\\delta$**. This validation step introduces a logarithmic dependency on $1/\\delta$ to the overall runtime.'}	$(kd)^{O(m^{3})}\cdot log(1/\delta)$	理论基础学科-理论计算机科学-机器学习	1341292	2	6
recuVDMq4U3785	For a k-deletion channel where $k$ symbols are deleted, the input entropy is a function of the output sequence. In the specific instance of a 1-deletion channel (k=1), where an input of length $n$ over alphabet $\Sigma_q$ yields an output of length $n-1$, find the minimum possible input entropy under uniform transmission.	{'step_1': 'The primary objective is to find the sequence $y$ that minimizes the input entropy for the 1-deletion channel, $H_{1-Del}^{ln}(y)$. We start with the general formula for this entropy, provided in Theorem 1, which expresses the entropy in terms of the run lengths $(r_1, r_2, ..., r_R)$ of the output sequence $y$: $H_{1-Del}^{ln}(y)=log(nq)-\\frac{1}{nq}\\sum_{i=1}^{\\rho(y)}\\mathcal{F}(r_i+1)$, where $\\mathcal{F}(a) = a \\log a$ (Concept_2).', 'step_2': 'According to Theorem 2, minimizing the entropy $H_{1-Del}^{ln}(y)$ is equivalent to maximizing the summation term $\\sum_{i=1}^{\\rho(y)}\\mathcal{F}(r_i+1)$.', 'step_3': 'The problem is now transformed into finding the run-length distribution that maximizes this summation. We first consider sequences with a fixed number of runs, $R$. According to Theorem 3, with $a=1$, the sum $\\sum_{i=1}^{R}\\mathcal{F}(r_i+1)$ is maximized when the sequence is skewed (Concept_1).', 'step_4': 'For a skewed sequence with $R$ runs and total length $m=n-1$, there are $R-1$ runs of length 1 and one run of length $m-(R-1) = n-R$. Substituting these run lengths into the entropy formula from Theorem 1 yields the minimum entropy for a fixed $R$, as stated in Theorem 4: $min_{y\\in\\Sigma_{q,R}^{m}}H_{1-Del}^{ln}(y)=log(nq)-\\frac{2(R-1)}{nq}-\\frac{(m-R+2)log(m-R+2)}{nq}$.', 'step_5': 'To find the overall minimum entropy, we need to find the value of $R$ (from $1$ to $m$) that minimizes the expression derived in the previous step. It is shown in the thesis that this expression is a decreasing function of $R$. Therefore, the minimum value is achieved at the smallest possible value for $R$, which is $R=1$.', 'step_6': 'Setting $R=1$ and $m=n-1$ in the formula from Theorem 4 gives the final answer. For $R=1$, the sequence has a single run of length $m=n-1$. The entropy is: $H_{1-Del}^{ln}(y) = log(nq) - \\frac{2(1-1)}{nq} - \\frac{((n-1)-1+2)log((n-1)-1+2)}{nq}$ $ = log(nq) - 0 - \\frac{(n)log(n)}{nq}$ $ = log(nq) - \\frac{log(n)}{q}$. This is the minimum possible input entropy for the 1-deletion channel.'}	{'concept_1': 'The function $\\mathcal{F}(a)$ is defined as $\\mathcal{F}(a) \\triangleq a \\log(a)$.', 'concept_2': 'For the 1-deletion channel, the set of length-$n$ supersequences of a length-$(n-1)$ sequence $y$ is dominated by structures formed by prolonging one of the existing runs of $y$. A run of length $r_i$ in $y$ corresponds to a supersequence with embedding number $r_i+1$.', 'concept_3': 'The minimum of a function over a set is found by first finding the minimum within subsets partitioned by a parameter $R$, and then finding the minimum over all possible values of $R$.', 'theorem_1': 'For a k-deletion channel, the input entropy for an output $y$ is $H_{k-Del}^{ln}(y)=log\\left(\\binom{n}{k}q^k\\right)-\\frac{1}{\\binom{n}{k}q^k} \\sum_{x \\in I_k(y)} \\mathcal{F}(\\omega_y(x))$, where $I_k(y)$ is the set of k-supersequences and $\\omega_y(x)$ is the embedding number.', 'theorem_2': 'Minimizing $H_{k-Del}^{ln}(y)$ is equivalent to maximizing the term $\\sum_{x \\in I_k(y)} \\mathcal{F}(\\omega_y(x))$.', 'theorem_3': 'For the 1-deletion channel, the summation term from theorem_2 is maximized by sequences with a skewed run-length profile.'}	log(nq)-\frac{log(n)}{q}	理论工程学科-信息论和通信理论-信道编码理论	427272	2	6
recuVHQ6KpGsNT	For the error-correcting code framework proposed for composite DNA-based storage, what is the redundancy in bits for the specific `(l=1, t=1)` improved Hamming code that operates on remainder classes? This calculation assumes a sufficiently large probability resolution `k` relative to the error magnitude `l`.	{'step_1': 'The instruction asks for the redundancy in bits for a specific coding scheme: the improved Hamming code for single, limited-magnitude errors (`l=1, t=1`) applied to remainder classes.', 'step_2': 'First, we establish the parameters of the finite field. According to the remainder class concept (concept_1), for an error magnitude of `l=1`, the probability vectors are mapped to `(2*1+1)³ = 27` classes. Therefore, the code operates over the finite field `GF(27)`, so `q=27`.', 'step_3': 'Next, we determine the structure of this specific improved Hamming code (concept_2, theorem_2). The goal is to correct a single error from a limited set of possible remainder error patterns.', 'step_4': 'We apply the optimal length formula (theorem_1) for this code. For `q=27`, the theorem states that `I_max = 2`. This means the length `n` of the improved Hamming code is related to its number of redundant symbols `r` by the formula `n = 2 * (27^r - 1) / (27 - 1)`.', 'step_5': 'We need to express the redundancy in terms of the code length `n`. Rearranging the formula from the previous step gives `(q-1)/I_{max} * n + 1 = q^r`. In our case, this is `(26/2) * n + 1 = 13n + 1 = 27^r`.', 'step_6': 'The number of redundant symbols is `r`, which can be expressed as `r = log_{27}(13n + 1)`.', 'step_7': 'Finally, we convert the number of redundant symbols into redundancy in bits. Since each symbol is an element of `GF(27)`, the total redundancy in bits is `r * log₂(27)`.', 'step_8': 'Substituting the expression for `r`: Redundancy = `log_{27}(13n + 1) * log₂(27)`. Using the change of base formula for logarithms (`log_b(x) * log_a(b) = log_a(x)`), this simplifies to `log₂(13n + 1)`. This calculation aligns with the approximation for large `k/l` (theorem_3), which allows us to ignore the smaller information content in the quotient vectors of the parity symbols.'}	"{'concept_1': 'Remainder Classes: Probability vectors are classified based on their remainder vector, which is obtained by dividing each of its four component values by `2l+1`. For `l=1`, this results in `(2*1+1)³ = 27` possible classes, which can be mapped to elements of a finite field `GF(27)`.', 'concept_2': 'Improved Hamming Code: A type of single-error-correcting code designed for scenarios where only a specific subset of error values is possible. It modifies the standard Hamming code construction to achieve a greater codeword length for the same number of parity check symbols by leveraging the limited nature of the errors.', 'theorem_1': 'Optimal Length of the Improved Hamming Code: The optimal length `n` of an improved Hamming code with `r` redundant symbols over a field `GF(q)` is `n = I_{max} * (q^r - 1) / (q - 1)`, where `I_{max}` is the maximum size of a set of scalars that keeps the error syndromes distinct. For `q = (2l+1)³`, it is established that `I_{max}=2`.', 'theorem_2': ""Parity Check Matrix of the Improved Hamming Code: The code's parity check matrix `H` is constructed by first including all 'major columns' (columns whose first non-zero element is 1) and then appending 'minor columns' (scaled versions of major columns) if they do not create ambiguity for the known set of possible error patterns."", 'theorem_3': 'Redundancy Calculation for Remainder Class Codes: When the resolution `k` is sufficiently large compared to `l`, the number of possible information messages represented in the quotient vectors of parity symbols becomes negligible. The redundancy in bits can then be approximated based solely on the parameters of the code operating on the remainder classes.'}"	log_{2}(13n+1)	理论工程学科-信息论和通信理论-信息纠错理论	31859	2	1
recuVNRyhHuY52	For a linear code defined over the ring $R = \mathbb{F}_2 + u\mathbb{F}_2$ (where $u^2=0$), what is the minimum Hamming distance of its binary Gray image, $\phi(\mathcal{C}_L)$? The code $\mathcal{C}_L$ is constructed using a defining set $L = \Delta_A^c + u(\Delta_B \setminus \Delta_{B'})$, where $\Delta_S$ represents a simplicial complex over $\mathbb{F}_2^m$ with a single maximal element whose support has cardinality $|S|$. The construction is subject to the specific constraints $|B|=m$ and $|A|=|B'|=m-1$.	"{'step_1': 'The objective is to find the minimum Hamming distance of a specific binary linear code, which is obtained through a multi-step construction process.', 'step_2': 'The process begins by defining a linear code $\\mathcal{C}_L$ over the ring $R = \\mathbb{F}_2 + u\\mathbb{F}_2$ (Concept_2).', 'step_3': ""The structure of this code is determined by its defining set, $L$. For this problem, the defining set is $L = \\Delta_A^c + u(\\Delta_B \\setminus \\Delta_{B'})$ (Concept_4). These sets are constructed from simplicial complexes over $\\mathbb{F}_2^m$ (Concept_1)."", 'step_4': ""The construction is further specified by the constraints on the cardinalities of the supports of these simplicial complexes: $|B|=m$ and $|A|=|B'|=m-1$."", 'step_5': 'The code $\\mathcal{C}_L$ over the ring is then transformed into a binary linear code, $\\phi(\\mathcal{C}_L)$, by applying the Gray map (Concept_3). The minimum Hamming distance of this binary code is the value we need to find.', 'step_6': 'Theorem_2 provides the explicit parameters for the resulting binary code $\\phi(\\mathcal{C}_L)$ under exactly these conditions. The parameters are given in the format $[n, k, d]$, where $n$ is the length, $k$ is the dimension, and $d$ is the minimum Hamming distance.', 'step_7': 'According to Theorem_2, the parameters of $\\phi(\\mathcal{C}_L)$ are $[2^{2m-1}, 2m, 2^{2m-2}]$. By definition, the third parameter in this triple is the minimum Hamming distance.', 'step_8': 'Therefore, the minimum Hamming distance of the code is $2^{2m-2}$. The theorem also states that this code is a Griesmer code (Theorem_1), confirming its optimality and the correctness of its parameters.'}"	"{'concept_1': 'A simplicial complex of $\\mathbb{F}_q^m$ with one maximal element whose support is $A \\subseteq [m]$, denoted $\\Delta_A$, can be viewed as an $|A|$-dimensional $\\mathbb{F}_q$-subspace of the finite field $\\mathbb{F}_{q^m}$.', 'concept_2': 'A linear trace code $\\mathcal{C}_L$ over the ring $R = \\mathbb{F}_q + u\\mathbb{F}_q$ can be constructed from a defining set $L \\subseteq \\mathcal{R} = \\mathbb{F}_{q^m} + u\\mathbb{F}_{q^m}$. Its codewords are of the form $c_z = (Tr(zx))_{x \\in L}$ for each $z \\in \\mathcal{R}$, where $Tr(\\cdot)$ is the trace function from $\\mathcal{R}$ to $R$.', 'concept_3': 'The Gray map $\\phi: (\\mathbb{F}_q + u\\mathbb{F}_q)^n \\rightarrow \\mathbb{F}_q^{2n}$ is defined by $\\phi(a+ub) = (b, a+b)$ for $a, b \\in \\mathbb{F}_q^n$. This map is an isometry that transforms the Lee distance between codewords in the ring to the Hamming distance between their binary images.', 'concept_4': ""The specific construction under consideration uses the defining set $L = \\Delta_A^c + u(\\Delta_B \\setminus \\Delta_{B'})$, where $\\Delta^c$ denotes the complement of the set $\\Delta$."", 'theorem_1': 'An $[n, k, d]$ linear code over $\\mathbb{F}_q$ is a Griesmer code if its parameters satisfy the Griesmer bound with equality: $n = \\sum_{i=0}^{k-1}\\lceil\\frac{d}{q^{i}}\\rceil$. Griesmer codes are optimal as they achieve the minimum possible length for a given dimension and distance.', 'theorem_2': ""For a code $\\mathcal{C}_L$ constructed with the defining set from Concept_4, under the specific constraints $q=2$, $|B|=m$, and $|A|=|B'|=m-1$, its Gray image $\\phi(\\mathcal{C}_L)$ is a binary linear code with parameters and is a Griesmer code.""}"	$2^{2m-2}$	理论工程学科-信息论和通信理论-信息论	182973	2	5
recuUTbsAW1bNx	What is the attenuation rate of the global solution of 2D Equations with linear velocity damping and without micropolar dissipative(angular viscosities > 4 dynamic microrotation viscosity) over large time?	"{'step_1': ""Establish the Spatial-Frequency Connection using Plancherel's Theorem. Converts spatial $L^2$-norms to frequency domain integrals: \\[\\|\\mathbf{u}(t)\\|_{L^2}^2 = \\int_{\\mathbb{R}^2} |\\hat{\\mathbf{u}}(\\xi,t)|^2 d\\xi, \\quad \\|\\omega(t)\\|_{L^2}^2 = \\int_{\\mathbb{R}^2} |\\hat{\\omega}(\\xi,t)|^2 d\\xi\\] Implication: Attenuation rates are determined by frequency-domain behavior."", 'step_2': 'Frequency Domain Splitting using the Generalized Fourier Splitting Method. Split frequency space into two regions for large $t$: \\begin{itemize}\\item \\textbf{Low-frequency region:} $|\\xi|^2 \\leq \\frac{C}{1+t}$\\begin{itemize}\\item Dominated by linear damping/weak viscosity \\item Determines overall decay rate \\end{itemize}\\item \\textbf{High-frequency region:} $|\\xi|^2 > \\frac{C}{1+t}$\\begin{itemize}\\item Dominated by strong viscous effects \\item Contributions decay exponentially \\end{itemize}\\end{itemize}', 'step_3': 'Analyze Velocity Field $\\mathbf{u}$ Decay. \\subsection{Step 3.1: Low-Frequency Contribution ($|\\xi|^2 \\leq \\frac{C}{1+t}$)}\\begin{itemize}\\item \\textbf{Region measure:} $\\pi \\cdot \\left(\\sqrt{\\frac{C}{1+t}}\\right)^2 = O\\left(\\frac{1}{1+t}\\right)$\\item \\textbf{Component decay:} $|\\hat{\\mathbf{u}}(\\xi,t)|^2 \\sim O\\left(\\frac{1}{(1+t)^2}\\right)$\\item \\textbf{Total contribution:}\\[ \\int_{\\text{low}} |\\hat{\\mathbf{u}}|^2 d\\xi \\sim O\\left(\\frac{1}{1+t}\\right) \\times O\\left(\\frac{1}{(1+t)^2}\\right) = O\\left(\\frac{1}{(1+t)^3}\\right) \\]\\end{itemize} \\subsection{Step 3.2: High-Frequency Contribution ($|\\xi|^2 > \\frac{C}{1+t}$)}\\begin{itemize}\\item \\textbf{Exponential decay:} $|\\hat{\\mathbf{u}}|^2 \\sim O(e^{-Ct})$\\item \\textbf{Total contribution:} $O\\left(\\frac{1}{(1+t)^N}\\right)$ (negligible for $N \\gg 3$)\\end{itemize} \\subsection{Step 3.3: Combine Contributions}\\[ \\|\\mathbf{u}(t)\\|_{L^2} \\sim \\sqrt{O\\left(\\frac{1}{(1+t)^3}\\right)} = O\\left((1+t)^{-3/2}\\right) \\]', 'step_4': 'Analyze Angular Velocity $\\omega$ Decay. \\subsection{Step 4.1: Low-Frequency Contribution ($|\\xi|^2 \\leq \\frac{C}{1+t}$)}\\begin{itemize}\\item \\textbf{Region measure:} $O\\left(\\frac{1}{1+t}\\right)$ (same as Step 3.1)\\item \\textbf{Component decay:} $|\\hat{\\omega}(\\xi,t)|^2 \\sim O\\left(\\frac{1}{1+t}\\right)$\\item \\textbf{Total contribution:}\\[ \\int_{\\text{low}} |\\hat{\\omega}|^2 d\\xi \\sim O\\left(\\frac{1}{1+t}\\right) \\times O\\left(\\frac{1}{1+t}\\right) = O\\left(\\frac{1}{(1+t)^2}\\right) \\]\\end{itemize} \\subsection{Step 4.2: High-Frequency Contribution ($|\\xi|^2 > \\frac{C}{1+t}$)}\\begin{itemize}\\item \\textbf{Exponential decay:} $|\\hat{\\omega}|^2 \\sim O(e^{-Ct})$\\item \\textbf{Total contribution:} $O\\left(\\frac{1}{(1+t)^N}\\right)$ (negligible for $N \\gg 2$)\\end{itemize} \\subsection{Step 4.3: Combine Contributions}\\[ \\|\\omega(t)\\|_{L^2} \\sim \\sqrt{O\\left(\\frac{1}{(1+t)^2}\\right)} = O\\left((1+t)^{-1}\\right) \\]', 'step_5': ""Validate Decay Rates. \\subsection{Step 5.1: Synergistic Effect Verification}\\begin{itemize}\\item Linear damping in $\\mathbf{u}$ reduces energy input to $\\omega$ via coupling terms \\item Prevents slower decay of $\\omega$ despite weaker angular viscosity \\end{itemize} \\subsection{Step 5.2: Nonlinear Term Control}\\begin{itemize}\\item Nonlinear terms decay as $N(\\mathbf{u},\\omega) \\sim (1+s)^{-5/2}$\\item Duhamel's principle shows integral contribution $\\sim (1+t)^{-3/2}$\\item Does not disrupt dominant linear decay rates \\end{itemize}"", 'step_6': 'Final Results. \\begin{itemize}\\item Velocity field: $\\|\\mathbf{u}(t)\\|_{L^2} \\leq C(1+t)^{-3/2}$\\item Angular velocity: $\\|\\omega(t)\\|_{L^2} \\leq C(1+t)^{-1}$\\end{itemize} Note: $C > 0$ is a constant independent of initial conditions.'}"	"{'concept_1': ""Linear Velocity Damping: Linear velocity damping is the addition of the linear term κu (κ > 0 is the dynamic microrotational viscosity) to the momentum equation. This damping term suppresses the random growth or oscillation of the velocity field by dissipating the 'energy' of the system."", 'concept_2': 'Generalized Fourier splitting method — A technique used to derive decay estimates for solutions to partial differential equations by dividing the Fourier space into low-frequency and high-frequency regions. Low-frequency (|ξ|^2 ≤ C/(1+t)): Large-scale structures where damping dominates.', 'concept_3': 'Low-Frequency Damping Enhancement — In the low-frequency region, the damping term −kappa u dominates, making the low-frequency component of u decay faster (similar to the behavior of the damping thermal equation. For ω, the low-frequency region is controlled by the viscosity term.', 'concept_4': 'Duhamel’s principle — Expresses nonlinear solutions as the sum of the linear homogeneous solution (damped) and an integral of nonlinear terms. Ensures nonlinear interactions do not disrupt the linear decay rate.', 'theorem_1': 'Energy estimates — Provides bounds on L2-norms of u,ω, and their gradients. For u, the damping term directly leads to faster dissipation of its energy, while for ω, the attenuation is mainly dependent on the angular viscosity term −γΔω, therefore ω decays more slowly.', 'theorem_2': 'Plancherel’s theorem — Allows conversion between spatial L^2-norms and Fourier L^2-norms, facilitating the analysis of decay in frequency space.'}"	The large-time decay rates of the L^2-norm of the global solution: - For the velocity field u: \|u(t)\|_{L^2} \leq C(1+t)^{-\frac{3}{2}} - For the microrotation angular velocity ω: \|ω(t)\|_{L^2} \leq C(1+t)^{-1}	理论工程学科-理论工程力学-流体力学	17646	2	4
recuUCrDYOxGr2	Under the Equalized Odds (EO) constraint, the prediction of the kernel regression model can be written as \[\hat{Y} = K\alpha.\] What is the closed-form solution of the coefficient vector \(\alpha\) in this case?	{'step_1': 'The goal is to derive the **closed-form solution** for kernel regression with EO constraints. The approach is: incorporate EO into ERM as a differentiable regularization term using the **conditional cross-covariance operator**, then reduce the infinite-dimensional problem to Gram matrices using the Representer Theorem (concept_1, concept_2, concept_4).', 'step_2': 'Construct a **fairness-aware ERM**: Minimize squared loss + kernel norm + EO regularization \\[\\min_{f\\in\\mathcal H}\\ \\tfrac1n\\sum_i(y_i-f(x_i))^2+\\lambda\\|f\\|_{\\mathcal H}^2+\\mu\\|C_{fS\\mid Y}\\|_{HS}^2,\\] where \\(\\|C_{fS\\mid Y}\\|_{HS}^2=0 \\iff \\hat Y\\perp\\!\\!\\!\\perp S\\mid Y\\), directly converting EO into an optimizable term (concept_1, concept_2). In the paper, Section 3.3 introduces EO in the form of a conditional operator, incorporated into the objective as a regularization term.', 'step_3': 'Apply the **Representer Theorem** (concept_4): Let \\(f(\\cdot)=\\sum_{i=1}^n\\alpha_i k(x_i,\\cdot)\\). This converts the infinite-dimensional optimization into a finite-dimensional optimization over \\(\\alpha\\in\\mathbb R^n\\). The paper adopts this dimensionality reduction approach when deriving the kernel form.', 'step_4': 'Express all terms as quadratic forms of \\(\\alpha\\) using **Gram matrices and projections** (concept_5): \\(\\|f\\|_{\\mathcal H}^2=\\alpha^\\top K\\alpha\\); the EO regularization corresponds to \\[\\alpha^\\top K\\,(I-\\Pi_y)\\,K_S\\,(I-\\Pi_y)\\,K\\,\\alpha,\\] where \\(K\\) is the input Gram matrix, \\(K_S\\) is the Gram matrix of sensitive attributes, and \\(\\Pi_y=Y(Y^\\top Y)^{-1}Y^\\top\\) is the projection onto the label subspace. This structure is directly used in the kernel solution of the paper (see the matrix blocks in the final solution).', 'step_5': 'Combine terms to obtain a strictly convex quadratic objective for \\(\\alpha\\), and apply first-order optimality conditions for \\(\\alpha\\): \\[\\Big(\\tfrac1nK^2+\\lambda K+\\mu\\,K(I-\\Pi_y)K_S(I-\\Pi_y)K\\Big)\\alpha=\\tfrac1nKY.\\] An equivalent transformation by left-multiplying with \\(K^{-1}\\) can convert \\(\\lambda K\\) into the form \\(\\lambda I\\), making it easier to observe strong convexity and invertibility.', 'step_6': '**Closed-form solution**: \\[\\boxed{\\ \\alpha=\\big(K+n\\lambda I+n\\mu\\,(I-\\Pi_y)K_S(I-\\Pi_y)\\,K\\big)^{-1}Y\\ }.\\]'}	{'concept_1': 'Fair ERM Framework: A learning framework where empirical risk minimization is extended by adding fairness regularization terms, enabling a trade-off between accuracy and fairness.', 'concept_2': 'Hilbert-Schmidt Independence Criterion (HSIC): A kernel-based statistical dependence measure, used to quantify independence between random variables via the Hilbert-Schmidt norm of cross-covariance operators.', 'concept_3': 'Conditional Cross-Covariance Operator: An operator in reproducing kernel Hilbert spaces (RKHS) that characterizes conditional independence between random variables by subtracting conditional expectations from joint dependencies.', 'concept_4': 'Representer Theorem: A fundamental result in RKHS stating that the solution of regularized empirical risk minimization problems can be expressed as a finite linear combination of kernel functions evaluated at training data points.', 'concept_5': 'Gram Matrix with Projections: A matrix formed by evaluating pairwise kernel functions on data points, often combined with projection operators (such as onto the label subspace) to enforce constraints or conditional structures in kernel methods.', 'theorem_1': 'Kernel Conditional Independence Test: A statistical test using conditional cross-covariance operators in RKHS to determine whether two random variables are conditionally independent given a third variable.'}	\alpha=\big(K+\lambda I+\mu\,(I-\Pi_y)^\top K_S (I-\Pi_y)\,K\big)^{-1}Y	理论工程学科-控制理论与控制工程-统计机器学习	1436936	3	5
recuV5g9UmQH2w	The exterior metric is $ds^{2} = -f(r)\,dt^{2} + f(r)^{-1}dr^{2} + r^{2}d\Omega^{2}$; the interior geometry of the collapsing star can be described by the spatially flat FLRW. The shell radius is $R(\tau)$, and we use units $G=c=1$. Please give the final expression for the shell’s density $\rho(\tau)$.	{'step_1': 'First, express the external geometry in the PG form $ds^2=-d\\tau^2+(dr+\\sqrt{1-f}\\,d\\tau)^2+r^2d\\Omega^2$ (theorem_1).', 'step_2': 'The internal flat FLRW can also be written in the PG form $ds^2=-d\\tau^2+(dr-rH\\,d\\tau)^2+r^2d\\Omega^2$, where $r(\\tau)=a(\\tau)r_c$ and $H=\\dot a/a$ (concept_1).', 'step_3': 'Then perform matching at the shell surface $r(\\tau)=R(\\tau)$:', 'step_4': '- From the external geometry, we obtain $\\dot R=-\\sqrt{1-f(R)}$ (theorem_1);', 'step_5': '- From the internal geometry, we obtain $\\dot R=R H$ (concept_1).', 'step_6': 'Combining these gives (concept_2):', 'step_7': '$\\left(\\frac{\\dot R}{R}\\right)^2=\\frac{1-f(R)}{R^2}.$', 'step_8': 'Finally, use the Friedmann equation $H^2=\\frac{8\\pi}{3}\\rho$ (theorem_2) and substitute $H=\\dot R/R$ to obtain the shell surface density:', 'step_9': '$\\boxed{\\rho(\\tau)=\\frac{3}{8\\pi}\\left(\\frac{\\dot R}{R}\\right)^2 =\\frac{3}{8\\pi}\\,\\frac{1-f\\!\\bigl(R(\\tau)\\bigr)}{R(\\tau)^2}}$ The units $G=c=1$ have been adopted.'}	{'concept_1': 'the FLRW model can also be cast into the PG coordinates, namely, $ds^2 = -d\\tau^2 + (dr - r H(\\tau) d\\tau)^2 + r^2 d\\Omega^2$, where $ r(\\tau) = a(\\tau) r_c $, $ H(\\tau) = \\dot{r}(\\tau)/r(\\tau) $, $ a(\\tau) $ is the scale factor and $ r_c $ is the comoving radial coordinate.', 'concept_2': 'On the star surface, $ r(\\tau) = R(\\tau) $, we obtain $\\frac{dR}{d\\tau} = -\\sqrt{1 - f(R)} = R H = \\dot{R}$, so $(\\dot{R}/R)^2 = \\frac{1-f(R)}{R^2}$.', 'theorem_1': 'Writing exterior metric in terms of PG coordinates, $ds^2 = -d\\tau^2 + (dr + \\sqrt{1 - f} d\\tau)^2 + r^2 d\\Omega^2$.', 'theorem_2': 'Friedmann equation gives $\\frac{\\dot{a}^2}{a^2} = \\frac{8\\pi}{3} \\rho(R)$.'}	$\rho(\tau) = \frac{3}{4 \pi} \frac{1 - f(R(\tau))}{2 R(\tau)^2}$	理论交叉学科-理论天体物理-宇宙学	261406	2	3
recuVDJJCp8IF5	Under the covariate shift setting, for importance-weighted kernel regression in an RKHS with a general spectral regularization family {g_λ}, what is the closed-form regularized estimator? Please express the answer only in operator form using the sampling operator S_x and the diagonal weight matrix B. Do not expand into specific inverse forms such as Tikhonov or truncated SVD; only provide the general operator representation with g_λ.	{'step_1': 'Covariate Shift and Importance Weighting (Concept_1, Concept_2): Under covariate shift, the conditional distribution P(y|x) is invariant across domains, but the marginal distributions differ. Thus, the target risk can be written as a weighted source risk: \\[\\mathbb{E}_T[(y-f(x))^2] = \\mathbb{E}_S[\\beta(x)(y-f(x))^2],\\] where \\(\\beta(x)=\\tfrac{d\\rho_T}{d\\rho_S}(x)\\). For finite samples, define the diagonal weight matrix \\[B = \\mathrm{diag}(\\beta(x_1), \\dots, \\beta(x_n)).\\]', 'step_2': 'Operator Formulation in RKHS (Concept_3, Concept_4): Define the sampling operator \\(S_x:\\mathcal H_K\\to\\mathbb R^n\\), \\(S_x f = (f(x_1),\\dots,f(x_n))\\), and its adjoint \\(S_x^*:\\mathbb R^n\\to\\mathcal H_K\\), \\(S_x^* v = \\sum_i v_i K(\\cdot,x_i)\\). Then we can define \\[T = S_x^* B S_x, \\quad h = S_x^* B y,\\] where \\(y=(y_1,\\dots,y_n)^\\top\\).', 'step_3': 'Spectral Regularization Framework (Concept_5): Spectral regularization applies a filter function \\(g_\\lambda\\) to the operator \\(T\\), stabilizing the inversion and covering methods such as Tikhonov, Landweber, and truncated SVD.', 'step_4': 'Closed-form Estimator: The final closed-form estimator is obtained as \\[\\boxed{f^{\\,z}_\\lambda = g_\\lambda(S_x^{*} B S_x)\\, S_x^{*} B y}.\\]'}	{'concept_1': 'Covariate Shift: A distributional setting where the conditional distribution P(y|x) remains the same across source and target domains, but the marginal distributions P(x) differ.', 'concept_2': 'Radon–Nikodým Derivative: Given that the target distribution ρ_T is absolutely continuous with respect to the source distribution ρ_S, the density ratio β(x)=dρ_T/dρ_S(x) exists, serving as importance weights between the two distributions.', 'concept_3': 'Sampling Operator: In an RKHS, the sampling operator S_x maps a function f to its vector of evaluations on sample points, S_x f = (f(x_1),...,f(x_n)).', 'concept_4': 'Adjoint Operator: The adjoint S_x^* maps a vector v in R^n to a linear combination of kernel functions, S_x^* v = Σ_i v_i K(·,x_i).', 'concept_5': 'Spectral Regularization Methods: A family of operator-function-based regularization techniques that apply spectral filter functions g_λ to control the bias-variance tradeoff in inverse problems.'}	\[\boxed{f^{\,z}_\lambda = g_\lambda(S_x^{*} B S_x)\, S_x^{*} B y}.\]	理论工程学科-控制理论与控制工程-统计机器学习	1500291	3	5
recuU8HuA6yoyv	What is the large-time decay rate of the $L^2$ norm for weak solutions to the $d$-dimensional micropolar Rayleigh-Bénard problem near hydrostatic equilibrium in $\mathbb{R}^d$ ($d=2,3$) with full dissipation ($\nu>0$) ?	"{'step_1': 'Energy Estimates (Using concept_2) Define the energy functional: \\[ E(t) = \\frac{1}{2} \\left( \\|u(t)\\|_{L^2}^2 + \\|w(t)\\|_{L^2}^2 + \\|\\theta(t)\\|_{L^2}^2 \\right). \\] Differentiate with respect to time and substitute the linearized equations (ignoring nonlinear terms due to small perturbations): \\[ \\frac{d}{dt} E(t) = \\int_{\\mathbb{R}^d} \\left( u \\cdot \\partial_t u + w \\cdot \\partial_t w + \\theta \\cdot \\partial_t \\theta \\right) dx. \\] Using the equations and integration by parts: \\begin{align*} \\nabla \\cdot u=0 \\int u \\cdot \\partial_t u dx &\\leq -\\mu \\|\\nabla u\\|_{L^2}^2 - (\\mu + \\lambda) \\|\\nabla \\cdot u\\|_{L^2}^2 + C \\left( \\|w\\|_{L^2}^2 + \\|\\theta\\|_{L^2}^2 \\right) \\\\ \\int w \\cdot \\partial_t w dx &\\leq -\\gamma \\|\\nabla w\\|_{L^2}^2 - 2\\chi \\|w\\|_{L^2}^2 + C \\|u\\|_{L^2}^2 \\\\ \\int \\theta \\partial_t \\theta dx &\\leq -\\kappa \\|\\nabla \\theta\\|_{L^2}^2 \\end{align*} Combining these estimates, there exist constants $c_1, c_2 > 0$ such that: \\[ \\frac{d}{dt} E(t) \\leq -C \\left( \\|\\nabla u\\|_{L^2}^2 + \\|\\nabla w\\|_{L^2}^2 + \\|w\\|_{L^2}^2 + \\|\\nabla \\theta\\|_{L^2}^2 + \\|\\nabla \\cdot u\\|_{L^2}^2 \\right) \\right). \\] Then we have \\[ E(t) + c_1 \\int_0^t \\left( \\|\\nabla u(s)\\|_{L^2}^2 + \\|\\nabla w(s)\\|_{L^2}^2 + \\|w(s)\\|_{L^2}^2 + \\|\\nabla \\theta(s)\\|_{L^2}^2 \\right) ds \\leq E(0). \\] This shows $E(t)$ is bounded but does not give the decay rate directly.', 'step_2': ""Fourier Analysis via Plancherel's Theorem (Using concept_4) By Plancherel's theorem, $\\|f\\|_{L^2} = \\|\\hat{f}\\|_{L^2}$, so: \\[ E(t) = \\frac{1}{2} \\int_{\\mathbb{R}^d} \\left( |\\hat{u}(\\xi,t)|^2 + |\\hat{w}(\\xi,t)|^2 + |\\hat{\\theta}(\\xi,t)|^2 \\right) d\\xi. \\] Split the frequency space into low-frequency ($|\\xi| \\leq R(t)$) and high-frequency ($|\\xi| > R(t)$) regions, with $R(t) = (1+t)^{-1/2}$. {High-Frequency Region ($|\\xi| > R(t)$)} From energy estimate: \\[ \\int_{|\\xi| > R(t)} |\\hat{U}(\\xi,t)|^2 d\\xi \\leq \\frac{1}{R(t)^2} \\int_{|\\xi| > R(t)} |\\xi|^2 |\\hat{U}|^2 d\\xi \\leq \\frac{1}{R(t)^2} \\|\\nabla U(t)\\|_{L^2}^2. \\] Since $\\int_0^t \\|\\nabla U(s)\\|_{L^2}^2 ds < \\infty$ and $\\|\\nabla U(t)\\|_{L^2}^2 \\to 0$, this decays as $O((1+t))$, faster than $(1+t)^{-d/2}$ for $d=2,3$. \\subsubsection*{Low-Frequency Region ($|\\xi| \\leq R(t)$)} In the linearized system, the solution in Fourier space satisfies: \\begin{align*} |\\hat{\\rho}(\\xi,t)|, |\\hat{u}(\\xi,t)|, |\\hat{\\theta}(\\xi,t)| &\\leq C e^{-c|\\xi|^2 t} |\\hat{U}_0(\\xi)| \\\\ |\\hat{w}(\\xi,t)| &\\leq C e^{-c t} |\\hat{w}_0(\\xi)| \\end{align*} Assuming $U_0 \\in L^1 \\cap L^2$ so that $\\|\\hat{U}_0\\|_{L^\\infty} \\leq \\|U_0\\|_{L^1} < \\infty$, we have: \\[ \\int_{|\\xi| \\leq R(t)} |\\hat{U}(\\xi,t)|^2 d\\xi \\leq \\|\\hat{U}_0\\|_{L^\\infty}^2 \\int_{|\\xi| \\leq R(t)} e^{-2c|\\xi|^2 t} d\\xi. \\] Changing variables $s = r\\sqrt{t}$, $dr = t^{-1/2}ds$: \\[ \\int_{|\\xi| \\leq R(t)} e^{-2c|\\xi|^2 t} d\\xi \\sim t^{-d/2} \\int_0^{R(t)\\sqrt{t}} s^{d-1} e^{-2c s^2} ds \\sim C t^{-d/2}, \\quad t \\to \\infty. \\] Thus: \\[ \\int_{|\\xi| \\leq R(t)} |\\hat{U}(\\xi,t)|^2 d\\xi \\leq C \\|U_0\\|_{L^1}^2 (1+t)^{-d/2}. \\]"", 'step_3': 'Handling Nonlinear Terms (Using concept_3) For nonlinear terms (e.g., $u \\cdot \\nabla u$), by integrating by parts, \\[ \\left| \\int u \\cdot (u \\cdot \\nabla u) dx \\right| \\leq =0. \\] When $\\|U\\|_{L^2}$ is small, nonlinear terms can be absorbed in the energy estimates and do not affect the leading decay rate.', 'step_4': 'Combining Estimates Combining low and high-frequency estimates: \\begin{align*} \\int_{|\\xi| \\leq R(t)} |\\hat{U}(\\xi,t)|^2 d\\xi &\\leq C (1+t)^{-d/2} \\\\ \\int_{|\\xi| > R(t)} |\\hat{U}(\\xi,t)|^2 d\\xi &\\leq C (1+t)^{-1} \\quad (\\text{faster}) \\\\ \\|w(t)\\|_{L^2}^2 &\\leq C e^{-ct} \\quad (\\text{exponentially fast}) \\end{align*} Thus: \\[ \\|U(t)\\|_{L^2}^2 = \\int_{\\mathbb{R}^d} |\\hat{U}(\\xi,t)|^2 d\\xi \\leq C (1+t)^{-d/2} \\] Taking square roots: \\[ \\|U(t)\\|_{L^2} \\leq C (1+t)^{-d/4}, \\quad d=2,3 \\] where $C$ depends on $\\|U_0\\|_{L^1 \\cap L^2}$ and dissipation coefficients.'}"	"{'concept_1': 'Weak solution definition \\[ (u,w,\\theta) \\in L^\\infty([0,T];L^2(\\mathbb{R}^d)) \\cap L^2([0,T];H^1(\\mathbb{R}^d)) \\]', 'concept_2': 'Energy estimates Fundamental for obtaining a priori bounds on the $L^2$ norms of solutions and their derivatives, forming the basis for decay analysis.', 'concept_3': 'Gagliardo-Nirenberg inequality Key for handling nonlinear terms in energy estimates and gradient decay proofs, e.g., bounding $\\|u\\|_{L^\\infty}$ in terms of $\\|u\\|_{L^2}$ and $\\|\\Delta u\\|_{L^2}$.', 'concept_4': ""Plancherel's theorem Essential for transitioning between spatial and frequency domains in Fourier-based decay proofs, enabling control of Fourier transforms of solutions.""}"	For initial data $(u_0, w_0, \theta_0) \in L^1(\mathbb{R}^d) \cap H^1(\mathbb{R}^d)$ : Full dissipation ($\nu > 0$): \[ \|u(t)\|_{L^2} + \|w(t)\|_{L^2}+ \|\theta(t)\|_{L^2} \leq C(t+1)^{-\frac{d}{4}}, d=2,3. \]	理论工程学科-理论工程力学-流体力学	52409	1	1
recuURbEBCDdYt	In graphene-channel flash memory devices with hBN as the tunneling layer, when the program voltage is |V_D,PROG| = 5 V and the effective memory window is 0.8 V, what is the order of the shortest programming pulse width?	{'Step_1': 'Relate the write time t_write to the required charge density σ_Q and the achievable injection current density J_inj (t_write ∝ σ_Q / J_inj). σ_Q is set by the device equivalent capacitance and target window, while t_write depends on whether J_inj can be large enough under the applied bias.', 'Step_2': 'Estimate charge density: σ_Q = C_eq · ΔV_mem = (5 fF/μm²) · 0.8 V = 4 fC/μm².', 'Step_3': 'Determine dominant programming mechanism: Considering Concept2 (high mobility and rapid thermalization), and Concept3 (|V_D,PROG| ≈ 5 V strong bias in 2D heterostructures), neither Direct Tunneling nor Fowler–Nordheim tunneling dominates; programming is mainly governed by Hot Carrier Injection.', 'Step_4': 'Build physical expression of injection current density: J_inj ≈ (attempt flux) × (emission probability). Attempt flux is determined by n_s v_sat / L_hot, ensured large by high mobility and quasi-ballistic transport (Concept1). Emission probability is determined by hot electron distribution: P_HE ≈ exp[−(Φ_B − qV_ox)/(k_B T_e)].', 'Step_5': 'Estimate attempt flux: Φ_att ≈ n_s v_sat / L_hot, where n_s ≈ C_g V_ov / q (C_g≈5 fF/μm², V_ov≈1 V ⇒ n_s≈3×10^16 m^-2); v_sat ≈ 1.5×10^5 m/s; L_hot ≈ 50 nm. Substituting gives current density upper bound q·n_s·v_sat/L_hot ≈ 1.5×10^10 A/m².', 'Step_6': 'Hot electron emission probability: P_HE ≈ exp[−(Φ_B − qV_ox)/(k_B T_e)]. Here V_ox = η_V V_prog, with η_V≈0.5 and V_prog=5 V ⇒ V_ox≈2.5 V; effective barrier height ≈ 1 eV; average energy gain per mean free path Δε ≈ qE_y λ_mfp with λ_mfp≈30 nm ⇒ Δε≈2.4 eV. With conversion factor α≈0.3, k_B T_e ≈ 0.72 eV. Thus P_HE ≈ exp(−1/0.7) ≈ 0.24.', 'Step_7': 'Final injection current density: J_inj ≈ J_ch,eff × P_HE ≈ (1.5×10^10 A/m²) × 0.24 ≈ 3.6×10^9 A/m². Considering angular distribution and scattering correction ξ≈0.001–0.003, J_inj ≈ 10^7–10^9 A/m². With σ_Q = 4×10^-3 C/m², t_write = σ_Q / J_inj ≈ (4×10^-3)/(1×10^7) = 4×10^-10 s = 400 ps.', 'Step_8': 'Conclusion: The shortest programming pulse width is in the picosecond regime.'}	{'concept_1': 'In graphene/hBN heterostructures, hBN serves as a flat substrate without dangling bonds, which significantly reduces scattering centers. This enables graphene to exhibit ultra-high carrier mobility and approach quasi-ballistic transport behavior.', 'concept_2': 'As the applied electric field increases, the carrier velocity in graphene tends to saturate. Meanwhile, electron–phonon scattering raises the electron temperature, leading to a non-equilibrium thermalized state that strongly affects transport properties.', 'concept_3': 'The charge injection process can be dominated by different tunneling mechanisms: Direct Tunneling (DT) under low fields and thin tunneling oxides, Fowler–Nordheim (FN) Tunneling under strong fields, and Hot Carrier Injection (HCI) where high-energy carriers surmount the barrier and inject into the floating gate. Depending on device structure and bias conditions, the dominant mechanism shifts.', 'concept_4': 'A probabilistic model describing the fraction of high-energy carriers that avoid scattering and directly reach the tunneling barrier for successful injection, used to evaluate programming efficiency and reliability.', 'concept_5': 'The formation of a 2D electron gas enhances the vertical electric field, thereby improving cross-barrier injection efficiency.'}	400 Picosecond scale (10^{-12}s)	理论交叉学科-计算材料学-半导体物理学	192748	3	5
recuUwG9Gr2YxF	In the rank metric, for a fixed list size L, what is the upper bound on the list-decoding radius of a linear code with rate R? (Give a single mathematical expression.)	{'step_1': 'Given a center \\(y\\in \\mathbb{F}_{q^m}^n\\), if the rank ball of radius \\(\\rho n\\) contains \\(L+1\\) codewords \\(c_0,\\dots,c_L\\) with \\(d_R(c_j,y)\\le \\rho n\\), define the difference vectors \\(v_j:=c_j-y\\). With the syndrome map \\(\\sigma(x)=Hx^{\\top}\\), we have \\(\\sigma(c_j)=\\sigma(y)\\). Hence, for any \\(j>0\\), \\[ u_j:=v_j-v_0\\in \\ker(H). \\]', 'step_2': 'For each difference vector, define the rank support \\(\\mathrm{RS}(v_j)\\subseteq \\mathbb{F}_q^n\\) with \\(\\dim \\mathrm{RS}(v_j)=\\operatorname{rank}_{\\mathbb{F}_q}(v_j)\\). Let \\[ W:=\\mathrm{RS}(v_0)+\\mathrm{RS}(v_1)+\\cdots+\\mathrm{RS}(v_L)\\subseteq \\mathbb{F}_q^n . \\] Since \\(d_R(c_j,y)=\\operatorname{rank}_{\\mathbb{F}_q}(v_j)\\le \\rho n\\), and by the subadditivity of the dimension of sum-spaces, we obtain \\[ \\dim W\\ \\le\\ \\sum_{j=0}^{L}\\dim \\mathrm{RS}(v_j)\\ =\\ \\sum_{j=0}^{L}\\operatorname{rank}_{\\mathbb{F}_q}(v_j)\\ \\le\\ (L+1)\\rho n. \\]', 'step_3': 'If the above “crowding” indeed occurs (there are \\(L+1\\) codewords in the ball), then necessarily \\[ \\dim W\\ >\\ L(n-k). \\] Conversely, to avoid crowding and make \\(C\\) \\((\\rho,L)\\)-list decodable, we require that any \\(W\\) generated by \\(L+1\\) difference vectors within radius \\(\\rho n\\) satisfies \\[ \\dim W\\ \\le\\ L(n-k). \\]', 'step_4': 'Comparing (A) with the crowding threshold (B), crowding requires \\[ (L+1)\\rho n\\ >\\ L(n-k). \\] Therefore, to preclude crowding and guarantee \\((\\rho,L)\\)-list decodability, the necessary upper bound is \\[ (L+1)\\rho n\\ \\le\\ L(n-k) \\quad\\Longleftrightarrow\\quad \\rho\\ \\le\\ \\frac{L}{L+1}\\Bigl(1-\\frac{k}{n}\\Bigr)\\ =\\ \\frac{L}{L+1}(1-R). \\]'}	{'concept_1': 'Code: \\(C \\subseteq \\mathbb{F}_{q^m}^n\\) with dimension \\(k\\) and rate \\(R = k/n\\). Parity-check matrix: \\(H \\in \\mathbb{F}_q^{(n-k)\\times n}\\) (thus the number of rows is \\(n-k\\)). Rank distance: \\(d_R(x, y) = \\operatorname{rank}_{\\mathbb{F}_q}(x - y)\\).', 'concept_2': 'There exists a center \\(y \\in \\mathbb{F}_{q^m}^n\\) such that the rank ball of radius \\(\\rho n\\) contains \\(L+1\\) codewords \\(c_0,\\dots,c_L\\) with \\(d_R(c_j, y) \\le \\rho n\\). Define the difference vectors \\(v_j = c_j - y\\). With the syndrome map \\(\\sigma(x) = H x^{\\top}\\), we have \\(\\sigma(c_j) = \\sigma(y)\\). For any \\(j>0\\), set \\(u_j := v_j - v_0 \\in \\ker(H)\\).', 'concept_3': 'Define the rank support \\(\\mathrm{RS}(v) \\subseteq \\mathbb{F}_q^n\\) whose dimension equals \\(\\operatorname{rank}_{\\mathbb{F}_q}(v)\\). Define the sum-space \\[ W := \\mathrm{RS}(v_0) + \\mathrm{RS}(v_1) + \\cdots + \\mathrm{RS}(v_L) \\subseteq \\mathbb{F}_q^n. \\]', 'concept_4': 'If the rank ball of radius \\(\\rho n\\) indeed contains these \\(L+1\\) codewords, then \\[ \\dim(W) > L \\cdot (n - k). \\] (Otherwise, there would exist a nonzero \\(\\mathbb{F}_q\\)-linear constraint simultaneously satisfied by these \\(L+1\\) vectors, a contradiction.) Therefore, for \\(C\\) to be \\((\\rho, L)\\)-list decodable, any such \\(W\\) generated by \\(L+1\\) difference vectors within radius \\(\\rho n\\) must satisfy \\[ \\dim(W) \\le L \\cdot (n - k). \\]'}	\(\rho \le \frac{L(1 - R)}{L + 1}\)	理论基础学科-理论计算机科学-计算机复杂度	880325	3	4
recuVeYQELiXnE	In the planar N-body problem, it is known that for a solution tending to a total collision, its normalized configuration curve must converge to the set of normalized central configurations. Specifically, if such a total collision solution tends to an isolated central configuration(up to rotation symmetry), it might theoretically converge to a circle (rather than a specific point on the circle) through infinite spin. The core question is: Does the phenomenon of infinite spin occur for a solution tending to a total collision in the planar N-body problem?	"{'step_1': ""By McGehee's blow-up, collision point becomes equilibrium points of new equations and collision solution becomes a solution converging to a equilibrium point."", 'step_2': 'From Center Mainfold Theorem, the flow of new equations near equilibrium point can be converged by flow on the center manifold.', 'step_3': 'By Lojasiewicz gradient inequality, the flow on the center manifold is approximately a gradient flow and the arclength of this flow is finite by the inequality, which means infinite spin does not occur.'}"	{'concept_1': 'McGehee’s blow-up: Introduce rescaled velocity variables $v=\\sqrt{r}\\rho, w=r^{\\frac{3}{2}}\\omega$ and a new time variable $\\tau$ such that $d\\tau=r^{-\\frac{3}{2}}dt.', 'concept_2': 'Center Manifold Theorem: Let $f:\\mathbb{R}^n \\to \\mathbb{R}^n$ be a vector field in $\\mathcal{C}^{k+1}$, with $f(0)=0$. Consider the matrix $A=Df(0)$, and let $V^s, V^u, V^c$ be the corresponding stable, unstable, center subspaces. Then there exists $\\delta>0$ and a local center manifold $\\mathcal{M}$ with the following properties. (i) There exists a $\\mathcal{C}^k$ function $\\phi:V^c \\to \\matbb{R}^n$ with $\\phi_c \\phi(x_c)=x_c$ such that $\\mathcal{M}=\\{\\phi(x_c): x_c\\in V^c, |x_c|<\\delta \\}$. (ii) The manifold $\\mathcal{M}$ is locally invariant for the flow of equation $\\dot{x}=f(x)$. (iii) $\\mathcal{M}$ is tangent to $V^c$ at the origin. (iv) Given an trajectory such that $x(t)\\to 0$ as $t\\to +\\infty$, there exists $\\eta>0$ and a trajectory $t\\to y(t)\\in \\mathcal{M}$ on the center manifold such that $e^{\\eta t}|x(t)-y(t)|\\to 0 as t \\to +\\infty$.', 'concept_3': 'Lojasiewicz gradient inequality: Let $f:U\\to \\matbb{R}$ be a real-analytic function an an open set $U$ in $\\mathbb{R}^n$. The for every $p\\in U$, there is a possibly smaller neighborhood $W$ of $p$ and constants $\\beta\\in (0,1)$ and $C>0$ such that for all $x\\in W$, $|f(x)-f(p)|^\\beta\\le C|\\nabla f(x)|$.'}	No(AI should mention that the question finally can be transferred to prove the integral θ(τ)=∫∣w(s)∣ds is finite by using Lojasiewicz gradient inequality)	理论基础学科-基础数学-分析	131126	2	3
recuUOxzww7BBL	Let G be a graph embedded on the Klein quartic surface. What is the mixing time of the Glauber dynamics for (Δ+Δ^0.7)-proper colorings on G? Answer the following: 1. summarize the technique you uses in one sentence. 2. the spectral norm of G. 3. express the mixing time with the big-O symbol of n and Δ.	"{'step 1': 'Recast Theorem 1 with k=Δ+Δ^0.7: in order to apply theorem 1, we need some constant ε∈(0,1) such that k ≥ Δ + ρ/(1-ε), i.e., Δ^0.7 ≥ ρ/(1-ε).', 'step 2': 'Notice that the Klein quartic surface has genus 3, so a graph embedded on it has bounded genus.', 'step 3': ""It's a property of bounded genus graphs that there exists an orientation with bounded maximum outdegree d (d is a function of the genus); then apply Theorem 2 to get ρ ≤ 2√(d(Δ-d)) = O(√Δ)."", 'step 4': 'Compare magnitudes: ρ/Δ^0.7 = O(Δ^-0.2) → 0; hence we can choose a constant ε bounded away from 0 (e.g., ε=½ for large Δ) to satisfy Theorem 1’s condition.', 'step 5': 'Substitute into Theorem 1 to conclude t_mix(δ) ≤ (n/ε)log(n/δ) = O(n log n) for fixed δ.'}"	{'theorem_1': 'Heat-bath Glauber dynamics on proper k-colorings of a graph with maximum degree Δ and principal eigenvalue ρ has mixing time τ(δ)≤ (n/ε)log(n/δ) whenever k≥ Δ + ρ/(1-ε), where n=|V|.', 'theorem_2': 'Let G be an undirected graph with maximum degree Δ that admits an orientation with maximum outdegree d≤ Δ/2; then the principal eigenvalue ρ=λ_max(A_G) satisfies ρ≤ 2√(d(Δ-d)).'}	1. The technique is to first bound the principal eigenvalue (spectral norm) of G, then bound the mixing time using this eigenvalue. 2. O(√Δ). 3. O(n log n).	理论基础学科-理论计算机科学-算法理论	83885	2	1
recuUTiYakLbC5	Under the flow matching framework, how to use the probabilistic path $p(\boldsymbol{x}_t | \boldsymbol{x}_0, \boldsymbol{x}_1)$ defined by the optimal transport theory to derive the expected form of the target velocity field for training neural networks?	"{'step_1': 'First, to derive the expected form of the target velocity field $\\boldsymbol{v}(\\boldsymbol{x}_t, t)$, it is necessary to integrate three core concepts: the flow matching framework, optimal transport coupling, and linear interpolation probabilistic path. The target velocity field defines the direction and magnitude of motion of data points during their transformation from the source distribution $p_0$ to the target distribution $p_1$.', 'step_2': 'According to the concept of **linear interpolation probabilistic path**, a path from a source point $\\boldsymbol{x}_0$ to a target point $\\boldsymbol{x}_1$ can be expressed as $\\boldsymbol{x}_t = (1-t)\\boldsymbol{x}_0 + t\\boldsymbol{x}_1$, where $t \\in [0, 1]$. This path is a linear path under the **optimal transport coupling** $\\pi(\\boldsymbol{x}_0, \\boldsymbol{x}_1)$. The instantaneous velocity of this path, i.e., its derivative with respect to time $t$, is $\\frac{d\\boldsymbol{x}_t}{dt} = \\boldsymbol{x}_1 - \\boldsymbol{x}_0$. However, this velocity is constant and cannot be directly used to define a time-varying velocity field. To obtain the average velocity of all particles located at $\\boldsymbol{x}_t$ at time $t$, we need to consider all pairs $(\\boldsymbol{x}_0, \\boldsymbol{x}_1)$ composed of all possible source points $\\boldsymbol{x}_0$ and target points $\\boldsymbol{x}_1$—specifically, those pairs whose linear interpolation paths pass through the point $\\boldsymbol{x}_t$ exactly at time $t$.', 'step_3': ""The target velocity field $\\boldsymbol{v}(\\boldsymbol{x}_t, t)$ is defined as the average of the instantaneous velocities of all trajectories passing through the point $\\boldsymbol{x}_t$ at time $t$. From the derivation in Step 2, the instantaneous velocity of each trajectory is $(\\boldsymbol{x}_1 - \\boldsymbol{x}_0)$. Therefore, we need to calculate the **conditional expectation** of $(\\boldsymbol{x}_1 - \\boldsymbol{x}_0)$ given $\\boldsymbol{x}_t$. By the definition of conditional expectation, the target velocity field can be expressed as: $\\boldsymbol{v}(\\boldsymbol{x}_t, t) = \\mathbb{E} \\left[ \\frac{d\\boldsymbol{x}_t}{dt} \\mid \\boldsymbol{x}_t \\right]$. Substituting the derivative of the linear interpolation path into the equation, we obtain: $\\boldsymbol{v}(\\boldsymbol{x}_t, t) = \\mathbb{E} \\left[ \\boldsymbol{x}_1 - \\boldsymbol{x}_0 \\mid \\boldsymbol{x}_t \\right]$. Furthermore, to make the denominator more intuitive (representing the 'remaining time'), we can transform the formula. Since $\\boldsymbol{x}_t = (1-t)\\boldsymbol{x}_0 + t\\boldsymbol{x}_1$, we can derive that $\\boldsymbol{x}_1 - \\boldsymbol{x}_0 = \\frac{\\boldsymbol{x}_1 - \\boldsymbol{x}_t}{1-t}$. Substituting this relationship into the above conditional expectation formula gives: $\\boldsymbol{v}(\\boldsymbol{x}_t, t) = \\mathbb{E} \\left[ \\frac{\\boldsymbol{x}_1 - \\boldsymbol{x}_t}{1-t} \\mid \\boldsymbol{x}_t \\right]$. To derive the final form in the paper, $\\boldsymbol{v}(\\boldsymbol{x}_t, t) = \\mathbb{E} \\left[ \\frac{\\boldsymbol{x}_1 - \\boldsymbol{x}_0}{1-t} \\mid \\boldsymbol{x}_t \\right]$, the derivation is actually based on a more precise reasoning: the velocity field is defined as the flow vector $\\boldsymbol{V}(\\boldsymbol{x},t)$, which is related to the probability flow through the equation $\\frac{\\partial p(\\boldsymbol{x},t)}{\\partial t} + \\nabla \\cdot (p(\\boldsymbol{x},t)\\boldsymbol{V}(\\boldsymbol{x},t))=0$. For the simplest linear interpolation flow that satisfies this equation, its velocity field can be proven to be $\\boldsymbol{v}(\\boldsymbol{x},t) = \\frac{\\mathbb{E} [\\boldsymbol{x}_1|\\boldsymbol{x}_t=\\boldsymbol{x}] - \\boldsymbol{x}}{1-t}$, which is equivalent to $\\mathbb{E} \\left[ \\frac{\\boldsymbol{x}_1 - \\boldsymbol{x}_0}{1-t} \\mid \\boldsymbol{x}_t \\right]$—since $\\boldsymbol{x}_t$ is a linear combination of $\\boldsymbol{x}_0$ and $\\boldsymbol{x}_1$. Thus, this formula is a direct result of the linear interpolation path and conditional expectation, and is consistent with the definition in the paper.""}"	"{'concept_1': ""Optimal Transport Coupling. This is a joint probability distribution $\\pi(\\boldsymbol{x}_0, \\boldsymbol{x}_1)$, where $\\boldsymbol{x}_0$ is a point from the source distribution $p_0$ and $\\boldsymbol{x}_1$ is a point from the target distribution $p_1$. This coupling describes how to 'transport' points from the source distribution to points in the target distribution at the minimum cost. In flow matching, it provides the basis for defining the probabilistic path from $p_0$ to $p_1$."", 'concept_2': 'Linear Interpolation Probabilistic Path. In the paper, an optimal transport coupling is used to define a straight-line path from the source distribution $p_0$ to the target distribution $p_1$. At time $t \\in [0, 1]$, the point $\\boldsymbol{x}_t$ on the path is a linear combination of the source point $\\boldsymbol{x}_0$ and the target point $\\boldsymbol{x}_1$, i.e., $\\boldsymbol{x}_t = (1-t)\\boldsymbol{x}_0 + t\\boldsymbol{x}_1$. The derivative of this path serves as the basis for the velocity field.', 'concept_3': 'Conditional Expectation. This is a mathematical concept used to calculate the expected value of a random variable given a specific condition. In the derivation of flow matching, to obtain the expectation of the target velocity field at a certain point $\\boldsymbol{x}_t$, it is necessary to average all possible trajectories passing through this point $\\boldsymbol{x}_t$, which is precisely achieved by computing the conditional expectation over the $(\\boldsymbol{x}_0, \\boldsymbol{x}_1)$ pairs.'}"	The target velocity field $\boldsymbol{v}(\boldsymbol{x}_t, t)$ can be defined as the conditional expectation of the average velocity at which all particles located at point $\boldsymbol{x}_t$ at time $t$ move to the target distribution $\boldsymbol{x}_1$ within the remaining time $(1-t)$. Its specific form is: $\boldsymbol{v}(\boldsymbol{x}_t, t) = \mathbb{E} \left[ \frac{\boldsymbol{x}_1 - \boldsymbol{x}_0}{1-t} \mid \boldsymbol{x}_t \right]$. This formula directly connects the probabilistic path defined in optimal transport with the concept of the velocity field, providing a clear training objective for neural network learning.	理论基础学科-理论计算机科学-算法理论	118671	1	3
recuVcTHwoeLyL	"Please use the following two parameters: effective internal free energy difference and fixed local environment approximation and above concepts to find out :the ""core mechanism"" by which nonequilibrium chemical reactions regulate nucleation kinetics? Please give the details about the regulation step by step. "	"{'step_1': 'Clarify the core question - Identify the core mechanism by which nonequilibrium chemical reactions regulate nucleation kinetics, starting from the classification of nonequilibrium chemical reactions in the paper.', 'step_2': 'Distinguish between homogeneous and heterogeneous nonequilibrium chemical reactions - According to the paper, homogeneous reactions have a constant \\(k_{I \\to B}\\) (independent of the local environment), while heterogeneous reactions have \\(k_{I \\to B}\\) dependent on the local potential energy \\(u\\) (following the Metropolis form). The paper emphasizes that only heterogeneous reactions can induce unique regulatory effects on nucleation kinetics, so the core mechanism must be associated with heterogeneous reactions.', 'step_3': 'Analyze the effect of heterogeneous reactions on phase free energy differences - Using the definition of the effective internal free energy difference \\(\\Delta f\\) in the paper (\\(\x08eta \\Delta f = -\\ln(\rho_B/\rho_I) + \\ln \\langle e^{-\x08eta \\Delta u_{I \\to B}} \rangle_I\\)), heterogeneous reactions cause \\(k_{I \\to B}\\) to depend on \\(u\\), leading to different \\(\\Delta f\\) values in the liquid (\\(\\Delta f_l\\)) and vapor (\\(\\Delta f_v\\)) phases, resulting in a non-zero phase free energy difference \\(\\Delta \\Delta f = \\Delta f_l - \\Delta f_v\\). This breaks the consistency of free energy differences across phases in equilibrium systems and is the initial driver of subsequent interfacial property changes.', 'step_4': 'Establish the link between \\(\\Delta \\Delta f\\) and non-equilibrium interfacial tension - Based on the Fixed Local Environment Approximation (FLEX) , which assumes that particle exchange with the reservoir equilibrates faster than changes in the local environment around a tagged lattice site, FLEX first realizes the mapping from the nonequilibrium steady state (NESS) to an effective equilibrium system by defining **effective fugacities** (the core tool for quantitative connection): the effective fugacity of bonding-state (B) particles is \\(\tilde{z}_B = \x0crac{\tilde{\rho}_B}{\tilde{\rho}_E} e^{\x08eta u}\\), and that of inert-state (I) particles is \\(\tilde{z}_I = \x0crac{\tilde{\rho}_I}{\tilde{\rho}_E}\\) , . Here, \\(\tilde{\rho}_B\\), \\(\tilde{\rho}_I\\), and \\(\tilde{\rho}_E\\) are the ""effective equilibrium densities"" mapped by FLEX (corresponding to the steady-state densities of B-state, I-state particles, and empty lattice sites in the nonequilibrium system), and \\(u\\) is the local potential energy determined by nearest-neighbor particle interactions. For heterogeneous reactions where \\(k_{I \\to B}\\) depends on \\(u\\) (decreasing as \\(u\\) decreases), the liquid phase, vapor phase, and liquid-vapor interface have distinct local \\(u\\) values, leading to different \\(\tilde{z}_B/\tilde{z}_I\\) ratios. Since the effective internal free energy difference \\(\x08eta \\Delta f = -\\ln(\tilde{z}_B/\tilde{z}_I)\\) , this further results in \\(\\Delta f\\) differences between phases (\\(\\Delta \\Delta f = \\Delta f_l - \\Delta f_v \neq 0\\)) and between the interface and bulk phases (with lower \\(\\Delta f\\) at the interface). The lower \\(\\Delta f\\) at the interface enhances the ""chemical activity"" of B-state particles, leading to the enrichment of B-state particles at the interface (i.e., higher \\(\tilde{\rho}_B\\) at the interface compared to the vapor phase and even the bulk liquid phase) . Such enrichment reduces the effective adatom bonding energy \\(\tilde{\\epsilon}\\) at the interface (satisfying \\(|\x08eta \tilde{\\epsilon}| \\leq |\x08eta \\epsilon|\\)), where \\(\tilde{\\epsilon}\\) is calculated via FLEX based on the interfacial \\(\tilde{\rho}_B\\) as \\(\x08eta \tilde{\\epsilon} = \\ln\\left[\x0crac{\tilde{\rho}_B}{1 - \tilde{\rho}_B}\right]_{u=\\epsilon} - \x08eta \\epsilon\\) . Finally, substituting \\(\tilde{\\epsilon}\\) into the interfacial tension formula of the solid-on-solid model (a tool for calculating interfacial tension under equilibrium-like conditions) yields the non-equilibrium interfacial tension \\(\\sigma\\), which deviates from the equilibrium interfacial tension \\(\\sigma_{eq}\\) .', 'step_5': 'Connect non-equilibrium interfacial tension to the nucleation free energy barrier - Extending Classical Nucleation Theory (CNT) to non-equilibrium steady states (NESS), the paper retains CNT’s core premise (rate-limiting step is critical nucleus formation) but replaces \\(\\sigma_{eq}\\) with non-equilibrium \\(\\sigma\\) in the free energy landscape. The nucleation free energy barrier \\(\\Delta F^* = F_{\\text{noneq}}(n^*) - F_{\\text{noneq}}(1)\\) is thus modified: reduced \\(\\sigma\\) lowers \\(\\Delta F^*\\), while increased \\(\\sigma\\) raises it. This directly alters the difficulty of nucleation.', 'step_6': 'Confirm the regulation of nucleation rate by \\(\\Delta F^*\\) - Per the paper’s CNT nucleation rate formula \\(J = \rho_1 D^* \\Gamma \\exp(-\x08eta \\Delta F^*)\\), each parameter’s physical meaning is as follows: \n- \\(\rho_1\\): Number density of bonding-state (B) monomers in the metastable vapor phase (the ""building blocks"" for nucleus formation). \n- \\(D^*\\): Diffusion coefficient of nucleus size (the reaction coordinate) near the critical nucleus size \\(n^*\\), quantifying how fast nucleus size fluctuates around this critical size (where the nucleus is equally likely to grow into liquid or dissolve back into vapor). \n- \\(\\Gamma\\): Zeldovich factor, correcting for the probability that a nucleus reaching \\(n^*\\) grows into a stable phase (instead of dissolving), calculated by fitting the nucleus’ ""commitment probability"" (to become stable liquid) to a harmonic barrier near \\(n^*\\). \n- \\(\x08eta\\): Inverse temperature parameter, \\(\x08eta = (k_B T)^{-1}\\) ( \\(k_B\\) = Boltzmann constant, \\(T\\) = absolute temperature). \n- \\(\\Delta F^*\\): Nucleation free energy barrier (\\(\\Delta F^* = F(n^*) - F(1)\\)), the minimum free energy needed to form a stable nucleus ( \\(F(n^*)\\) = free energy of critical nucleus, \\(F(1)\\) = free energy of one B-monomer). \\(\\Delta F^*\\) relates exponentially to \\(J\\) (rate of stable nucleus formation per unit volume). Heterogeneous reactions reduce non-equilibrium interfacial tension \\(\\sigma\\), lowering \\(\\Delta F^*\\) and greatly increasing \\(J\\); a higher \\(\\Delta F^*\\) (from larger \\(\\sigma\\)) slows \\(J\\). This closes the regulatory chain: heterogeneous non-equilibrium reactions → modulated interfacial properties → changed \\(\\Delta F^*\\) → regulated nucleation rate.', 'step_7': 'Summarize the core mechanism - Heterogeneous nonequilibrium chemical reactions first induce a non-zero \\(\\Delta \\Delta f\\), which leads to non-equilibrium interfacial tension \\(\\sigma\\) via FLEX and interface \\(B\\)-state enrichment. \\(\\sigma\\) modifies the nucleation free energy barrier \\(\\Delta F^*\\) in the extended CNT framework, ultimately regulating the nucleation rate. This is the core mechanism, distinguishing it from equilibrium systems where nucleation rate depends solely on thermodynamic driving forces (e.g., supersaturation).'}"	"{'concept_1': 'Heterogeneous (Inhomogeneous) Chemical Reactions - Nonequilibrium reactions where the rate constant \\(k_{I \\to B}\\) (particle conversion rate from inert state \\(I\\) to bonding state \\(B\\)) depends on the local potential energy \\(u\\) (potential energy from interactions between a lattice site and its nearest-neighbor particles). The rate constant follows the Metropolis form: \\(k_{I \\to B} = k^\\circ \\min\\left[1, \\exp(-\\beta u - \\beta \\Delta f_{res} - \\beta \\Delta \\mu)\\right]\\), where: \n- \\(k^\\circ\\): Constant reference rate (ratio of timescales for chemical reactions and particle transport, constant for homogeneous reactions), \n- \\(\x08eta\\): Inverse temperature parameter (\\(\x08eta = (k_B T)^{-1}\\), \\(k_B\\) is Boltzmann constant, \\(T\\) is absolute temperature), \n- \\(\\Delta f_{res}\\): Internal free energy difference between \\(B\\) and \\(I\\) states in the particle reservoir, \n- \\(\\Delta \\mu\\): Nonequilibrium chemical potential drive (breaks time-reversal symmetry when \\(\\Delta \\mu \\neq 0\\)). \nThis type of reaction is distinct from homogeneous reactions, where \\(k_{I \\to B} = k^\\circ\\) (constant, independent of local environment).', 'concept_2': ""Non-Equilibrium Interfacial Tension (\\(\\sigma\\)) - Interfacial tension at the liquid-vapor interface of a NESS system, which deviates from the equilibrium interfacial tension (\\(\\sigma_{eq}\\)). It is generated by the enrichment of \\(B\\)-state particles at the interface (caused by heterogeneous reactions) and is derived via the solid-on-solid model using the formula: \\(\\sigma(\\tilde{\\epsilon}) = \\sqrt{\\frac{4 \\tilde{\\epsilon} \\beta^{-2}}{\\pi \\chi(\\beta)} \\int_{\\beta_c}^{\\beta} K'\\left(\\frac{8\\left[\\cosh(\\beta' \\tilde{\\epsilon}) - 1\\right]}{\\left[\\cosh(\\beta' \\tilde{\\epsilon}) + 1\\right]^2}\\right) \\left[\\frac{\\cosh(\\beta' \\tilde{\\epsilon}) - 3}{\\sinh(\\beta' \\tilde{\\epsilon})}\\right] d\\beta'}\\), where: \n- \\(\tilde{\\epsilon}\\): Effective adatom bonding energy (bonding energy of a single \\(B\\)-state adatom attached to the liquid-vapor interface, satisfying \\(|\\beta \\tilde{\\epsilon}| \\leq |\\beta \\epsilon|\\), \\(\\epsilon\\) is equilibrium bonding energy of \\(B\\)-state particles), \n- \\(\\chi(\\beta)\\): Correction term for interfacial tension calculation (\\(\\chi(\\beta) = [1 - \\sinh^{-4}(\\beta \\tilde{\\epsilon}/2)]^{1/8}\\)), \n- \\(\x08eta_c\\): Inverse critical temperature (temperature at the critical point of phase separation, satisfying \\(\x08eta_c |\\tilde{\\epsilon}| = 2\\ln(1+\\sqrt{2})\\)), \n- \\(K'\\): Elliptic integral of the first kind (mathematical function for integrating non-linear terms in interfacial tension), \n- \\(\x08eta'\\): Dummy variable of integration (corresponds to \\(\x08eta\\) at different temperatures)."", 'theorem_1': 'Classical Nucleation Theory (CNT) for NESS - An extended framework of equilibrium CNT, applicable to nucleation at NESS. It retains the core premise of equilibrium CNT (the rate-limiting step of nucleation is the formation of a critical nucleus of the stable phase) but modifies the interfacial tension term to account for non-equilibrium effects. Key components include: \n1. Equilibrium CNT free energy landscape (basis for modification): \\(\x08eta F(n) = \x08eta \\sigma_{eq} \\sqrt{4\\pi n} - \x08eta \\Delta \\Phi n + \\frac{5}{4}\\ln n + d\\), where: \n - \\(n\\): Nucleus size (number of \\(B\\)-state particles in the nucleus), \n - \\(\\sigma_{eq}\\): Equilibrium interfacial tension (interfacial tension at thermal equilibrium), \n - \\(\\Delta \\Phi\\): Bulk thermodynamic driving force (difference in thermodynamic potential between the stable liquid phase and metastable vapor phase), \n - \\(d\\): Constant (chosen to match the \\(B\\)-state monomer number density in the vapor phase \\(\rho_1\\) with \\(\\exp[-\\beta F(1)]\\)). \n2. Non-equilibrium nucleation free energy barrier: \\(\\Delta F^* = F_{\\text{noneq}}(n^*) - F_{\\text{noneq}}(1)\\), where: \n - \\(F_{\\text{noneq}}(n)\\): Non-equilibrium free energy landscape (obtained by replacing \\(\\sigma_{eq}\\) with non-equilibrium \\(\\sigma\\) in the equilibrium landscape), \n - \\(n^*\\): Critical nucleus size (nucleus size where the commitment probability to the stable phase is 1/2, i.e., \\(\\phi(n^*) = 1/2\\)).'}"	"The ""core molecular mechanism"" by which nonequilibrium chemical reactions regulate nucleation kinetics is that   1. Inhomogeneous chemical reactions change the local particle state distribution,   2. This results in a non - zero effective free energy difference (ΔΔf) between the liquid and vapor phases.   3. This generates a nonequilibrium interfacial tension (σ),   4. Then it in turn changes the nucleation barrier (ΔF*), and ultimately regulates the nucleation rate. ( Please carefully check the steps.)"	理论基础学科-理论化学-化学反应动力学理论	262985	2	6
recuVhQ0yiUSLT	What is the role of STN1 in the metastasis of pancreatic ductal adenocarcinoma (PDAC) and its molecular mechanisms, and can it serve as a potential therapeutic target?  Hint:  HINT_1: By analyzing the TCGA-GTEx database, GEO (Gene Expression Omnibus), and scRNA-seq datasets, 692 genes abnormally upregulated in tumor tissues and malignant ductal cells were identified, among which 12 genes were associated with poor overall survival and disease-free interval.  HINT_2: STN1 expression is elevated in PDAC tumor tissues; this phenotype was confirmed by the TCGA-GTEx and tissue microarray datasets.  HINT_3: Enhanced STN1 expression is present in all types of cells within tumor tissues, with the highest expression in malignant ductal cells compared to normal pancreas.  HINT_4: Other CST complex components: CTC1 did not show the same expression pattern as STN1, while TEN1 had a similar expression pattern, but its high expression led to better outcomes in PDAC patients, opposite to STN1.  HINT_5: STN1 protein levels are elevated in four human pancreatic cancer cell lines (MIA PaCa-2, PANC-1, AsPC-1, and BxPC-3). This phenotype was also observed in tumor tissues by immunohistochemical (IHC) staining.  HINT_6: STN1 protein levels are independent of the size of the primary tumor.  HINT_7: High STN1 expression is significantly associated with reduced survival.  HINT_8: Altering STN1 protein levels does not markedly affect pancreatic cancer cell proliferation.  HINT_9: STN1 knockdown significantly weakens pancreatic cancer cell migration and invasion, both of which are restored upon STN1 reintroduction.  HINT_10: The metastatic and invasive abilities of MIA PaCa-2 cells increase proportionally with STN1 protein levels.  HINT_11: STN1 protein levels were examined in various organs of Stn1 heterozygous (Stn1-/+) mice, showing a marked reduction without compensatory upregulation.  HINT_12: Offspring of Stn1-/+ mice crossed with KPC mice (KPCS mice) displayed normal embryonic development and pancreatic phenotypes.  HINT_13: KPCS mice exhibited significantly reduced liver and lung metastasis compared with KPC mice.  HINT_14: Using JASPAR, four potential upstream transcription factors of STN1 were identified: SNAI2, PPARD, HOXB7, and ZBTB7A. They are upregulated in PDAC tumor tissues and correlate with STN1 expression levels in PDAC patients.  HINT_15: Reduction of HOXB7 and ZBTB7A weakened STN1 mRNA levels to a greater extent than SNAI2 and PPARD reduction.  HINT_16: Elevated HOXB7 levels significantly enhanced transcriptional activity of the STN1-luciferase reporter system; this enhancement was abolished by mutating the HOXB7 binding site in the STN1 promoter.  HINT_17: ChIP-qPCR results showed that HOXB7 directly binds to the STN1 promoter region.  HINT_18: Analysis of cDNA from 55 human PDAC tissues revealed co-expression of HOXB7 and STN1.  HINT_19: Reduction of HOXB7 expression led to decreased STN1 protein levels.  HINT_20: ZBTB7A overexpression had no significant effect on STN1-luciferase transcriptional activity.  HINT_21: In STN1 knockdown PANC-1 cells, the EMT pathway was significantly downregulated; epithelial markers were increased, while mesenchymal markers and EMT-related transcription factors (EMT-TFs) were decreased.  HINT_22: STN1 knockdown inhibited EMT progression in PANC-1, AsPC-1, and BxPC-3 cells, which was restored after STN1 reintroduction.  HINT_23: In MIA PaCa-2 cells, STN1 reduction decreased vimentin expression, while E-cadherin levels remained unchanged.  HINT_24: In KPC mice, reduced STN1 led to decreased epithelial marker E-cadherin and increased mesenchymal marker vimentin.  HINT_25: In STN1 knockdown cell lines and KPCS mouse tumor tissues, STN1 reduction was associated with decreased mRNA and protein expression of the EMT-TF ZEB1, but not with SNAI1/SNAI2.  HINT_26: STN1 enhanced transcriptional activity of the ZEB1-luciferase reporter system.  HINT_27: STN1 bound extensively to the promoter region of ZEB1.  HINT_28: Reintroducing ZEB1 into STN1 knockdown MIA PaCa-2 cells enhanced their invasive and migratory abilities.  HINT_29: Co-IP confirmed an interaction between STN1 and STAT3.  HINT_30: Reduction of STN1 was associated with decreased STAT3 binding affinity for the ZEB1 promoter.  HINT_31: CUT&Tag results showed that STN1 and STAT3 can bind to the same region of the ZEB1 promoter.  HINT_32: STAT3 inhibition weakened STN1’s enhancement of ZEB1-luciferase activity.  HINT_33: Deletion of STN1’s OB fold and wHTH2 domains did not affect its interaction with STAT3, but deletion of the wHTH1 domain disrupted STAT3 binding.  HINT_34: The isolated wHTH1 domain alone can bind STAT3.  HINT_35: Reintroducing STN1-wHTH2D and STN1-wHTH1 effectively restored ZEB1 expression, while reintroducing STN1-OBD and STN1-wHTH1D did not.  HINT_36: Reintroduction of STN1-wHTH2D into STN1 knockdown cells significantly rescued impaired migration and invasion, whereas reintroducing STN1-OBD had no effect on tumor metastasis in vitro or in vivo.  HINT_37: STN1 may bind to ssDNA displaced near STAT3’s binding site in the ZEB1 promoter R-loop, and STAT3 can bind to this region through interaction with STN1 at the R-loop.  HINT_38: Telomere length was not different between KPC and KPCS mice.  HINT_39: Reduced STN1 in pancreatic cancer cell lines and in the pancreas of KPC mice caused telomere dysfunction.  HINT_40: STN1 significantly rescued MIA PaCa-2 cell metastatic ability, whereas STN1-OBD reintroduction failed to restore migration in shSTN1 cells.  HINT_41: Treating STN1-overexpressing cells with S3I-201 reduced transcriptional activity of the ZEB1-luciferase reporter system.  HINT_42: In the presence of a STAT3 inhibitor, both STN1-overexpressing and PCDH-control cell lines exhibited reduced invasion and migration compared with vector controls.  HINT_43: Treating STN1 knockdown pancreatic cells with a STAT3 inhibitor also led to significantly decreased ZEB1 expression and slowed EMT progression.	1. Bioinformatics analysis and mRNA expression detection in PDAC tumor tissues revealed that STN1 is highly expressed in both pancreatic cancer cell lines and tumor tissues. STN1 overexpression is independent of tumor size but is associated with poorer patient survival, suggesting that STN1 may function in PDAC metastasis rather than tumor growth and has potential as a clinical therapeutic target.    2. Since altering STN1 protein levels does not affect pancreatic cancer cell proliferation, but STN1 knockdown reduces the migratory and invasive abilities of tumor cells—phenotypes that are rescued upon STN1 reintroduction—and elevated STN1 enhances the migration and invasion of MIA PaCa-2 cells, this indicates that STN1 promotes PDAC progression by enhancing metastatic capacity rather than proliferation.    3. KPCS mice exhibit normal embryonic development and pancreatic phenotypes but significantly reduced liver and lung metastases, indicating that STN1 plays a critical role in promoting tumor metastasis and thereby accelerating PDAC progression.    4. HOXB7 and STN1 are co-expressed; elevated HOXB7 enhances STN1 transcription, while decreased HOXB7 reduces STN1 protein levels. Moreover, HOXB7 directly binds the STN1 promoter region. Therefore, HOXB7 is an upstream transcription factor of STN1, and its high expression in PDAC patients may contribute to increased STN1 expression.    5. Alterations in STN1 expression affect epithelial markers (e.g., E-cadherin) and mesenchymal markers (e.g., vimentin) in different pancreatic cancer cell lines and mouse models. STN1 knockdown suppresses EMT, while reintroduction of STN1 restores it, indicating that STN1 promotes PDAC metastasis by facilitating EMT progression.    6. Dual-luciferase reporter assays showed that STN1 enhances the transcriptional activity of the ZEB1-luciferase reporter system. ChIP-qPCR demonstrated strong enrichment of STN1 at the ZEB1 promoter region, but not at the ACTB locus (negative control). Furthermore, reintroducing ZEB1 into STN1-knockdown MIA PaCa-2 cells restored invasive and migratory abilities, indicating that STN1 promotes PDAC metastasis by enhancing ZEB1 transcription.    7. S-Tag pull-down and Co-IP assays revealed an interaction between STN1 and STAT3. ChIP-qPCR showed that STN1 knockdown significantly reduced STAT3 binding at the ZEB1 promoter, and CUT&Tag sequencing confirmed co-occupancy of STN1 and STAT3 at the same ZEB1 promoter region. These results indicate that STN1 recruits STAT3 to the ZEB1 promoter to cooperatively promote its transcription.    8. Treatment with the STAT3 inhibitor S3I-201 markedly reduced STN1’s ability to activate the ZEB1 promoter, suggesting that STN1-mediated transcriptional activation of ZEB1 depends on STAT3 activity.    9. Domain-deletion mutants of STN1 showed that the wHTH1 domain is essential for interaction with STAT3, while the OB domain mediates ssDNA binding but is dispensable for STN1–STAT3 interaction. Co-IP confirmed that deleting the OB fold or wHTH2 domain did not disrupt interaction with STAT3, whereas deleting wHTH1 abolished it; notably, the wHTH1 domain alone was sufficient to bind STAT3. Functional assays showed that reintroduction of the wHTH2 domain restored invasion and migration defects caused by STN1 knockdown, whereas the OB domain had no effect. These findings suggest that STN1 promotes PDAC migration and ZEB1 transcriptional activation through its protein-binding (wHTH1) and ssDNA-binding (OB) affinities.    10. STAT3 inhibitors significantly suppressed the migration and invasion of STN1-overexpressing cells, with stronger inhibitory effects than in controls. In MIA PaCa-2 and PANC-1 cells overexpressing STN1, treatment with STAT3 inhibitors (e.g., S3I-201, napabucasin) led to a greater reduction in migration and invasion compared with controls. 3D spheroid invasion assays confirmed that STAT3 inhibition more strongly suppressed invasion in STN1-overexpressing cells. These results indicate that PDAC cells with high STN1 expression are more sensitive to STAT3 inhibition, supporting the potential of STAT3 inhibitors in suppressing metastasis.    11. STAT3 inhibitors markedly reduced the invasive area of STN1-overexpressing cells, with stronger effects than in controls, further validating their heightened sensitivity to STAT3 inhibition.    12. Following STAT3 inhibitor treatment, ZEB1 expression decreased, epithelial marker E-cadherin increased, and mesenchymal marker vimentin decreased, with more pronounced changes in STN1-overexpressing cells. This suggests that STAT3 inhibitors suppress tumor metastasis by inhibiting ZEB1 transcription and EMT progression, and the effect is stronger in the context of STN1 overexpression.	Concept_1: STN1 is an essential member of the CST complex (CTC1-STN1-TEN1) for maintaining telomere length and genome stability. This complex participates in preserving DNA stability at the genomic level, with a preference for binding GC-rich regions, unfolded G-quadruplex structures, and facilitating RAD51 recruitment in response to replication stress.  Concept_2: EMT (epithelial-mesenchymal transition) is a reversible cellular program in which malignant tumor cells transition from a stable and orderly epithelial state to a more motile mesenchymal state.  Concept_3: HOXB7 is a member of the transcription factor HOX gene family. By targeting downstream genes within oncogenic hierarchies, it is associated with tumor progression and poor outcomes in multiple cancer types.  Concept_4: ZEB1 is a key factor in PDAC metastasis; deletion of Zeb1 in KPC mice is sufficient to suppress tumor metastasis.  Concept_5: During transcription, RNA-DNA hybrids generate a special chromatin structure called an “R-loop.” This structure can trigger replication fork collisions and Pol II stalling, posing risks to effective transcription elongation and contributing to disease development.  Concept_6: KPC mice carry the LSL-Kras G12D oncogenic mutation, the LSL-Trp53 R172H (p53 loss-of-function mutation), and Pdx1-Cre. These three genetic drivers synergize to induce spontaneous pancreatic ductal adenocarcinoma (PDAC).  Concept_7: KPCS mice are generated by crossing KPC mice with Stn1 heterozygous mice, resulting in a 50% reduction in STN1 protein levels. These mice exhibit normal embryonic development and no pancreatic defects, ruling out embryotoxicity from STN1 haploinsufficiency. Compared with KPC mice, KPCS mice show over 80% reduction in lung and liver metastases, confirming STN1 as a key driver of PDAC metastasis.  Concept_8: The Dual-Luciferase Reporter Assay (DLRA) is a highly sensitive molecular biology technique primarily used to measure transcriptional activity of specific genes within cells. Using DLRA, researchers demonstrated that HOXB7 activates the STN1 promoter, STN1 activates the ZEB1 promoter, and STAT3 is required for STN1-mediated activation of ZEB1.  Concept_9: STN1-OBD is a mutant form of STN1 with a disrupted oligonucleotide/oligosaccharide-binding domain, which is critical for ssDNA recognition and interaction.	HOXB7 transcriptionally activates STN1 expression, and STN1 recruits STAT3 to the ZEB1 promoter through its ssDNA-binding ability, activates ZEB1 transcription, and drives the EMT process; ZEB1-mediated EMT enhances the migration and invasion ability of pancreatic cancer cells and promotes their distant metastasis. STN1 can serve as a potential therapeutic target for PDAC patients with high STN1 expression.  【Key points of the answer: ① STN1 is upregulated through transcriptional activation by HOXB7; ② STN1 interacts with STAT3, binds to the ZEB1 promoter, and activates ZEB1 transcription; ③ ZEB1 expression induces the EMT process, thereby promoting PDAC metastasis and enhancing its migration and invasion ability; ④ STN1 can serve as a potential therapeutic target for PDAC patients with high STN1 expression. 】	理论基础学科-生物学-细胞生物学、免疫学	209430	4	5
recuWh2PsWSZwH	In the radiation-dominated era, in Newtonian gauge and neglecting anisotropic stress, derive the transfer function T_\phi(x) and provide its closed-form expression.	"{'step_1': 'First, by theorem_1, we can take a single gravitational potential \\phi=\\psi, so we only need to track the evolution of \\phi.', 'step_2': ""Next, in the RD background of theorem_2 with w=c_s^2=\\tfrac{1}{3}, using conformal time and the conformal Hubble rate (\\eta,\\ \\mathcal H \\equiv a'/a), we have a\\propto\\eta \\Rightarrow \\mathcal H=1/\\eta."", 'step_3': ""Then, according to concept_1, the first-order Newtonian potential satisfies \\phi'' + 4\\mathcal H\\,\\phi' - \\frac{1}{3}\\nabla^2\\phi = 0, and going to momentum space (Fourier component\\phi_k) gives \\phi_k''+\\frac{4}{\\eta}\\phi_k'+\\frac{k^2}{3}\\phi_k=0. (★)"", 'step_4': 'Afterwards, following concept_2, define the dimensionless variable x\\equiv k\\eta and normalize by matching to the superhorizon limit and the primordial curvature perturbation \\zeta_k, writing \\phi_k=\\frac{2}{3}\\,\\zeta_k\\,T_\\phi(x),\\qquad T_\\phi(0)=1.', 'step_5': ""Rewriting (★) as an equation for T_\\phi yields T_\\phi''+\\frac{4}{x}T_\\phi' + \\frac{1}{3}T_\\phi = 0. (◇)"", 'step_6': 'Finally, solving (◇) and imposing superhorizon regularity, we obtain the explicit transfer function T_\\phi(x)=3\\,\\frac{\\sin y - y\\cos y}{y^{3}},\\qquad y=\\frac{x}{\\sqrt{3}}.'}"	"{'concept_1': ""Under RD conditions, the first-order Newtonian potential satisfies \\phi'' + 4\\mathcal H\\,\\phi' - \\frac{1}{3}\\nabla^2\\phi = 0, switch to momentum space (Fourier component \\phi_k)."", 'concept_2': 'Define the dimensionless variable x \\equiv k\\eta \\qquad, and normalize by matching to the superhorizon limit and the primordial curvature perturbation \\zeta_k; define the transfer function T_\\phi(x): \\phi_k = \\frac{2}{3}\\,\\zeta_k\\,T_\\phi(x), \\qquad T_\\phi(0)=1.', 'theorem_1': 'In first-order scalar perturbations in the Newtonian gauge (also called longitudinal gauge), the gravitational potentials are \\phi,\\psi. Neglecting anisotropic stress, one has \\phi=\\psi.', 'theorem_2': ""During the radiation-dominated era (RD) we havew = c_s^2 = \\frac{1}{3}. Use conformal time \\eta and the conformal Hubble rate \\mathcal H \\equiv a'/a.""}"	T_\phi(x)=\frac{9}{(x)^2}\!\left(\frac{\sqrt3}{x}\sin\frac{x}{\sqrt3}-\cos\frac{x}{\sqrt3}\right).	理论交叉学科-理论天体物理-宇宙学与引力波	336520	2	4
recuWhaz2amG1f	"In a 1+1-dimensional spatiotemporally modulated medium, the refractive index \( n \) is constant, and the impedance is a function \( Z(u) \) that varies with the composite ""motion"" coordinate \( u = x - vt \). The modulation velocity is equal to the phase velocity, i.e., \( v = c/n \). The medium occupies a finite-thickness interval \( 0 \leq x \leq d \), with vacuum (impedance \( \eta_0 \)) on both sides. Assume the eigenfunction of the internal field is \( G(w) \) (where \( w = x + vt \)). Question: What is the necessary and sufficient condition for this finite-thickness medium to achieve strictly zero reflection for any incident side (please provide a simple mathematical condition on \( G \))?"	"{'step_1': ""Set up moving coordinates $u = x - vt$ and $w = x + vt$ with $v = c/n$, and assume $Z = Z(u)$, $n = \\text{const}$. This choice simplifies Maxwell's equations for the 1+1D slab $0 \\leq x \\leq d$ embedded in vacuum with impedance $\\eta_0$ on both sides."", 'step_2': ""Rewrite Maxwell's equations in $(u, w)$. Under $Z = Z(u)$ and $v = c/n$, they reduce to $\\partial_u \\ln[(E_y - ZH_z)/\\sqrt{Z}] = 0$ and a companion relation, yielding $E_y - ZH_z = \\sqrt{Z} g(w)$, where $g$ is arbitrary. Integrating $g$ gives $G$ with $G'(w) = g(w)$."", 'step_3': ""Solve for the field inside the slab: $E_y = \\frac{1}{2}\\{ - [\\sqrt{Z(u)}]' G(w) + \\sqrt{Z(u)} G'(w) + h(u) \\}$, $H_z = [2Z(u)]^{-1}\\{ - [\\sqrt{Z(u)}]' G(w) - \\sqrt{Z(u)} G'(w) + h(u) \\}$, where $h(u)$ is an arbitrary function capturing the wave co-propagating with the moving impedance (right-going internal mode with no internal reflection)."", 'step_4': ""Form the interface-going combinations $E_\\pm \\coloneqq \\frac{1}{2}(E_y \\pm \\eta_0 H_z)$, which correspond to right-/left-going waves in vacuum and are continuous at $x = 0$ and $x = d$. Imposing zero outgoing wave on the transmission side fixes $h(u)$ in terms of $G$ via two boundary constraints: (i) Left incidence (entering at $x = 0$): impose $E_-(x = d) = 0 \\implies h(u) = [\\sqrt{Z(u)}]' G(2d - u) - \\frac{1 + \\eta_0/Z(u)}{1 - \\eta_0/Z(u)} \\sqrt{Z(u)} G'(2d - u)$. (ii) Right incidence (entering at $x = d$): impose $E_+(x = 0) = 0 \\implies h(u) = [\\sqrt{Z(u)}]' G(-u) - \\frac{1 - \\eta_0/Z(u)}{1 + \\eta_0/Z(u)} \\sqrt{Z(u)} G'(-u)$."", 'step_5': ""Evaluate the reflected field at the entrance face. For left incidence, substitute the $h(u)$ from Step 4(i) back into $E_-$ and then evaluate at $x = 0$ ($u = -vt$, $w = vt$). Terms appear only as differences of $G$ and $G'$ evaluated at $w$ and $w + 2d$, e.g., $G'(vt) - G'(vt + 2d)$ and $G(vt) - G(vt + 2d)$, multiplied by impedance-dependent prefactors. Therefore $E_-(x = 0) \\equiv 0$ for all $t$ if and only if $G(w + 2d) = G(w)$ (hence also $G'$ periodic), i.e., period $2d$."", 'step_6': ""Repeat for right incidence: use Step 4(ii), substitute into $E_+$ and evaluate at $x = d$ ($u = d - vt$, $w = d + vt$). The reflected field at $x = d$ again reduces to impedance-prefactor times $[G(w) - G(w - 2d)]$ and $[G'(w) - G'(w - 2d)]$. Hence $E_+(x = d) \\equiv 0$ for all $t$ if and only if $G(w + 2d) = G(w)$."", 'step_7': ""Necessity and sufficiency: (Sufficiency) If $G$ has period $2d$, the difference terms vanish identically, giving zero reflection for either incidence direction. (Necessity) Conversely, demanding zero reflection for all $t$ requires those difference terms to vanish pointwise, which enforces $G$ and $G'$ equality under a $2d$ shift; the only time-translation-invariant solution for all $t$ is $G(w + 2d) = G(w)$. This matches the paper's identification of “reflectionless eigenpulses” when $G$ is $2d$-periodic."", 'step_8': 'Conclusion (condition on $G$): The finite-thickness, $v = c/n$ moving-impedance slab is strictly reflectionless from either side if and only if the internal-field eigenfunction satisfies $G(w + 2d) = G(w)$ (period $2d$).'}"	"{'concept_1': 'The modeling framework for uniform-velocity space–time media / spacetime crystals. This framework characterizes the ""motion"" of the medium using the coordinates $u = x - vt$ and $w = x + vt$, and enables analytical treatment under the condition where ""the flow velocity equals the phase velocity"" (luminal). It serves as the fundamental premise for the adoption of $Z = Z(x - vt)$ and $v = c/n$ in this paper.', 'concept_2': 'The concept of eigenpulses in time-varying media. This concept states that there exist special incident waveforms that can achieve reflectionless propagation in a given spatiotemporal distribution.', 'concept_3': 'Time-modulation-based nonreciprocal propagation and unidirectional functions. Time-varying media can exhibit strongly asymmetric reflection/transmission properties for incidences from both sides (the idea of unidirectional mirrors), which provides a physical basis for designing finite-thickness structures and discussing the boundary conditions for incidences from the left/right.'}"	$G(w+2d)=G(w) \quad\text{(with period }2d\text{)}$	理论基础学科-电磁理论与量子电磁学-麦克斯韦方程组理论解	67043	1	3
recuWhnw3wDaGd	In a flat FLRW universe and conformal Newtonian gauge, for adiabatic scalar perturbations, write the linear evolution equation for the Bardeen potential \Phi in real space using conformal time \eta with \mathcal H=a'/a and primes denoting \mathrm d/\mathrm d\eta, and c_s^2\equiv \mathrm d p/\mathrm d\rho. Define all symbols.	"{'step_1': ""First, in the **conformal Newtonian gauge** and ignoring anisotropic stress (theorem_1), we have \\Psi=\\Phi. The $00$ constraint from the Einstein equations at the same order yields a Poisson-like relation: 4\\pi G\\,a^2\\,\\delta\\rho= -3\\mathcal H\\bigl(\\Phi'+\\mathcal H\\Phi\\bigr)+\\nabla^2\\Phi, which expresses the density perturbation entirely in terms of \\Phi."", 'step_2': 'Next, using the first-order scalar equation (concept_2): \\Phi\'\'+3\\mathcal H\\Phi\'+\\bigl(2\\mathcal H\'+\\mathcal H^2\\bigr)\\Phi =4\\pi G\\,a^2\\,\\delta p, and adopting the **adiabatic approximation** \\delta p=c_s^2\\,\\delta\\rho (quantifying the ""pressure information"" via the sound speed c_s^2).', 'step_3': ""Subsequently, rewrite the right-hand side (RHS) of the above equation as: 4\\pi G\\,a^2\\,\\delta p = c_s^2\\!\\left[-3\\mathcal H\\bigl(\\Phi'+\\mathcal H\\Phi\\bigr)+\\nabla^2\\Phi\\right]. Substitute this back into the first-order scalar equation, move the RHS to the left-hand side (LHS), and combine like terms. The conclusion is obtained as follows: express \\delta\\rho in terms of \\Phi using (theorem_1 + $00$ constraint), then close the system and rearrange using (concept_2 + \\delta p=c_s^2\\delta\\rho). This exactly gives the target equation: \\Phi'' + 3\\mathcal{H}(1 + c_s^2)\\Phi' + \\bigl(2\\mathcal{H}' + (1 + 3c_s^2)\\mathcal{H}^2 - c_s^2 \\nabla^2\\bigr)\\Phi = 0. This is the **standard evolution equation for the Newtonian gauge potential \\Phi in an adiabatic fluid**.""}"	"{'theorem_1': 'In the conformal Newtonian gauge, the perturbed metric reads: ds^2 = a^2(\\eta) \\left\\{ -(1 + 2\\Phi) d\\eta^2 + \\left[ (1 - 2\\Phi)\\delta_{ij} + \\frac{1}{2} h_{ij} \\right] dx^i dx^j \\right\\}, where \\Phi denotes the scalar perturbation (we neglect anisotropic stress so \\Phi = \\Psi), and h_{ij} is the transverse-traceless (TT) tensor perturbation.', 'theorem_2': ""Expanding the Einstein equations to the second order yields: h_{ij}'' + 2H h_{ij}' - \\nabla^2 h_{ij} = -4 S_{ij}^{\\rm TT}, with primes indicating derivatives with respect to the conformal time \\eta and \\mathcal{H} = a'/a the conformal Hubble parameter."", 'concept_1': ""S_{ij}^{\\rm TT} is the TT projection of the quadratic source, S_{ij} &= 4 \\Phi \\partial_i \\partial_j \\Phi + 2 \\partial_i \\Phi \\partial_j \\Phi - \\frac{4}{3(1 + \\omega)\\mathcal{H}^2} \\partial_i (\\Phi' + \\mathcal{H} \\Phi) \\partial_j (\\Phi' + \\mathcal{H} \\Phi), where \\omega \\equiv p/\\rho is the background equation-of-state (EoS) parameter."", 'concept_2': ""Linearized Einstein equations for scalar perturbations (first order) in the above gauge: \\Phi''+3\\mathcal H\\Phi'+\\bigl(2\\mathcal H'+\\mathcal H^2\\bigr)\\Phi = 4\\pi G\\,a^2\\,\\delta p.""}"	\Phi'' + 3\mathcal{H}(1 + c_s^2)\Phi' + (2\mathcal{H}' + (1 + 3c_s^2)\mathcal{H}^2 - c_s^2 \nabla^2)\Phi = 0	理论交叉学科-理论天体物理-引力波	391338	2	4
recuWjyZXWbRCq	Consider second–order scalar-induced GWs. Align axes with $(e_{+},e_{ imes},\hat{\mathbf{k}})$ and introduce v \equiv \frac{p}{k}, \qquad u \equiv \frac{|\mathbf{k}-\mathbf{p}|}{k}. Show that the polarization-summed geometric kernel is \sum_{s=+, imes} \frac{Q_s(\mathbf{k},\mathbf{p})}{k^{2}}\, \frac{Q_s(\mathbf{k},\mathbf{k}-\mathbf{p})}{k^{2}}=？	"{'Step_1': 'Goal: With axes aligned to (e_+, e_×, k̂) and u ≡ |k−p|/k, v ≡ p/k, show the value of ∑_{s=+,×} [Q_s(k,p)/k^2][Q_s(k,k−p)/k^2].', 'Step_2': ""Polarization & definition (theorem_1): Use TT tensors ε^s_ij(k̂) with ε^s_ij ε^{s'}_ij = 2 δ^{ss'}. Define Q_s(k,a) ≡ ε^s_ij(k̂) a_i a_j."", 'Step_3': 'Spherical coordinates (theorem_2): For p = (p,θ,φ) w.r.t. k̂, Q_+(k,p)/k^2 = v^2 sin^2θ cos2φ and Q_×(k,p)/k^2 = v^2 sin^2θ sin2φ.', 'Step_4': 'Geometry for q ≡ k−p (theorem_2): Let its angles be (θ′,φ′). Because q_T = −p_T, we have φ′ = φ + π and sinθ′ = (v/u) sinθ.', 'Step_5': 'Build the polarization sum (concept_2): ∑_s [Q_s(k,p)/k^2][Q_s(k,q)/k^2] = v^2 u^2 sin^2θ sin^2θ′ [cos2φ cos2φ′ + sin2φ sin2φ′]. Using φ′ = φ + π, the bracket equals 1, giving v^2 u^2 sin^2θ sin^2θ′.', 'Step_6': 'Eliminate angles (theorem_2 & concept_1): Law of cosines u^2 = 1 + v^2 − 2v cosθ ⇒ sin^2θ = [4v^2 − (1 + v^2 − u^2)^2]/(4v^2). Also sinθ′ = (v/u) sinθ ⇒ sin^2θ sin^2θ′ = (v^2/u^2) sin^4θ.', 'Step_7': 'Combine: v^2 u^2 × (v^2/u^2) sin^4θ = v^4 sin^4θ = v^4 × {[4v^2 − (1 + v^2 − u^2)^2]/(4v^2)}^2 = {[4v^2 − (1 + v^2 − u^2)^2]/4}^2.', 'Final_Answer': '\\left[\\frac{4v^{2}-(1+v^{2}-u^{2})^{2}}{4}\\right]^{2}'}"	"{'theorem_1': ""Let \\epsilon^{s}_{ij}(\\hat{\\mathbf{k}}) be the TT polarization tensors with \\epsilon^{s}_{ij}(\\hat{\\mathbf{k}})\\,\\epsilon^{s'}_{ij}(\\hat{\\mathbf{k}})=2\\,\\delta^{ss'}, \\qquad Q_s(\\mathbf{k},\\mathbf{p}) \\equiv \\epsilon^{s}_{ij}(\\hat{\\mathbf{k}})\\,p_i p_j ."", 'theorem_2': 'Choose spherical coordinates (p,\\theta,\\phi) for the internal momentum \\mathbf{p} with the z–axis along \\hat{\\mathbf{k}}. Then Q_{+}(\\mathbf{k},\\mathbf{p})=\\frac{p^{2}}{\\sqrt{2}}\\sin^{2}\\theta\\cos 2\\phi, \\qquad Q_{\\times}(\\mathbf{k},\\mathbf{p})=\\frac{p^{2}}{\\sqrt{2}}\\sin^{2}\\theta\\sin 2\\phi . Introduce v\\equiv p/k and u\\equiv|\\mathbf{k}-\\mathbf{p}|/k. By the law of cosines, u^{2}=1+v^{2}-2v \\cos\\theta, hence \\sin^{2}\\theta=\\frac{4v^{2}-\\big(1+v^{2}-u^{2}\\big)^{2}}{4v^{2}}.', 'concept_1': 'Divide each Q_s by k^{2}, substitute |\\mathbf{k}-\\mathbf{p}|=uk, eliminate \\mu using u^{2}=1+v^{2}-2v \\cos\\theta, and sum over s=+,\times, where u^2 = 1 + v^2 -2 v cos \\theta.', 'concept_2': ""sum over s=+,\times: \\sum_{s}\\frac{Q_s(\\mathbf k,\\mathbf p)}{k^{2}}\\, \\frac{Q_s(\\mathbf k,\\mathbf q)}{k^{2}} &= v^{2}u^{2}\\sin^{2}\\theta\\,\\sin^{2}\\theta' \\Big[\\cos 2\\phi\\,\\cos 2\\phi' + \\sin 2\\phi\\,\\sin 2\\phi'\\Big]. Using \\phi'=\\phi+\\pi, the bracket equals 1, and we obtain \\sum_{s}\\frac{Q_s(\\mathbf k,\\mathbf p)}{k^{2}} \\frac{Q_s(\\mathbf k,\\mathbf q)}{k^{2}} = v^{2}u^{2}\\sin^{2}\\theta\\,\\sin^{2}\\theta'. We have \\sin\\theta'=\\dfrac{v}{u}\\sin\\theta, hence \\sin^{2}\\theta\\,\\sin^{2}\\theta' =\\frac{v^{2}}{u^{2}}\\sin^{4}\\theta.""}"	\left[\frac{4v^{2}-(1+v^{2}-u^{2})^{2}}{4}\right]^{2}	理论交叉学科-理论天体物理-宇宙学演化	262141	2	4
recuVUkyIjq6dk	As for the Fe1.5Ni0.5/CeO2 catalyst, which dopant, among Ti, La, and Zr, in CeO2 support leads to the highest activity in reaction between CO2 and C2H6 to generate C2H4?	{'step_1': 'The doping of Ti into the CeO2 matrix can induce the formation of solid solution and the exposure of (111) and (110) crystal planes. Moreover, the Ti dopant also increases the concentration of Ce3+ species on CeO2, suggesting the increment of oxygen vacancies.', 'step_2': 'According to the concept 1, the oxygen vacancy is favorable of CO2 conversion. Among Zr-, La-, and Ti-doped catalysts, the Ti-doped catalyst has the highest oxygen vacancy concentration, which favors the conversion of CO2 and C2H6 with high selectivity to C2H4. Therefore, the Ti-doped catalyst with the high oxygen vacancy proportion shows the high activity.', 'step_3': 'The doping of Zr can favor the formation of oxygen vacancies in CeO2. However, the Zr dopant is favorable of the oxidation of CO to CO2, which is the reverse reaction of the conversion of CO2. Therefore, the Zr dopant shows the lower activity in comparison with Ti dopant.'}	{'concept_1': 'Oxygen vacancy on the CeO2 (111) is favorable of CO2 conversion.', 'concept_2': 'The doping of Ti in CeO2 matrix can increase the concentration of Ce3+ species on CeO2.', 'concept_3': 'The doping of Zr in CeO2 favors the formation of oxygen vacancies and the oxidation of CO to CO2.'}	The Ti dopant enables the highest activity.	理论基础学科-理论化学-计算化学	54993	1	3
recuWj3Wo7JwAe	Consider an NH₃–H₂O double-effect, double-stage absorption refrigeration system comprising two generator–absorber pairs in series (a high-temperature generator G2 supplied by external heat whose rejected heat is internally recovered to drive a lower-temperature generator G1), two absorbers, a condenser, an evaporator. Holding geometry, working pair, and internal heat-exchanger effectiveness constant, rank the following operating parameters by their influence on COP (from most to least influential): (1) absorber-1 outlet temperature T_{\mathrm{abs,1}}; (2) condensation temperature T_{\mathrm{cond}}; (3) high-temperature generator (G2) temperature T_{\mathrm{gen,2}}; (4) evaporation temperature T_{\mathrm{evap}}. Please order 1–2–3–4 from highest to lowest influence on COP and briefly justify your ranking.	{'step_1': '\\mathrm{COP}=Q_{\\mathrm{evap}}/Q_{\\mathrm{gen}}. Any change that raises Q_{\\mathrm{evap}} and/or reduces generator duty at the same lift improves COP.', 'step_2': 'Evaporation temperature has the strongest lever (Concept 3). Raising T_{\\mathrm{evap}} shrinks the thermal lift (T_{\\mathrm{cond}}-T_{\\mathrm{evap}}), lowers solution circulation and rectifier/generator duties, so Q_{\\mathrm{gen}} drops while Q_{\\mathrm{evap}} is maintained/increased—giving the steepest COP gain among the variables.', 'step_3': 'Sink-side temperatures are next (Concept 1). Lowering T_{\\mathrm{cond}} (and, secondarily, the absorber-1 outlet T_{\\mathrm{abs,1}}) strengthens absorption driving force and reduces circulation, improving COP—but their leverage is weaker than T_{\\mathrm{evap}} because they do not reduce the lift as directly.', 'step_4': 'High-T generator setpoint is least influential (Concept 2). Increasing T_{\\mathrm{gen,2}} tends to raise rectification/reflux and internal duties, often flattening or reducing COP; its sensitivity is smaller than that of the sink-side and evaporator temperatures.'}	{'concept1': 'Raising the heat-sink temperatures （absorber/condenser）weakens absorption driving force, increases solution circulation ratio, and degrades COP; conversely, lowering sink temperatures improves COP.', 'concept2': 'Increasing the high-temperature generator setpoint can raise rectifier/reflux duties and internal heat loads; COP sensitivity is weaker than to sink-side temperatures and can even decline beyond a practical optimum.', 'concept3': 'Raising the evaporation temperature T_{\\mathrm{evap}} reduces the thermal lift (T_{\\mathrm{cond}}-T_{\\mathrm{evap}}), lowers the solution circulation ratio and generator duty, and thereby increases \\mathrm{COP}=Q_{\\mathrm{evap}}/Q_{\\mathrm{gen}}'}	Ranking (most → least influence on COP): evaporation temperature T_{\mathrm{evap}} > condensation temperature T_{\mathrm{cond}} > absorber-1 outlet temperature T_{\mathrm{abs,1}} > high-temperature generator T_{\mathrm{gen,2}}; raising T_{\mathrm{evap}} most strongly increases COP, lowering T_{\mathrm{cond}} also increases COP, lowering T_{\mathrm{abs,1}} yields a smaller COP gain, while increasing T_{\mathrm{gen,2}} typically reduces COP due to higher rectification/	其他理论学科-理论热力学与统计物理-能量效率	109309	1	3
recuVJjeMcm6kF	Near the stellar center (ξ → ξ_c, equivalently P → P_c), use the central pressure P_c and central energy density ε_c to write the expression for v(ξ). Choose a volume-type coordinate v ≡ r^3.	"{'Step_1': 'Goal: derive, in one sentence and near the stellar center (ξ→ξ_c, equivalently P→P_c), the expression v(ξ) in terms of the central quantities P_c and ε_c.', 'Step_2': ""Field equations (Concept_1): start from the TOV hydrostatic equilibrium in natural units (G=c=1), dP/dr = -[(ε+P)(m(r)+4πr^3P)]/[r(r-2m)], with m(r)=4π∫_0^r ε(r') r'^2 dr'."", 'Step_3': 'Central regularity (Concept_2): impose smooth, finite center expansions m(r)= (4π/3) ε_c r^3 + O(r^5), P(r)= P_c + O(r^2), ε(r)= ε_c + O(r^2), and note that a dimensionless radius ξ scales linearly with r near the center (r ∝ ξ).', 'Step_4': 'Center-linearized gradient (Concept_3): substitute the central expansions into TOV and keep the leading term to obtain dP/dr ≃ -4π (ε_c + P_c) (P_c + ε_c/3) r.', 'Step_5': 'Quadratic central pressure profile (Concept_4): integrate Step_4 with the boundary condition P(0)=P_c to get P_c - P(r) ≃ 2π (ε_c + P_c) (P_c + ε_c/3) r^2.', 'Step_6': 'Algebraic inversion (from Concept_4): solve the previous relation for r^2 to obtain r^2 ≃ [P_c - P]/[2π (ε_c + P_c)(P_c + ε_c/3)].', 'Step_7': 'Definition of the volume-type coordinate: choose v ≡ r^3 (or any variable proportional to r^3) so that v = (r^2)^{3/2}, which converts the quadratic pressure profile into a 3/2-power dependence on (P_c - P).', 'Step_8': 'Final one-sentence result: v(ξ)= \\left( \\dfrac{P_c - P(ξ)}{2\\pi\\, (\\varepsilon_c + P_c)\\, (P_c + \\varepsilon_c/3)} \\right)^{3/2} \\quad (G=c=1).'}"	"{'concept_1': ""TOV hydrostatic equilibrium (natural units G=c=1)\n\\[\n\\frac{dP}{dr}=-\\frac{(\\varepsilon+P)\\,[\\,m(r)+4\\pi r^{3}P\\,]}{r\\,[\\,r-2m(r)\\,]},\\qquad \nm(r)=4\\pi\\!\\int_{0}^{r}\\varepsilon(r')\\,r'^{2}\\,dr'.\n\\]"", 'concept_2': '**Central regularity and local linear scaling of the dimensionless radius\n\\[\nm(r)=\\tfrac{4\\pi}{3}\\varepsilon_c r^{3}+O(r^{5}),\\quad \nP(r)=P_c+O(r^{2}),\\quad \\varepsilon(r)=\\varepsilon_c+O(r^{2}),\\quad \n(\\xi\\to\\xi_c)\\ \\Rightarrow\\ r\\propto \\xi .\n\\]', 'concept_3': 'Center-linearized TOV (lowest order)\n\\[\n\\frac{dP}{dr}\\simeq -4\\pi\\,(\\varepsilon_c+P_c)\\,\\bigl(P_c+\\varepsilon_c/3\\bigr)\\,r .\n\\]', 'concept_4': 'Quadratic central pressure profile and algebraic inversion\nIntegrate the previous relation with P(0)=P_c to get\n\\[\nP_c-P(r)\\simeq 2\\pi\\,(\\varepsilon_c+P_c)\\,\\bigl(P_c+\\varepsilon_c/3\\bigr)\\,r^{2}\n\\quad\\Longleftrightarrow\\quad\nr^{2}\\simeq \\frac{P_c-P}{\\,2\\pi(\\varepsilon_c+P_c)\\,(P_c+\\varepsilon_c/3)\\,}.\n\\]'}"	v(ξ)=\left( \frac{P_c-P}{2\pi\,(\varepsilon_c+P_c)\,\bigl(P_c+\varepsilon_c/3\bigr)} \right)^{3/2}\qquad(G=c=1)\,.	理论交叉学科-理论天体物理-中子星	175552	2	1
recuU9q0jwzs3S	The brain consists of complex neural networks composed of neurons that exhibit significant heterogeneity in their physiological properties and spiking characteristics. How neural heterogeneity affects macroscopic neural dynamics and how it may contribute to neural computation represents one of the most important research topics of concern in both theoretical and experimental neuroscience fields. However, the study of neural networks has been hindered by various factors including high data dimensionality and complex dynamical characteristics. Therefore, in a landmark 2024 study, the authors derived mean-field limit approximation equations for neural network heterogeneity, greatly simplifying the population analysis process by expressing the complex neural activities of neural populations using a simple four-dimensional first-order differential equation system. Given that individual neuron activity follows the Izhikevich equations, the neuronal heterogeneity in the model is manifested in the spiking threshold v_{θ,i}. To ensure the equation system has an analytical form, v_{θ,i} follows a Lorentzian distribution with width Δ_v and center v̄_{θ}. Please derive the four-dimensional first-order mean-field limit approximation equation system based on the above information.	{'step_1': 'Individual membrane potential and population variables are written as:\n\n$$\\begin{aligned} \nC\\dot{v}_i &= k(v_i-v_r)(v_i - v_{\\theta,i}) - u + I(t) + g_s\\,(E-v_i), &&\\text{[1]}\\\\ \n\\tau_u\\dot{u} &= -u + b\\!\\left(-v_r + \\frac{1}{N}\\sum_{j=1}^N v_j\\right) + \\tau_u\\kappa r, &&\\text{[2]}\\\\ \n\\tau_s\\dot{s} &= -s + J\\tau_s r, &&\\text{[3]}\\\\ \nr(t) &= \\frac{1}{N}\\sum_{j=1}^N\\sum_k \\delta\\!\\big(t-t^k_j\\big). &&\\text{[4]} \n\\end{aligned}$$', 'step_2': 'Using threshold heterogeneity (Lorentzian distribution) given in the concept:\n\n$$p(v_\\theta)=\\frac{1}{\\pi}\\frac{\\Delta_v}{(v_\\theta-\\bar{v}_\\theta)^2+\\Delta_v^2}. \\quad\\text{[5]}$$', 'step_3': 'From this, derive the continuum representation and continuity equation:\n\nUnder $N\\!\\to\\!\\infty$, using conditional density $\\rho(v,t\\mid v_\\theta)$:\n\n$$\\frac{\\partial}{\\partial t}\\rho = -\\frac{\\partial}{\\partial v}\\!\\left[ \\frac{1}{C}\\big(k(v-v_r)(v-v_\\theta)-u+I+g_s(E-v)\\big)\\rho \\right].\\quad\\text{[6]}$$', 'step_4': 'Introducing complex variable Lorentzian ansatz (two parameters):\n\n$$\\rho(v,t\\mid v_\\theta)=\\frac{1}{\\pi}\\frac{x(t,v_\\theta)}{[v-y(t,v_\\theta)]^2+x(t,v_\\theta)^2}. \\quad\\text{[7]}$$', 'step_5': 'Equivalence between macroscopic quantities and ansatz parameters:\n\n$$\\frac{C\\pi}{k}\\,r(t)=\\!\\!\\int x(t,v_\\theta)\\,p(v_\\theta)\\,dv_\\theta = x(t),\\quad v(t)=\\!\\!\\int y(t,v_\\theta)\\,p(v_\\theta)\\,dv_\\theta = y(t). \\quad\\text{[8],[9]}$$\n(Using the limits $v_p\\!\\to\\!\\infty,\\ v_0\\!\\to\\!-\\infty$)', 'step_6': 'Complex variabilization (encapsulation into univariate complex ODE):\n\nLet $z(t,v_\\theta)=x+iy$, substituting into [6] yields:\n\n$$C\\,\\partial_t z = i\\Big[-kz^2+i\\big(k(v_r+v_\\theta)+g_s\\big)z + k v_r v_\\theta + g_sE - u + I\\Big]. \\quad\\text{[10]}$$', 'step_7': 'Perform residue integration over $v_\\theta$ (selecting poles to ensure $r\\!>\\!0$):\n\nFor \n$$\\dot{z}=\\frac{1}{C}\\int \\partial_t z(t,v_\\theta)\\,p(v_\\theta)\\,dv_\\theta \\quad\\text{[11]}$$\nUsing [5] to evaluate at simple poles $v_\\theta=\\bar{v}_\\theta\\pm i\\Delta_v$ in the complex plane; pole selection follows $x(t)=\\frac{\\pi C}{k}r(t)>0$: if $v-v_r<0$, take the lower half-plane $(\\bar{v}_\\theta-i\\Delta_v)$, otherwise take the upper half-plane. And using\n\n$$z\\big(t,\\bar{v}_\\theta+i\\Delta_v\\big)=x+iy=\\frac{\\pi C}{k}r(t)+iv(t)$$\nto connect $z$ with $(r,v)$.', 'step_8': 'Finally obtain the four-dimensional closed system:\n\n$$\\begin{aligned}\nC\\dot{r}&=\\frac{\\Delta_v k^2}{\\pi C}\\,\\sigma_v\\,(v-v_r) + r\\big(k(2v-v_r-\\bar{v}_\\theta)-g_s\\big), &&\\text{[12]}\\\\\nC\\dot{v}&=k v\\,(v-v_r-\\bar{v}_\\theta) -\\pi C r\\Big(\\Delta_v\\sigma_v+\\frac{\\pi C}{k}r\\Big) + k v_r\\bar{v}_\\theta - u + I + g_s(E-v), &&\\text{[13]}\\\\\n\\tau_u\\dot{u}&=b(v-v_r)-u+\\tau_u\\kappa r, &&\\text{[14]}\\\\\n\\tau_s\\dot{s}&=-s+\\tau_s J r. &&\\text{[15]}\n\\end{aligned}$$'}	{'concept_1': 'The four-dimensional first-order Izhikevich equation system for neural populations is a variant of the two-dimensional first-order Izhikevich equations for individual neurons, where synaptic activation term s and spike rate population macroscopic quantity r are introduced, and has the following form:\n\n$$C\\dot{v}_i = k(v_i - v_r)(v_i - v_t) - u + I(t) + gs(E - v_i),\\tag{1}$$\n\n$$\\tau_u \\dot{u}_i = -u + b\\left(-v_r + \\frac{1}{N} \\sum_{j=1}^N v_j\\right) + \\tau_u \\kappa r$$\n\n$$\\tau_s \\dot{s} = -s + J r \\tau_s，$$\n\n$$r(t) = \\frac{1}{N} \\sum_{j=1}^N \\sum_{k=1}^{S_j} \\delta(t - t_j^k).$$', 'concept_2': 'To ensure the equations have analytical solutions, the heterogeneous Spiking Threshold Lorentzian distribution has the following form:\n\n$$p(v_\\theta) = \\frac{1}{\\pi} \\frac{\\Delta_v}{(v_\\theta - \\bar{v}_\\theta)^2 + \\Delta_v^2}.$$', 'concept_3': 'The mean-field limit approximation of neurons is essentially an analysis of the average properties of neural populations using statistical physics methods, which employs the continuity assumption for fluid populations:\n\n$$\\frac{\\partial}{\\partial t} \\rho(v,t|v_\\theta) = -\\frac{\\partial}{\\partial v}\\left[\\frac{1}{C}\\left(k(v-v_r)(v-v_t) - u + I + gs(E-v)\\right) \\rho(v,t|v_\\theta)\\right].$$', 'concept_4': 'When the two-dimensional partial differential equation is inconvenient to solve, the method of constructing complex variables is often used in the field of statistical mechanics. First, a one-dimensional complex partial differential equation is constructed, and then the residue theorem from complex function theory is applied for solving. This allows the construction of a partial differential equation of the form:\n\n$$C \\frac{\\partial}{\\partial t} z(t,v_\\theta) = i[-k z(t,v_\\theta)^2 + i(k(v_r+v_\\theta) - u + I + gs(E-v)) + kv_r v_\\theta + gsE - u + I],$$\nwhere $z(t,v_\\theta) = x(t,v_\\theta) + i y(t,v_\\theta)$.', 'concept_5': 'The final form should contain the population variables r, v as well as the recovery term u and convolution signal s, which is the standard form of mean-field equations.'}	Cr = \frac{\Delta_v k^2 \sigma_v}{\pi C}(v - v_r) + r\big(k(2v - v_r - \bar{v}_\theta) - gs\big),C\dot{v} = kv(v - v_r - \bar{v}_\theta) - \pi Cr\Big(\Delta_v \sigma_v + \frac{\pi C}{k}r\Big) + kv_r \bar{v}_\theta - u + I + gs(E - v),\tau_u \dot{u} = b(v - v_r) - u + \tau_u \kappa r,\tau_s \dot{s} = -s + \tau_s Jr,	理论基础学科-生物学-理论神经科学	65725	1	5
recuUqZziw5wn0	Suppose $X$ is a subvariety of $\mathbb{P}^4$ defined by the equation $x_1^5+x_2^5+x_3^5=k$ where $k\in \mathbb{C}$. Is the $GL_2(\mathbb{C})$ character variety of $X$ rigid?	{'step_1': 'Use the concept 1, we can compute that X is not smooth when k=0 and X is smooth when k\\neq0.', 'step_2': 'Use the concept 2, we know that the base space is not zero.', 'step_3': 'Let X be a smooth projective variety with Picard number one. Then the GL2(C) character variety of X is rigid if and only if b1(X) = 0. The concept 2 gives that b_1(X)\\neq0 when k=0, and thus we know that the $GL_2(\\mathbb{C})$ character variety of $X$ is not rigid when k=0. When k\\neq0, we can see that X is actually smooth and thus the corresponding character variety of $X$ is rigid.'}	{'concept_1': 'A projective variety X determined by the equation f in P^n is smooth if there is no point in X such that the derivative df is zero. Otherwise it is not smooth.', 'concept_2': '$H^0(X,\\text{Sym}^2\\Omega_{X}^1)\\neq0$ if k=0'}	The $GL_2(\mathbb{C})$ character variety of $X$ is not rigid when k=0 and when k\neq0, it is rigid.	理论基础学科-基础数学-几何	606394	1	2
recuUSrw35heEZ	What is the learning rate of a function satisfying the (L_0, L_1) -smoothness condition? Don't omit the coefficients. Don't omit the time subscript t. Double vertical bars are used for norm symbols.	{'step_1': 'According to concept_3, in transformer-based objectives, the optimization dynamics are governed by gradient magnitudes, which explains why adaptive optimizers outperform SGD. This is because transformer models exhibit unique smoothness properties where local smoothness depends on the gradient norm, suggesting that the learning rate should scale as the reciprocal of the gradient norm to handle varying curvature effectively.', 'step_2': 'This aligns with concept_1, the (L_0, L_1)-smooth condition, which states that for a differentiable and lower-bounded function f, there exist constants L_0, L_1 > 0 such that for all w_1, w_2 in \\mathbb{R}^d with \\|w_1 - w_2\\| \\leq \\frac{1}{L_1}, the gradient difference satisfies \\|\\nabla f(w_1) - \\nabla f(w_2)\\| \\leq (L_0 + L_1 \\|\\nabla f(w_1)\\|) \\|w_1 - w_2\\|. In Transformer optimization landscapes, this condition captures how local smoothness is bounded by the local gradient norm, providing the foundation for adaptive learning rate requirements.', 'step_3': 'Building on theorem_1, under this (L_0, L_1)-smoothness, the local smoothness constant L(w) is bounded by L_0 + L_1 \\|\\nabla f(w)\\|, meaning the curvature increases with the gradient magnitude, which risks overshooting without adjustment. This necessitates adaptive learning rates that scale inversely with the gradient norm to ensure the function value decreases, deriving the bound \\eta_t \\leq \\frac{1}{L_0 + L_1 \\|\\nabla f(w_t)\\|}, balancing descent and curvature for convergence.', 'step_4': 'To implement this bound practically in Transformer training without knowing L_0 and L_1 exactly, theorem_1 and concept_3 imply approximating the gradient magnitude. Here, concept_2 offers a solution: momentum, as an exponential moving average of historical gradients, provides a robust proxy for estimating \\|\\nabla f(w_t)\\| without second-moment computations, enabling efficient adaptive optimization while adhering to the learning rate upper bound derived from the smoothness properties.'}	{'concept_1': '(L_0, L_1)-smooth condition | Assuming that f is differentiable and lower bounded, there exist constants L_0, L_1 > 0, such that \\forall w_1, w_2 \\in \\mathbb{R}^d satisfying \\|w_1 - w_2\\| \\leq \\frac{1}{L_1}, \\|\\nabla f(w_1) - \\nabla f(w_2)\\| \\leq (L_0 + L_1 \\|\\nabla f(w_1)\\|) \\|w_1 - w_2\\|.', 'theorem_1': 'Under (L_0, L_1)-smoothness, the local smoothness L(w) is bounded by L_0 + L_1 \\|\\nabla f(w)\\|, necessitating adaptive learning rates that scale inversely with gradient magnitude to avoid overshooting | When Assumption 2.1 holds, the local smoothness of the objective function is bounded by the linear form of the gradient norm (i.e., L(w) \\leq L_0 + L_1 \\|\\nabla f(w)\\|). We know that the smoothness constant L(w) governs how much the gradient can change locally. If L(w) scales with \\|\\nabla f(w)\\|, the curvature (and thus the risk of overshooting) increases with the gradient’s magnitude. This necessitates a smaller learning rate when the gradient is large and allows a larger rate when the gradient is small.', 'concept_2': 'Momentum, as an exponential moving average of historical gradients, provides a robust proxy for estimating gradient magnitudes without second-moment computations | Momentum, an exponential average of historical gradients, can provide a good and robust estimate of gradient magnitude without the need for complex second-moment computations.', 'concept_3': 'In NLP-based objectives (such as language modeling, which are foundational to modern large language models including transformer architectures), the optimization dynamics are governed by gradient magnitudes, explaining why adaptive optimizers outperform SGD | Models in NLP tasks, which dominate modern large language models (LLMs), exhibit unique smoothness properties in their optimization landscapes. Specifically, the local smoothness of these objectives is governed by gradient magnitudes, which suggests that the learning rate should be proportional to the reciprocal of the gradient norm at each iteration, explaining why Adam optimizer beats SGD on training NLP architectures like those in language modeling.'}	\eta_t \leq \frac{1}{L_0 + L_1 \|\nabla f(w_t)\|}	理论基础学科-理论计算机科学-机器学习	331348	4	3
recuUo3KlCjoXZ	Given an unknown probability distribution $p$, what is the tight upper bound on the sample complexity $m(n,\epsilon)$ required for an algorithm to distinguish between the case where $|supp(p)| \le n$ and the case where $p$ is $\epsilon$-far from any distribution with support size at most $n$? The analysis must be based on a linear estimator constructed from shifted and scaled Chebyshev polynomials, considering the trade-offs that determine the final complexity terms. To control the estimator's variance, the degree $d$ of the Chebyshev polynomial is constrained to be $d=O(\log n)$. This constraint further provides the $\frac{1}{\log n}$ factor that improves upon the baseline complexity. Also consider the trivial/baseline tester which succeeds with $m_0=\Theta\left(\frac{n}{\epsilon}\right)$ samples; the final achievable sample complexity is the minimum between this baseline and the Chebyshev-based bound.	"{'step_1': 'Problem setup (concept_1). Define the testing goal: given $n,\\epsilon$ and $m$ i.i.d. samples, accept if $|\\mathrm{supp}(p)|\\le n$ and reject if $p$ is $\\epsilon$-far from every distribution of support $\\le n$.', 'step_2': 'Test statistic (concept_2). Use a linear estimator built from the sample histogram of the form $\\hat S=\\sum_i (1+f(N_i))$ and a threshold test on $\\hat S$. The function $f$ is chosen via a polynomial approximation so that $\\mathbb E[\\hat S]$ approximates the true support size and the statistic concentrates.', 'step_3': 'Poissonization (concept_3). Analyze in the Poissonized model $N_i\\sim\\mathrm{Poi}(m p_i)$ to make counts independent and simplify computation of bias and variance of $\\hat S$.', 'step_4': 'Baseline bound (concept_4). A simple/plug-in tester yields the baseline sufficient sample size $m_0=\\Theta\\!\\Big(\\frac{n}{\\epsilon}\\Big)$, since the dominant error term decays like $e^{-c m/n}$ and solving for error $\\le\\epsilon$ gives the $n\\log(1/\\epsilon)$ type scaling which in this form provides $m_0=\\Theta(n/\\epsilon)$ as the baseline regime.', 'step_5': ""Chebyshev-based estimator construction (theorem_1). Construct a polynomial $P_d$ (shifted & scaled Chebyshev) so that $P_d(0)=-1$ and $P_d(x)$ is small on a chosen safe interval $[l,r]$. Use this polynomial to define $f$ and thereby a linear estimator whose bias for 'medium'-sized probabilities is tightly controlled."", 'step_6': 'Refined small-probability analysis (theorem_2). For probabilities below the safe interval, apply the differential/derivative inequality analysis to further bound the contribution of very small probabilities; this analysis introduces a $\\log(1/\\epsilon)$-type factor into the candidate sample size required for the Chebyshev estimator to achieve error $\\epsilon$.', 'step_7': ""Variance control via polynomial degree constraint. To keep the estimator's variance small, restrict the polynomial degree to $d=O(\\log n)$. This choice limits variance growth and produces a multiplicative improvement of $1/\\log n$ over the baseline in the Chebyshev branch."", 'step_8': 'Two candidate sufficient sample sizes. Baseline (plug-in): $m_0=\\Theta\\!\\big(\\dfrac{n}{\\epsilon}\\big)$. Chebyshev branch (after applying theorem_2 and using $d=O(\\log n)$): $m_{\\text{cheb}}=\\Theta\\!\\left(\\frac{n}{\\epsilon\\log n}\\,\\log\\frac{1}{\\epsilon}\\right).$', 'step_9': 'Take the minimum (concept_5). Success requires only one of the sufficient conditions, so an achievable (upper-bound) sample complexity is $m(n,\\epsilon)=\\min\\{m_0,\\; m_{\\text{cheb}}\\}.$', 'step_10': 'Algebraic factoring (final form). Factor out the common multiplier $\\dfrac{n}{\\epsilon\\log n}$: $\\min\\!\\left\\{\\frac{n}{\\epsilon},\\ \\frac{n}{\\epsilon\\log n}\\log\\frac{1}{\\epsilon}\\right\\} =\\frac{n}{\\epsilon\\log n}\\cdot\\min\\!\\left\\{\\log n,\\ \\log\\frac{1}{\\epsilon}\\right\\}.$', 'step_11': 'Final tight upper bound (concise statement). Thus $\\boxed{\\,m(n,\\epsilon)=O\\!\\left(\\frac{n}{\\epsilon\\log n}\\cdot\\min\\Big\\{\\log\\frac{1}{\\epsilon},\\ \\log n\\Big\\}\\right)\\,}$ which explicitly displays the $\\frac{1}{\\log n}$ improvement from the Chebyshev degree constraint and the regime switch governed by $\\min\\{\\log(1/\\epsilon),\\log n\\}$'}"	"{'concept_1': 'A support-size tester is an algorithm that takes parameters $n$ and $\\epsilon$, draws $m$ samples, and decides if $|supp(p)| \\le n$ or if $p$ is $\\epsilon$-far from having such support.', 'concept_2': 'A test statistic $\\hat{S}$ is constructed as $\\hat{S}=\\sum_{i\\in\\mathbb{N}}(1+f(N_{i}))$, where $N_i$ is the number of times element $i$ appears in the sample. The tester accepts if $\\hat{S}$ is below a threshold related to $n$.', 'concept_3': 'Poissonization is an analysis technique that simplifies the analysis by making the counts $N_i$ for each element mutually independent random variables, $N_i \\sim Poi(mp_i)$.', 'concept_4': 'The fundamental sample complexity for distinguishing distributions based on $\\epsilon$-distance scales proportionally to the support parameter $n$ and inversely proportionally to the distance parameter $\\epsilon$. This establishes a baseline dependency of $m \\propto \\frac{n}{\\epsilon}$, which more advanced methods refine.', 'theorem_1': ""Shifted and scaled Chebyshev polynomials are used to construct a polynomial $P_d(x)$ such that $P_d(0)=-1$ and its value is minimized over a 'safe interval' $[l,r]$, making the estimator accurate for probabilities within that range."", 'theorem_2': 'A refined analysis of the estimator for probabilities smaller than the safe interval, using a differential inequality, provides a tighter performance guarantee. This improvement introduces the $\\log(1/\\epsilon)$ factor into the complexity.'}"	$O(\frac{n}{\epsilon \log n}\cdot \min\{\log(1/\epsilon), \log n\})$ (also equivalent to $ O( \min( \frac{n}{\epsilon}, \frac{n \log(1/\epsilon)}{\epsilon \log n})$)	理论基础学科-理论计算机科学-算法理论	298371	3	3
recuUYABk4Gv6B	In the context of testing low stabilizer complexity states, the maximum fidelity $F(\phi)$ of a quantum state $|\phi\rangle$ with a stabilizer state can be lower-bounded by its Gowers 3-norm, $|||\phi\rangle||_{U^{3}}^{8}\ge\gamma$, using the relation $\sqrt{F(\phi)} \ge \frac{\gamma^{C_2}}{C_1}$. Based on the proof that applies additive combinatorics tools like the Balog-Szemerédi-Gowers theorem, what is the analytical expression for the exponent $C_2$? Hint: A corollary combining the Balog-Szemerédi-Gowers theorem and the result on Marton's conjecture shows that if $Pr_{z_1, z_2 \in S}[z_1+z_2 \in S] \ge \epsilon$, there exists an affine subspace $V$ such that $|S \cap V| \ge \frac{\epsilon^{K_2}}{K_1}|S|$.	"{'step_1': 'The objective is to find the analytical expression for the exponent $C_2$ as defined in the main theorem (theorem_1), which states the fidelity lower bound as $|\\langle \\phi|s\\rangle| \\ge \\frac{\\gamma^{C_2}}{C_1}$. According to concept_2, $|\\langle s | \\phi \\rangle|$ is the square root of the stabilizer fidelity, $\\sqrt{F(\\phi)}$.', 'step_2': ""The proof of this theorem, which leverages additive combinatorics tools like the Balog-Szemerédi-Gowers theorem (theorem_2) and the result on Marton's conjecture (theorem_3), leads to a more detailed intermediate inequality. As the hint suggests, combining these theorems introduces a constant $K_2$, which appears in the expression from concept_3: $|\\langle s | \\phi \\rangle| \\ge \\delta \\frac{\\epsilon^{2K_2+2}}{K_1^2}$."", 'step_3': 'To connect this intermediate result back to the Gowers norm bound $\\gamma$, we utilize the relationships provided in concept_3: $\\delta \\propto \\gamma^2$ and $\\epsilon \\propto \\gamma^2$. We substitute these proportionalities into the inequality.', 'step_4': 'After substitution, the right-hand side of the inequality becomes proportional to $(\\gamma^2) \\cdot (\\gamma^2)^{2K_2+2}$. The next step is to simplify this expression to find the total exponent of $\\gamma$.', 'step_5': 'The total exponent is the sum of the exponents from each part: the exponent from the $\\delta$ term is 2, and the exponent from the $\\epsilon$ term is $2 \\times (2K_2+2) = 4K_2+4$. Adding them together gives $2 + (4K_2+4) = 4K_2+6$.', 'step_6': 'By comparing our derived expression, which is proportional to $\\gamma^{4K_2+6}$, with the form given in theorem_1 ($\\gamma^{C_2}$), we can equate the exponents. This gives the final analytical expression for $C_2$: $C_2 = 4K_2+6$.'}"	"{'concept_1': ""The Gowers 3-norm for a quantum state $|\\phi\\rangle = \\frac{1}{\\sqrt{N}}\\sum_x g(x)|x\\rangle$ is defined via its amplitude function $g(x)$. Its 8th power is given by $|||\\phi\\rangle||_{U^3}^8 = \\frac{1}{N}\\sum_{z}f(z)^2$, where $f(z)$ is the state's characteristic function."", 'concept_2': 'Stabilizer fidelity, denoted as $F(\\phi)$, is the maximum squared overlap of a quantum state $|\\phi\\rangle$ with any stabilizer state, defined as $F(\\phi) = \\max_{|s\\rangle \\in Stab_n} |\\langle s | \\phi \\rangle|^2$.', 'concept_3': 'The final fidelity bound is derived from the expression $|\\langle s | \\phi \\rangle| \\ge \\delta \\frac{\\epsilon^{2K_2+2}}{K_1^2}$, where $\\delta$ and $\\epsilon$ are parameters dependent on the initial Gowers norm value $\\gamma$ as $\\delta \\propto \\gamma^2$ and $\\epsilon \\propto \\gamma^2$.', 'theorem_1': 'For a quantum state with a large Gowers 3-norm, $|||\\phi\\rangle||_{U^3}^8 \\ge \\gamma$, its fidelity with some stabilizer state is lower-bounded by a polynomial in $\\gamma$, specifically $|\\langle \\phi|s\\rangle| \\ge \\frac{\\gamma^{C_2}}{C_1}$ for some absolute constants $C_1, C_2 > 0$.', 'theorem_2': ""The Balog-Szemerédi-Gowers theorem states that for an Abelian group $G$ and a subset $S \\subset G$ with $Pr_{z_1, z_2 \\in S}[z_1+z_2 \\in S] \\ge \\epsilon$, there exists a subset $S' \\subset S$ with $|S'| \\ge \\frac{\\epsilon|S|}{3}$ and $|S'+S'| \\le (\\frac{6}{\\epsilon})^8 |S|$."", 'theorem_3': ""A result on Marton's conjecture by Gowers, Green, Manners, and Tao states that if a subset $S \\subset \\mathbb{F}_2^n$ has a small doubling constant, $|S+S| \\le K|S|$, it can be covered by $(2K)^C$ translations of a subspace $V \\subset \\mathbb{F}_2^n$ with $|V| \\le |S|$.""}"	The analytical expression is given by $C_2 = 4K_2 + 6$	理论基础学科-理论计算机科学-计算复杂度	230666	2	3
recuV26pFqhi7f	Under fixed bit-width $b$ quantized arithmetic, given an SSM with $L$ layers and hidden state dimension $d$, if it can accept some input word $w$, provide an upper bound on the length of an acceptable word $w'$, expressed only in terms of $L,d,b$.	"{'step_1': 'By the $b$-bit quantization assumption, each coordinate takes at most $2^{b}$ values. Hence a single-layer $d$-dimensional hidden state has\n \\[\n S_1=(2^{b})^{d}=2^{bd}\n \\]\n possibilities.', 'step_2': 'Concatenating $L$ layers at the same position gives the combined layer state count\n \\[\n S=(S_1)^{L}=(2^{bd})^{L}=2^{bLd}.\n \\]', 'step_3': 'To preserve semantics under segment deletion, it is not enough that single-time states match; we require boundary consistency across the cut. Concretely, for each layer $1\\le l\\le L$, both pairs\n \\[\n (h_{i-1}^l,h_i^l)\\quad\\text{and}\\quad(h_{j-1}^l,h_j^l)\n \\]\n must coincide. By Steps 1--2, each $h_t^l$ ranges over at most $2^{bd}$ values, so each layer-wise ordered pair has at most $2^{2bd}$ possibilities, and across $L$ layers the number of effective pair-configurations is\n \\[\n (2^{2bd})^{L}=\\bigl(2^{bd}\\bigr)^{2L}=\\bigl(2^{bLd}\\bigr)^{2}=S^{2}.\n \\]\n Thus, if the input length $n>S^2$, determinism over this finite set implies there exist $1\\le i<j\\le n$ with identical effective configurations.', 'step_4': 'By deletion invariance under matched boundaries, removing $a_{i+1}\\cdots a_{j}$ yields a word still accepted.', 'step_5': ""Repeating the previous step until no such long segment remains, we obtain an accepted word $w'$ with\n \\[\n |w'|\\le S^{2}=\\bigl(2^{\\,bLd}\\bigr)^{2}=2^{\\,2bLd}.\n \\]""}"	"{'concept_1': 'For a system under fixed bit-width $b$ with hidden dimension $d$ and $L$ layers, the effective configuration at time $t$ is defined as the ordered pair $(H_{t-1},H_t)$ of consecutive layer-concatenated hidden states. The set of such configurations is finite and determined only by $b,d,L$.', 'concept_2': ""In deterministic evolution with finite states, any sufficiently long run must necessarily contain two points where the overall configuration is identical. This is only a qualitative fact of ``existence of repetition.''"", 'concept_3': 'Once the overall configuration is identical at two positions, the subsequent evolution is consistent, thus, the intermediate segment can be removed while preserving semantics, yielding an acceptable word that is not much longer.'}"	|w'|\ \le\ 2^{\,2Ldb}.	理论基础学科-理论计算机科学-计算复杂度	198978	2	3
recuV4t2Gbf53n	In an internal flow field, if the stream-function coordinate is used instead of the physical coordinate, please provide the transformation relationship. \(\psi\) denotes the stream function, and \(\beta\) denotes the streamline, where when \(d\psi = 0\), \(\beta = \text{const}\); \(\phi\) denotes the potential function, and \(\alpha\) denotes the equipotential line, where when \(d\phi = 0\), \(\alpha = \text{const}\).	{'step_1': 'Expressions for streamlines and equipotential lines in the Cartesian coordinate system.', 'step_2': 'Expression for the flow direction in the Cartesian coordinate system.', 'step_3': 'Derive the coordinate-transformation relations based on the streamline, equipotential-line, and flow-direction expressions.'}	{'concept_1': 'The relationships for streamlines, equipotential lines, and the flow direction with respect to Cartesian coordinates, \\beta(x, y)=\\text { const }, \\quad \\frac{\\mathrm{d} y}{\\mathrm{~d} x}=\\tan \\theta. where theta is the flow angle, alpha is the equipotential line, beta is streamline.'}	The expression is : \begin{array}{l} \frac{\partial y}{\partial \alpha}=\tan \theta \frac{\partial x}{\partial \alpha} \\ \frac{\partial y}{\partial \beta}=-\cot \theta \frac{\partial x}{\partial \beta} \end{array}. The x and y are Cartesian coordinate systems. Where theta is the flow angle, alpha is the equipotential line, and beta is streamline.	理论工程学科-理论工程力学-空气动力学理论	20652	1	1
recuV4xvWPHIM3	Let the parameter vector of an Exponential Random Graph Model (ERGM) be \(\beta = (\beta_1, \beta_2, \dots, \beta_k) \in \mathbb{R}^k\), where \(\beta_i\) corresponds to the coefficient of the \(i\)-th sufficient statistic \(H_i\) in the model. Subcritical region of ERGMs satisfying two conditions: (1) There exists a unique solution \(p = p(\beta)\) to the equation \(\varphi_{\beta}(a) = a\) in \((0,1)\); (2) \(\varphi'_{\beta}(p) < 1\). Here, \(\varphi_{\beta}(a) = \frac{e^{2\Phi_{\beta}(a)}}{e^{2\Phi_{\beta}(a)} + 1}\) and \(\Phi_{\beta}(a) = \sum_{i=1}^{k} \beta_{i} e_{i} a^{e_{i}-1}\), where \(e_i\) is the number of edges in the subgraph \(H_i\). Please calculate the asymptotic variance of the total number of edges in ERGMs In the subcritical region of parameters.	"{'step_1': 'Define the Total Number of Edges and Edge Indicators: Let \\(G\\) be an ERGM on \\(n\\) vertices. Using **Concept 3** (exchangeable pairs involve edge indicator vectors \\(Y = \\{Y_{ij}\\}\\)): - Define \\(Y_{ij} \\in \\{0,1\\}\\) as the edge indicator: \\(Y_{ij} = 1\\) if edge \\((i,j)\\) exists in \\(G\\), and \\(Y_{ij} = 0\\) otherwise. - Let \\(W_n\\) denote the **total number of edges** in \\(G\\): \\(W_n = \\sum_{1 \\leq i < j \\leq n} Y_{ij}\\), where \\(N = \\binom{n}{2}\\) is the total number of possible edges in an \\(n\\)-vertex graph.', 'step_2': ""Link Edge Probability to the Subcritical Region: By **instruction** (subcritical region), the parameter vector \\(\\beta\\) satisfies: 1. A unique fixed point \\(p = p(\\beta) \\in (0,1)\\) solves \\(\\varphi_\\beta(a) = a\\), where \\(\\varphi_\\beta(a) = \\frac{e^{2\\Phi_\\beta(a)}}{e^{2\\Phi_\\beta(a)} + 1}\\) and \\(\\Phi_\\beta(a) = \\sum_{j=1}^k \\beta_j e_j a^{e_j - 1}\\); 2. \\(\\varphi'_\\beta(p) < 1\\) (weak dependence condition). For large \\(n\\) (asymptotic regime), the edge indicator \\(Y_{ij}\\) has an approximate expectation equal to the fixed point \\(p\\): \\(\\mathbb{E}[Y_{ij}] \\approx p\\). This follows because \\(\\varphi_\\beta(p) = p\\) is the equilibrium edge probability (the conditional probability of an edge, given the rest of the graph, converges to \\(p\\) for large \\(n\\))."", 'step_3': 'Center \\(W_n\\) for the Exchangeable Pair Theorem (Theorem 1): **Theorem 1** requires a centered random variable \\(W\\) with \\(\\mathbb{E}[W] = 0\\). Define the **centered total edge count**: \\(\\tilde{W}_n = W_n - \\mathbb{E}[W_n]\\). By linearity of expectation and \\(\\mathbb{E}[Y_{ij}] \\approx p\\): \\(\\mathbb{E}[W_n] = \\sum_{1 \\leq i < j \\leq n} \\mathbb{E}[Y_{ij}] \\approx Np\\), so \\(\\tilde{W}_n \\approx W_n - Np\\).', 'step_4': ""Construct an Exchangeable Pair for \\(\\tilde{W}_n\\): Using **Concept 1** (exchangeable pairs of edge vectors): - Construct \\(Y'\\) (exchangeable with \\(Y\\)) by flipping a uniformly random edge: select a pair \\((i,j) \\sim \\text{Uniform}(1,\\dots,N)\\), set \\(Y'_{kl} = Y_{kl}\\) for \\((k,l) \\neq (i,j)\\), and \\(Y'_{ij} = 1 - Y_{ij}\\). - The corresponding centered total edge count is \\(\\tilde{W}'_n = W'_n - \\mathbb{E}[W'_n]\\). Since \\((Y,Y')\\) is exchangeable, \\(\\mathbb{E}[W'_n] = \\mathbb{E}[W_n]\\), so: \\(\\tilde{W}'_n - \\tilde{W}_n = W'_n - W_n = (1 - 2Y_{ij})\\)."", 'step_5': ""Compute Key Quantities for Theorem 1: We calculate two critical terms from **Theorem 1**: 5.1 Expectation of the Squared Difference: Since \\(Y_{ij} \\in \\{0,1\\}\\), \\(Y_{ij}^2 = Y_{ij}\\). Thus: \\(\\mathbb{E}\\left[(\\tilde{W}'_n - \\tilde{W}_n)^2\\right] = \\mathbb{E}\\left[(1 - 2Y_{ij})^2\\right] = \\mathbb{E}\\left[1 - 4Y_{ij} + 4Y_{ij}^2\\right] = 1 - 4p + 4p = 1\\). 5.2 Conditional Expectation and \\(\\lambda\\): By **Theorem 1**, \\(\\mathbb{E}[\\tilde{W}'_n \\mid \\tilde{W}_n] = (1 - \\lambda)\\tilde{W}_n\\) for some \\(\\lambda \\in (0,1)\\) (dependency strength). For large \\(N\\) (asymptotic regime), \\(\\mathbb{E}[Y_{ij} \\mid W_n] \\approx \\frac{W_n}{N}\\) (uniform edge selection). Substituting into the conditional expectation: \\(\\mathbb{E}[W'_n \\mid W_n] = W_n + \\mathbb{E}[1 - 2Y_{ij} \\mid W_n] \\approx W_n + 1 - \\frac{2W_n}{N}\\). The constant term \\(1\\) vanishes as \\(N \\to \\infty\\), so: \\(\\mathbb{E}[\\tilde{W}'_n \\mid \\tilde{W}_n] \\approx \\left(1 - \\frac{2}{N}\\right)\\tilde{W}_n\\). Here, \\(\\lambda \\approx \\frac{2}{N}\\) in the raw count, but we refine \\(\\lambda\\) using the subcritical derivative \\(\\varphi'_\\beta(p)\\)."", 'step_6': ""Relate \\(\\lambda\\) to the Subcritical Derivative \\(\\varphi'_\\beta(p)\\), compute the derivative of \\(\\varphi_\\beta(a)\\) at the fixed point \\(p\\): 1. First, differentiate \\(\\varphi_\\beta(a) = \\frac{e^{2\\Phi_\\beta(a)}}{e^{2\\Phi_\\beta(a)} + 1}\\): \\(\\varphi'_\\beta(a) = \\frac{2\\Phi'_\\beta(a)e^{2\\Phi_\\beta(a)}}{\\left(e^{2\\Phi_\\beta(a)} + 1\\right)^2} = 2\\Phi'_\\beta(a)\\varphi_\\beta(a)(1 - \\varphi_\\beta(a))\\). 2. Substitute \\(\\varphi_\\beta(p) = p\\) (fixed point) and \\(\\Phi'_\\beta(p) = \\sum_{j=1}^k \\beta_j e_j(e_j - 1)p^{e_j - 2}\\) (derivative of \\(\\Phi_\\beta(a)\\)): \\(\\varphi'_\\beta(p) = 2p(1 - p)\\sum_{j=1}^k \\beta_j e_j(e_j - 1)p^{e_j - 2} = \\sum_{j=1}^k \\beta_j e_j(e_j - 1)2p^{e_j - 1}(1 - p)\\). 3. For the single-edge subgraph \\(H_1\\) (where \\(e_1 = 1\\)), \\(e_1(e_1 - 1) = 0\\), so the \\(j=1\\) term vanishes: \\(\\varphi'_\\beta(p) = \\sum_{j=2}^k \\beta_j e_j(e_j - 1)2p^{e_j - 1}(1 - p)\\)."", 'step_7': ""Solve for the Asymptotic Variance: By **Theorem 1**, the variance parameter is: \\(\\sigma^2 = \\frac{1}{4\\lambda} \\mathbb{E}\\left[(\\tilde{W}'_n - \\tilde{W}_n)^2\\right]\\). In the subcritical region, weak dependence implies \\(1 - \\lambda \\approx \\varphi'_\\beta(p)\\), so \\(\\lambda \\approx 1 - \\varphi'_\\beta(p)\\). Substituting \\(\\mathbb{E}[(\\tilde{W}'_n - \\tilde{W}_n)^2] = 1\\) and scaling by \\(Np(1 - p)\\) (the binomial variance of independent edges) gives the **asymptotic variance** of \\(W_n\\): \\(\\sigma_n^2 = \\frac{N p(1 - p)}{1 - \\sum_{j=2}^{k} \\beta_{j} e_{j}(e_{j}-1) \\cdot 2 p^{e_{j}-1}(1-p)}\\), where: - \\(N = \\binom{n}{2}\\) (total possible edges), - \\(p\\) is the unique fixed point of \\(\\varphi_\\beta(a) = a\\) in \\((0,1)\\), - \\(\\beta_j\\) and \\(e_j\\) are the parameter and edge count of subgraph \\(H_j\\), respectively.""}"	"{'concept_1': ""Exchangeable pairs in ERGM analysis: An exchangeable pair \\((Y, Y')\\) consists of two ERGM edge indicator vectors \\(Y = \\{Y_{ij}\\}\\) and \\(Y'\\) with identical joint distribution \\(\\mathcal{L}(Y, Y') = \\mathcal{L}(Y', Y)\\)."", 'theorem_1': ""Let \\((W, W')\\) be an exchangeable pair of random variables with \\(\\mathbb{E}[W] = 0\\) and suppose there exists \\(\\lambda \\in (0,1)\\) such that \\(\\mathbb{E}[W' \\mid W] = (1 - \\lambda)W\\). Define the variance parameter by \\(\\sigma^2 = \\frac{1}{2\\lambda} \\mathbb{E}\\left[\\frac{(W' - W)^2}{2}\\right]\\) and let \\(Z \\sim \\mathcal{N}(0, \\sigma^2)\\). Then for some absolute constant \\(C > 0\\), \\(\\|\\mathcal{L}(W) - \\mathcal{L}(Z)\\|_{\\text{W}} \\leq C \\left( \\lambda + \\mathbb{E}\\left| \\frac{(W' - W)^2}{2\\lambda} - \\sigma^2 \\right| + \\mathbb{E}|W' - W|^3 \\right)\\). Here, the first term \\(\\lambda\\) controls the dependency strength, the second term measures deviation from the variance condition, and the third term quantifies higher-order moment behavior.""}"	The asymptotic variance of the total number of edges in exponential random graph models (ERGMs) within the subcritical region of parameters is explicitly given by: \(\sigma_{n}^{2} = \sigma_{n}^{2}(\beta) = \frac{N p(1-p)}{1 - \sum_{j=2}^{k} \beta_{j} e_{j}(e_{j}-1) \cdot 2 p^{e_{j}-1}(1-p)}\) where: - \(N = \binom{n}{2}\) (representing the total number of possible edges in a graph with \(n\) vertices), - \(p\) denotes the unique solution to the equation \(\varphi_{\beta}(a) = a\) in the interval \((0,1)\), - \(\beta_j\) is the parameter corresponding to the \(j\)-th subgraph statistic \(H_j\) in the ERGM definition, - \(e_j\) stands for the number of edges in the subgraph \(H_j\).	理论基础学科-基础数学-概率论	91122	2	2
recuV5mtyLghI1	Given a memory pattern matrix $\Xi \in \mathbb{R}^{d \times M}$ and a query matrix $X \in \mathbb{R}^{d \times L}$, where $\|\Xi\|_{\max} \leq B$ and $\|X\|_{\max} \leq B$ with $B = O(\sqrt{\log \tau})$ and $\tau = \max\{M, L\}$. Let the singular values of the association matrix $A = \exp(\beta \Xi^\top X)$ satisfy exponential decay: $\sigma_k(A) \leq C \exp(-k)$ ($C > 0$). A low-rank approximation is used for computation: $Z = \Xi D^{-1} A_k$, where $D = \text{diag}(A_k \mathbf{1}_L)$ and $A_k$ denotes the rank-$k$ approximation of $A$. It is known that the complexity of matrix multiplication is $T_{\text{mat}}(a,b,c) = O(abc)$. Let $\delta_A = \tau^{-c}$ (where $c > 0$ is a constant), and there exists a rank-$k = O(\log(1/\delta_A))$ approximation $A_k$ such that $\|A - A_k\|_2 \leq \delta_A$. Derive the asymptotic expression for the total time complexity $T_{\text{total}}$.	{'step_1': 'Let $\\delta_A = \\tau^{-c}$, then $\\log(1/\\delta_A) = O(\\log \\tau)$. From the phase transition condition $B = O(\\sqrt{\\log \\tau})$, we obtain $B^2 = O(\\log \\tau)$. According to theorem_1, the degree of the polynomial is $g = O(\\beta B^2 d + \\log(1/\\delta_A)) = O(\\beta d \\log \\tau + \\log \\tau) = O(d \\log \\tau).$ Furthermore, from the given condition on the approximation rank, we have $k = O(\\log(1/\\delta_A)) = O(\\log \\tau).$', 'step_2': 'From theorem_2, the time complexity is: $T_{A_k} = O(\\tau k g) = O(\\tau \\cdot \\log \\tau \\cdot d \\log \\tau) = O(d \\tau \\log^2 \\tau).$', 'step_3': 'Using the low-rank decomposition from theorem_2: $A_k \\mathbf{1}_L = U(V^\\top \\mathbf{1}_L)$ Compute $V^\\top \\mathbf{1}_L$: $O(k L) = O(\\tau \\log \\tau)$ Compute $U v$: $O(k M) = O(\\tau \\log \\tau)$ Total time complexity: $T_D = O(\\tau \\log \\tau).$', 'step_4': 'Using lemma_1 and lemma_2: $Z = \\Xi \\underbrace{(D^{-1} U)}_{O(M k)} V^\\top$ Compute $D^{-1} U$: $O(M k) = O(\\tau \\log \\tau)$ Compute $\\Xi Q$: $O(d M k) = O(d \\tau \\log \\tau)$ Compute $R V^\\top$: $O(d k L) = O(d \\tau \\log \\tau)$ Total time complexity: $T_Z = O(d \\tau \\log \\tau).$', 'step_5': '$T_{\\text{total}} = \\underbrace{O(d \\tau \\log^2 \\tau)}_{T_{A_k}} + \\underbrace{O(\\tau \\log \\tau)}_{T_D} + \\underbrace{O(d \\tau \\log \\tau)}_{T_Z}.$ Dominant term analysis: As $\\tau \\to \\infty$, $\\log^2 \\tau \\gg \\log \\tau$, so: $T_{\\text{total}} = O(d \\tau \\log^2 \\tau).$'}	{'theorem_1': 'For a matrix $Y$, there exists a polynomial $P_g$ such that $\\|\\exp(Y) - P_g(Y)\\|_2 \\leq \\delta_A \\exp(\\|Y\\|_2)$, with the degree $g = O(\\|Y\\|_2 + \\log(1/\\delta_A))$, where $\\|Y\\|_2 \\leq \\beta \\|\\Xi\\|_2 \\|X\\|_2 \\leq \\beta B^2 d$.', 'theorem_2': 'The time complexity for computing $A_k = UV^\\top$ is $O(\\tau k g)$, where $\\tau = \\max\\{M, L\\}$, $k$ is the approximation rank, and $g$ is the degree of the polynomial.', 'lemma_1': 'The time for computing $\\Xi D^{-1} A_k$ is $O(d k \\tau)$, which is achieved by calculating $\\Xi(D^{-1} U)V^\\top$ in parts.', 'lemma_2': 'The time complexity for computing $D^{-1} U$ is $O(\\tau k)$.'}	$T_{\text{total}} = O(d \tau \log^2 \tau)$	理论基础学科-理论计算机科学-计算复杂性	1027700	1	4
recuVaTDWqiMHN	In a Distributed Hypothesis Testing (DHT) model, the source \(\boldsymbol{X^n}\) and side information \(\boldsymbol{Y^n}\) are related by \(\boldsymbol{Y^n} = \boldsymbol{X^n} \oplus \boldsymbol{E^n}\), where the elements of \(\boldsymbol{X^n}\) and \(\boldsymbol{E^n}\) are independent and identically distributed (i.i.d.) binary random variables. The source bits satisfy \(P(X=1)=1/2\), and the noise bits satisfy \(P(E=1)=p\). The two hypotheses to be tested concern different values of the noise parameter \(p\): - \(H_0: p = p_0\) - \(H_1: p = p_1\) A quantizer-only scheme is used, built from a linear block code of length \(n\) and message length \(m\) (with generator matrix \(\boldsymbol{G_q}\)). The encoder quantizes a source sequence \(\boldsymbol{x^n}\) to a message sequence \(\boldsymbol{z_q^m}\) such that the Hamming distance between the codeword \(\boldsymbol{x_q^n} = \boldsymbol{G_q} \boldsymbol{z_q^m}\) and \(\boldsymbol{x^n}\) is minimized. After receiving \(\boldsymbol{z_q^m}\), the detector reconstructs the codeword \(\boldsymbol{x_q^n}\). It performs a Neyman-Pearson test where the test statistic is the Hamming distance between the reconstructed codeword \(\boldsymbol{x_q^n}\) and the side information \(\boldsymbol{Y^n}\). The decision rule is to decide for \(H_0\) if \(d(\boldsymbol{x_q^n}, \boldsymbol{Y^n}) < \lambda_q\), where \(\lambda_q\) is an integer threshold. - Let \(E_{\gamma}^{(q)}\) be the number of source sequences \(\boldsymbol{x^n}\) of Hamming weight \(\gamma\) that are quantized to the all-zero codeword. - Let \(N_{0}^{(q)} = \sum_{\gamma=0}^{d_{max}^{(q)}} E_{\gamma}^{(q)}\). Based on the model and definitions above, provide the exact analytical expressions for the Type-I and Type-II error probabilities, \(\alpha_{n}^{(q)}\) and \(\beta_{n}^{(q)}\).	{'Step_1': 'First, the problem is framed using the Neyman-Pearson Hypothesis Testing Framework (Concept_1). This defines the goal: to compute the Type-I error \\(\\alpha_n^{(q)}\\) as 1 - P(decide \\(H_0\\)|\\(H_0\\)) and the Type-II error \\(\\beta_n^{(q)}\\) as P(decide \\(H_0\\)|\\(H_1\\)). The decision rule P(decide \\(H_0\\)) is based on the probability that the test statistic (Hamming distance) is below a threshold, P(d(\\(x_q^n\\), \\(Y^n\\)) ≤ \\(\\lambda_q\\)), thus setting the overall structure for the final formulas.', 'Step_2': 'The probability calculation must account for all possible source sequences \\(x^n\\) that map to a single codeword (e.g., the all-zero codeword). The Law of Total Probability (Theorem_1) is applied to average the outcome over these sequences. This is achieved by partitioning the source sequences by their Hamming weight \\(\\gamma\\) and weighting the conditional probability for each weight class by its prevalence \\(E_\\gamma^{(q)} / N_0^{(q)}\\), which introduces the outer summation structure into the formulas.', 'Step_3': 'To compute the inner conditional probability (for a given source weight \\(\\gamma\\)), the physical relationship between the source, noise, and side information is modeled. The Binary Symmetric Channel (BSC) Model (Concept_2), describing \\(Y^n = X^n \\oplus E^n\\), is used. This model provides the probability of a single, specific noise vector \\(E^n\\) of weight \\(j\\) occurring, which is \\(p^j(1-p)^{n-j}\\). This introduces the core probabilistic term into the summation, evaluated with \\(p_0\\) for \\(H_0\\) and \\(p_1\\) for \\(H_1\\).', 'Step_4': 'For a given source weight \\(\\gamma\\) and a resulting side-information weight \\(\\lambda\\), multiple different noise vectors of a certain weight \\(j\\) could have caused this outcome. We must count these possibilities. The Combinatorial Weight/Distance Identity for GF(2) Vectors (Theorem_2) is used to derive the exact formula for this count, \\(\\Gamma_{\\lambda,j,\\gamma}\\). This term bridges the gap between the probability of a single microscopic event (a specific noise vector) and the probability of the macroscopic observation (the test statistic value).', 'Step_5': 'Finally, all components are assembled to form the complete expressions. The total probability P(decide \\(H_0\\)|\\(H_i\\)) is calculated by summing, over all relevant outcomes (\\(\\lambda\\), \\(\\gamma\\), \\(j\\)), the product of the number of ways an outcome can occur (\\(\\Gamma_{\\lambda,j,\\gamma}\\) from Step 4) and the probability of any one of those ways occurring (\\(p_i^j(1-p_i)^{n-j}\\) from Step 3), and then averaging this sum over all source types (using the weights from Step 2). This final probability is then substituted back into the expressions defined in Step 1 to yield the final formulas for \\(\\alpha_n^{(q)}\\) and \\(\\beta_n^{(q)}\\).'}	"{'Concept_1': 'Neyman-Pearson Hypothesis Testing Framework: A statistical methodology for deciding between two simple hypotheses, \\(H_0\\) and \\(H_1\\), by constructing a decision rule based on a test statistic and a threshold (\\(\\lambda_q\\)). It aims to minimize the Type-II error probability (\\(\\beta\\)) for a fixed Type-I error probability (\\(\\alpha\\)). This framework defines the fundamental structure of the problem and the goal of calculating \\(\\alpha_n^{(q)}\\) and \\(\\beta_n^{(q)}\\).', 'Concept_2': ""Binary Symmetric Channel (BSC) Model: A fundamental channel model where each binary digit transmitted is independently flipped with a crossover probability 'p'. This model mathematically describes the relationship \\(Y^n = X^n \\oplus E^n\\) and is the source of the binomial probability terms \\(p^j(1-p)^{n-j}\\), which represent the likelihood of a noise vector \\(E^n\\) with Hamming weight 'j'."", 'Theorem_1': 'Law of Total Probability: A foundational theorem stating that the probability of an outcome can be computed by summing the conditional probabilities over a set of mutually exclusive and exhaustive events. It is used here to average the overall error probability by conditioning on the Hamming weight (\\(\\gamma\\)) of the source sequence \\(X^n\\), weighting each case by its prevalence (\\(E_\\gamma^{(q)} / N_0^{(q)}\\)) within the quantization cell.', 'Theorem_2': 'Combinatorial Weight/Distance Identity for GF(2) Vectors: A theorem relating the Hamming weights of three binary vectors under XOR addition (A ⊕ B = C). It is used to derive the combinatorial term \\(\\Gamma_{\\lambda,j,\\gamma}\\), which counts the number of ways a source vector of weight \\(\\gamma\\) and a received vector of weight \\(\\lambda\\) can produce an error vector of weight \\(j\\), thereby linking the macroscopic test statistic (\\(\\lambda\\)) to the microscopic error events (\\(j\\)).'}"	The resulting error probabilities are: Type-I Error Probability (\(\alpha_n^{(q)}\)): The probability of deciding \(\mathcal{H}_1\) when \(\mathcal{H}_0\) is true. \[\alpha_{n}^{(q)} = 1-\frac{1}{N_{0}^{(q)}}\sum_{\lambda=0}^{\lambda_{q}}\sum_{\gamma=0}^{d_{max}^{(q)}}\sum_{j=0}^{n}E_{\gamma}^{(q)}\Gamma_{\lambda,j,\gamma}p_{0}^{j}(1-p_{0})^{n-j}\] Type-II Error Probability (\(\beta_n^{(q)}\)): The probability of deciding \(\mathcal{H}_0\) when \(\mathcal{H}_1\) is true. \[\beta_{n}^{(q)} = \frac{1}{N_{0}^{(q)}}\sum_{\lambda=0}^{\lambda_{q}}\sum_{\gamma=0}^{d_{max}^{(q)}}\sum_{j=0}^{n}E_{\gamma}^{(q)}\Gamma_{\lambda,j,\gamma}p_{1}^{j}(1-p_{1})^{n-j}\] where for \(j = \gamma + \lambda - 2u\), and \(0 ≤ u ≤ min(\gamma, \lambda)\). \[\Gamma_{\lambda,j,\gamma} = \binom{\gamma}{u}\binom{n-\gamma}{\lambda-u}\]	理论工程学科-信息论和通信理论-香农信息论	84362	1	4
recuVeOSkxGfQN	In the translational dynamics of a dual-arm aerial manipulation system, what is the explicit expression of the equivalent force term $F_d$ induced by the arm motions in the inertial frame?	{'step_1': '$ \\mathbf r_{CM}=\\frac{1}{M}\\sum_i m_i\\,\\mathbf r_i,\\ \\ \\dot{\\mathbf r}_{CM}=\\frac{1}{M}\\sum_i m_i\\,\\dot{\\mathbf r}_i,\\ \\ \\ddot{\\mathbf r}_{CM}=\\frac{1}{M}\\sum_i m_i\\,\\ddot{\\mathbf r}_i $. In our notation (body-frame): $ \\mathbf r^{B}_{OC},\\ \\dot{\\mathbf r}^{B}_{OC},\\ \\ddot{\\mathbf r}^{B}_{OC} $.', 'step_2': 'Pose mapping: $ \\mathbf x^{W} = {}^{W}\\!R_B\\,\\mathbf x^{B} $; Time derivative (first-order transport form): $ \\tfrac{d}{dt}\\big({}^{W}\\!R_B\\,\\mathbf x^{B}\\big) = {}^{W}\\!R_B\\big(\\boldsymbol{\\omega}_B\\times \\mathbf x^{B} + \\dot{\\mathbf x}^{B}\\big) $. Thus the CoM velocity: $ {}^{W}\\!\\dot{\\mathbf r}_{OC} = {}^{W}\\!R_B\\big(\\boldsymbol{\\omega}_B\\times \\mathbf r^{B}_{OC} + \\dot{\\mathbf r}^{B}_{OC}\\big) $.', 'step_3': '$ {}^{W}\\!\\mathbf a_{OC} = {}^{W}\\!\\ddot{\\mathbf p}_B + {}^{W}\\!R_B\\!\\big[\\dot{\\boldsymbol{\\omega}}_B\\times \\mathbf r^{B}_{OC} + \\boldsymbol{\\omega}_B\\times(\\boldsymbol{\\omega}_B\\times \\mathbf r^{B}_{OC}) + 2\\,\\boldsymbol{\\omega}_B\\times \\dot{\\mathbf r}^{B}_{OC} + \\ddot{\\mathbf r}^{B}_{OC}\\big] $.', 'step_4': '$ m_t\\,{}^{W}\\!\\mathbf a_{OC} = \\mathbf U + \\mathbf F_d - m_t g\\,\\mathbf e_3 $. Substitute Step 3 and gather the terms other than thrust $ \\mathbf U $ and gravity $ -m_t g\\,\\mathbf e_3 $ into $ \\mathbf F_d $, yielding $ F_d = -\\,m_t\\,{}^{W}\\!R_B\\!\\big[\\boldsymbol{\\omega}_B\\times(\\boldsymbol{\\omega}_B\\times \\mathbf r^{B}_{OC}) + \\dot{\\boldsymbol{\\omega}}_B\\times \\mathbf r^{B}_{OC} + 2\\,\\boldsymbol{\\omega}_B\\times \\dot{\\mathbf r}^{B}_{OC} + \\ddot{\\mathbf r}^{B}_{OC}\\big] $.'}	{'concept_1': '$ \\mathbf r_{CM}=\\frac{1}{M}\\sum_i m_i,\\mathbf r_i,\\ \\ \\dot{\\mathbf r}_{CM}=\\frac{1}{M}\\sum_i m_i,\\dot{\\mathbf r}i,\\ \\ \\ddot{\\mathbf r}{CM}=\\frac{1}{M}\\sum_i m_i,\\ddot{\\mathbf r}_i ,.$', 'concept_2': '$ \\ {}^{W}!\\mathbf a = {}^{W}!\\mathbf a_{\\text{origin}} + {}^{W}!R_B!\\big[\\dot{\\boldsymbol{\\omega}}_B\\times \\mathbf r^{B} + \\boldsymbol{\\omega}_B\\times(\\boldsymbol{\\omega}_B\\times \\mathbf r^{B}) + 2,\\boldsymbol{\\omega}B\\times \\dot{\\mathbf r}^{B} + \\ddot{\\mathbf r}^{B}\\big] ,,$ with $\\mathbf r^{B}=\\mathbf r^{B}{OC}$ in this problem.', 'concept_3': '$ \\sum \\mathbf F_{\\text{ext}} = M,\\mathbf a_{CM} ,,$ which in this problem corresponds to $ m_t,{}^{W}!\\mathbf a_{OC} = \\mathbf U + \\mathbf F_d - m_t g,\\mathbf e_3 ,.$'}	$F_d = -\,m_t\,{}^{W}\!R_B\!\big[\boldsymbol{\omega}_B\times(\boldsymbol{\omega}_B\times \mathbf r^{B}_{OC}) + \dot{\boldsymbol{\omega}}_B\times \mathbf r^{B}_{OC} + 2\,\boldsymbol{\omega}_B\times \dot{\mathbf r}^{B}_{OC} + \ddot{\mathbf r}^{B}_{OC}\big]$ where $m_t$: total mass of the system. ${}^{W}\!R_B$: rotation matrix from the body frame $B$ to the inertial frame $W$. $\mathbf r^{B}_{OC}$: position of the overall center of mass $C$ relative to the body-frame origin $O$ (expressed in $B$). $\dot{\mathbf r}^{B}_{OC}$, $\ddot{\mathbf r}^{B}{OC}$: first/second time derivatives of $\mathbf r^{B}{OC}$ (induced by joint velocities and accelerations of the arms). $\boldsymbol{\omega}_B$, $\dot{\boldsymbol{\omega}}_B$: body angular velocity and angular acceleration (expressed in $B$). $\times$: the 3-D vector cross product.	理论工程学科-控制理论与控制工程-非线性系统	89584	2	3
recuVfJj6JEr5w	For pairs of graphs over n vertices that are distinguishable by k-dimensional Weisfeiler-Leman (k-WL), determine the lower bound on the number of refinement iterations required for a $(k+c-1)$-dimensional WL algorithm to distinguish them, expressed as a function of n, k, and c, where $1 \le c \le k-1$. This problem investigates the trade-off between the dimension (number of variables) and the iteration number (quantifier depth) in the Weisfeiler-Leman algorithm.	"{'step_1': 'The goal is to determine a lower bound on the refinement iterations for a $(k+c-1)$-dimensional Weisfeiler-Leman (WL) algorithm, expressed as a specific function of n, k, and c, as per the instruction.', 'step_2': 'Following Concept_2, the problem is translated from the domain of the WL algorithm to the cop-robber game. We need to find the number of rounds a robber can survive against $k+c-1$ cops to find the corresponding WL iteration count.', 'step_3': ""To derive a strong, non-trivial trade-off, the analysis uses the advanced techniques of Graph Compression (Concept_3) and the Compressed Cop-Robber Game (Concept_4). The lower bound is derived from the robber's performance in this specific game variant."", 'step_4': ""To proceed and get a functional form, a quantitative theorem specifying the robber's survival time is necessary. This theorem, which is missing from the provided concepts, would need to state that a robber can survive for a number of rounds T(n, k, c) against $k+c-1$ cops on a specially constructed graph."", 'step_5': ""With the robber's survival time T from the missing theorem, we apply the provided Theorem_1. This theorem establishes a direct translation, stating that T rounds of robber survival implies a lower bound of T iterations for the corresponding WL algorithm."", 'step_6': 'Therefore, the final answer for the WL iteration lower bound would be exactly the function T(n, k, c) provided by the missing quantitative theorem from Step 4. Without that specific theorem, the final functional form cannot be derived.'}"	"{'concept_1': 'The Weisfeiler-Leman (WL) algorithm is an iterative procedure for graph isomorphism testing. The k-dimensional version (k-WL) refines a coloring of k-tuples of vertices at each step. The number of steps until the coloring stabilizes is the iteration number.', 'concept_2': 'The cop-robber game is played on a graph where k cops attempt to capture a robber. There is a close correspondence between this game and the WL algorithm, where the number of cops relates to the WL dimension and the number of game rounds relates to the WL iteration number.', 'concept_3': 'Graph compression is a technique where vertices and edges of a graph are identified via an equivalence relation. This creates a new, compressed structure on which a modified game can be played.', 'concept_4': 'The compressed cop-robber game is a variant played on a compressed graph. In this game, cops occupy all vertices equivalent to their chosen position, and the robber must make moves that are closed under the edge equivalence relation.', 'theorem_1': ""A lower bound of T rounds for a robber's survival against k cops in the (compressed) cop-robber game directly translates to a lower bound of T iterations for the k-dimensional Weisfeiler-Leman algorithm to distinguish certain graph pairs.""}"	\(\Omega(n^{k/(c+1)})\)	理论基础学科-理论计算机科学-计算复杂度	341907	2	2
recuVgrbk7thHs	Let the operator $L_{p,q}$ be defined as $L_{p,q}(�egin{matrix}u\\ v\end{matrix})=(\begin{matrix}0&-1\\ 1&0\end{matrix})\frac{d}{dt}(\begin{matrix}u\\ v\end{matrix})+(\begin{matrix}-p&q\\ q&p\end{matrix})(\begin{matrix}u\\ v\end{matrix})$, where $p(x)\in L^2(0,\infty)$, $q(x)\in L^2(0,\infty)$, and $\lim_{x\rightarrow\infty}\sup x\sqrt{p^{2}(x)+q^{2}(x)}=A$ for some $A>0$. If $A<1/2$, does the operator $L_{p,q}$, under the boundary condition $u(0)\sin \phi_0 - v(0)\cos \phi_0 = 0$ for some $\phi_0 \in [0,\pi)$, have eigenvalues within its essential spectrum?	"{'step_1': 'The condition $A < \\infty$ implies that $\\sqrt{p^{2}(x)+q^{2}(x)} = O(1/x)$, which is $o(1)$ as $x \\to \\infty$. By **Theorem 1**, the essential spectrum of $L_{p,q}$ is $(-\\infty, \\infty)$. Therefore, any potential eigenvalue would be an embedded eigenvalue. The condition for a number $\\lambda$ to be an eigenvalue is that the amplitude $R(x)$ of its corresponding solution is square-integrable, i.e., $R \\in L^2(0, \\infty)$. The proof proceeds by showing $R \\notin L^2(0, \\infty)$.', 'step_2': ""Using **Concept 2**, we define the potential's amplitude as $V(x) = \\sqrt{p^2(x)+q^2(x)}$. The initial condition becomes $\\limsup_{x\\rightarrow\\infty} xV(x) = A < 1/2$. From the polar coordinate transformation **Concept 2**, the solution's amplitude $R(x)$ is governed by the differential equation $\\frac{d}{dx}\\ln R(x) = -V(x)\\cos(2\\theta(x)-\\varphi(x))$."", 'step_3': 'Using **Theorem 3** (the definition of limsup), the condition $\\limsup_{x\\rightarrow\\infty} xV(x) = A$ implies that for any $\\epsilon > 0$, we can find an $x_0$ such that for all $x > x_0$, the inequality $V(x) \\le \\frac{A+\\epsilon}{x}$ holds. We can choose $\\epsilon$ to be small enough such that $A+\\epsilon < 1/2$.', 'step_4': 'We integrate the equation for $\\ln R(x)$ from $x_0$ to $x$ and apply the bound on $V(x)$ (since $|\\cos(\\cdot)| \\le 1$): $\\ln R(x) = \\ln R(x_0) - \\int_{x_0}^x V(t)\\cos(2\\theta(t)-\\varphi(t))dt \\ge C - \\int_{x_0}^x \\frac{A+\\epsilon}{t} dt$. This yields $\\ln R(x) \\ge C - (A+\\epsilon)\\ln x$, which means for large $x$, $R(x)$ decays no faster than $kx^{-(A+\\epsilon)}$ for some constant $k > 0$.', 'step_5': 'For $R(x)$ to correspond to an eigenfunction, we need the integral $\\int^\\infty R(x)^2 dx$ to converge. However, we have the lower bound $R(x)^2 \\ge k^2 x^{-2(A+\\epsilon)}$. Since we chose $\\epsilon$ such that $2(A+\\epsilon) < 1$, the exponent is greater than -1, causing the integral $\\int^\\infty x^{-2(A+\\epsilon)} dx$ to diverge. Therefore, $R \\notin L^2(0, \\infty)$, which proves that there are no eigenvalues.'}"	{'Theorem_1': 'If $\\sqrt{p^{2}(x)+q^{2}(x)}=o(1)$, then the essential spectrum of $L_{p,q}$ is $(-\\infty,\\infty)$.', 'Concept_2': 'The Cartesian coordinates x and y can be converted to polar coordinates r and φ with r ≥ 0 and φ in the interval (−π, π] by: r＝√(x²＋y²)，φ＝arctan y/x.', 'Theorem_3': 'Let $\\limsup_{x\\rightarrow\\infty} f(x) = A$. Then, for any given $\\epsilon > 0$, there exists a number $x_0$ such that for all $x > x_0$, the inequality $f(x)\\le A+\\epsilon$ holds.'}	No. (Guessing the answer by analogy with similar operators or considering only special cases is not correct. Moreover, after changing A to 1, the model still gives such similar answers. Note that, however, when A>1/2, L_{p,q} has a eigenvalue within its essential spectrum.)	理论基础学科-基础数学-偏微分方程	259091	2	3
recuVgLcWqyGkY	Give a description of representations of Nichols Hopf algebras without concrete fusion rules and dimension information.	{'step_1': 'To describe representations of Nichols Hopf algebras, start with the definition of irreducible modules in an FTC (concept_1), which are simple objects with no nontrivial subobjects. For $K_n$, these are the 1-dimensional $V_{\\epsilon}$ (trivial action) and $V_{\\bar{K}}$ (sign action).', 'step_2': 'Next, using the concept of projective covers (concept_2), which are projective indecomposable modules surjecting onto irreducible modules, we identify $P_{\\epsilon}$ and $P_{\\bar{K}}$ as projective covers of $V_{\\epsilon}$ and $V_{\\bar{K}}$, constructed as submodules of $K_n$ generated by $\\Lambda^{*}E \\cdot (1 + K)$ and $\\Lambda^{*}E \\cdot (1 - K)$ respectively.', 'step_3': 'Finally, based on the property of tensor products in FTCs (concept_3) – that tensor products of projectives are projective – we derive fusion rules: $V_{\\bar{K}} \\otimes P_{\\epsilon}$ (a projective module) must be $P_{\\bar{K}}$ (matching dimension and Hom properties), and tensor products of $P_{\\epsilon}$ and $P_{\\bar{K}}$ decompose into equal sums of both projectives due to dimension counting and linearity of Hom functors.'}	{'concept_1': 'Irreducible module in finite tensor category (FTC): A simple object in an FTC, with no nontrivial subobjects. For $K_{n}$-mod, these are $V_{\\epsilon}$ and $V_{\\bar{K}}$.', 'concept_2': 'Projective cover in FTC: For each irreducible module $V$, a projective indecomposable module $P(V)$ such that there exists a surjective homomorphism $P(V) \\to V$ with superfluous kernel. For $K_{n}$-mod, $P_{\\epsilon}$ and $P_{\\bar{K}}$ are projective covers of $V_{\\epsilon}$ and $V_{\\bar{K}}$.', 'concept_3': 'Tensor product of modules in FTC: The tensor product of a projective module with any module is projective, and fusion rules describe the decomposition of tensor products into direct sums of indecomposable projectives.'}	There are two non-isomorphic irreducible $\Kn$-modules $V_\epsilon, V_{\bar K}$ of dimension 1, and each has a projective cover $P_\epsilon, P_{\bar K}$ (resp.).	理论基础学科-基础数学-代数	455033	1	1
recuVlpJt54hyn	Let $n\geq 2$, $\Omega\subset \mathbb{R}^n$ be a bounded domain with boundary $\Gamma$ of class $C^2$. Suppose $u\in C^2(\overline{\Omega})$ is a solution to the following equation    \[\begin{cases}\Delta u=n &~\text{in }\Omega\\u=0&~\text{on }\partial\Omega\end{cases}.\]   Define   \[R=\frac{1}{|\Gamma|}\int_{\Gamma}^{}|\nabla u|dS, \qquad \delta=\left\| |\nabla u|-R \right\|_{L^{\infty}(\Gamma)}.\]   It's already established that $u$ will satisfy the following integral inequality:   \[\int_{\Omega}(-u)|D^2h|^2dx\leq C(n, d_\Omega, r_i, r_e)\delta\]   where $h=q-u$, $q=\frac{1}{2}|x-z|^2$, $d_\Omega$ is the diameter of $\Omega$, $r_i$ is the radius of the uniform interior ball, $r_e$ is the radius of the uniform exterior ball and $C(n, d_\Omega, r_i, r_e)$ denotes a constant depends on these quantities.     Take $z$ as the global minimum point of u so that $\nabla h(z)=0$, and define $\rho_e=\max_{\Gamma}|x-z|$, $\rho_i=\min_{\Gamma}|x-z|$. Utilizing the integral inequlity and Sobolev embedding theorem, can we obtain the stability inequality $\rho_e-\rho_i\leq C(n, d_\Omega, r_i, r_e)\delta^{1/(n-1)}$ for given $n\geq 4$? If your answer is yes, you should list all the Sobolev inequality that you use with concrete exponent.	{'step_1': 'apply the weighted Sobolev embedding theorem with $r=2N/(N-3)$ and $p=\\frac{2N}{N-1}$ to $h$ to obtain the estimate: $\\left\\|h-h_\\Omega\\right\\|_{L^{2N/(N-3)}}\\leq C\\left\\|\\nabla h\\right\\|_{L^{2N/(N-1)}}$.', 'step_2': 'apply the weighted Sobolev embedding theorem with $r=2N/(N-1)$, $p=2$, $\\alpha=1/2$ to $\\nabla h$ to obtain the estimate: $\\left\\|\\nabla h\\right\\|_{L^{2N/(N-1)}}\\leq C\\left\\|\\delta_{\\Gamma}^{1/2}\\nabla^2h\\right\\|_{L^2}$.', 'step_3': 'choose $p=2N/(N-3)$ in Theorem 1 to obtain a stability inequality in terms of the diviation $\\left\\|h-h_\\Omega\\right\\|_{L^{2N/(N-3)}}^{2/(N-1)}$.', 'step_4': 'conclude with the three inequalities establishe in step 1, 2, 3 to obtain a primary inequality $\\rho_e-\\rho_i\\leq C\\left\\|\\delta_\\Gamma^{1/2}\\nabla^2h\\right\\|_{L^2}^{2/(N-1)}$.', 'step_5': 'apply the given integral inequality that $u$ satisfies and the lower bound on $-u$, together with the inequality established in step 4 to obtain $\\rho_e-\\rho_i\\leq C\\delta^{1/(N-1)}$.'}	{'concept/theorem_1': 'With the same assumption on $\\Omega$ and $u$, the following estimate holds:\n \\[\\rho_e-\\rho_i\\leq C(n, p, d_\\Omega, r_i, r_e)\\left\\|h-h_\\Omega\\right\\|_{L^p(\\Omega)}^{p/(n+p)} \\text{ for any } 1\\leq p<\\infty,\\] \n where $h_\\Omega=\\frac{1}{|\\Omega|}\\int_\\Omega h(x)dx$ denotes the mean of $h$ over $\\Omega$.', 'concept/theorem_2': 'With the same assumption on $\\Omega$, the following weighted Sobolev embedding holds:\n \\[\\left\\|h-h_\\Omega\\right\\|_{L^r(\\Omega)}\\leq C(n, r_i, r_e, d_\\Omega)\\left\\|\\nabla h\\right\\|_{L^p(\\Omega)}\\]\n for all $r, p>0$ such that $1\\leq p\\leq r\\leq Np/(N-p)$ and $p<N$ and $h$ that is harmonic in $\\Omega$.', 'concept/theorem_3': 'With the same assumption on $\\Omega$, the following weighted Sobolev embedding holds:\n \\[\\left\\|h\\right\\|_{L^r(\\Omega)}\\leq C(n, r_i, r_e, d_\\Omega, p, r, \\alpha)\\left\\|\\delta_\\Gamma^\\alpha \\nabla h\\right\\|_{L^p(\\Omega)}\\]\n for all $r, p>0$, $0\\leq \\alpha\\leq 1$ such that $1\\leq p\\leq r\\leq Np/(N-p(1-\\alpha))$ and $p(1-\\alpha)<N$ and harmonic $h$ in $\\Omega$ such that $h(z)=0$, where $\\delta_\\Gamma(x)$ denotes the distance of $x$ to $\\partial\\Omega$.', 'concept/theorem_4': 'With the same assumption on $\\Omega$ and $u$, $u$ admits the following estimate\n \\[(-u)\\geq \\frac{r_i}{2}\\delta_\\Gamma \\text{ in } \\Omega\\]'}	Yes. The following are the Sobolev inequalities used in the proof:  1. $\left\|h-h_\Omega\right\|_{L^{2N/(N-3)}}\leq C\left\|\nabla h\right\|_{L^{2N/(N-1)}}$.  2. $\left\|\nabla h\right\|_{L^{2N/(N-1)}}\leq C\left\|\delta_\Gamma^{1/2}\nabla^2h\right\|_{L^2}$.	理论基础学科-基础数学-偏微分方程	100329	2	3
recuVn5VMX99A8	In different nonlinear multimode waveguide optical systems, when the system satisfies ergodicity and within the framework of the microcanonical ensemble, what relationship is satisfied between the mode occupation number |c_j|^2 after the system thermalizes and the eigenvalue ε_j of the linear supermode Hamiltonian?	{'step_1': 'Microcanonical ensemble: the multimode nonlinear waveguide system is closed (no pump/loss) and ergodic. In the weak-nonlinearity regime, the linear-part energy U is quasi-invariant alongside the strictly conserved total power P.', 'step_2': 'From the discrete nonlinear Schrödinger equation (DNLS) or the appropriate equations of motion, recognize the Hamiltonian structure and write explicit expressions for the conserved total power P and the Hamiltonian H.', 'step_3': 'Expand the field in the linear supermode basis to define modal amplitudes c_j; decompose the Hamiltonian as H = H_L + H_{\\mathrm{NL}}. While H_NL depends on the specific nonlinearity. one can define a quasi-invariant internal energy U = \\sum_j \\varepsilon_j |c_j|^2, in weak nonlinearity, where \\varepsilon_j are the eigenvalues of the linear supermode Hamiltonian.', 'step_4': 'Macrostates and entropy: use modal occupations n_j \\equiv |c_j|^2 as macro-variables and adopt the optical entropy S = \\sum_j \\ln(n_j), (up to an additive constant), since phases are uniformly distributed under ergodicity and do not affect macrostates.', 'step_5': 'Constrained maximization: maximize S subject to \\sum_j n_j = P, \\quad \\sum_j \\varepsilon_j n_j = U, using Lagrange multipliers \\alpha, \\beta. Define the Lagrangian \\mathcal{L} = \\sum_j \\ln(n_j) - \\alpha \\left( \\sum_j n_j - P \\right) - \\beta \\left( \\sum_j \\varepsilon_j n_j - U \\right).', 'step_6': 'Stationarity:\\frac{\\partial \\mathcal{L}}{\\partial n_j} = 0, hence n_j = \\frac{1}{\\alpha + \\beta \\varepsilon_j}. This is the Rayleigh–Jeans form for modal occupancies. Determine \\alpha, \\beta from the invariants P and U; .'}	{'concept_1': 'Relation between the discrete nonlinear Schrödinger equation and the Hamiltonian. In a nonlinear multimode waveguide optical system, the field propagation in a waveguide array can be described by the discrete nonlinear Schrödinger equation (DNLS). For example, in the Kerr-nonlinearity case: i \\frac{d a_m}{dz} + a_{m-1} + a_{m+1} + |a_m|^2 a_m = 0, where a_m is the complex amplitude in the m-th waveguide; the nearest-neighbor coupling terms a_{m\\pm1} represent coupling between adjacent waveguides, and the |a_m|^2 a_m term represents the Kerr nonlinear effect. Under weak-nonlinearity conditions, the linear-part energy U = \\sum_{j=1}^M \\varepsilon_j |c_j|^2 is approximately conserved (here \\varepsilon_j are the eigenvalues of the linear eigenmodes).', 'concept_2': 'Definition of the microcanonical ensemble in optics. The microcanonical ensemble is the statistical-mechanics framework used to describe isolated systems. The total energy (and other conserved quantities such as total particle number) is fixed, and the system is assumed to uniformly explore all microstates consistent with those constraints. In optical thermodynamics, the microcanonical ensemble corresponds to a closed system (no external pumping or loss) with conserved quantities: the total power P and the (quasi-)linear energy U.', 'concept_3': 'Common definition of optical entropy in optical thermodynamics. Within the microcanonical framework for multimode optical systems, a frequently used definition of optical entropy is S = \\sum_{j=1}^M \\ln\\big(|c_j|^2\\big), where c_j is the complex amplitude of the j-th linear eigenmode. This form arises from the logarithm of the phase-space volume (neglecting phase degrees of freedom when phases are uniformly distributed over [0,2\\pi]). Maximizing this entropy corresponds to the system reaching thermal equilibrium.'}	Rayleigh–Jeans distribution, or specifically in the form of |c_j|^2 = \frac{1}{\alpha + \beta \varepsilon_j} for all of the nonlinear process	其他理论学科-电磁理论与量子电磁学-光子热力学	50454	1	3
recuVoRgdoVCJJ	In the context of fairly allocating indivisible chores among agents with additive disutility preferences, the concept of envy-freeness up to any chore (EFX) is a compelling fairness criterion. An allocation is $\alpha$-EFX if for any pair of agents $(i, h)$, the disutility for agent $i$ of their own bundle minus any single chore $j$ is at most $\alpha$ times the disutility for agent $i$ of agent $h$'s bundle ($d_{i}(x_{i}\backslash\{j\})\le \alpha\cdot d_{i}(x_{h})$). Given that the existence of an exact EFX ($\alpha=1$) allocation is a major open question, what is the constant-factor approximation guarantee $\alpha$ for which an $\alpha$-EFX allocation is proven to exist for any chore instance?	"{'step_1': 'The process begins by using an Earning-Restricted (ER) Competitive Equilibrium (Concept_1) to generate a fractional allocation, which is then rounded. This allows for the partitioning of chores and agents into low-payment/high-payment groups ($N_0$ and $N_H$) (Concept_2).', 'step_2': 'The analysis focuses on finding the worst-case envy, which occurs with agents in the $N_0$ group. This is because their bundles have the highest potential payment relative to the guaranteed minimum payment for any other agent.', 'step_3': ""The key to quantifying this envy is Concept_4, which states that for any agent, the ratio of their disutility for two different bundles is equal to the ratio of the bundles' payments."", 'step_4': ""Theorem_2 provides the numerator for this worst-case ratio: the maximum possible payment for an agent's bundle in group $N_0$ is 2."", 'step_5': ""To complete the ratio, we need the denominator, which is the guaranteed minimum payment for any agent's bundle. In the underlying allocation framework, this minimum payment is established as 1/2."", 'step_6': 'Using these bounds, the constant factor $\\alpha$ is calculated by taking the ratio of the maximum possible payment to the minimum possible payment: $\\alpha = \\frac{\\text{max_payment}}{\\text{min_payment}} = \\frac{2}{1/2} = 4$.', 'step_7': 'For the remaining agents in group $N_H$, their envy is resolved to a guarantee that is at least as good as this worst-case bound by using a combination of EFX re-allocations (Theorem_1) and ordered Chore Swaps (Concept_3, Theorem_3).', 'step_8': 'Since the highest possible envy ratio across all scenarios is 4, it is concluded that a 4-EFX allocation is guaranteed to exist.'}"	"{'concept_1': 'Earning-Restricted (ER) Competitive Equilibrium: A fractional allocation framework that imposes an upper limit on the total earnings agents can collectively derive from any single chore. This equilibrium reveals more information about agent preferences than an unrestricted one.', 'concept_2': 'Partitioning by Payments: Based on an ER equilibrium, chores are partitioned into a set L of low-paying chores (payment $\\le 1/2$) and a set H of high-paying chores (payment $> 1/2$). Agents are correspondingly partitioned into $N_0$ (receiving only chores from L) and $N_H$ (receiving at least one chore from H).', 'concept_3': ""Chore Swap: An operation to resolve envy for an agent $i \\in N_H$. The agent $i$ transfers their single high-paying chore to the agent $l$ they envy most, and in return, receives agent $l$'s entire bundle of chores."", 'concept_4': 'Payment as a Proxy for Disutility: For any given agent $i$, the disutility they feel for any bundle $x_h$ is directly proportional to the total payment $p(x_h)$ of that bundle. Therefore, the ratio of disutilities for two different bundles is equal to the ratio of their payments.', 'theorem_1': 'EFX Allocation for Limited Chores: An exact EFX allocation can be computed in polynomial time when the number of chores $m$ is at most twice the number of agents $n$.', 'theorem_2': 'Worst-Case Payment Ratio Bounds: The initial integral allocation establishes the bounds for the worst-case envy scenario. The bundle payment for an agent in the set $N_0$ can be as high as 2.', 'theorem_3': 'Envy Resolution for Agents with High-Paying Chores: For agents in $N_H$, a combination of EFX re-allocation of their high-paying chores and a series of ordered Chore Swaps can resolve their envy to a constant-factor EFX guarantee.'}"	4	理论基础学科-理论计算机科学-算法理论	153977	2	5
recuVqCOpFM6Fu	For the unsteady Prandtl's system (boundary layer equations), under the favorable pressure condition (\(partial xP (x, t) \leq 0 for t > 0, 0 < x \leq L) and the initial and boundary data satisfying the monotonicity conditions (\(\partial_y u_0 > 0, \partial_y u_1 > 0\)), does the global weak solution exist? Does the weak solution have uniqueness and continuous dependence on the initial and boundary data? And is the weak solution smooth in the interior of the domain? The two-dimensional unsteady Prandtl's system: \(\left\{ \begin{array}{ll}\partial_t u + u\partial_x u + v \partial_y u + \partial_x P = \nu \partial_y^2 u, & 0 < x < L, \, y > 0, \\partial_x u + \partial_y v = 0, & \\u\big|_{t=0} = u_0(x, y), \, u\big|_{y=0} = 0, & \\v\big|_{y=0} = v_0(x, t), \, u\big|_{x=0} = u_1(y, t), & \\u(x, y, t) \to U(x, t), & y \to +\infty\end{array}\right.\)	"{'step_1': 'First, the ""monotonicity conditions of initial and boundary data"" in the Instruction correspond to concept_2, which is an essential premise for constructing and analyzing weak solutions in the document, laying the foundation for subsequent research on weak solutions.', 'step_2': 'For the existence of global weak solutions, the document uses concept_3 (Direct BV Estimate) to directly analyze and bound the total variation of the solution, obtain key a priori estimates such as the boundedness of the solution and its derivatives, and thus derive the existence of the global weak solution.', 'step_3': 'For the uniqueness and continuous dependence of weak solutions, concept_5 (Kruzkov-type \\(L^1\\)-Contraction Argument) is adopted: by constructing entropy functions and test functions, an \\(L^1\\)-norm contraction estimate is established, proving that the difference between two weak solutions is bounded by the difference of initial and boundary data, thereby verifying the uniqueness and continuous dependence of the weak solution.', 'step_4': 'For the interior smoothness of weak solutions, the document first uses concept_4 (Crocco Transformation) to simplify the original Prandtl system into an ultraparabolic equation through specific variable substitutions, then applies concept_1 (Regularity Theory for Ultraparabolic Equations) — the ""weak"" Poincaré inequality connects the solution with its average value, and the oscillation estimate proves that the solution\'s oscillation decays with the domain scale, jointly deriving the smoothness of the weak solution in the interior of the domain.', 'step_5': ""Synthesizing these analyses, the final Answer consistent with the document's conclusions is obtained.""}"	"{'concept_1': 'Regularity Theory for Ultraparabolic Equations: A theoretical system for analyzing the smoothness of ultraparabolic equation solutions, including the ""weak"" Poincaré inequality and oscillation estimate.', 'concept_2': 'Monotonicity Conditions of Initial and Boundary Data for the Prandtl System: Conditions that \\(\\partial_y u_0(x,y) > 0\\) (monotonicity of initial velocity \\(u_0\\) in y) and \\(\\partial_y u_1(t,y) > 0\\) (monotonicity of boundary velocity \\(u_1\\) in y), which are essential for constructing and analyzing weak solutions.', 'concept_3': 'Direct BV Estimate: A method to obtain a priori estimates (e.g., boundedness of functions and their derivatives) by analyzing and bounding the total variation of functions with finite total variation.', 'concept_4': 'Crocco Transformation: A variable transformation method that simplifies the original equation via specific substitutions.', 'concept_5': 'Kruzkov-type \\(L^1\\)-Contraction Argument: A method using entropy and test functions to establish \\(L^1\\)-norm contraction estimates, proving solution differences are bounded by initial/boundary data differences.'}"	For the unsteady Prandtl's system (boundary layer equations), under the favorable pressure condition (\(\partial_x P \leq 0\)) and the initial and boundary data satisfying the monotonicity conditions (\(\partial_y u_0 > 0, \partial_y u_1 > 0\)), the global weak solution exists; the weak solution has uniqueness and continuous dependence on the initial and boundary data; and the weak solution is smooth in the interior of the domain.	理论工程学科-理论工程力学-流体力学	598872	1	5
recuVr8vkzTFXk	A single-bit Pseudorandom Code (PRC) is used to encode a message $m \in \{0, 1\}$ into a codeword $c$. An attacker's goal is to break the code's adaptive robustness by finding a corrupted codeword $c'$ that the decoder cannot uniquely map back to the correct original message. What is the highest value of the robustness parameter $\delta$, representing the fraction of tolerable errors, that is theoretically impossible for any such PRC to achieve?	"{'step_1': ""An attacker's goal, as stated in the instruction, is to create a single string $c'$ that causes decoding ambiguity for a single-bit PRC. To begin, the attacker considers the codewords for both possible messages, '0' and '1', as established by concept_1. Let's call these representative codewords $c_0$ and $c_1$."", 'step_2': 'Concept_2 is the critical insight: $c_0$ and $c_1$ are computationally indistinguishable from independent, uniformly random binary strings of length $n$. A direct consequence of this property is that they are expected to agree on approximately half of their bit positions ($n/2$) and disagree on the other half ($n/2$). Their mutual Hamming distance is therefore approximately $n/2$.', 'step_3': ""The attacker now constructs the malicious string $c'$. The strategy is to find a point in the middle of $c_0$ and $c_1$. For the $n/2$ positions where $c_0$ and $c_1$ agree, $c'$ is set to that same value. For the $n/2$ positions where they disagree, the attacker sets the bits of $c'$ to match $c_0$ in the first half of these positions ($n/4$ bits) and to match $c_1$ in the second half ($n/4$ bits)."", 'step_4': ""By this construction, we can calculate the distance of $c'$ from the original codewords. The string $c'$ differs from $c_0$ only in those positions where it was forced to match $c_1$. This occurred in $n/4$ positions. Thus, the fractional distance is $(n/4) / n = 1/4$. Symmetrically, the fractional distance from $c'$ to $c_1$ is also $1/4$."", 'step_5': ""Now, we apply the failure condition defined in concept_3. The attacker has successfully produced a string $c'$ that is simultaneously within a fractional distance of $1/4$ of a codeword for '0' and a codeword for '1'."", 'step_6': ""If a PRC were to claim a robustness of $\\delta = 1/4$, this string $c'$ would meet the condition for being a valid corruption of both $c_0$ and $c_1$. However, concept_4 states the decoder must output a single, unique result. It cannot resolve this ambiguity."", 'step_7': 'Therefore, any PRC claiming a robustness of $\\delta \\ge 1/4$ can be broken by this attack. This means that achieving a robustness of $1/4$ (or higher) is theoretically impossible. The highest value of $\\delta$ that is impossible to achieve begins exactly at this boundary.'}"	"{'concept_1': 'A single-bit PRC generates a codeword for each of the two messages, 0 and 1.', 'concept_2': 'Pseudorandomness dictates that the generated codewords for different messages are computationally indistinguishable from independent, uniformly random binary strings.', 'concept_3': ""The $\\delta$-robustness of a PRC is violated if an adversary can produce a string $c'$ such that it is within a fractional distance $\\delta$ of a valid codeword for message '0' and also within a fractional distance $\\delta$ of a valid codeword for message '1'."", 'concept_4': 'The decoder is deterministic and must output only one message (0, 1, or failure) for any given input string.'}"	2026-01-04 00:00:00	理论基础学科-理论计算机科学-密码学理论	687693	4	2
recuVthcAnXt8x	Let the actuated variable $q_a$ and the unactuated variable $q_u$ be the generalized coordinates of the following kinematic model of 2-DOF underactuated mechanical system. Given its equilibrium $(\dot q_{ud}, q_{ud} )=(0,0)$, and the reference $q_{ar}$ of $q_a$ with the finite-time convergence property of $q_{ar}(t)=q_{ad}, \dot q_{ar}(t)=\ddot q_{ar}(t)=0, \forall t>t_1$. Then, defining the error signals $e_a=q_a-q_{ar}, e_u=q_u-q_{ud}$, an online trajectory generation law is $\dot q_a=\dot q_{ar}-\rho_a^2(q_a-q_{ar})+\rho_c^2\Delta$, where $\rho_a, \rho_c, \Delta $ are generated by $\rho_a(\dot q_uM_{ua}\dot e_a+\dot q_u C_{ua} e_a)+\dot \rho_a(k_1+2\dot q_uM_{ua} e_a)=0$, $\rho_c \dot q_uC_{ua}\Delta-\dot \rho_c(k_3-2\dot q_uM_{ua} \Delta)=0$, $\dot \Delta=k_2M_{ua}\dot q_u$, with the parameters $k_1>0, k_2>0, k_3>0$. Without any other assumptions, applying the online trajectory generation law to the kinematic model, is the conclusion $\dot e_a \rightarrow 0, e_a \rightarrow 0, \dot e_u \rightarrow 0, e_u \rightarrow 0, as t\rightarrow \infty $ true?	{'step_1': 'Leverage Concept 2 to Clarify System Properties and Error Definitions\nFirst, we use \\textbf{Concept 2 (Property of 2-DOF underactuated systems)} to establish foundational system characteristics:\n- The unactuated mass matrix $M_{uu}$ is positive-definite ($M_{uu} > 0$), and its time derivative satisfies $\\dot{M}_{uu}/2 = C_{uu}$ (a critical relation for simplifying energy derivatives).\n- The coupling term $M_{ua} \\neq 0$ (ensures interaction between actuated and unactuated dynamics, essential for transmitting control effects to $q_u$).\n- The gravity term for the unactuated variable satisfies $G_u = 0 \\iff e_u = 0$ (links the potential energy state to the unactuated error $e_u = q_u - q_{ud} = q_u$, since $q_{ud} = 0$).\n- The potential energy derivative is $\\dot{U} = \\dot{q}_u G_u = \\dot{e}_u G_u$ (as $\\dot{q}_u = \\dot{e}_u$, directly relating unactuated velocity error to potential energy change).\nFor the actuated side, the reference $q_{ar}(t)$ has finite-time convergence ($q_{ar}(t) = q_{ad}, \\dot{q}_{ar}(t) = \\ddot{q}_{ar}(t) = 0$ for $t > t_1$), so the actuated error $\\dot{e}_a = \\dot{q}_a - \\dot{q}_{ar} = \\dot{q}_a$ and $\\ddot{e}_a = \\ddot{q}_a$ for $t > t_1$—this simplifies subsequent trajectory law analysis.', 'step_2': 'Apply Concept 3 to Construct the Lyapunov Function\nPer \\textbf{Concept 3 (Energy-based control method)}, the Lyapunov function for underactuated systems typically combines mechanical energy (kinetic + potential) and positive functions of auxiliary variables (e.g., $\\rho_a, \\rho_c, \\Delta$). We construct the candidate Lyapunov function as:\n\\[\nV = \\underbrace{\\frac{1}{2}M_{uu}\\dot{e}_u^2 + U}_{\\text{Mechanical Energy (from Concept 3)}} + \\underbrace{\\frac{1}{2}k_1\\rho_a^2 + \\frac{1}{2}k_3\\rho_c^2 + \\frac{1}{2k_2}\\Delta^2}_{\\text{Positive Auxiliary Term (from Concept 3)}}\n\\]\n- **Non-negativity of $V$**: $M_{uu} > 0$ (Concept 2) implies $\\frac{1}{2}M_{uu}\\dot{e}_u^2 \\geq 0$; potential energy $U \\geq 0$ (bounded below for mechanical systems); and the auxiliary term is positive since $k_1, k_2, k_3 > 0$ and all terms are squared. Thus, $V \\geq 0$ (non-negative definite), and $V = 0$ only if $\\dot{e}_u = 0, U = 0$ (hence $G_u = 0 \\iff e_u = 0$ by Concept 2), $\\rho_a = 0, \\rho_c = 0, \\Delta = 0$.', 'step_3': 'Compute $\\dot{V}$ and Prove Its Non-Positivity\nTo apply LaSalle’s Theorem, we first derive $\\dot{V}$ and show $\\dot{V} \\leq 0$, using the system’s kinematic model and the given online trajectory generation law:\n1. **Derivative of Mechanical Energy**: Using $\\dot{M}_{uu}/2 = C_{uu}$ (Concept 2) and $\\dot{U} = \\dot{e}_u G_u$ (Concept 2), we expand the mechanical energy derivative:\n \\[\n \\dot{(\\frac{1}{2}M_{uu}\\dot{e}_u^2 + U)} = C_{uu}\\dot{e}_u^2 + M_{uu}\\dot{e}_u\\ddot{e}_u + \\dot{e}_u G_u\n \\]\n From the 2-DOF Euler-Lagrange equation (unactuated row, $\\tau_u = 0$), we have $M_{uu}\\ddot{e}_u + C_{uu}\\dot{e}_u + G_u = -M_{ua}\\ddot{e}_a - C_{ua}\\dot{e}_a$ (since $\\ddot{q}_u = \\ddot{e}_u$ and $\\dot{q}_a = \\dot{e}_a$ for $t > t_1$). Substituting this into the mechanical energy derivative eliminates $C_{uu}\\dot{e}_u^2$ and $\\dot{e}_u G_u$, leading to:\n \\[\n \\dot{(\\text{Mechanical Energy})} = -M_{ua}\\dot{e}_u\\ddot{e}_a - C_{ua}\\dot{e}_u\\dot{e}_a\n \\]\n2. **Derivative of Auxiliary Term**: Using the given differential equations for $\\rho_a, \\rho_c, \\Delta$:\n - $\\dot{\\Delta} = k_2 M_{ua}\\dot{e}_u$ (given) implies $\\frac{\\Delta\\dot{\\Delta}}{k_2} = M_{ua}\\dot{e}_u\\Delta$;\n - Rearranging $\\rho_a(\\dot{e}_u M_{ua}\\dot{e}_a + \\dot{e}_u C_{ua}e_a) + \\dot{\\rho}_a(k_1 + 2\\dot{e}_u M_{ua}e_a) = 0$ (given) gives $k_1\\rho_a\\dot{\\rho}_a = -\\rho_a^2\\dot{e}_u(M_{ua}\\dot{e}_a + C_{ua}e_a) - 2\\rho_a\\dot{\\rho}_a\\dot{e}_u M_{ua}e_a$;\n - Rearranging $\\rho_c\\dot{e}_u C_{ua}\\Delta - \\dot{\\rho}_c(k_3 - 2\\dot{e}_u M_{ua}\\Delta) = 0$ (given) gives $k_3\\rho_c\\dot{\\rho}_c = \\rho_c^2\\dot{e}_u C_{ua}\\Delta + 2\\rho_c\\dot{\\rho}_c\\dot{e}_u M_{ua}\\Delta$.\n3. **Combine Derivatives for $\\dot{V}$**: Using the online trajectory law $\\dot{e}_a = -\\rho_a^2 e_a + \\rho_c^2\\Delta$ (from $\\dot{q}_a = \\dot{q}_{ar} - \\rho_a^2 e_a + \\rho_c^2\\Delta$ and $\\dot{q}_{ar} = 0$ for $t > t_1$), substitute $\\ddot{e}_a$ (derived from $\\dot{e}_a$) and the auxiliary term derivatives into $\\dot{V}$. After canceling cross terms (e.g., $-C_{ua}\\dot{e}_u\\dot{e}_a$ and $\\rho_c^2\\dot{e}_u C_{ua}\\Delta$), the key intermediate result is:\n \\[\n \\dot{V} = -\\rho_a^2\\dot{e}_u^2(M_{ua}e_a)^2 - \\rho_c^2\\dot{e}_u^2(M_{ua}\\Delta)^2 \\leq 0\n \\]\n This is non-positive because $M_{ua} \\neq 0$ (Concept 2) and all terms are products of squares and negative signs.', 'step_4': 'Apply Theorem 1 (LaSalle’s Invariance Theorem) to Prove Convergence\nWe verify the conditions of LaSalle’s Theorem and identify the largest invariant set:\n1. **Autonomous System**: For $t > t_1$, $\\dot{q}_{ar} = \\ddot{q}_{ar} = 0$, so the system dynamics (governed by the trajectory law and Euler-Lagrange equations) is autonomous.\n2. **Compact Positively Invariant Set $\\Omega$**: Define $\\Omega = \\{x \\mid V \\leq V_0\\}$ for some $V_0 > 0$. Since $\\dot{V} \\leq 0$, solutions starting in $\\Omega$ never leave it (positive invariance), and $\\Omega$ is compact (bounded by $V \\leq V_0$).\n3. **Invariant Set $E$ and $M$**: Let $E = \\{x \\in \\Omega \\mid \\dot{V} = 0\\}$. From $\\dot{V} = 0$, we get $\\rho_a\\dot{e}_u e_a = 0$ and $\\rho_c\\dot{e}_u\\Delta = 0$ (since $M_{ua} \\neq 0$). The largest invariant set $M \\subset E$ is analyzed as follows:\n - If $\\dot{e}_u \\neq 0$, then $\\rho_a e_a = 0$ and $\\rho_c\\Delta = 0$. However, $\\dot{e}_u \\neq 0$ would imply $\\dot{U} = \\dot{e}_u G_u \\neq 0$ (Concept 2), leading to unbounded $U$ (contradicting $V \\leq V_0$). Thus, $\\dot{e}_u = 0$.\n - If $\\dot{e}_u = 0$, then $\\dot{\\Delta} = 0$ (from $\\dot{\\Delta} = k_2 M_{ua}\\dot{e}_u$), $\\dot{\\rho}_a = 0$ (from $k_1\\rho_a\\dot{\\rho}_a = 0$ and $k_1 > 0$), and $\\dot{\\rho}_c = 0$ (from $k_3\\rho_c\\dot{\\rho}_c = 0$ and $k_3 > 0$). For invariance, $\\dot{e}_a = 0$ (otherwise $\\ddot{e}_a \\neq 0$, implying $G_u \\neq 0$ and $e_u \\neq 0$ by Concept 2), so $e_a = 0$ (from $\\dot{e}_a = -\\rho_a^2 e_a + \\rho_c^2\\Delta$) and $\\Delta = 0$. Finally, $\\dot{e}_u = 0$ and $G_u = 0$ (from bounded $U$) imply $e_u = 0$ (Concept 2).\nThus, $M = \\{x \\mid e_u = 0, \\dot{e}_u = 0, e_a = 0, \\dot{e}_a = 0\\}$. By LaSalle’s Theorem, all solutions starting in $\\Omega$ approach $M$ as $t \\to \\infty$, proving $\\dot{e}_a \\to 0, e_a \\to 0, \\dot{e}_u \\to 0, e_u \\to 0$. The conclusion is therefore true.'}	{'concept_1': '(Definition of `` online trajectory planning’’) Online trajectory planning technique of underactuated system is to design a proper $\\dot q_a$. With $\\dot q_a$ and $\\ddot q_a$ applied to the kinematic model, an asymptotic convergence result of $e_u, \\dot e_u$ is desired.', 'concept_2': '(Property of 2-DOF underactuated systems) $M_{uu}$ is positive-definite with $\\dot M_{uu}/2=C_{uu}$; $M_{ua}\\neq 0$; $M_{ua}\\neq 0$; $G_u=0$ indicates $e_u=0$; the potential energy $U$ holds $\\dot U=\\dot q_uG_u$ .', 'concept_3': '(Energy-based control method)Part of Lyapunov function for underactuated system is usually selected as the sum of kinetic energy and potential energy, i.e. $V=M_{uu} {\\dot q}_u^2+U+f(t)$, where $f(t)$ is a positive function of other variables .', 'theorem_1': '(LaSalle’s invariance theorem) Consider an autonomous system $\\dot{x} = f(x),$ where $f : D \\to \\mathbb{R}^n$ is a locally Lipschitz function from a domain $D \\subset \\mathbb{R}^n$ into $\\mathbb{R}^n$. Let $\\Omega \\subset D$ be a compact set that is positively invariant with respect to the system. Let $V : D \\to \\mathbb{R}$ be a continuously differentiable function such that $\\dot{V}(x) \\leq 0 \\quad \\text{for all } x \\in \\Omega.$ Let $E = \\left\\{ x \\in \\Omega : \\dot{V}(x) = 0 \\right\\},$ and let $M$ be the largest invariant set in $E$. Then, every solution starting in $\\Omega$ approaches $M$ as $t \\to \\infty$.'}	Yes, it's true.	理论工程学科-控制理论与控制工程-非线性控制	46611	1	2
recuVuXTqRyi9f	Consider an iterative learning procedure designed for the agnostic setting with access to population-level statistics. The procedure runs for $T$ steps. In each step, it computes an optimal monotonic function for a fixed linear predictor and then performs a single Projected Gradient Descent (PGD) update on that linear predictor. The final output is an aggregation of all $T$ predictors. What is the sufficient number of update steps $T$ required for this procedure to guarantee that its final aggregated predictor achieves an error of at most $\epsilon$ relative to the best single-index model? Assume the feature vectors are confined to a ball of radius $L$ and the linear predictor's weights are confined to a ball of radius $R$.	"{'step_1': ""The overall goal is to ensure the final error of the aggregated predictor is at most $\\epsilon$. Due to the error metric's convexity (Concept_5), this final error is bounded by the average of the excess losses of the individual predictors from each iteration. Thus, our goal is to bound this average excess loss."", 'step_2': 'For any step $t$, the excess loss is upper-bounded by the instantaneous PGD regret (Concept_1).', 'step_3': 'We then apply the standard PGD analysis (Concept_2) to this regret term, yielding an upper bound on the single-step excess loss: $ExcessLoss_t \\le \\frac{1}{2\\eta}(\\|w_{t}-w\\|_{2}^{2}-\\|w_{t+1}-w\\|_{2}^{2})+\\frac{\\eta}{2}\\|\\nabla_{w}f_t(w_t)\\|_{2}^{2}$.', 'step_4': 'We sum this inequality over all $T$ steps. The term $\\sum_{t=0}^{T-1}(\\|w_{t}-w\\|_{2}^{2}-\\|w_{t+1}-w\\|_{2}^{2})$ forms a telescoping sum. Using the initial conditions and weight space constraints from Concept_4 ($w_0=0, \\|w\\| \\le R$), this sum is bounded by $\\|w_0 - w\\|_2^2 \\le R^2$.', 'step_5': 'The sum of the second term is bounded using the uniform gradient bound from Concept_3: $\\sum_{t=0}^{T-1} \\frac{\\eta}{2}\\|\\nabla_{w}f_t(w_t)\\|_{2}^{2} \\le \\frac{T\\eta L^2}{2}$.', 'step_6': 'Combining these results gives a bound on the sum of excess losses: $\\sum_{t=0}^{T-1} ExcessLoss_t \\le \\frac{R^2}{2\\eta} + \\frac{T\\eta L^2}{2}$. The average excess loss is therefore bounded by $\\frac{1}{T}\\sum ExcessLoss_t \\le \\frac{R^2}{2\\eta T} + \\frac{\\eta L^2}{2}$.', 'step_7': 'This upper bound on the average excess loss is minimized by choosing the learning rate $\\eta = \\frac{R}{L\\sqrt{T}}$. Substituting this choice of $\\eta$ gives an average excess loss of at most $\\frac{LR}{\\sqrt{T}}$.', 'step_8': 'To guarantee the final error is at most $\\epsilon$, we set this bound to be less than or equal to $\\epsilon$: $\\frac{LR}{\\sqrt{T}} \\le \\epsilon$.', 'step_9': 'Solving the inequality for $T$ provides the sufficient number of iterations: $T \\ge \\frac{L^2 R^2}{\\epsilon^2}$.'}"	{'concept_1': 'The suboptimality of the predictor at iteration $t$, defined as its excess matching loss relative to any comparator model $w$, is upper-bounded by the instantaneous regret of the PGD update, given by the inner product $\\langle\\nabla_{w} f_t(w_t), w_t - w\\rangle$.', 'concept_2': 'A standard analysis of a PGD step provides an upper bound on the instantaneous regret: $\\langle\\nabla_{w} f_t(w_t), w_t - w\\rangle \\le \\frac{1}{2\\eta}(\\|w_{t}-w\\|_{2}^{2}-\\|w_{t+1}-w\\|_{2}^{2})+\\frac{\\eta}{2}\\|\\nabla_{w}f_t(w_t)\\|_{2}^{2}$, where $\\eta$ is the learning rate.', 'concept_3': 'The squared Euclidean norm of the gradient of the matching loss function is uniformly bounded by $L^2$.', 'concept_4': 'The space of valid weight vectors is a convex set defined by $\\|w\\|_2 \\le R$. The initial weight vector $w_0$ is the zero vector.', 'concept_5': 'The final error metric is a convex function of the predictor, meaning the error of the averaged predictor is no more than the average of the errors of individual predictors.'}	$ T \geq \frac{L^2 R^2}{\epsilon^2} $	理论基础学科-理论计算机科学-机器学习	142835	2	2
recuVxqriUzjYe	In wall turbulence, what is the relationship between the dominant contribution of the slip velocity variance(⟨w_{s}'^{2}⟩), the fluid fluctuating velocity variance(⟨w_{f}'^{2}⟩), and the particle fluctuating velocity variance(⟨w_{p}'^{2}⟩)?	"{'step_1': ""Provide the vertical momentum equation for a single particle based on theorem_1 (particle vertical momentum equation) \\frac{\\mathrm{d}w_p}{\\mathrm{d}t} = \\frac{w_f - w_p}{\\tau_p} - g. Here, w_f is the instantaneous fluid velocity at the particle's position, \\tau_p is the particle relaxation time, and g is the gravitational acceleration. This equation ignores the added mass, Basset force, and lift force."", 'step_2': 'Define the instantaneous slip velocity according to concept_1 (definition of slip velocity) w_s \\equiv w_f - w_p, Rewrite the vertical momentum equation of a single particle as \\frac{\\mathrm{d}w_p}{\\mathrm{d}t} = \\frac{w_s}{\\tau_p} - g. Explicitly take the slip velocity w_s as the direct driving force for the drag force on the particles.', 'step_3': 'According to concept_2 (conservation of the second moment of the particle phase space PDF) P(z,w,t) = \\langle \\delta(z_p - z)\\,\\delta(w_p - w) \\rangle, Multiply it by w^2 and integrate all w to obtain the conservation equation of the second moment of the vertical height \\frac{\\mathrm{d}}{\\mathrm{d}z}\\bigl[\\varrho\\langle w_p^3 \\rangle_z\\bigr]-2\\varrho\\langle a_p w_p \\rangle_z = 0, Among them, \\varrho is the local particle number density, and a_p is the conditional acceleration.', 'step_4': 'Substitute the vertical momentum equation of a single particle into the second-order moment conservation equation and obtain it using the instantaneous slip velocity. \\frac{\\mathrm{d}}{\\mathrm{d}z}\\bigl[\\varrho\\langle w_p^3 \\rangle_z\\bigr]-\\frac{2\\varrho}{\\tau_p}\\langle w_s w_p \\rangle_z+2\\varrho g\\langle w_p \\rangle_z = 0, Couple the third-order moment of particles with the slip-particle velocity correlation function.', 'step_5': 'Organize to obtain the equation for the variance of the slip velocity: \\langle w_s^{\\prime 2}\\rangle_z = \\bigl(\\langle w_f^{\\prime 2}\\rangle_z - \\langle w_p^{\\prime 2}\\rangle_z\\bigr) +R_t + R_g', 'step_6': 'According to concept_3 (local homogeneous approximation) \\bigl|R_t\\bigr| + \\bigl|R_g\\bigr| \\ll \\bigl|\\langle w_f^{\\prime 2}\\rangle_z - \\langle w_p^{\\prime 2}\\rangle_z\\bigr|, Therefore, the higher-order terms R_t,R_g can be regarded as small quantities.', 'step_7': 'Final Conclusion: \\langle w_s^{\\prime 2}\\rangle_z \\approx \\langle w_f^{\\prime 2}\\rangle_z - \\langle w_p^{\\prime 2}\\rangle_z. This indicates that within the logarithmic layer, the variance of the slip velocity is mainly determined by the difference between the variance of the local ""observed"" fluid fluctuating velocity and the variance of the particle fluctuating velocity.'}"	"{'concept_1': 'Instantaneous slip velocity(w_s \\equiv w_f − w_p), where w_f is the instantaneous fluid velocity at the particle position, and w_p is the instantaneous particle velocity.', 'concept_2': 'P(z,w,t) \\equiv \\langle \\delta(z_p - z)\\,\\delta(w_p - w) \\rangle, It is used to convert a discrete set of Lagrangian particles into a continuous z–w phase space statistical description.', 'concept_3': ""In the logarithmic region far from the wall, the gradient of turbulence statistics with height is sufficiently small, so that the contribution of higher-order moments caused by inhomogeneity R_t and R_g is at least an order of magnitude smaller than \\langle w_{f}'^{2}\\rangle− \\langle w_{p}'^{2}\\rangle."", 'theorem_1': 'The equation of motion of a point particle under the action of Stokes drag and gravity directly determines the coupling relationship between \\frac{\\mathrm{d}w_p}{\\mathrm{d}t} = \\frac{w_f - w_p}{\\tau_p} - g, w_p and w_f .'}"	The dominant contribution comes from the difference between the variance of the fluid fluctuation velocity and the variance of the particle fluctuation velocity observed locally:\langle w_{f}'^{2}\rangle-\langle w_{p}'^{2}\rangle	理论工程学科-理论工程力学-流体力学	171506	3	4
recuVD1HLTDcnu	In a spherically symmetric interior solution in Kerr–Schild form, if all curvature invariants are required to be finite at r=0, what condition(s) must be satisfied?	"{'step_1': 'First, according to concept_1, require that the scalar curvature \\(R(r)\\) be expandable at \\(r=0\\) with no negative powers: \\[R(r)=\\sum_{n=2}^{\\infty} C_n\\, r^{\\,n-2}.\\]', 'step_2': ""Substituting the relation in theorem_2, \\[R(r)=\\frac{2 r\\, m''(r)+4 m'(r)}{r^2},\\] and integrating twice with respect to \\(r\\), one obtains the general solution \\[m(r)= M - \\frac{Q^2}{2r} + \\frac12 \\sum_{n=2}^{\\infty} \\frac{C_n\\, r^{\\,n+1}}{(n+1)(n+2)},\\] where \\(M\\) and \\(Q\\) are integration constants corresponding to the exterior Schwarzschild and Reissner–Nordström–type “singular” contributions, respectively."", 'step_3': 'Next, using theorem_1, \\[ds^2=-f(r)\\,dt^2+\\frac{dr^2}{f(r)}+r^2 d\\Omega^2,\\qquad f(r)=1-\\frac{2\\,m(r)}{r},\\] plugging the above \\(m(r)\\) in gives \\[f(r)=1-\\frac{2M}{r}+\\frac{Q^2}{r^2} -\\frac12\\sum_{n=2}^{\\infty}\\frac{C_n\\,r^{\\,n}}{(n+1)(n+2)}.\\] Since the summation starts at \\(n=2\\) (i.e., \\(\\mathcal O(r^2)\\)), it remains finite as \\(r\\to0\\); whereas \\[\\lim_{r\\to0}\\frac{Q^2}{r^2}=+\\infty,\\qquad \\lim_{r\\to0}\\frac{M}{r}=+\\infty,\\] introduce \\(r^{-2}\\) and \\(r^{-1}\\) divergences in \\(g_{tt}=-f(r)\\) and \\(g_{rr}=1/f(r)\\), respectively. To ensure a regular center (finite metric components at \\(r=0\\)), these divergent terms must be removed, which forces \\(Q=0\\) and \\(M=0\\).', 'step_4': 'Therefore, under the regularity constraint of concept_1, together with theorem_2 and theorem_1 and the limiting analysis, we obtain \\[\\boxed{Q=0,\\; M=0}.\\]'}"	"{'concept_1': 'Demand that \\(R(r)\\) be expandable at \\(r=0\\) with no negative powers: \\(R(r)=\\sum_{n=2}^{\\infty} C_n\\, r^{\\,n-2}\\). Integrating twice yields the general solution \\(m(r)= M - \\frac{Q^2}{2r} + \\frac12 \\sum_{n=2}^{\\infty} \\frac{C_n\\, r^{\\,n+1}}{(n+1)(n+2)}\\). Here \\(M\\) and \\(Q\\) are integration constants, corresponding to the “singular” contributions of the exterior Schwarzschild and Reissner–Nordström terms.', 'theorem_1': 'The interior metric is written as \\(ds^2=-f(r)\\,dt^2+\\frac{dr^2}{f(r)}+r^2 d\\Omega^2\\) \\(f(r)=1-\\frac{2\\,m(r)}{r}\\).', 'theorem_2': ""The relation between the Ricci scalar and \\(m(r)\\) is \\(R=\\frac{2 r\\, m''(r)+4 m'(r)}{r^2}\\).""}"	M=0 and Q=0	理论交叉学科-理论天体物理-黑洞几何	51570	1	3
recuVDosjnzWmq	write down the expression for the local entropy density $s_{\text{bin}}(x,y,\eta_s)$.Answer in one sentence without using the internet.	{'Step_1': 'Goal: write down the expression for the binary-collision contribution to the initial local entropy density stextbin(x,y,etas)s_{\\text{bin}}(x,y,\\eta_s) in a (3+1)D hydrodynamic initialization, to be stated in one sentence.', 'Step_2': 'Target identification (Concept_1): identify stextbin(x,y,etas)s_{\\text{bin}}(x,y,\\eta_s) as the binary-collision contribution to the initial local entropy density.', 'Step_3': 'Factorization & scaling (Concept_2–3): adopt a factorization ansatz into a longitudinal envelope and a transverse profile with overall scaling K/tau0K/\\tau_0 to ensure correct normalization/dimensions at formation time tau0\\tau_0.', 'Step_4': 'Transverse profile (Concept_4): build from binary-collision points with Gaussian smearing width sigmar\\sigma_r: \\\\(\\displaystyle \\tilde s_{\\text{bin}}(x,y)=\\sum_{i\\in \\text{bin}}\\frac{1}{2\\pi\\sigma_r^2}\\exp\\!\\Big[-\\frac{(x-x_i)^2+(y-y_i)^2}{2\\sigma_r^2}\\Big].\\\\)', 'Step_5': 'Longitudinal envelope (Concept_5): choose a Gaussian centered at \\\\eta_w with width \\\\sigma_w and support enforced by a step function: \\\\(\\displaystyle H_s^{\\text{bin}}(\\eta_s)=\\exp\\!\\Big[-\\frac{(\\eta_s-\\eta_w)^2}{2\\sigma_w^2}\\Big],\\theta(|\\eta_s|-\\eta_w).\\\\)', 'Step_6': 'Variables (Concept_6): x,yx,y are transverse coordinates, etas\\eta_s is space–time rapidity, and \\\\theta\\ is the Heaviside step function.', 'Step_7': 'Assembly (Concept_7): combine the longitudinal envelope and transverse profile with the scale factor to obtain the one-line result \\\\(s_{\\text{bin}}(x,y,\\eta_s)=\\dfrac{K}{\\tau_0}\\,H_s^{\\text{bin}}(\\eta_s)\\,\\tilde s_{\\text{bin}}(x,y).\\\\)', 'Step_8': 'Scope note (Concept_8): this construction covers only the binary-collision channel (distinct from the participant contribution) and is intended for event-by-event initialization inputs.'}	{'Concept_1': 'Identify the target as the **binary-collision contribution** to the initial local entropy density $s_{\\text{bin}}(x,y,\\eta_s)$ in a $(3+1)$D hydrodynamic initialization.', 'Concept_2': 'Adopt a **factorization ansatz**: the binary part separates into a **longitudinal envelope** and a **transverse profile**, scaled overall by $K/\\tau_0$.', 'Concept_3': 'Use the **overall magnitude and formation-time scaling** $K/\\tau_0$ to set the correct normalization/dimensions of the entropy density at $\\tau_0$.', 'Concept_4': 'Build the **transverse profile** from binary collision points with Gaussian smearing width $\\sigma_r$: \\\\(\\displaystyle \\tilde s_{\\text{bin}}(x,y)=\\sum_{i\\in \\text{bin}}\\frac{1}{2\\pi\\sigma_r^2}\\exp\\!\\Big[-\\frac{(x-x_i)^2+(y-y_i)^2}{2\\sigma_r^2}\\Big].\\\\)', 'Concept_5': 'Choose a **longitudinal envelope** centered at $\\eta_w$ with width $\\sigma_w$ and support enforced by a step function:\\theta(|\\eta_s|-\\eta_w).$', 'Concept_6': 'Specify variables: $x,y$ are **transverse coordinates**, $\\eta_s$ is **space–time rapidity**, $\\theta$ is the **Heaviside step**.', 'Concept_7': '**Assemble** longitudinal and transverse pieces with the scale factor to obtain $s_{\\text{bin}}$ (the requested one-line expression).', 'Concept_8': 'Note that this construction is the **binary-collision channel only** (distinct from the participant contribution) and is intended for **event-by-event** initialization inputs.'}	s_{\text{bin}}(x,y,\eta_s)=\frac{K}{\tau_0}\,H_s^{\text{bin}}(\eta_s)\,\tilde s_{\text{bin}}(x,y).	理论基础学科-理论物理-重离子碰撞	101934	2	2
recuVDZy1iiliU	Let $N\geq 2$, $\Omega\subset \mathbb{R}^N$ be a bounded domain with $C^2$ boundary $\Gamma$. Assume $u\in C^2(\overline{\Omega})$ is the solution to the Poisson equation with constant Neumann boundary condition: \[\begin{cases}\Delta u=N,&~\text{in }\Omega\\u_\nu=R,&~\text{on }\partial\Omega\end{cases}\] where $R$ is a constant. Assume $f\in C^2(\overline{\Omega})$ is the solution to the Poisson equation with zero Dirichlet boundary data \[\begin{cases}\Delta f=N,&~\text{in }\Omega\\f=0,&~\text{on }\partial\Omega\end{cases}\] Denote $\overline{u}=\max_{\Omega}u$; $h=q-u$, where $q(x)=\frac{1}{2} |x-z|^2$ and $z\in\Omega$ is a global minimum point of $z$ so that $\nabla h(z)=0$; $\nu$ the unit outer normal to $\Gamma$; $H>0$ the mean curvature of $\Gamma$; $\nabla_\Gamma u$ the tangential derivative of $u$ along $\Gamma$; $\delta_\Gamma(x)=\mathrm{dist}(x;\Gamma)$ the distance of $x$ to $\Gamma$; $r_i$ ($r_e$) the radius of uniform interior (exterior) ball of $\Omega$; $d_\Omega$ the diameter of $\Omega$; $\rho_e=\max_{\Gamma}|x-z|$; $\rho_i=\min_{\Gamma}|x-z|$; $\left\| \overline{u}-u \right\|_{1,2,\tau,\Gamma}=\left\| \overline{u}-u \right\|_{L^2(\Gamma)}+\left\| \nabla_\Gamma u \right\|_{L^2(\Gamma)}$. $C$ always denotes a constant depending on $N, d_\Omega, r_e, r_i$. Assume $|h_\nu|\leq 1+d_\Omega$, $|RH|\leq C$, $\mathrm{osc}_{\Omega}\leq 1$ (in particular $0\leq\overline{u}-u\leq 1$). It's already established that $u$ will satisfy the following integral inequality \[\int_{\Omega}^{}(\overline{u}-u)\left|\nabla^2h\right|^2dx\leq C \left(\left\| \overline{u}-u \right\|_{1,2,\tau,\Gamma}^2+\left\| \overline{u}-u \right\|_{1,2,\tau,\Gamma}\left\| h_\nu \right\|_{L^2(\Gamma)}+\int_{\Gamma}^{}\left|\left\langle \nabla\nu (\nabla_\Gamma u), \nabla_\Gamma u\right\rangle\right| dS_x.\right)\] and that $\overline{u}-u$ admits the lower bound $\overline{u}-u\geq \frac{r_i}{2}\delta_\Gamma$. Follow these steps to get the optimal exponent $\tau_N$ (depending on $N$) for $N\geq 100$ in the stable inequality $\rho_e-\rho_i\leq C\left\| \overline{u}-u \right\|_{1,2,\tau,\Gamma}^{\tau_N}$. Step 1: choose proper coordinate on each $x\in\Gamma$ to get pointwise control for $\left|\left\langle \nabla\nu (\nabla_\Gamma u), \nabla_\Gamma u\right\rangle\right|$. Step 2: apply trace inequality to estimate $\left\| h_\nu \right\|_{L^2(\Gamma)}$. Step 3: apply comparison theorem to $\overline{u}-u$ and $f$ to obtain $\overline{u}-u\geq -f$ in $\Omega$, thus conclude the estimate for $\left\| (\overline{u}-u)^{1/2}\nabla^2h \right\|_{L^2(\Omega)}$. Step 4: choose $p$ properly in the following interpolation inequality \[\rho_e-\rho_i\leq C(N, p, r_i)\left\| \nabla h \right\|_{L^\infty(\Omega)}^{(N-p)/N}\left\| \nabla h \right\|_{L^p(\Omega)}^{p/N}\leq C(N, p, r_i, r_e, d_\Omega)\left\| \nabla h \right\|_{L^p(\Omega)}^{p/N}\text{ for } 1\leq p<N,\] apply Poincare-type inequality to relate to the LHS of the integral inequality to get optimal constant $\tau_N$. In your answer, you should return the estimate for $\left|\left\langle \nabla\nu (\nabla_\Gamma u), \nabla_\Gamma u\right\rangle\right|$ you obtained in Step 1 and the value of $p$ you chose in Step 4. You may use the fact that if $t^2\leq C(M_1t+M_2)$, then $t^2\leq C(M_1^2+M_2)$ with $t=\left\| (\overline{u}-u)^{1/2}\nabla^2h \right\|_{L^2(\Omega)}$.	{'step_1': 'estimate $\\left\\langle \\nabla\\nu(\\nabla_\\Gamma u), \\nabla_\\Gamma u\\right\\rangle$. By theorem 1, we may assume WLOG that $\\nabla\\nu$ is a diagonal matrix, so theorem 2 implies that \\[\\min \\left\\{\\kappa_1,\\dots,\\kappa_{N-1}\\right\\}|\\nabla_\\Gamma u|^2\\leq\\left\\langle \\nabla\\nu(\\nabla_\\Gamma u), \\nabla_\\Gamma u\\right\\rangle\\leq \\max \\left\\{\\kappa_1,\\dots,\\kappa_{N-1}\\right\\}|\\nabla_\\Gamma u|^2.\\] Since $\\Omega$ is a $C^2$ domain, each $\\kappa_i$ can be controlled by the radius of the uniform interior ball and exterior ball, thus $\\left|\\left\\langle \\nabla\\nu (\\nabla_\\Gamma u),\\nabla_\\Gamma u\\right\\rangle\\right|\\leq C(r_i, r_e)\\left|\\nabla_\\Gamma u\\right|^2$.', 'step_2': 'estimate $\\left\\| h_\\nu \\right\\|_{L^2(\\Gamma)}$. Apply comparison theorem to $\\overline{u}-u$ and $f$ gives $\\overline{u}-u\\geq -f$ in $\\Omega$, so by theorem 3 \\[\\left\\| h_\\nu \\right\\|_{L^2(\\Gamma)}\\leq C \\left\\| (-f)\\nabla^2h \\right\\|_{L^2(\\Gamma)}\\leq C \\left\\| (\\overline{u}-u)\\nabla^2h \\right\\|_{L^2(\\Gamma)}.\\]', 'step_3': 'combine the estimates obtained in Step 2 and Step 3, we get \\[\\left\\| (\\overline{u}-u)\\nabla^2h \\right\\|_{L^2(\\Gamma)}^2\\leq C \\left(\\left\\| \\overline{u}-u \\right\\|_{1,2,\\tau,\\Gamma}^2+\\left\\| \\overline{u}-u \\right\\|_{1,2,\\tau,\\Gamma} \\left\\| (\\overline{u}-u)\\nabla^2h \\right\\|_{L^2(\\Gamma)}\\right).\\] View it as a quadratic polynomial of $\\left\\| (\\overline{u}-u)\\nabla^2h \\right\\|_{L^2(\\Gamma)}$, we obtain \\[\\left\\| (\\overline{u}-u)\\nabla^2h \\right\\|_{L^2(\\Gamma)}\\leq C \\left\\| \\overline{u}-u \\right\\|_{1,2,\\tau,\\Gamma}.\\]', 'step_4': 'relate LHS of the inequality derived in Step 3 to $\\rho_e-\\rho_i$: take $p=2$, $\\alpha=1/2$ in theorem 4, apply theorem 4 to $\\nabla h$, by the interpolation inequality we have \\[\\rho_e-\\rho_i\\leq C \\left\\| \\overline{u}-u \\right\\|_{1,2,\\tau,\\Gamma}^{p/N}\\text{ for any }1\\leq p\\leq \\min \\left\\{\\frac{2N}{N-1}, N\\right\\}.\\] To obtain optimal exponent $\\tau_N$, we need to maximize $p$, since $N\\geq 100$, we have $2N/(N-1)\\leq N$, thus the maximum choice of $p$ is $2N/(N-1)$, which also gives $\\tau_N=2/(N-1)$.'}	{'concept/theorem_1': 'With the same assumption on $\\Omega$, at each point $x\\in\\Gamma$, we can choose coordinate at this point such that \\[\\nabla\\nu=\\mathrm{diag}(\\kappa_1, \\kappa_2, \\dots, \\kappa_{N-1}, 0),\\] where $\\kappa_i$ denotes the $i$-th principle curvature.', 'concept/theorem_2': 'If $A$ is a real symmetric matrix, then it holds that $\\lambda_{\\min}\\left\\| x \\right\\|^2\\leq\\left\\langle x, Ax\\right\\rangle\\leq \\lambda_{\\max}\\left\\| x \\right\\|^2$ for any $x\\in \\mathbb{R}^N$, where $\\lambda_{\\min}$($\\lambda_{\\max}$) denotes the smallest(largest) eigenvalue of $A$.', 'concept/theorem_3': 'With the same assumption on $\\Omega$, $u$ and $f$, it holds that \\[\\int_{\\Gamma}^{}(h_\\nu)^2 dS_x\\leq C(N, d_\\Omega, r_i, r_e)\\int_{\\Omega}^{}(-f)|\\nabla^2 h|^2dx.\\]', 'concept/theorem_4': 'For any numbers $r$, $p$, $\\alpha$ such that $0\\leq \\alpha\\leq 1$, $1\\leq p\\leq r\\leq \\frac{Np}{N-p(1-\\alpha)}$, $p(1-\\alpha)<N$, the following weighted Sobolev inequality \\[\\left\\| v \\right\\|_{L^r(\\Omega)}\\leq C(N, r, p, \\alpha, d_\\Omega, r_i)\\left\\| \\delta_\\Gamma^\\alpha\\nabla^2v \\right\\|_{L^p(\\Omega)}\\] holds for any harmonic function $v$ in $\\Omega$ such that $v(z)=0$.'}	$|\left\langle \nabla\nu(\nabla_\Gamma u), \nabla_\Gamma u\right\rangle|\leq C|\nabla_\Gamma u|^2$ $p=2N/(N-1)$	理论基础学科-基础数学-偏微分方程	39481	2	3
recuVH3SEhT5ga	"For a `k`-CSP that is `l`-null-constraining, what is the minimum level `d` of the Bounded Width (BW) hierarchy that a random instance (with `n` variables and `Δn` constraints) is shown to possess a solution for, thereby ""fooling"" the hierarchy and indicating its limitation in solving the CSP efficiently? This level `d` serves as a lower bound on the hierarchy's power. A `k`-CSP is a constraint satisfaction problem where each constraint involves `k` variables. It is `l`-null-constraining if every simple constraint path of length at least `l` imposes no restriction on its endpoints."	"{'step_1': 'The core of the proof relies on a constructive theorem (Theorem_1), which states that a hierarchy solution of level `d = t/(1+k/β)` can be built if two conditions are met: the CSP must be ""extensible"" and its instance graph must be ""confined"".', 'step_2': ""The first condition is satisfied by the problem's premise. A key result in the paper (related to Concept_2) is that any `l`-null-constraining `k`-CSP is proven to be BW-fully-extensible. This property is essential as it guarantees that local solutions can be consistently glued together to form a global, albeit relaxed, solution."", 'step_3': 'The second condition relates to the structure of a typical instance. A random CSP instance corresponds to a random `k`-uniform hypergraph. According to standard probabilistic arguments (Theorem_2), such a random hypergraph is, with uniformly positive probability, `(2t, γ/l)`-confined for `t = \\Omega(n)` (Concept_3). This confinement ensures that the local variable and constraint dependencies required to define the solution at any given point remain small.', 'step_4': 'With both conditions of Theorem_1 met, we can conclude that a BW hierarchy solution exists for a random instance of an `l`-null-constraining `k`-CSP.', 'step_5': 'The level `d` of this constructed solution is given by the formula `d = t/(1+k/β)`. Substituting the value `t = \\Omega(n)` from the properties of random graphs (Theorem_2), we find that `d = \\Omega(n) / (1+k/(γ/l)) = \\Omega(n)`, since `k`, `γ`, and `l` are constants.', 'step_6': 'Therefore, the hierarchy can be ""fooled"" up to a level linear in the number of variables `n`, establishing a lower bound of `\\Omega(n)` on the level required to solve the problem.'}"	"{'concept_1': 'A `k`-CSP is `l`-null-constraining if for every simple constraint path of length at least `l`, any pair of values for the endpoint variables can be extended to a satisfying assignment for the entire path.', 'concept_2': 'A CSP is BW-fully-extensible if for any instance and any ""insular family"" of variable subsets, any satisfying assignment to the minimal subset can be extended to a full, consistent set of satisfying assignments for all subsets in the family.', 'concept_3': 'A hypergraph is `(t, β)`-confined if the union of any two `t`-small, `S`-closed subgraphs remains `r`-small for an `(rβ)`-small vertex set `S`. This property ensures that local analyses do not spread uncontrollably through the graph.', 'theorem_1': 'If a `k`-CSP is extensible and its instance graph (viewed as a hypergraph) is `(2t, β)`-confined, then there exists a hierarchy solution of level `d = t / (1 + k/β)`.', 'theorem_2': 'With uniformly positive probability, a random `k`-uniform hypergraph with `n` variables and `Δn` constraints is expanding, which implies it is `(2t, γ/l)`-confined for some constant `γ > 0` and for `t = \\Omega(n)`.'}"	\Omega(n)	理论基础学科-理论计算机科学-计算复杂度	187326	2	3
recuVHnPzNyQ92	In comparison with m-ZrO2, the t-ZrO2 possess the higher lattice oxygen proportion. Among m-ZrO2 and t-ZrO2, which provides the higher activity for plasma-catalyzed conversion of H2S-CO2 to syngas?	{'step_1': 'Based on the concept 1, the oxygen species mainly consists of lattice oxygen species and the surface oxygen species on the oxygen vacancies. Thus, the higher lattice oxygen proportion means the lower oxygen vacancy proportion. In comparison with t-ZrO2, the m-ZrO2 with the lower lattice oxygen proportion has the higher oxygen vacancy proportion.', 'step_2': 'In comparison with lattice oxygen, the adsorbed oxygen species on oxygen vacancies shows the weaker Zr-O bond, which is vital in H2S dissociation.', 'step_3': 'The m-ZrO2 with more oxygen vacancies exhibits the higher adsorption strength of CO2 and CH4 as well as the lower reaction energy barriers. Therefore, the m-ZrO2 provides the higher activity for plasma-catalyzed conversion of H2S-CO2 to syngas.'}	{'concept_1': 'The oxygen species on ZrO2 mainly consists of lattice oxygen species and the surface oxygen species on the oxygen vacancies. The amount of weakly bound oxygen species is very low.', 'concept_2': 'The Zr-O surface sites are thought to be involved in H2S dissociation.', 'concept_3': 'The m-ZrO2 surface has more oxygen vacancies.'}	m-ZrO2	理论基础学科-理论化学-计算化学	87630	1	1
recuVHogjyBVj5	In relativistic hydrodynamics, for a diffusive mode with dispersion relation \(\omega(k) = i \sum_{n=1}^\infty \beta_{2n} k^{2n}\), determine the causality-imposed lower bound on the dimensionless combination \(R\beta_2\). For the lowest-order analysis, use the moments \(a_0=1\) and \(a_1 = \frac{1}{3} - \frac{\pi}{8} r \beta_2\). Finally, take the limit \(r \to R\) to obtain the tightest bound.	{'step_1': 'Reformulate the causality constraint of the diffusive mode as a moment problem. Express this via the positive semidefiniteness (PSD) of Hankel matrices constructed from the spectral measure \\(\\mu(x) = 1 - \\sum_{n=1}^\\infty r^{2n-1} \\beta_{2n} (1-x^2)^{-1/2} T_{2n}(x)\\), ensuring microscopic causality (concept1) is satisfied at all orders.', 'step_2': 'For the lowest-order analysis N=2, compute the relevant moments of \\(\\mu(x)\\): \\(a_0 = 1\\) (normalization condition) \\(a_1 = 1/3 - (\\pi/8) r \\beta_2\\) (linear relation to \\(\\beta_2\\) at first order) These moments encode the spectral weight of the diffusive mode and serve as inputs for Hankel matrix construction.', 'step_3': 'Apply the PSD conditions from concept2 to these Hankel matrices: \\(H_2^1 = a_1 \\ge 0 \\implies 1/3 - (\\pi/8) r \\beta_2 \\ge 0 \\implies r \\beta_2 \\le 8/(3\\pi)\\) (upper bound from first moment) \\(H_1^0 - H_2^1 = a_0 - a_1 \\ge 0 \\implies 1 - (1/3 - (\\pi/8) r \\beta_2) \\ge 0 \\implies 2/3 + (\\pi/8) r \\beta_2 \\ge 0 \\implies r \\beta_2 \\ge -16/(3\\pi)\\) (lower bound from Hankel difference) These inequalities illustrate how the lower and upper bounds on \\(\\beta_2\\) arise naturally from matrix positivity and the structure of the moments.', 'step_4': 'Take the physical limit \\(r \\to R\\) to obtain the tightest causality bounds on the dimensionless combination: \\(-16/(3\\pi) \\le R \\beta_2 \\le 0\\) This step ensures that the bounds reflect the largest physically relevant length scale while preserving causality constraints.', 'step_5': 'Conclude that the causality-imposed lower bound on the diffusive mode is: \\(R \\beta_2 \\ge -16/(3\\pi)\\) This result encapsulates the interplay between microscopic causality, spectral moment positivity, and the Hankel matrix framework, providing a rigorous constraint on transport coefficients in relativistic hydrodynamics.'}	"{'concept1': ""Physical modes' dispersion relations \\(\\omega(k)\\) must satisfy the inequality \\(v_{LC} |\\mathrm{Im},k| - \\mathrm{Im},\\omega(k) \\ge 0\\), where \\(v_{LC}\\) is the lightcone speed (set to 1). This is the fundamental constraint derived from microscopic causality."", 'concept2': 'Hankel matrices constructed from the moments \\(a_n\\) of the measure \\(\\mu(x)\\) must be positive semidefinite. The key conditions include \\(H_N^0 \\succeq 0\\), \\(H_N^1 \\succeq 0\\), \\(H_{N-1}^0 - H_N^1 \\succeq 0\\), and \\(H_{N-1}^1 - H_N^2 \\succeq 0\\), where \\((H_N^\\ell)_{ij} = a_{i+j+\\ell}\\).'}"	The causality-imposed lower bound is \[R\beta_2 \;\ge\; -\frac{16}{3\pi}.\]	理论工程学科-理论工程力学-流体力学	70688	3	2
recuVIAg8GQrkq	Establish a lower bound for the estimation error of a parameter $\theta$ under a general $L_q$ loss function using an information-theoretic approach, overcoming the traditional limitation of he van Trees inequality to squared $L_q$ loss.	{'step_1': 'First, clarify the original forms of the three key theorems (critical for avoiding dimensional errors in subsequent steps): \\n\\n### Van Trees Inequality (Theorem 1)\\nFor $L_2$ loss, the mean squared error lower bound is: \\n$$\\mathbb{E}\\left[\\|\\theta-\\hat{\\theta}\\|_2^2\\right] \\geq \\frac{d}{\\det\\left(I_X(\\theta)+J(\\pi)\\right)^{1/d}}$$\\n\\n### Efroimovich’s Inequality (Theorem 2)\\nRelates conditional entropy $h(\\theta \\mid X)$ to total Fisher information: \\n$$\\frac{1}{2\\pi e} e^{\\frac{2}{d} h(\\theta \\mid X)} \\geq \\frac{1}{\\det\\left(I_X(\\theta)+J(\\pi)\\right)^{1/d}}$$\\n\\n### Maximum Entropy Under $L_q$ Moment Constraints (Theorem 3)\\nFor any random variable $Z$, its differential entropy is upper-bounded by: \\n$$h(Z) \\leq \\log\\left(C_{ME}(q) \\cdot \\mathbb{E}[|Z|^q]^{1/q}\\right)$$', 'step_2': 'Define Estimation Error & Use Entropy Translation Invariance Estimation Error: Let $$Z = \\hat{\\theta}(X) - \\theta$$ (d-dimensional vector, where $$Z_i$$ = $i$-th component of $Z$). Our goal is to bound $\\mathbb{E}_{\\theta,X}\\left[\\|Z\\|_q^q\\right] = \\mathbb{E}_{\\theta,X}\\left[\\sum_{i=1}^d |Z_i|^q\\right]$. \\nEntropy Invariance: For fixed $X$, $$\\theta = \\hat{\\theta}(X) - Z_i$$ (a translation of $Z$). Since differential entropy is invariant to translation: \\n $$h(\\theta \\mid X) = h(Z \\mid X)$$', 'step_3': 'Derive Lower Bound for $$h(\\theta \\mid X)$$ (via Efroimovich’s Inequality)\\nWe use Theorem 2 to find a lower bound for $h(\\theta \\mid X)$: \\nStart with Efroimovich’s Inequality: \\n $$\\frac{1}{2\\pi e} e^{\\frac{2}{d} h(\\theta \\mid X)} \\geq \\frac{1}{\\det\\left(I_X(\\theta)+J(\\pi)\\right)^{1/d}}$$\\nTake the natural logarithm of both sides (preserves inequality, as $$\\ln(\\cdot)$$ is increasing): \\n $$\\ln\\left(\\frac{1}{2\\pi e}\\right) + \\frac{2}{d} h(\\theta \\mid X) \\geq -\\frac{1}{d} \\ln\\left(\\det\\left(I_X(\\theta)+J(\\pi)\\right)\\right)$$\\nRearrange terms to solve for $h(\\theta \\mid X)$: \\n $$h(\\theta \\mid X) \\geq \\frac{d}{2} \\ln(2\\pi e) - \\frac{1}{2} \\ln\\left(\\det\\left(I_X(\\theta)+J(\\pi)\\right)\\right) \\tag{1}$$', 'step_4': 'We use Theorem 3 to find an upper bound for $h(Z \\mid X)$ (the entropy of the $d$-dimensional error $Z$):\\nSplit $d$-dimensional entropy into component-wise entropies using subadditivity: \\nFor a $d$-dimensional random vector $Z$, its total conditional entropy satisfies the subadditivity property (no independence assumption is required): \\n$$h(Z \\mid X) \\leq \\sum_{i=1}^d h(Z_i \\mid X)$$\\nApply Theorem 3 to each 1-dimensional component $Z_i \\mid X$: \\nFor each $Z_i$, the maximum entropy principle gives: \\n$$h(Z_i \\mid X) \\leq \\log\\left(C_{ME}(q) \\cdot \\mathbb{E}[|Z_i|^q \\mid X]^{1/q}\\right)$$\\nSum over all components to get the upper bound for $h(Z \\mid X)$: \\n$$h(Z \\mid X) \\leq \\sum_{i=1}^d \\log\\left(C_{ME}(q) \\cdot \\mathbb{E}[|Z_i|^q \\mid X]^{1/q}\\right) \\tag{2}$$', 'step_5': 'Combine Entropy Bounds (Link Entropy to Error)\\nFrom Step 2, $h(\\theta \\mid X) = h(Z \\mid X)$. Substitute (1) and (2) to 联立 (link entropy to the error $Z$): \\n$$\\frac{d}{2} \\ln(2\\pi e) - \\frac{1}{2} \\ln\\left(\\det\\left(I_X(\\theta)+J(\\pi)\\right)\\right) \\leq \\sum_{i=1}^d \\log\\left(C_{ME}(q) \\cdot \\mathbb{E}[|Z_i|^q \\mid X]^{1/q}\\right)$$\\nSimplify the right-hand side (RHS) using logarithmic properties ($\\sum \\log a_i = \\log\\left(\\prod a_i\\right)$): \\n$$\\text{RHS} = \\log\\left(\\prod_{i=1}^d \\left[C_{ME}(q) \\cdot \\mathbb{E}[|Z_i|^q \\mid X]^{1/q}\\right]\\right) = \\log\\left(C_{ME}(q)^d \\cdot \\prod_{i=1}^d \\mathbb{E}[|Z_i|^q \\mid X]^{1/q}\\right)$$', 'step_6': 'Apply AM-GM Inequality to Error Moments\\nTo connect the product of component moments to the total $L_q$ error, use the **Arithmetic Mean-Geometric Mean (AM-GM) Inequality**: \\nFor non-negative terms $\\mathbb{E}[|Z_i|^q \\mid X]$ ($i=1,...,d$), AM-GM states: \\n$$\\frac{1}{d} \\sum_{i=1}^d \\mathbb{E}[|Z_i|^q \\mid X] \\geq \\left(\\prod_{i=1}^d \\mathbb{E}[|Z_i|^q \\mid X]\\right)^{1/d}$$\\nTake the $1/q$-th power of both sides (preserves inequality, as $x^{1/q}$ is increasing for $x \\geq 0$): \\n$$\\left(\\frac{1}{d} \\sum_{i=1}^d \\mathbb{E}[|Z_i|^q \\mid X]\\right)^{1/q} \\geq \\left(\\prod_{i=1}^d \\mathbb{E}[|Z_i|^q \\mid X]\\right)^{1/(dq)}$$\\nNote that $\\sum_{i=1}^d \\mathbb{E}[|Z_i|^q \\mid X] = \\mathbb{E}[\\|Z\\|_q^q \\mid X]$. Rearrange to bound the product term: \\n$$\\prod_{i=1}^d \\mathbb{E}[|Z_i|^q \\mid X]^{1/q} \\leq d^{d/q} \\cdot \\mathbb{E}[\\|Z\\|_q^q \\mid X]^{d/q} \\tag{3}$$', 'step_7': 'Solve for Lower Bound of Conditional $L_q$ Error\\nSubstitute (3) into the combined entropy inequality (from Step 5) and solve for $\\mathbb{E}[||Z||_q^q \\mid X]$: \\nSubstitute (3) into the RHS: \\n $$\\frac{d}{2} \\ln(2\\pi e) - \\frac{1}{2} \\ln\\left(\\det\\left(I_X(\\theta)+J(\\pi)\\right)\\right) \\leq \\log\\left(C_{ME}(q)^d \\cdot d^{-d/q} \\cdot \\mathbb{E}[||Z||_q^q \\mid X]^{d/q}\\right)$$\\nExponentiate both sides (preserves inequality, as $$\\exp(\\cdot)$$ is increasing): \\n $$(2\\pi e)^{d/2} \\cdot \\det\\left(I_X(\\theta)+J(\\pi)\\right)^{-1/2} \\leq C_{ME}(q)^d \\cdot d^{-d/q} \\cdot \\mathbb{E}[||Z||_q^q \\mid X]^{d/q}$$\\nRearrange terms to solve for $\\mathbb{E}[||Z||_q^q \\mid X]$: \\n $$\\mathbb{E}[||Z||_q^q \\mid X] \\geq \\frac{(2\\pi e)^{q/2} \\cdot d}{\\det\\left(I_X(\\theta)+J(\\pi)\\right)^{q/(2d)} \\cdot C_{ME}(q)^q} \\tag{4}$$', 'step_8': 'Take Expectation Over $\\theta$ and (Final Bound)\\nThe inequality (4) holds for all $X$. To get the unconditional lower bound, take the expectation over $\\theta$ and (expectation is monotonic, so the lower bound is preserved): \\n$$\\mathbb{E}_{\\theta,X}\\left[||Z||_q^q\\right] \\geq \\frac{(2\\pi e)^{q/2} \\cdot d}{\\det\\left(I_X(\\theta)+J(\\pi)\\right)^{q/(2d)} \\cdot C_{ME}(q)^q}$$\\nRewrite into the standard form by grouping constants: \\n$$\\boxed{\\mathbb{E}_{\\theta, X}\\left[\\|\\hat{\\theta}(X)-\\theta\\|_q^q\\right] \\geq\\left(\\frac{\\sqrt{2 \\pi e}}{C_{M E}(q)}\\right)^q \\frac{d}{\\operatorname{det}\\left(I_X(\\theta)+J(\\pi)\\right)^{\\frac{q}{2 d}}}}$$', 'step_9': 'Validate Consistency with Van Trees Inequality (Special Case $q=2$)\\nTo confirm correctness, substitute $$q=2$$ (where $C_{ME}(2)=\\sqrt{2\\pi e}$): \\n$$\\left(\\frac{\\sqrt{2\\pi e}}{\\sqrt{2\\pi e}}\\right)^2 \\cdot \\frac{d}{\\det\\left(I_X(\\theta)+J(\\pi)\\right)^{2/(2d)}} = \\frac{d}{\\det\\left(I_X(\\theta)+J(\\pi)\\right)^{1/d}}$$\\nThis matches the Van Trees Inequality (Theorem 1), verifying the derivation.'}	"{'concept_1': 'For a family of probability distributions $\\{P_\\theta\\}$ parameterized by $\\theta$, the Fisher Information Matrix (FIM) quantifies the amount of information that an observable random variable $X$ (distributed according to $P_\\theta$) carries about the unknown parameter $\\theta$. The $(i,j)$-th component of the FIM is formally defined as: \\n$$\\left[I_X(\\theta)\\right]_{ij} \\triangleq \\int_X \\frac{\\frac{\\partial}{\\partial \\theta_i} f(x; \\theta) \\cdot \\frac{\\partial}{\\partial \\theta_j} f(x; \\theta)}{f(x; \\theta)} d\\lambda(x)$$\\nwhere $f(x; \\theta)$ denotes the probability density function (pdf) of $X$ under parameter $\\theta$, and $\\lambda$ is the dominating measure (e.g., Lebesgue measure for continuous $X$). As a fundamental quantity in estimation theory, the FIM captures the *local difficulty* of the parameter estimation problem—directly shaping the tightness of the lower bounds in Theorems 1 and 2.', 'theorem_1': 'For an estimator $\\hat{\\theta}$ of a parameter $\\theta \\sim \\pi(\\theta)$ (where $\\pi$ denotes the prior distribution of $\\theta$), the mean squared error (MSE) is lower bounded by the inverse of a function of the Fisher information, as follows: \\n$$\\mathbb{E}\\left[\\|\\theta - \\hat{\\theta}\\|_2^2\\right] \\geq \\frac{d}{\\det\\left(I_X(\\theta) + J(\\pi)\\right)^{1/d}}$$\\nThis theorem establishes a foundational information-theoretic lower bound on estimation error. Notably, its traditional formulation is restricted to $L_2$ loss.', 'theorem_2': ""This entropic inequality connects the conditional entropy of the parameter $\\theta$ given the observation $X$ (denoted $h(\\theta \\mid X)$) to the Fisher information. Mathematically, it is expressed as: \\n$$\\frac{1}{2\\pi e} e^{\\frac{2}{d} h(\\theta \\mid X)} \\geq \\frac{1}{\\det\\left(I_X(\\theta) + J(\\pi)\\right)^{1/d}}$$\\nCritically, this inequality serves as a key bridge for generalizing beyond squared error ($L_2$ loss). Unlike the Van Trees Inequality, which directly bounds variance (and thus MSE), Efroimovich's Inequality operates by bounding entropy—enabling extensions to broader classes of loss functions."", 'theorem_3': ""For any random variable $X$, if its $q$-th moment is fixed, the differential entropy $h(X)$ is maximized by the $L_q$ maximum-entropy distribution. This principle yields the following inequality: \\n$$h(X) \\leq \\log\\left(C_{ME}(q) \\cdot \\mathbb{E}\\left[|X|^q\\right]^{1/q}\\right)$$\\nHere, $C_{ME}(q)$ denotes a constant dependent only on $q$ (the order of the moment) and the dimension of $X$. A core application of this theorem lies in converting the general entropic bound from Efroimovich's Inequality into a concrete lower bound on $L_q$ loss—facilitating the generalization of estimation error bounds beyond $L_2$.""}"	\begin{aligned}\n&\text { The expected } L_q \text { estimation error is lower bounded by: }\n&\mathbb{E}_{\theta, X}\left[\|\hat{\theta}(X)-\theta\|_q^q\right] \geq\left(\frac{\sqrt{2 \pi e}}{C_{M E}(q)}\right)^q \frac{d}{\operatorname{det}\left(I_X(\theta)+J(\pi)\right)^{\frac{q}{2 d}}}\n\end{aligned}	理论工程学科-信息论和通信理论-分布式通信	355230	1	3
recuVISOGHiBy5	What is the largest source of uncertainty when predicting future changes in the Southern Ocean circulation (such as the intensity of the ACC)? Is it the differences between different climate forcing scenarios (such as SSP scenarios) or the differences in model initial conditions (such as the phase of centennial oscillations)?	"{'step_1': 'Identify potential sources of uncertainty L: According to the problem, there are three types of possible sources of uncertainty: 1. Different climate forcing scenarios (such as Shared Socioeconomic Pathways). 2. Model differences. 3. Initial conditions (for example, the phase of centennial oscillations).', 'step_2': 'Starting from Concept 1 (the existence of the Southern Ocean Centennial Oscillation) ：Concept 1 points out that the deep circulation, sea ice extent, and circulation intensity of the Southern Ocean undergo spontaneous oscillations on a centennial scale, and these oscillations can significantly affect the global climate. This indicates that even under the same climate scenarios and model settings, the internal system can trigger completely different evolutionary paths due to differences in initial conditions (phase differences).', 'step_3': 'Incorporating Concept 2 (Centennial-scale oscillations simulated by HadGEM3-GC3.1-LL)：In the control experiments of CMIP6, the simulation results of HadGEM3-GC3.1-LL show that centennial-scale oscillations of sea ice and deep currents naturally emerge around 500 years. These oscillations originate from the energy imbalance (TOA flux imbalance) existing under the initial conditions of the model. Different phases can cause the system to evolve in different directions. This indicates that under the same climate scenario, simply changing the initial conditions can trigger completely different trends in the circulation changes of the Southern Ocean.', 'step_4': ""Introduce Concept 3 (Uncertainties in Antarctic Bottom Water Transport): Concept 3 emphasizes that the formation and transport of Antarctic Bottom Water (AABW) are highly sensitive to different research settings: some studies suggest that significant mixing occurs before transoceanic transport, while others argue that the water masses in the source regions remain independent. This sensitivity exacerbates the system's internal dependence on the setting of initial conditions: different initial states may amplify or weaken the role of AABW, thereby enhancing the dominance of centennial oscillations in future predictions."", 'step_5': ""Compare the importance of the three sources of uncertainty: 1. Differences in climate forcing scenarios: They determine long-term trends (such as greenhouse gas levels), but their impacts on the Southern Ocean circulation at the regional scale will be masked or delayed by internal oscillations. 2. Model differences: They do lead to different simulation results, but centennial oscillations are widespread in multi-model experiments, indicating that they are not the product of a single model. 3. Initial conditions (phase of centennial oscillations): They directly determine whether the system is in the 'rising phase' or 'declining phase' of the oscillation, and have an overwhelming impact on short-to-medium-term predictions (on the scale of decades to a century). Therefore, it can be deduced that the largest source of uncertainty is not scenario differences or model differences, but initial conditions (the phase of centennial oscillations). This is because these oscillations can occur spontaneously under the same scenario and with the same model, leading to greater discrepancies in the results.""}"	"{'concept_1': 'The existence of the Southern Ocean Centennial Oscillation: There are centennial-scale oscillation phenomena within the Southern Ocean. These oscillations usually manifest as coordinated changes in deep convection, sea ice extent, and circulation intensity. They can occur spontaneously in models and have a significant impact on the global climate.', 'concept_2': ""Centennial-scale oscillations of HadGEM3-GC3.1-LL: In the pre-industrial control experiment (piControl) of CMIP6, the HadGEM3-GC3.1-LL simulation exhibits centennial-scale oscillations in Antarctic sea ice and global temperature. These oscillations begin to emerge around year 500 and are associated with deep convection events in the Ross Sea and Weddell Sea. Their root cause lies in the positive top-of-atmosphere (TOA) energy imbalance present in the model's initial conditions, which leads to the gradual accumulation of heat in the deep ocean and eventually triggers convection, thereby causing climate state transitions and centennial-scale fluctuations."", 'concept_3': 'Uncertainties in simulating Antarctic Bottom Water (AABW) and its related processes in CMIP models: In CMIP5 models, there are uncertainties in the simulation of Antarctic Bottom Water formation. Some models fail to correctly reproduce polar processes related to the formation of Antarctic Bottom Water (for example, the generation of high-density water along the Antarctic continental shelf), resulting in an unclear connection between changes in surface fluxes and the weakening of low-latitude Meridional Overturning Circulation (MOC) cells associated with Antarctic Bottom Water.'}"	Differences in initial conditions (the phase of centennial oscillations) are a more important source of uncertainty than differences in various forcing scenarios.	理论交叉学科-理论环境模型-大气环流模型	124133	2	3
recuVKl5fbSct3	For a 2-Prover-1-Round projection game with alphabet size q, what is the theoretical limit on the soundness error that can be achieved while maintaining NP-hardness?	"{'step_1': 'Any 2-Prover-1-Round game with alphabet size q has value at least 1/q under random assignment, since each constraint is satisfied with probability 1/q when assignments are chosen uniformly at random.', 'step_2': 'The question asks for the best (smallest) soundness error that can be proven NP-hard to achieve, meaning the smallest value in the ""NO"" case that maintains computational hardness.', 'step_3': 'Since random assignment achieves value 1/q, it is information-theoretically impossible to have soundness error less than 1/q, because every instance has value at least 1/q.', 'step_4': 'Theorem 1.3 establishes that for any ε > 0, it is NP-hard to distinguish between value ≥ 1-δ versus value ≤ 1/q^(1-ε). Since 1/q^(1-ε) = q^ε/q, this soundness error is actually greater than 1/q when ε > 0.', 'step_5': 'The soundness error 1/q^(1-ε) approaches the theoretical minimum 1/q from above as ε → 0, since (1-ε) approaches 1 and thus q^ε approaches 1.', 'step_6': 'The paper states that the best one can hope for is γ = 1 - o(1) in the exponent, meaning soundness error 1/q^(1-o(1)), which represents near-optimal hardness approaching the information-theoretic barrier.', 'step_7': 'Since 1/q^(1-ε) approaches 1/q from above as ε → 0, and 1/q is the information-theoretic lower bound, this represents the theoretical limit achievable while maintaining NP-hardness.'}"	"{'concept_1': 'A bipartite graph G = (L ∪ R, E) with alphabets ΣL and ΣR, where each edge e ∈ E specifies a constraint map φe : ΣL → ΣR, and the value is the maximum fraction of edges that can be satisfied by any assignment.', 'concept_2': 'In a PCP, the maximum value achievable in the ""NO"" case when it should be hard to distinguish from the ""YES"" case, representing how well a supposedly invalid proof can fool the verifier.', 'concept_3': 'A fundamental lower bound showing that a random assignment to any 2-Prover-1-Round game with alphabet size q achieves value at least 1/q in expectation, providing an information-theoretic barrier.', 'concept_4': 'The relationship between alphabet size q and achievable soundness error in PCPs, where smaller alphabet sizes generally force larger soundness errors.', 'concept_5': 'Achieving soundness error 1/q^(1-ε) for arbitrarily small ε > 0, which approaches the information-theoretic optimum of 1/q from above.', 'concept_6': ""The paper's technical approach using vertices as ℓ-dimensional subspaces of F_q^n with specific adjacency structure, enabling the subspace encoding that achieves near-optimal parameters."", 'concept_7': ""Setting ℓ_2 = (1-δ)ℓ_1 where the alphabet size becomes approximately q^ℓ_1 and soundness error becomes q^(-(1-δ')ℓ_1) for small δ'.""}"	1/q^(1-ε)	理论基础学科-理论计算机科学-计算复杂度	451229	2	5
recuVKmbDduAgB	In a heavy-ion collision experiment, to eliminate jet-trigger efficiency effects on two-particle angular correlations, the raw pair counts are normalized using the mixed-event method. The following definitions are provided: \begin{itemize} \item Raw pair counts: $N_{\text{pair}}(\Delta\eta,\Delta\phi)$ \item Total number of trigger jets: $N_{\text{jet}}$ \item Mixed-event distribution: $\displaystyle\mathrm{ME}(\Delta\eta,\Delta\phi)= \frac{N_{\text{mixed}}(\Delta\eta,\Delta\phi)}{N_{\text{mixed}}^{\text{tot}}}$ \item Tracking-efficiency correction factor: $S(\Delta\eta,\Delta\phi)$ \end{itemize} If the \textbf{differential two-dimensional angular yield per trigger jet} $\frac{1}{N_{\text{jet}}}\frac{d^{2}N}{d\Delta\eta\,d\Delta\phi}$ is required to equal $1$ at $\Delta\eta=\Delta\phi=0$ \emph{and} the factor $S(\Delta\eta,\Delta\phi)$ must appear in the \textbf{numerator} of the final expression, please write the unique expression that satisfies these conditions. Answer in one sentence without using the internet.	{'Step_1': 'Goal: derive the unique two-dimensional angular yield per trigger jet that equals 1 at $\\Delta\\eta=\\Delta\\varphi=0$ and has $S(\\Delta\\eta,\\Delta\\varphi)$ in the numerator, to be stated in one sentence.', 'Step_2': 'Observable definition (Concept_1): identify the per-trigger-jet differential yield $\\mathcal{Y}(\\Delta\\eta,\\Delta\\varphi)=\\displaystyle\\frac{1}{N_{\\text{jet}}}\\frac{\\mathrm{d}^2N}{\\mathrm{d}\\Delta\\eta\\,\\mathrm{d}\\Delta\\varphi}$.', 'Step_3': 'Mixed-event normalization (Concept_2): define $\\text{ME}(\\Delta\\eta,\\Delta\\varphi)=\\displaystyle\\frac{N_{\\text{mixed}}(\\Delta\\eta,\\Delta\\varphi)}{N_{\\text{mixed}}^{\\text{tot}}}$ to flatten detector acceptance.', 'Step_4': 'Efficiency correction (Concept_3): multiply by the tracking-efficiency factor $S(\\Delta\\eta,\\Delta\\varphi)$ to correct for reconstruction losses.', 'Step_5': 'Ansatz construction (Concept_4): postulate $\\mathcal{Y}(\\Delta\\eta,\\Delta\\varphi)=\\displaystyle\\frac{\\text{ME}(0,0)\\,S(\\Delta\\eta,\\Delta\\varphi)}{\\text{ME}(\\Delta\\eta,\\Delta\\varphi)\\,S(0,0)}$, explicitly placing $S(\\Delta\\eta,\\Delta\\varphi)$ in the numerator.', 'Step_6': 'Boundary condition (Concept_5): verify $\\mathcal{Y}(0,0)=1$.', 'Step_7': 'Efficiency normalization (Concept_6): note that $S(\\Delta\\eta,\\Delta\\varphi)$ is conventionally normalized to $S(0,0)=1$ at the reference point $\\Delta\\eta=\\Delta\\varphi=0$.'}	{'Concept_1': 'Identify the observable as the \\textbf{per-trigger-jet differential yield} $\\mathcal{Y}(\\Delta\\eta,\\Delta\\phi)\\equiv \\frac{1}{N_{\\text{jet}}}\\frac{d^{2}N}{d\\Delta\\eta\\,d\\Delta\\phi}.$', 'Concept_2': 'Normalize raw pair counts with the \\textbf{mixed-event distribution} $\\mathrm{ME}(\\Delta\\eta,\\Delta\\phi)= \\frac{N_{\\text{mixed}}(\\Delta\\eta,\\Delta\\phi)}{N_{\\text{mixed}}^{\\text{tot}}}.$', 'Concept_3': 'Correct for detector efficiency with the \\textbf{tracking-efficiency factor} $S(\\Delta\\eta,\\Delta\\phi).$', 'Concept_4': 'Impose the \\textbf{physical ansatz} $\\mathcal{Y}(\\Delta\\eta,\\Delta\\phi)= \\frac{\\mathrm{ME}(0,0)\\,S(\\Delta\\eta,\\Delta\\phi)} {\\mathrm{ME}(\\Delta\\eta,\\Delta\\phi)\\,S(0,0)},$ which places $S(\\Delta\\eta,\\Delta\\phi)$ in the numerator.', 'Concept_5': 'Verify the \\textbf{boundary condition} $\\mathcal{Y}(0,0)=1.$', 'Concept_6': '\\textbf{tracking-efficiency factor} $S(\\Delta\\eta,\\Delta\\phi).$'}	$\frac{1}{N_{\text{jet}}}\frac{d^{2}N}{d\Delta\eta\, d\Delta\phi} = \frac{\mathrm{ME}(0,0)}{\mathrm{ME}(\Delta\eta,\Delta\phi)}\, S(\Delta\eta,\Delta\phi).$	理论基础学科-理论物理-重离子碰撞	159697	2	1
recuVKxGnLijDz	In the static spherically symmetric regular black hole with two shape functions, where the metric is given by \( ds^{2}=-f(r)\,dt^{2}+f(r)^{-1}A(r)^{2}dr^{2}+r^{2}d\Omega^{2} \), what is the minimal constraint on the power series expansion to ensure that the curvature invariants remain finite as \( r\to0 \)?	{'step_1': 'First, according to \\concept_1, perform a power series expansion of the two shape function metrics as $r\\to0$: \\[ A(r)=A_0+A_1 r + A_2 r^2 + \\cdots,\\qquad f(r)=B_0+B_1 r + B_2 r^2 + \\cdots . \\]', 'step_2': 'Next, using concept_2, examine the leading singular term of the Ricci scalar: \\[ R=\\frac{2\\bigl(1 - f(r)/A(r)^2\\bigr)}{r^{2}} + O(1). \\] Substituting the expansions shows that if $A_0\\neq B_0$, a $1/r^2$ divergence occurs. Therefore, to ensure $R$ is finite, the following must hold: \\[ A_0=B_0 . \\]', 'step_3': 'Then, in accordance with concept_3, eliminate all $1/r$ terms using the invariants composed of the Ricci tensor from the Zakhary\\textendash McIntosh (ZM) set (which is equivalent to examining the singular parts of $R_{tt}$ and $R_{rr}$). From \\[ R_{tt}=\\frac{B_0 B_1}{A_0}\\,\\frac{1}{r}+O(1) \\] under the condition $A_0=B_0$, it is deduced that $B_1=0$; further, from \\[ R_{rr}=\\Bigl(\\frac{A_1}{A_0}-\\frac{B_1}{B_0}\\Bigr)\\frac{1}{r}+O(1) \\] under the conditions $A_0=B_0$ and $B_1=0$, it is obtained that $A_1=0$.'}	"{'concept_1': 'Expansion for the case of two shape functions: A(r)=A_{0}+A_{1} r + A_{2} r^{2}+\\cdots, f(r)=B_{0}+B_{1} r + B_{2} r^{2}+\\cdots .', 'concept_2': 'First eliminate the zero-order term using the condition that the ""singular part of the Ricci scalar R is 0"", where R=\\frac{2(1 - f(r)/A(r)^2)}{r^2} + O(1);', 'concept_3': 'ZM is a complete set (17 in total) of second-order curvature invariants, among which multiple items are directly composed of the Ricci tensor. Therefore, requiring them to be finite is equivalent to setting the coefficients of the singular terms of these components to zero. For this purpose, the singular parts of the items containing R_{tt} and R_{rr} in the ZM set are set to 0.'}"	A_0 = B_0 and A_1 = B_1 = 0.	理论交叉学科-理论天体物理-黑洞几何	71684	1	3
recuVMql7D1CY0	Consider the problem of determining the fundamental limit on the asymptotic transformation ratio between two statistical experiments, each composed of a pair of probability distributions, $P=(p^{(1)}, p^{(2)})$ and $Q=(q^{(1)}, q^{(2)})$. Let this limit be $r(P \rightarrow Q) = \sup\{m/n \mid P^{\otimes n} \ge Q^{\otimes m}\}$, where '$\ge$' denotes majorization. Find the expression for this limit under the condition that the set of possible outcomes for $p^{(1)}$ is a non-trivial, strict subset of the outcomes for $p^{(2)}$.	"{'step_1': ""The instruction asks for the 'fundamental limit on the asymptotic transformation ratio,' $r(P \\rightarrow Q)$, which is a formal way of asking for the optimal rate. The transformation itself is defined by majorization (Concept_1)."", 'step_2': 'The general method to find this limit is to compute the minimum ratio of certain information-theoretic quantities known as monotones (Concept_2, Concept_3).', 'step_3': 'The core of the problem is to identify the correct family of monotones for the given support condition: $supp~p^{(1)}$ is a strict subset of $supp~p^{(2)}$ (Concept_5).', 'step_4': ""Concept_4 introduces the mathematical definition of a crucial family of monotones without naming them 'Rényi divergences.'"", 'step_5': 'Theorem_1 provides the critical link: it states that for the specific strict-subset support condition, the set of all necessary monotones is precisely the family defined in Concept_4, evaluated for $(p^{(1)}, p^{(2)})$, over the parameter range $\\alpha \\in [0, \\infty]$.', 'step_6': ""We can now apply the general formula from Concept_2. We replace the abstract 'monotone $D$' with the specific functional form from Concept_4 (which we recognize as Rényi divergence), and we perform the minimization over the parameter $\\alpha \\in [0, \\infty]$ as specified by Theorem_1."", 'step_7': 'This procedure directly yields the final answer, expressing the rate as the minimum ratio of these information measures.'}"	"{'concept_1': ""The relation '$P \\ge Q$' (P majorizes Q) holds if Q can be obtained from P via a stochastic map, meaning Q is a noisy version of P."", 'concept_2': 'The fundamental transformation limit $r(P \\rightarrow Q)$ is given by the infimum, over a set of valid information-theoretic monotones, of the ratio of the monotone evaluated on P to that on Q.', 'concept_3': 'A monotone is a function $D$ that does not increase under the application of a stochastic map, i.e., $D(P) \\ge D(TP)$.', 'concept_4': 'A key family of monotones for comparing two distributions $p$ and $q$ is defined by the functional form $D(p||q) = \\frac{1}{\\alpha-1}\\log\\sum_i p_i^\\alpha q_i^{1-\\alpha}$, parameterized by a real number $\\alpha$.', 'concept_5': 'The specific set of monotones relevant for determining the transformation limit depends on the support properties of the distributions in experiment $P$.', 'theorem_1': 'When the support of $p^{(1)}$ is a strict subset of the support of $p^{(2)}$, the only relevant family of monotones for the rate calculation are those of the form defined in Concept_4, with $p=p^{(1)}$ and $q=p^{(2)}$, for all $\\alpha \\in [0, \\infty]$.'}"	$$r(P \rightarrow Q) = \min_{\alpha \in [0, \infty]} \frac{D_{\alpha}(p^{(1)}||p^{(2)})}{D_{\alpha}(q^{(1)}||q^{(2)})}$$	理论工程学科-信息论和通信理论-信息测度	287265	3	4
recuVMBduqXvjK	For a system with $M$ sequences where exactly one is an outlier generated from $P_A$ and the rest from $P_N$, we are interested in a sequential decision process. This process is constrained such that its expected duration is uniformly bounded across all possible distribution pairs $(P_N, P_A)$. Determine the optimal performance characteristic that describes the exponential relationship between the error probability and this uniform time bound.	"{'step_1': ""The objective is to determine the highest possible misclassification exponent ($E^{EST}$) for any sequential test that adheres to the Expected Stopping Time Universality Constraint in an 'exactly one outlier' setting (Concept_1, Concept_2, Concept_3). This requires finding a bound that is both achievable by some test and serves as an upper limit for all possible tests."", 'step_2': 'To establish achievability, a specific sequential test, denoted $\\Phi^{EST}$, is constructed. This test employs a minimum scoring function rule (Concept_4). The score for each hypothesis $H_i$, $S_i(x^k)$, is calculated using a function based on the sum of KL-divergences, $G_i$, which measures the similarity of the empirical distributions of all sequences except the i-th one (Concept_5). The test stops when any score drops below a predefined, time-varying threshold, and the decision corresponds to the hypothesis with the minimum score.', 'step_3': 'The performance of this test, specifically its misclassification exponent, is analyzed using large deviations theory. An error occurs under hypothesis $H_i$ if the test wrongly decides $H_j$ for some $j \\neq i$. The probability of this event is bounded. The analysis shows that the exponential decay rate of this probability is determined by a minimization problem over all possible empirical distributions, which has the form $\\min_{Q\\in\\mathcal{P}(X)} D(Q||P_{A})+(M-2)D(Q||P_{N})$.', 'step_4': 'This specific optimization problem is then connected to a standard information measure. By applying the variational form of the Rényi divergence (Theorem_1) with the parameter $\\alpha = M-2$, the expression $\\min_{Q\\in\\mathcal{P}(X)} (M-2)D(Q||P_{N}) + D(Q||P_{A})$ is shown to be exactly equal to the Rényi divergence of order $\\frac{M-2}{M-1}$, i.e., $D_{\\frac{M-2}{M-1}}(P_{N}||P_{A})$.', 'step_5': 'This result demonstrates that the proposed test $\\Phi^{EST}$ can achieve an exponent of at least $D_{\\frac{M-2}{M-1}}(P_{N}||P_{A})$, thus establishing the achievability part of the tight bound (Theorem_2).', 'step_6': 'To prove that this bound is tight, a converse is required. The converse proof (Theorem_3) uses the data processing inequality for KL-divergence to show that for *any* sequential test satisfying the expected stopping time universality constraint, the misclassification exponent cannot exceed a certain value. By optimizing over auxiliary distributions, this upper bound is shown to be precisely $D_{\\frac{M-2}{M-1}}(P_{N}||P_{A})$.', 'step_7': 'Since the achievability result shows a test can attain this exponent and the converse result shows no test can exceed it, the value $D_{\\frac{M-2}{M-1}}(P_{N}||P_{A})$ is established as the tight bound on the misclassification exponent.'}"	{'concept_1': 'Outlier Identification Problem: A scenario with $M-1$ sequences from a nominal source $P_N$ and one from an anomalous source $P_A$.', 'concept_2': 'Uniform Expected Stopping Time Constraint: The sequential process must terminate within a pre-defined average time limit, regardless of the underlying data statistics.', 'concept_3': 'Error Exponent: The performance is measured by the rate at which the logarithm of the error probability scales linearly with the stopping time bound.', 'theorem_1': 'Rényi Divergence: A family of divergence measures, $D_{\\alpha}(P||Q)$, parametrized by an order $\\alpha$.', 'theorem_2': 'Information-Theoretic Bounds: The solution to problems of this nature is often found by establishing matching achievability and converse bounds, which define the precise theoretical limit of performance.'}	$D_{\frac{M-2}{M-1}}(P_{N}||P_{A})$	理论工程学科-信息论和通信理论-假设检验	167467	2	3
recuVMGhp0EmSB	A rational agent sequentially samples from one of $n$ available information sources to distinguish between competing hypotheses. The agent's long-term strategy is a stationary probability distribution over the sources. The ultimate performance of such a test is measured by its error exponent, which is fundamentally limited by the expected rate of information accumulation. Formulate the mathematical expression for this performance limit for distinguishing a declared hypothesis ($m$) from an alternative ($\theta$).	"{'step_1': 'The primary objective is to formulate the expression for the error exponent, which, according to the Performance Limit Principle (Concept_3), is equivalent to the average information gained per sample.', 'step_2': 'First, we must quantify the information provided by a single sample. According to the Information Measure (Concept_2), the information gained from a sample from a specific source $j$ for distinguishing the distribution $P_{m,j}$ (under hypothesis $m$) from $P_{\\theta,j}$ (under hypothesis $\\theta$) is the Kullback-Leibler (KL) divergence, $D(P_{m,j}||P_{\\theta,j})$.', 'step_3': ""Next, we consider the agent's behavior. The agent does not stick to a single source but chooses among them according to a stationary strategy. As defined by Agent Strategy (Concept_1), this strategy is a probability distribution over the $n$ sources, denoted by the vector $\\beta^m = (\\beta_1^m, \\dots, \\beta_n^m)$, where $\\beta_j^m$ is the probability of selecting source $j$."", 'step_4': 'To find the *average* information gain across all sources, we must average the information from each source, weighted by the probability of selecting it. This is a direct application of the Law of Total Expectation (Concept_4).', 'step_5': 'Applying this law, the overall expected information per sample is the sum of the conditional expectations for each source (the KL divergence $D(P_{m,j}||P_{\\theta,j})$) multiplied by the probability of choosing that source ($\\beta_j^m$).', 'step_6': 'This calculation yields the final mathematical expression: $\\sum_{j=1}^{n}\\beta_{j}^{m}D(P_{m,j}||P_{\\theta,j})$. This expression represents the average rate of information accumulation, which, by Concept_3, is the error exponent limit.'}"	"{'concept_1': ""Agent Strategy: An agent's stationary strategy is a probability vector $\\pi = (\\pi_1, ..., \\pi_n)$ where $\\pi_j$ is the probability of choosing to sample from source $j$."", 'concept_2': ""Information Measure: The information provided by a single sample from source $j$ for discriminating a declared hypothesis's distribution $P_m$ from an alternative $P_\\theta$ is the expectation, under $P_m$, of the logarithmic ratio of their likelihoods."", 'concept_3': 'Performance Limit Principle: The asymptotic decay rate of the error probability (the exponent) is equal to the average information gained per sample across all sources.', 'concept_4': 'Law of Total Expectation: The overall expected value of a quantity that depends on a choice is the sum of the conditional expectations for each choice, weighted by the probability of making that choice.'}"	$\sum_{j=1}^{n}\beta_{j}^{m}D(P_{m,j}||P_{\theta,j})$	理论工程学科-信息论和通信理论-信息度量	661203	2	3
recuVNmsHLOjyY	Consider an interface system of two semi-infinite one-dimensional monatomic chains: the left side is linear, the right side is weakly cubic-nonlinear. Each mass has value m; adjacent masses are connected by linear springs of stiffness k_1; the right side additionally has cubic stiffness k_3. The equations of motion are: （1）Linear region (j<0): m*\ddot{u}_j+k_1*(2u_j-u_{j-1}-u_{j+1})=0, (2) Nonlinear region (j>0): m*\ddot{u}_j+k_1(2u_j-u_{j-1}-u_{j+1})+\varepsilon*k_3*\big[(u_j-u_{j-1})^3+(u_j-u_{j+1})^3\big]=0, (3) At the interface j=-1,0: m*\ddot{u}_{-1}+k_1*(2*u_{-1}-u_{-2}-u_0)=0, m*\ddot{u}_{0}+k_1*(2*u_{0}-u_{-1}-u_{1})+\varepsilon*k_3*(u_0-u_1)^3=0. Suppose a right-going Bloch wave \frac{1}{2}A e^{i(\omega t - \mu j)}+\text{c.c.}, is incident from the far left side j = -\infinite, and \mu,\omega satisfying the linear dispersion \omega(\mu)=\sqrt{\frac{2*k_1}{m}(1-\cos{\mu})}. When using the first-order method of multiple scales to solve the problem, how do we set the form of the zeroth-order solutions u_j^(0)? Note that the final expression needs to be brought back to the field of t.	{'step_1': 'Use the method of multiple scales in Concept_1 by expanding time as T_0=t,\\;T_1=\\varepsilon t, and expand displacement as u_j=u_j^{(0)}+\\varepsilon u_j^{(1)}+\\dots. Time derivatives become D_0=\\partial/\\partial T_0,\\;D_1=\\partial/\\partial T_1. The zeroth-order equation contains only D_0^2 and yields the linear eigenwave problem; hence, the zeroth-order solution must be taken as the linear Bloch-wave basis so higher orders can introduce amplitude-dependent corrections.', 'step_2': 'In the linear left half (j<0) take the Bloch-wave form including incident right-going and reflected left-going parts: \\frac{1}{2}A e^{i(\\omega T_0-\\mu j)}+\\frac{1}{2}A_{1h}^- e^{i(\\omega T_0+\\mu j)}+\\text{c.c.}. This directly uses Concept_2 and ensures compliance with the linear dispersion \\omega(\\mu)=\\sqrt{\\frac{2k_1}{m}(1-\\cos\\mu)} at zeroth order.', 'step_3': 'In the right half (j\\ge0) take the zeroth-order solution as a transmitted right-going component \\frac{1}{2}A_{1h}^+ e^{i(\\omega T_0-\\mu_0 j)}+\\text{c.c.}. Although the right side is weakly nonlinear, the zeroth-order waveform is still linear, but its wavenumber \\mu_0 may differ from the left-side \\mu, reflecting the potential amplitude-dependent dispersion of Concept_3.', 'step_4': 'Represent the frequency with a slow correction \\omega=\\omega_0+\\varepsilon\\sigma (i.e. \\omega_1=\\sigma) and let amplitudes depend on slow time T_1: A=A(T_1),\\;A_{1h}^{\\pm}=A_{1h}^{\\pm}(T_1). This follows Concept_1 and sets up the first-order problem that will produce amplitude/phase modulation and nonlinear frequency shift (Concept_3).', 'step_5': 'Impose matching at the interface j=-1,0 by substituting the zeroth-order left and right expressions into the zeroth-order interface equations; these matching conditions link A,\\,A_{1h}^{\\pm},\\;\\mu,\\;\\mu_0,\\;\\sigma. The actual nonlinear amplitude-dependent corrections (e.g. shift of \\mu_0 or \\sigma) are determined at first order—this ordering respects the multiple-scales logic and keeps usage of Concepts 1–3 mathematically consistent.', 'step_6': 'Therefore, the zeroth-order solutions are (matching the provided answer): For j<0: u_j^{(0)}=\\tfrac{1}{2}A\\,e^{i(\\omega T_0-\\mu j)}+\\tfrac{1}{2}A_{1h}^{-}\\,e^{i(\\omega T_0+\\mu j)}+\\text{c.c.}. For j\\ge0: u_j^{(0)}=\\tfrac{1}{2}A_{1h}^{+}\\,e^{i(\\omega T_0-\\mu_0 j)}+\\text{c.c.}, with \\omega=\\omega_0+\\varepsilon\\sigma and \\omega_0(\\mu_0)=\\sqrt{\\tfrac{2k_1}{m}(1-\\cos\\mu_0)}. This formulation explicitly invokes Concept_1, Concept_2 and Concept_3; their usages are consistent and logically valid in the multiple-scales perturbation framework.'}	{'concept_1': 'Method of Multiple Scales | In weakly nonlinear systems, the time variable is asymptotically expanded into multiple scales (T_n = \\varepsilon^n t), and the displacement solution is expressed as a series u_j = u_j^(0) + \\varepsilon*u_j^(1) + …. Time derivatives are expanded as d/dt = D_0 + \\varepsilon*D_1 + … and d^2/dt^2 = D_0^2 + 2*\\varepsilon*D_0*D_1 + …. This enables order-by-order separation of the problem. Zero-order problem will provide the linear form of displacement. The first- or higher-order problems will bring the expression of nonlinear frequency and nonlinear displacement.', 'concept_2': 'Bloch wave | In periodic linear chains, wave propagation solutions adopt the Bloch wave form \\frac{1/2}*A*e^{i*(\\omega*T_0 - \\mu*j)} + c.c., where frequency \\omega and wavenumber \\mu satisfy the linear dispersion relation.', 'concept_3': 'Nonlinear dispersion solution | The nonlinear dispersion relation denotes the dispersion relation of a nonlinear periodic system, which is different from that of the linear compartment, meaning that the wavenumber may be changed, from \\mu to \\mu_0, at fixed frequency when the wave amplitude is changed.'}	u_j^(0) = \frac{1}{2}*A*e^{i*(\omega*T_0 - \mu*j)} + \frac{1}{2}*A_{1h}^{-}*e^{i*(\omega*T_0 + \mu*j)} + c.c., for j < 0. u_j^(0) = \frac{1}{2}*A_{1h}^{+}*e^{i*(\omega*T_0-\mu_0*j)} + c.c., for j >= 0, where \omega = \omega_0 + \varepsilon*\sigma. Note, for the nonlinear case, \mu_0 isn’t equal to \mu. If using the same symbols to represent the linear and nonlinear wave numbers, the results is wrong, can not give a correct prediction for nonlinear behaviour. The \omega_0 and \mu_0 satisfy \omega_0(\mu_0)=\sqrt{\frac{2*k_1}{m}(1-\cos{\mu_0})}. Other symbols A_{1h}^{-} and A_{1h}^{+} can be replaced according to preference, such as using B and C.	理论工程学科-理论工程力学-非线性波动理论	112596	2	3
recuVOAMbAyzVh	For the ensemble of random Staircase Generator Matrix Codes (SGMCs), what is the average partial weight spectrum corresponding to codewords generated by information sequences where the rightmost '1' is at a specific position $l$? A random SGMC is a binary $[n,k]$ linear code whose generator matrix has a staircase structure defined by a profile $w=(w_0, \dots, w_{k-1})$, with random binary elements below the staircase.	"{'step_1': 'The objective is to find the average partial weight spectrum, denoted $B^{(l)}(X)$, which corresponds to the set of codewords $\\mathbb{G}_l$ from the random SGMC ensemble (Concept_3, Concept_5).', 'step_2': 'A random SGMC is defined by its staircase generator matrix, specified by a profile $w$, with random elements below the staircase (Concepts 1 & 2).', 'step_3': ""The set of codewords $\\mathbb{G}_l$ is generated by the $2^l$ information sequences in the subset $\\mathcal{U}_l$, whose rightmost '1' is at position $l$ (Concept_4)."", 'step_4': 'We first analyze the structure of a single codeword $c$ generated by an input $u \\in \\mathcal{U}_l$. Due to the structure of the random generator matrix, any such codeword is a random vector with a specific three-part structure (Theorem_1).', 'step_5': 'The weight enumerator of such a codeword can be determined by analyzing its segments: * The all-zero segment of length $n-n_l$ contributes a factor of $X^0=1$. * The all-one segment of length $w_l$ has a fixed weight of $w_l$, contributing a factor of $X^{w_l}$. * The first segment of length $n_l - w_l$ is totally random. The weight enumerator of a totally random binary vector of length $N$ is $(1/2 + 1/2X)^N$. Thus, this segment contributes a factor of $(1/2 + 1/2X)^{n_l - w_l}$.', 'step_6': 'The average weight enumerator for a single codeword in $\\mathbb{G}_l$ is the product of the enumerators of its independent segments: $X^{w_l}(1/2 + 1/2X)^{n_l - w_l}$.', 'step_7': 'The set $\\mathbb{G}_l$ contains $2^l$ such codewords, each generated by an information vector in $\\mathcal{U}_l$. Since each codeword in $\\mathbb{G}_l$ follows the same random distribution, the average partial weight spectrum for the entire set is $2^l$ times the average weight enumerator of a single codeword.', 'step_8': 'Therefore, the final expression for the average partial weight spectrum is $B^{(l)}(X) = 2^l X^{w_l} (1/2 + 1/2X)^{n_l - w_l}$.'}"	{'concept_1': 'A Staircase Generator Matrix Code (SGMC) is a binary $[n,k]$ linear code whose generator matrix has a staircase-like structure defined by a profile of integers $w=(w_0, w_1, \\dots, w_{k-1})$ such that $\\sum_{l=0}^{k-1} w_l = n$.', 'concept_2': 'A random SGMC ensemble is a collection of SGMCs where the binary elements in the generator matrix below the defined staircase structure are sampled independently from a Bernoulli process with a success probability of 1/2.', 'concept_3': 'The weight spectrum of a code ensemble is the average weight enumerator polynomial, $B(X) = \\sum_{d=0}^n B_d X^d$, where $B_d$ is the average number of codewords with Hamming weight $d$.', 'concept_4': 'The set of $2^k-1$ non-zero information sequences is partitioned into $k$ disjoint subsets, $\\mathcal{U}_l$ for $l=0, \\dots, k-1$. An information sequence $u$ belongs to $\\mathcal{U}_l$ if its rightmost non-zero element is at position $l$.', 'concept_5': 'The set of codewords $\\mathbb{G}_l$ is defined as $\\{c=uG | u \\in \\mathcal{U}_l\\}$. The average partial weight spectrum is the weight spectrum of the codewords within this specific set $\\mathbb{G}_l$.', 'theorem_1': 'For any information sequence $u \\in \\mathcal{U}_l$, a corresponding codeword $c = uG$ from the random SGMC ensemble is a random vector composed of three distinct segments: a totally random binary segment of length $n_l - w_l$, a fixed all-one segment of length $w_l$, and a fixed all-zero segment of length $n-n_l$.'}	$2^l X^{w_l} (1/2 + 1/2X)^{n_l - w_l}$, where $n_l = \sum_{i=0}^{l} w_i$.	理论工程学科-信息论和通信理论-信道编码理论	170875	3	2
recuVTejQQ3Buw	Let $N\geq 2$, $\Omega\subset \mathbb{R}^N$ be a bounded domain with $C^2$ boundary $\partial\Omega$. Assume $u\in C^2(\overline{\Omega})\cap C^\infty(\overline{\Omega})$ is a solution to the following Poisson equation: \[\begin{cases}\Delta u=N,&~\text{in }\Omega\\u=0,&~\text{on }\partial\Omega\end{cases}.\] Denote by $H$ the mean curvature of $\Omega$, $G=\max_{\overline{\Omega}}|\nabla u|$, $R=\frac{1}{|\partial\Omega|}\int_{\partial\Omega}^{}u_\nu dS$, $H_0=\frac{1}{R}$, $\delta=\int_{\partial\Omega}^{}(H_0-H)^+dS$. It's known that $u$ satisfies the following inequality \[\left|\int_{\Omega}^{}(-u)dx-C(N, R)|\Omega|\right|\leq C(N, d_\Omega)\delta^{\tau_N},\] where $C(N, R)$ denotes a constant depends on $N$ and $R$ and $C(N, d_\Omega)$ denotes a constant depends on $N$ and $d_\Omega$, the diameter of $\Omega$. Find the expression for $C(N, R)$ by considering the special case when $\Omega$ is a ball, and figure out the value of $\tau_N$ by establishing a similar estimate in detail (details are needed) for $\left|\int_{\Omega}^{}(-u)dx-C(N, R)|\Omega|\right|$ . Your answer should return these two values.	{'step_1': 'find $C(N, R)$ through a special case. When $\\Omega$ is a ball centered at $0$, the Poisson equation admits the unique solution $u(z)=\\frac{1}{2}(|x|^2-R^2)$, so the boundary condition $u=0$ implies that $\\Omega=B_R$. Direct computation shows that $\\int_{\\Omega}^{}(-u)dx=\\frac{\\omega_N R^{N+2}}{N+2}$ and $\\Omega=\\omega_NR^N$, where $\\omega_N=|B_1|$. In this special case $H=H_0$, thus $\\int_{\\Omega}^{}(-u)dx=C(N, R)|\\Omega|$, which gives $C(N,R)=\\frac{R^2}{N+2}$.', 'step_2': 'estimate $\\left|\\int_{\\Omega}^{}(-u)dx-\\frac{R^2}{N+2}|\\Omega|\\right|$ in terms of $\\left\\| u_\\nu-R \\right\\|_{L^2(\\partial\\Omega)}$. Apply theorem 2 to $u$, since $\\Delta u=N$ and $u=0$ on $\\partial\\Omega$, we get \\[\\int_{\\Omega}^{}(-u)dx=\\frac{1}{N}\\int_{\\Omega}^{}|\\nabla u|^2 dx.\\] Apply theorem 1, we find \\[\\int_{\\Omega}^{}(-u)dx=\\frac{1}{N(N+2)}\\int_{\\partial\\Omega}^{}u_\\nu^2 \\left\\langle x-z,\\nu\\right\\rangle dS.\\] So \\begin{align*} \\left|\\int_{\\Omega}^{}(-u)-\\frac{R^2}{N+2}|\\Omega|\\right|=&~\\left|\\frac{1}{N(N+2)}\\int_{\\partial\\Omega}^{}(u_\\nu^2-R^2)\\left\\langle x-z,\\nu\\right\\rangle\\right| \\\\ \\leq&~ \\frac{1}{N(N+2)}(R+G)d_\\Omega\\int_{\\partial\\Omega}^{}|u_\\nu-R|dx \\\\ \\leq&~ \\frac{1}{N(N+2)}(R+G)d_\\Omega |\\partial\\Omega|^{1/2}\\left\\| u_\\nu-R \\right\\|_{L^2(\\partial\\Omega)} \\\\ \\leq&~2G d_\\Omega |\\partial\\Omega|^{1/2} \\left\\| u_\\nu-R \\right\\|_{L^2(\\partial\\Omega)},\\end{align*} where in the second inequality we use the Cauchy-Schwarz inequality, theorem 4, and in the last inequality we use the fact that \\[R=\\frac{1}{|\\partial\\Omega|}\\int_{\\partial\\Omega}^{}u_\\nu \\leq G.\\]', 'step_3': 'relate $|\\partial\\Omega|^{1/2}\\left\\| u_\\nu-R \\right\\|_{L^2(\\partial\\Omega)}$ to $\\delta$. By theorem 3, since $|\\nabla u|^2-\\frac{(\\Delta u)^2}{N}\\geq 0$, we have \\[\\frac{1}{R}\\int_{\\partial\\Omega}^{}(u_\\nu-R)^2\\leq G^2\\delta.\\] Recall the definition of $R$: \\[R=\\frac{1}{\\partial\\Omega}\\int_{\\partial\\Omega}^{}u_\\nu=\\frac{1}{|\\partial\\Omega|}\\int_{\\Omega}^{}\\Delta u=\\frac{N|\\Omega|}{|\\partial\\Omega|}.\\] Thus $|\\partial\\Omega|^{1/2}\\left\\| u_\\nu-R \\right\\|_{L^2(\\partial\\Omega)}\\leq G (N|\\Omega|)^{1/2}\\delta^{1/2}$.', 'step_4': 'find $\\tau_N$. Combine all the estimates obtained in step 2 and step 3 we have \\[\\left|\\int_{\\Omega}^{}(-u)-\\frac{R^2}{N+2}|\\Omega|\\right|\\leq C(N, d_\\Omega, |\\Omega|)G^2\\delta^{1/2}\\leq C(N, d_\\Omega)G^2 \\delta^{1/2},\\] which gives $\\tau_N=\\frac{1}{2}$. '}	"{'concept/theorem_1': 'With the same assumption on $\\Omega$ and $u$, the following Pohozaev equality holds: \\[\\int_{\\Omega}^{}|\\nabla u|^2=\\frac{1}{N+2}\\int_{\\partial\\Omega}^{}u_\\nu^2 \\left\\langle x-z, \\nu\\right\\rangle dS,\\] where $z\\in \\mathbb{R}^N$ is a given point.', 'concept/theorem_2': ""Let $\\Omega$ be a domain for which the divergence theorem holds and $u\\in C^2(\\Omega)$, then we have the following Green's first identity: \\[\\int_{\\Omega}^{}\\left(u\\Delta u+|\\nabla u|^2\\right)dx=\\int_{\\partial\\Omega}^{}u\\frac{\\partial u}{\\partial\\nu}dS.\\]"", 'concept/theorem_3': 'With the same assumption on $\\Omega$ and $u$, $u$ will satisfy the following integral identity: \\[\\frac{1}{N-1}\\int_{\\Omega}^{}\\left\\{|\\nabla^2 u|^2-\\frac{(\\Delta u)^2}{N}\\right\\}dx + \\frac{1}{R}\\int_{\\partial\\Omega}^{}(u_\\nu-R)^2dS=\\int_{\\partial\\Omega}^{}(H_0-H)(u_\\nu)^2dS.\\]', 'concept/theorem_4': ""The Holder's inequality holds: For any $f\\in L^p(\\Omega)$, $g\\in L^q(\\Omega)$ such that $p, q\\geq 1$ and $\\frac{1}{p}+\\frac{1}{q}=1$, we have \\[\\left|\\int_{\\Omega}^{}fg\\right|\\leq \\left\\| f \\right\\|_{L^p}\\left\\| g \\right\\|_{L^q}.\\]""}"	$C(N, R)=\frac{R^2}{N+2}$ and $\tau_N=\frac{1}{2}$.	理论基础学科-基础数学-偏微分方程	106918	2	4
recuVTKPQPtHAh	What is the pessimistic slightly superexponential capacity, denoted as \(\dot{C}_{DI}(W)\), for a general memoryless channel `W` with a finite output alphabet and arbitrary input? This capacity is defined in the context of deterministic identification (DI), where the number of identifiable messages \(N_{DI}\) scales with the block length `n` as \(N_{DI} \sim 2^{\dot{C}_{DI}(W) \cdot n \log n}\). The capacity is related to the lower Minkowski dimension, `d`, of the set formed by taking the square root of each of the channel's output probability distributions, a set denoted as \(\sqrt{\tilde{\mathcal{X}}}\).	{'step_1': 'The objective is to determine the pessimistic slightly superexponential capacity, \\(\\dot{C}_{DI}(W)\\). This requires finding the maximum number of codewords `N` for a DI code (Concept_1) and relating its asymptotic scaling to the block length `n` via the capacity definition (Concept_2).', 'step_2': 'We start with an arbitrary \\((n, N, \\lambda_1, \\lambda_2)\\)-DI code. The reliability condition (Concept_1) implies that the output distributions for any two distinct codewords must be highly distinguishable. Using the first part of Theorem_1, this high distinguishability requires the corresponding sequences of spherised vectors to be separated by a minimum Euclidean distance. This transforms the coding problem into a geometric packing problem.', 'step_3': 'To limit the size `N` of this packing, we use a covering argument. We cover the single-letter spherised output set \\(\\sqrt{\\tilde{\\mathcal{X}}}\\) (Concept_4) with a finite number of small balls. The total number of distinct codeword sequences can be no more than \\(|\\text{covering set}|^n\\). By relating the size of this covering to the lower Minkowski dimension `d` of \\(\\sqrt{\\tilde{\\mathcal{X}}}\\) (Concept_3), we can bound `N`. Taking the logarithm and applying the definition of \\(\\dot{C}_{DI}(W)\\) yields the upper bound: \\(\\dot{C}_{DI}(W) \\le \\frac{1}{2}d\\).', 'step_4': 'To find the lower bound, we construct a code. The goal is to find a large set of codewords whose output distributions are highly distinguishable. We begin by creating a geometric packing of the single-letter spherised output set \\(\\sqrt{\\tilde{\\mathcal{X}}}\\) with a specific minimum separation. The number of points in this packing is lower-bounded by the lower Minkowski dimension `d` (Concept_3).', 'step_5': 'We use the points from this single-letter packing as an alphabet to construct `n`-length codeword sequences. We choose all possible sequences formed from this alphabet, creating a codebook of size \\(N = |\\text{packing set}|^n\\).', 'step_6': 'For any two distinct codeword sequences from this construction, we can lower-bound the sum of squared Euclidean distances between them. Using the second part of Theorem_1, this geometric separation is strong enough to guarantee that the total variation distance between the corresponding product output distributions is close to 1, fulfilling the reliability requirement for a DI code (Theorem_2).', 'step_7': 'The size of this constructible code `N` is determined by the size of the initial single-letter packing. By relating the packing size to the lower Minkowski dimension `d` and applying the capacity definition (Concept_2), we find that a rate of at least \\(\\frac{1}{2}d\\) is achievable. Thus, the lower bound is \\(\\dot{C}_{DI}(W) \\ge \\frac{1}{2}d\\).', 'step_8': 'Since the upper bound from the converse (Step 3) is \\(\\dot{C}_{DI}(W) \\le \\frac{1}{2}d\\) and the lower bound from achievability (Step 7) is \\(\\dot{C}_{DI}(W) \\ge \\frac{1}{2}d\\), the bounds are tight. Therefore, the capacity is exactly equal to \\(\\frac{1}{2}d\\).'}	"{'concept_1': 'A DI code is a family of pairs \\(\\{(u_j, \\mathcal{E}_j) : j \\in [N]\\}\\), where \\(u_j \\in \\mathcal{X}^n\\) are deterministic input codewords of block length `n`, and \\(\\mathcal{E}_j \\subset \\mathcal{Y}^n\\) are corresponding decoding sets in the output space. The code must satisfy two error probability conditions for all \\(j \\ne k\\): a missed identification probability \\(W^n(\\mathcal{Y}^n \\setminus \\mathcal{E}_j | u_j) \\le \\lambda_1\\) and an incorrect identification probability \\(W^n(\\mathcal{E}_k | u_j) \\le \\lambda_2\\).', 'concept_2': 'For deterministic identification, the capacity that captures the \\(n \\log n\\) scaling in the exponent of the number of messages \\(N_{DI}\\) is defined as \\(\\dot{C}_{DI}(W) := \\inf_{\\lambda_1, \\lambda_2 > 0} \\liminf_{n \\to \\infty} \\frac{1}{n \\log n} \\log N_{DI}(n, \\lambda_1, \\lambda_2)\\). This is also referred to as the pessimistic capacity.', 'concept_3': 'For a bounded set \\(F\\) in a metric space, the lower Minkowski dimension is a way to measure its fractal dimension. It is defined using the scaling of the maximum number of disjoint balls of radius \\(\\delta\\) that can be packed into the set, \\(\\Pi_{\\delta}(F)\\), as the radius \\(\\delta\\) approaches zero: \\(\\underline{d}_M(F) = \\liminf_{\\delta \\to 0} \\frac{\\log \\Pi_{\\delta}(F)}{-\\log \\delta}\\).', 'concept_4': ""A probability distribution \\(P\\) over a finite alphabet \\(\\mathcal{Y}\\) can be mapped to a unit vector \\(\\sqrt{P} = (\\sqrt{P(y)})_{y \\in \\mathcal{Y}}\\) on the non-negative part of a hypersphere. This 'spherisation' allows the use of Euclidean geometry and distances to analyze relationships between probability distributions."", 'theorem_1': ""The distinguishability of two product distributions, \\(W_{x^n}\\) and \\(W_{x'^n}\\), is directly tied to the Euclidean distance between their 'spherised' vector sequences, \\(\\bigoplus_{i=1}^n \\sqrt{W_{x_i}}\\) and \\(\\bigoplus_{i=1}^n \\sqrt{W_{x'_i}}\\). The key relationships are: \\((\\frac{1}{2}\\|W_{x^n} - W_{x'^n}\\|_1)^2 \\le \\sum_{i=1}^n \\|\\sqrt{W_{x_i}} - \\sqrt{W_{x'_i}}\\|_2^2\\) for the converse, and \\(-\\ln(1 - \\frac{1}{2}\\|W_{x^n} - W_{x'^n}\\|_1) \\ge \\frac{1}{2}\\sum_{i=1}^n \\|\\sqrt{W_{x_i}} - \\sqrt{W_{x'_i}}\\|_2^2\\) for achievability."", 'theorem_2': 'A reliable DI code can be constructed directly from a sufficiently dense Euclidean packing of the spherised output distributions. The size of the DI code is determined by the number of points that can be packed while maintaining a minimum separation distance.'}"	The pessimistic slightly superexponential capacity \(\dot{C}_{DI}(W)\) is equal to half the lower Minkowski dimension `d`: \[ \dot{C}_{DI}(W) = \frac{1}{2}d \]	理论工程学科-信息论和通信理论-信号检测	427500	3	5
recuVOcYTWds4t	In the limited feedback model for binary error-correcting codes, what is the minimum number of feedback bits required from the receiver (Bob) to the sender (Alice) to allow the error resilience to surpass the classical no-feedback limit of 1/4 for a k-bit message?	"{'step_1': 'Establish the Contradiction Hypothesis\nWe proceed by contradiction. The goal is to find the minimum feedback required to achieve an error resilience of \\alpha > 1/4. We begin by assuming the conclusion is false. That is, we hypothesize that there exists a coding scheme, based on the principles of error-correcting codes with feedback, that can achieve an error resilience of \\alpha = 1/4 + \\delta (for some constant \\delta > 0) while using a sub-logarithmic number of feedback bits, specifically \\zeta = o(\\log k) bits for a k-bit message. This assumption directly challenges the no-feedback limit, which states that without feedback, an error resilience greater than 1/4 is impossible.', 'step_2': ""Construct the Message Distinguishability Graph and Model Feedback as Coloring\nBased on the assumption in Step 1, we construct a complete graph G where the vertex set V consists of all K=2^k possible messages Alice might send. For the coding scheme to be successful, the protocol must provide a way to distinguish between any two distinct messages, say v_i and v_j, even in the presence of errors. This means every edge (v_i, v_j) in the graph G must be accounted for.\nThe feedback mechanism is the tool used to ensure this distinguishability. Since we assumed a total of \\zeta feedback bits, there are l = 2^\\zeta possible feedback strings that Bob can send. We model the protocol's strategy as an edge-coloring of the graph G with l colors. We assign a color to the edge (v_i, v_j) corresponding to the specific feedback string that the protocol would rely on to resolve the ambiguity between messages v_i and v_j. For the protocol to be valid, every edge in the complete graph must be colored."", 'step_3': ""Apply Ramsey Theory to Reach a Contradiction\nWe now have a complete graph on K=2^k vertices, edge-colored with l=2^\\zeta colors. We apply Ramsey's Theorem. This theorem states that if the number of vertices K is sufficiently large compared to the number of colors l, the graph must contain a large monochromatic clique. A monochromatic clique in this context represents a large subset of messages for which the protocol relies on the exact same feedback string to distinguish between any pair of them.\nFor this subset of messages, the feedback is fixed and provides no new information. The problem of distinguishing between them is therefore reduced to a standard coding problem without feedback. This sub-code must adhere to the constraints of the no-feedback limit. However, the Plotkin bound puts a strict upper limit on the number of messages that can exist in a code while maintaining the distance required for an error resilience of 1/4 + \\delta. A sufficiently large monochromatic clique, whose existence is guaranteed by Ramsey's Theorem for our assumed small number of colors l, would imply a sub-code that violates this very bound. This is a contradiction."", 'step_4': ""Conclusion and Underlying Logic\nThe contradiction forces us to reject our initial assumption from Step 1. Therefore, no scheme can achieve an error resilience greater than 1/4 with only o(\\log k) feedback bits.\nThe cornerstone of this final quantitative step is the bound on multicolor Ramsey numbers. This theorem provides the explicit relationship between the number of vertices (K), the size of the monochromatic clique to be avoided (s, a constant that depends on \\delta), and the necessary number of colors (l). The bound states that to avoid such a clique, the number of colors l must grow at least as fast as a logarithmic function of K. Formally:\nl = \\Omega(\\log K)\nSince the number of feedback bits is \\zeta = \\log_2 l and the number of vertices is K=2^k, we can substitute these into the bound:\n\\zeta = \\log_2 l \\ge \\log_2(\\Omega(\\log K)) = \\log_2(\\Omega(\\log 2^k)) = \\Omega(\\log k)\nThis proves that \\Omega(\\log k) feedback bits are necessary. This entire investigation is motivated by Berlekamp's thesis, which showed that with unlimited feedback, resilience can be improved to 1/3, establishing that feedback is a valuable resource and making the question of its minimal cost a critical one.""}"	"{'concept_1': 'Error-Correcting Codes with Feedback | In this communication model, a sender (Alice) transmits a sequence of bits to a receiver (Bob) over a noisy channel. Bob can send information back to Alice over a separate, noiseless feedback channel. Alice uses this feedback to adapt her subsequent transmissions, aiming to counteract errors introduced by an adversary.', 'concept_2': ""Error Resilience | This metric, denoted \\alpha, is the maximum fraction of adversarially corrupted bits (e.g., bit flips) in Alice's transmission that a coding scheme can tolerate while still guaranteeing that Bob can uniquely and correctly decode the original message."", 'concept_3': 'No-Feedback Limit (Plotkin Bound) | In the classical model without feedback, there is a strict theoretical upper bound on error resilience. If an adversary can corrupt more than a 1/4 fraction of the transmitted bits, it is impossible to guarantee unique decodability. Thus, the maximum error resilience without feedback is \\alpha = 1/4.[1, 2]', 'concept_4': ""Ramsey's Theorem (Multicolor Version) | This theorem from combinatorics states that for any number of colors l and any desired clique size s, there exists a number R(s; l) such that any complete graph with at least R(s; l) vertices whose edges are colored with l colors must contain a monochromatic clique of size s (a subgraph of size s where all edges have the same color).[3, 4]"", 'theorem_1': ""Full-Feedback Improvement (Berlekamp's Thesis) | Berlekamp's foundational work demonstrated that if the feedback channel is unlimited and instantaneous, the error resilience can be improved significantly, from \\alpha = 1/4 to \\alpha = 1/3. This establishes that feedback is a powerful resource for improving error resilience."", 'theorem_2': 'Bound on Multicolor Ramsey Numbers | A well-known quantitative bound on multicolor Ramsey numbers states that if a complete graph on K vertices is edge-colored with l colors such that it contains no monochromatic clique of size s, then the number of colors l must satisfy a lower bound that grows with K. A common form of this bound implies that l must be at least on the order of (\\log K / \\log \\log K).[2, 5]'}"	\Omega(\log k)	理论工程学科-信息论和通信理论-编码理论	212871	4	4
recuW0yr54nXwK	Let \(\Omega\) be a bounded, open, and connected set (a domain) in the plane. We define the following:  * **Neumann Eigenvalue, \(\mu_k(\Omega)\)**: This symbol represents the \(k\)-th value \(\mu\) for which the eigenvalue problem \(-\Delta u = \mu u\) in \(\Omega\), with the boundary condition \(\partial_n u = 0\) on \(\partial\Omega\) (the outward normal derivative is zero), has a non-trivial solution \(u\). The eigenvalues are ordered starting from the first non-zero one: \(0 < \mu_1(\Omega) \le \mu_2(\Omega) \le \dots\).  * **\(k\)-Exterior Self-Domain**: A convex domain \(\omega\) is defined as a \(k\)-exterior self-domain if it is the solution to its own exterior maximization problem. Rigorously, for any convex domain \(D\) such that \(\omega \subset D\), the inequality \(\mu_k(D) \le \mu_k(\omega)\) must hold.    With these definitions, is the unit square a \(1\)-exterior self-domain (for the case where \(k=1\))?	{'step_1': 'Let \\(Q\\) be the unit square, which we can define as \\(Q = [0, 1] \\times [0, 1]\\) without loss of generality. Its area is \\(|Q|=1\\). Let \\(\\Omega\\) be any convex domain such that \\(Q \\subset \\Omega\\). We want to prove that \\(\\mu_1(\\Omega) \\le \\mu_1(Q)\\). If \\(\\Omega=Q\\), the equality is trivial. Therefore, we will focus on the case where \\(\\Omega\\) strictly contains \\(Q\\).', 'step_2': 'By elementary calculations, it is easy to see that \\(|\\Omega| \\ge w_\\Omega\\). For example, we can prove it as follows. Because \\(\\Omega\\) is a convex set strictly containing the unit square \\(Q\\), we can identify points N, W, S, E on the boundary of \\(\\Omega\\) that lie, respectively, to the North, West, South, and East of the square. Let \\(h_N, h_W, h_S, h_E\\) be the minimum distances from these points to the corresponding sides of the square \\(Q\\). Since \\(\\Omega\\) strictly contains \\(Q\\), at least one of these distances is positive.\nThe width of \\(\\Omega\\) in the vertical direction is at least \\(1 + h_N + h_S\\), and its width in the horizontal direction is at least \\(1 + h_W + h_E\\). The minimum width \\(w_\\Omega\\) must be less than or equal to the width in any direction, so we have:\n \\(w_\\Omega \\le 1 + h_N + h_S \\quad \\text{and} \\quad w_\\Omega \\le 1 + h_W + h_E\\)\nBy convexity, \\(\\Omega\\) must contain the four triangles formed by connecting each point (N, W, S, E) to the two corresponding vertices of the square. The areas of these triangles are \\(h_N/2, h_W/2, h_S/2, h_E/2\\), respectively.\nThis gives us a lower bound for the area of \\(\\Omega\\):\n \\(|\\Omega| \\ge |Q| + \\frac{h_N+h_W+h_S+h_E}{2} = 1 + \\frac{1}{2}(h_N+h_W+h_S+h_E)\\)\nFrom the width inequalities, we know that \\(2w_\\Omega \\le (1+h_N+h_S) + (1+h_W+h_E)\\), which implies \\(\\frac{1}{2}(h_N+h_W+h_S+h_E) \\ge w_\\Omega - 1\\).\nSubstituting this into the area inequality yields the crucial geometric relationship:\n \\(|\\Omega| \\ge 1 + (w_\\Omega - 1) \\implies |\\Omega| \\ge w_\\Omega\\)', 'step_3': 'From **Concept 1**, we have the inequality for the first non-zero Neumann eigenvalue:\n \\(\\mu_1(\\Omega) \\le \\frac{\\pi^2 w_\\Omega^2}{|\\Omega|^2}\\)\nUsing our geometric result \\(|\\Omega| \\ge w_\\Omega\\), which implies \\(\\frac{w_\\Omega}{|\\Omega|} \\le 1\\), we can bound the term on the right:\n \\(\\frac{\\pi^2 w_\\Omega^2}{|\\Omega|^2} = \\pi^2 \\left(\\frac{w_\\Omega}{|\\Omega|}\\right)^2 \\le \\pi^2\\)\nCombining these gives us:\n \\(\\mu_1(\\Omega) \\le \\pi^2\\)\nFrom **Concept 2**, we know the value for the unit square:\n \\(\\mu_1(Q) = \\pi^2\\)\nTherefore, we arrive at the final inequality:\n \\(\\mu_1(\\Omega) \\le \\mu_1(Q)\\)'}	{'concept_1': 'For any convex planar domain \\(\\Omega\\), the first non-zero Neumann eigenvalue is bounded by an inequality involving its area and minimum width: \\(\\mu_1(\\Omega) \\le \\frac{\\pi^2 w_\\Omega^2}{|\\Omega|^2}\\), where \\(w_\\Omega\\) is the minimum width of \\(\\Omega\\).', 'concept_2': 'The first non-zero Neumann eigenvalue for the unit square \\(Q\\) is known to be \\(\\mu_1(Q) = \\pi^2\\).'}	Yes. (It's not correct to draw conclusions based on a rough intuitive analysis or calculations based on a few examples.)	理论基础学科-基础数学-偏微分方程	95486	2	2
recuW1AcCDUHSh	Let $N\geq 2$, $\Omega\subset \mathbb{R}^N$ be a bounded $C^2$ domain. Let $f$ be a locally Lipschitz function such that $f>\phi_0$ for some positive constant $\phi_0$. Let $u\in C^{1,\alpha}(\overline{\Omega})$ be a weak solution to \[\begin{cases}-\Delta_pu=f(u)&~\text{in }\Omega\\u>0 &~\text{in }\Omega\\u=0&~\text{on }\partial\Omega\end{cases}\] where $-\Delta_p:=-\mathrm{div}\left(|\nabla|^{p-2}\nabla u\right)$ and $p>N$. Denote $M=\max_{\overline{\Omega}}u$, $H$ the mean curvature of $\partial\Omega$ and $\mathcal{M}_0^-=\max \left\{-H, 0\right\}$. For each $\delta>0$, define $\mathcal{V}(\delta)=\left\{x\in\Omega:\mathrm{dist}(x,\partial\Omega)\leq\delta\right\}$. For any open subset $E\Subset \Omega$, define $\delta_+(E)=\sup_{x\in E}\mathrm{dist}(x,\partial\Omega)$, $\delta_-(E)=\inf_{x\in E}\mathrm{dist}(x,\partial\Omega)$. It's already established that the following integral inequality holds for $r=\frac{1}{2}$: \begin{align*} \int_E \frac{d x}{|\nabla u(x)|^{(p-1) r}} \leq &~ C \frac{\mathcal{V}\left(\delta_{+}(E)\right)^{\frac{1-r+r p}{p}}}{\delta_{-}(E)}\|\nabla u\|_{L^p(\Omega)} + \frac{C}{(1-r)^2} \frac{\mathcal{V}\left(\delta_{+}(E)\right)^{\frac{(p-1) r-p+2}{p}}}{\delta_{-}(E)^2}\|\nabla u\|_{L^p(\Omega)}^{(p-1)(2-r)} \\ &~+\frac{C}{1-r}\left\|f^\prime\right\|_{L^{\infty}([0, M])}\|\nabla u\|_{L^p(\Omega)}^{p-(p-1) r}, \end{align*} where $C$ is a constant depends on $N$, $p$ and $\phi_0$. Let $I\subset \mathbb{R}$ be an open interval, for each $\sigma>0$, the following estimate holds: \[\left|u^{-1}(I)\right|\leq C\left(\sigma^{\alpha}+|I|\sigma^{-\beta}\right)\] for some $\alpha$, $\beta>0$, where $C>0$ is a positive constant depends on $N$, $p$, $|\Omega|$, $|\partial\Omega|$, $\mathcal{M}_0^-$, $\phi_0$, $M$, $\left\| f^\prime \right\|_{L^\infty([0,M])}$. Find the value of $\alpha$ and $\beta$ by splitting $\Omega$ into $\left\{|\nabla u|>\sigma\right\}$, $\left\{|\nabla u|\leq\sigma, \mathrm{dist}(x, \partial\Omega)>\sigma^\theta\right\}$, $\left\{|\nabla u|\leq\sigma, \mathrm{dist}(x, \partial\Omega)\leq\sigma^\theta\right\}$ and establishing a similar inequality.	{'step_1': 'estimate $u^{-1}(I)\\cap \\left\\{|\\nabla u|>\\sigma\\right\\}$. Take $g=|\\nabla u|^{-1}$, $U=u^{-1}(I)\\cap\\left\\{|\\nabla u|>\\sigma\\right\\}$ in theorem 1 and apply theorem 3, we get \\begin{align*} \\left|u^{-1}(I)\\cap \\left\\{|\\nabla u|>\\sigma\\right\\}\\right|=&~\\int_{I}^{}dt \\int_{\\left\\{u=t\\right\\}\\cap \\left\\{|\\nabla u|>\\sigma\\right\\}}^{}\\frac{dS}{|\\nabla u|} \\\\ \\leq &~ \\sigma^{-1} \\int_{I}^{}\\mathcal{H}^{N-1}\\left(\\left\\{u=t\\right\\}\\cap \\left\\{|\\nabla u|>\\sigma\\right\\}\\right)dt, \\end{align*} where $\\mathcal{H}^{N-1}$ denotes the $(N-1)$-dimensional Hausdorff measure. By theorem 3 again \\begin{align*} \\mathcal{H}^{N-1}\\left(\\left\\{u=t\\right\\}\\cap \\left\\{|\\nabla u|>\\sigma\\right\\}\\right)\\leq&~ \\sigma^{1-p}\\int_{\\left\\{u=t\\right\\}}^{}|\\nabla u|^{p-1}dS \\\\ =&~\\sigma^{1-p}\\int_{\\left\\{u>t\\right\\}}^{}f(u)dx \\\\ \\leq&~ \\left\\| f \\right\\|_{L^\\infty([0,M])}|\\Omega|\\sigma^{1-p}, \\end{align*} where we have used theorem 2 in the second equality, so combine these estimates we derive \\[\\left|u^{-1}(I)\\cap \\left\\{|\\nabla u|>\\sigma\\right\\}\\right|\\leq C \\sigma^{-p}.\\] This gives $\\beta=p$.', 'step_2': 'estimate $\\left\\{|\\nabla u|\\leq\\sigma, \\mathrm{dist}(x, \\partial\\Omega)>\\sigma^\\theta\\right\\}$. Denote $E=\\left\\{x\\in\\Omega: \\mathrm{dist}(x, \\partial\\Omega)>\\sigma^\\theta\\right\\}$, by theorem 3 and the integral inequality, note that $\\delta_-(E)=\\sigma^\\theta$ and $\\delta_+(E)\\leq d_\\Omega$, we have for $r=\\frac{1}{2}$ \\begin{align*} \\left|\\left\\{|\\nabla u|\\leq\\sigma, \\mathrm{dist}(x, \\partial\\Omega)>\\sigma^\\theta\\right\\}\\right|\\leq&~\\int_{E}^{}\\frac{\\sigma^{(p-1)r}}{|\\nabla u|^{(p-1)r}}dS \\\\ \\leq&~ C \\left(\\sigma^{(p-1)r-\\theta}+\\sigma^{(p-1)r-2\\theta}+\\sigma^{(p-1)r}\\right). \\end{align*} Since $\\theta>0$, the dominated term is $\\sigma^{(p-1)r-2\\theta}$. ', 'step_3': 'estimate $\\left\\{|\\nabla u|\\leq\\sigma, \\mathrm{dist}(x, \\partial\\Omega)\\leq\\sigma^\\theta\\right\\}$. By theorem 4 \\[\\left|\\left\\{|\\nabla u|\\leq\\sigma, \\mathrm{dist}(x, \\partial\\Omega)\\leq\\sigma^\\theta\\right\\}\\right|\\leq \\left|\\mathrm{dist}(x,\\partial\\Omega)\\leq\\sigma^\\theta\\right|\\leq C\\sigma^\\theta.\\] In order to make the two parts having same decay rate, we need $(p-1)r-2\\theta=\\theta$, which gives $\\theta=\\frac{(p-1)r}{3}=\\frac{p-1}{6}$, and thus $\\alpha=\\theta=\\frac{p-1}{6}$.'}	{'concept/theorem_1': 'Suppose $U$ is an open set in $\\mathbb{R}^n$ and $u$ is a rea-valued Lipschitz function on $U$, then for any $g\\in L^1(U)$, the following co-area formula holds: \\[\\int_{U}^{}g(x)|\\nabla u(x)|dx=\\int_{\\mathbb{R}}^{}dt \\int_{u^{-1}(t)}^{}g(x)dS(x)\\]', 'concept/theorem_2': 'With the same assumption on $\\Omega$ and $u$, define $I(t)=\\int_{\\left\\{u>t\\right\\}}f(u)$, then \\[I(t)=\\int_{\\left\\{u=t\\right\\}}^{}|\\nabla u|^{p-1}dS\\] for a.e. $t\\in (0, M)$.', 'concept/theorem_3': 'Let $(X, \\Sigma, \\mu)$ be a measure space, $f$ a real-valued measurable function, then for any $\\sigma>0$ and $p\\in (0, \\infty)$, it holds that \\[\\mu \\left(\\left\\{x\\in\\Omega:|f(x)|\\geq t\\right\\}\\right)\\leq \\frac{1}{t^p}\\int_{X}^{}|f|^pd\\mu\\]', 'concept/theorem_4': 'With the same assumption on $\\Omega$, the following estimate for the boundary piece holds \\[\\left|\\left\\{x\\in\\Omega: \\mathrm{dist}(x, \\partial\\Omega)\\leq\\delta\\right\\}\\right|\\leq (1+\\delta \\mathcal{M}_0^-)^{N-1}\\delta.\\]'}	$\alpha=\frac{p-1}{6}$, $\beta=p$	理论基础学科-基础数学-偏微分方程	65096	2	4
recuW2aBSTw4ST	Answer the following questions without searching online:    On a smooth, compact, connected Riemannian surface M of genus 0, is the second Steklov eigenfunction $u_2$ is necessarily a Morse function?	"{'step_1': 'We assume that $u_2$ is not a Morse function. This implies the existence of at least one degenerate critical point $p \\in M$ for $u_2$. By definition, this means that the Hessian of $u_2$ vanishes at $p$.', 'step_2': ""Define an auxiliary function $w := u_2 - u_2(p)$. Clearly, $w$ is harmonic ($\\Delta w = 0$), and $p$ is a zero of $w$ such that both its gradient and Hessian vanish, i.e., $\\nabla w(p)=0$ and $D^2 w(p)=0$. According to the local theory of harmonic functions, at a zero that is also a degenerate critical point, the function's nodal set (zero level set) consists of at least three curves intersecting at that point with equal angles. These nodal curves divide a small geodesic disk $D$ around $p$ into $2k$ ($k \\ge 3$) disjoint regions, $D_i$ ($i=1, ..., 2k$)."", 'step_3': ""To arrive at a contradiction, we first assume that $u_2$ (and thus $w$) has at most 3 nodal domains in M. Since there are at least 6 regions $D_i$ with alternating signs around $p$, the pigeonhole principle implies that at least three non-adjacent regions (let us call them $D_1, D_2, D_3$) must belong to the same nodal domain, $\\Omega$. We can construct a simple closed curve $\\gamma_{12}$ that passes through $p$, remains within the nodal domain $\\Omega$, and connects the regions $D_1$ and $D_2$. Because $D_1$ and $D_2$ are non-adjacent, there must be a region between them that belongs to a different nodal domain, $\\Omega'$. **Crucially, because the manifold M is of genus 0, the curve $\\gamma_{12}$ separates M, and therefore the nodal domain $\\Omega'$ must be contained in the bounded region enclosed by $\\gamma_{12}$.** By repeating this argument for the pairs of regions $(D_1, D_3)$ and $(D_2, D_3)$, we can construct two other closed curves, $\\gamma_{13}$ and $\\gamma_{23}$, which respectively enclose two other distinct nodal domains, $\\Omega''$ and $\\Omega'''$. We have thus shown that the function $w$ must have at least four nodal domains ($\\Omega, \\Omega', \\Omega'', \\Omega'''$). This contradicts our starting assumption of at most 3 nodal domains, thereby proving that $w$ must have at least 4 interior nodal domains."", 'step_4': ""Let $\\Omega_i$ ($i=1, ..., m$, with $m \\ge 4$) be the interior nodal domains of $w$. We construct a test function $\\phi := \\sum_{i=1}^m a_i w_i$, where $w_i$ is the restriction of $w$ to $\\Omega_i$ (and zero outside). Since $m \\ge 4$, it is always possible to find a non-trivial set of real coefficients $\\{a_i\\}$ that simultaneously solve the following two linear systems: (a) $\\int_{\\partial M} \\phi = \\sum_{i=1}^m a_i \\int_{\\partial \\Omega_i} w_i = 0$ (b) $\\sum_{i=1}^m a_i^2 \\int_{\\partial \\Omega_i} w_i = 0$. Condition (a) ensures that $\\phi$ is a valid test function for the second eigenvalue $\\sigma_2$. By the min-max principle, we have $\\sigma_2 \\le \\frac{\\int_M |\\nabla \\phi|^2}{\\int_{\\partial M} \\phi^2}$. We now compute the Rayleigh quotient for $\\phi$. The denominator is $\\int_{\\partial M} \\phi^2 = \\sum_{i=1}^m a_i^2 \\int_{\\partial \\Omega_i} w^2$. For the numerator, using Green's identity and the Steklov condition, we have $\\int_{\\Omega_i} |\\nabla w|^2 = \\sigma_2 \\int_{\\partial \\Omega_i} w^2 + \\sigma_2 u_2(p) \\int_{\\partial \\Omega_i} w$. Therefore, $\\sum_{i=1}^m a_i^2 \\int_{\\Omega_i} |\\nabla w|^2 = \\sigma_2 \\sum_{i=1}^m a_i^2 \\int_{\\partial \\Omega_i} w^2 + \\sigma_2 u_2(p) \\sum_{i=1}^m a_i^2 \\int_{\\partial \\Omega_i} w$. By our choice of coefficients $a_i$ (Condition b), the second term on the right-hand side vanishes. The numerator is thus exactly equal to $\\sigma_2 \\sum a_i^2 \\int_{\\partial \\Omega_i} w^2$. This means the Rayleigh quotient for $\\phi$ is precisely $\\sigma_2$. It follows that $\\phi$ is also a second Steklov eigenfunction. However, by its construction, $\\phi$ has at least four nodal domains, which contradicts Courant's Nodal Domain Theorem (any second eigenfunction can have at most two nodal domains).""}"	{'concept_1': 'A smooth function $f: M \\to \\mathbb{R}$ defined on a smooth manifold M is called a **Morse function** if all of its critical points (points where the gradient $\\nabla f$ vanishes) are non-degenerate. A critical point $p$ is **non-degenerate** if the Hessian matrix of the function at that point (composed of second partial derivatives) is invertible, i.e., its determinant is non-zero. If a critical point is not non-degenerate, it is called **degenerate**.', 'concept_2': 'Let $u_k$ be the k-th eigenfunction of the Steklov problem. A **nodal domain** is a connected component of the set $\\{x \\in M \\mid u_k(x) \\neq 0\\}$. The theorem states that $u_k$ has at most k nodal domains.', 'concept_3': 'The second Steklov eigenvalue $\\sigma_2$ can be characterized variationally using the Rayleigh quotient, $R(u) = \\frac{\\int_M |\\nabla u|^2 dV}{\\int_{\\partial M} u^2 dS}$. Its value is the minimum of the Rayleigh quotient over the space of all non-zero functions $u \\in H^1(M)$ that are orthogonal to the first eigenfunction, i.e., satisfying $\\int_{\\partial M} u dS = 0$. Any function that satisfies this condition and for which the Rayleigh quotient achieves this minimum value $\\sigma_2$ is a second eigenfunction.'}	Yes. (Guessing the answer by roughly enumerating the properties of harmonic functions or other properties is not correct. The correct answer requires at least using Courant's nodal domain theorem to deduce the contradiction.)	理论基础学科-基础数学-偏微分方程	81178	2	2
recuW2wddZto9U	let \tau(t) be travel time of a vehicle entering at time t, consider aggressive driving behavior, characterized by a concave speed-density relationship in the free-flow regime (i.e., \( v''(k) < 0 \) for densities \( k < k_{\text{crit}} \)). Under these conditions, the path described by the expected value \( E[\tau(t)] \) and variance \( \text{Var}[\tau(t)] \) forms a what curve (shape and direction) in the mean-variance phase plane?	{'step_1': 'Theorem 1 focuses on **aggressive driving behavior**, which is defined by a concave speed-density relationship in the free-flow regime (i.e., \\(v^{\\prime \\prime}(k) \\leq 0\\) for densities \\(k \\leq k_{crit}\\), where \\(k_{crit}\\) is the density that maximizes traffic flow). The core goal is to prove that the path formed by the expected travel time \\(\\mathbb{E}[\tau(t)]\\) and travel time variance \\(Var[\tau(t)]\\) (for vehicles entering the corridor at different times \\(t\\) during rush hour) forms a **counterclockwise loop** in the mean-variance phase plane.', 'step_2': 'Lemma 3.1 provides the critical criterion for variance dominance, which is essential to proving the counterclockwise loop. Its core content is: - Consider two vehicles (A and B) entering the corridor at times \\(t_1 < t_2\\) with identical expected travel times (\\(\\mathbb{E}[\tau_1] = \\mathbb{E}[\tau_2]\\)). - Let \\(\\Delta \tau = \tau(t_2) - \tau(t_1)\\) (the travel time difference between B and A) be a function of peak flow \\(q_p\\). If there exists an \\(x_0\\) such that: 1. \\(\\Delta \tau(x) < 0\\) when \\(x < x_0\\), 2. \\(\\Delta \tau(x) > 0\\) when \\(x > x_0\\) (i.e., \\(\\Delta \tau\\) undergoes exactly one sign change from negative to positive), 3. \\(\tau_1\\) (travel time of vehicle A) is an increasing function of \\(q_p\\), - Then, \\(Var[\tau_2] \\geq Var[\tau_1]\\) holds. This lemma is foundational because a counterclockwise loop in the mean-variance plane requires variance to increase as the expected travel time evolves (consistent with \\(Var[\tau_2] \\geq Var[\tau_1]\\) when \\(\\mathbb{E}[\tau_1] = \\mathbb{E}[\tau_2]\\)).', 'step_3': 'In the scenario of Theorem 1 (rush hour congestion on a single corridor with stochastic peak flow \\(q_p\\)): - For **low peak flow \\(q_p < x_0\\)** (where \\(x_0\\) is a threshold peak flow): Vehicle A (entering earlier at \\(t_1\\)) experiences less congestion because the upstream flow is lower. Thus, \\(\tau_1 < \tau_2\\), leading to \\(\\Delta \tau = \tau_2 - \tau_1 < 0\\). - For **high peak flow \\(q_p > x_0\\)** : As \\(q_p\\) increases, the upstream flow intensifies, and congestion accumulates. Vehicle B (entering later at \\(t_2\\)) is more affected by this increased congestion—its travel time \\(\tau_2\\) rises faster than \\(\tau_1\\) (since A entered before the congestion peaked). Eventually, \\(\tau_2 > \tau_1\\), leading to \\(\\Delta \tau = \tau_2 - \tau_1 > 0\\). This means \\(\\Delta \tau\\) has exactly one sign change (from negative to positive) at \\(x_0\\), satisfying the first two conditions of Lemma 3.1.', 'step_4': 'In the context of Theorem 1: - Peak flow \\(q_p\\) directly determines the severity of rush hour congestion: a larger \\(q_p\\) means more vehicles enter the corridor, increasing traffic density \\(k\\) and reducing speed \\(v\\) (per the speed-density relationship). - Vehicle A’s travel time \\(\tau_1\\) depends on the congestion level during its trip. As \\(q_p\\) increases, congestion becomes more severe, so \\(\tau_1\\) (the time A takes to traverse the corridor) increases. Thus, \\(\tau_1\\) is an increasing function of \\(q_p\\), satisfying the third condition of Lemma 3.1.', 'step_5': 'Combining the above steps with the unimodality of \\(\\mathbb{E}[\tau(t)]\\) (a key property stated in the paper: \\(\\mathbb{E}[\tau(t)]\\) is a unimodal function of departure time, approaching the free-flow travel time \\(\tau_{free}\\) for very early/late departures and peaking during rush hour): - When \\(\\mathbb{E}[\tau(t)]\\) **increases** (from early to peak departure times): For any two vehicles A (earlier) and B (later) with \\(\\mathbb{E}[\tau_1] = \\mathbb{E}[\tau_2]\\), Lemma 3.1 gives \\(Var[\tau_2] \\geq Var[\tau_1]\\)—so variance rises with the expected travel time. - When \\(\\mathbb{E}[\tau(t)]\\) **decreases** (from peak to late departure times): For another pair of vehicles (A’ and B’) with \\(\\mathbb{E}[\tau_{A’}] = \\mathbb{E}[\tau_{B’}]\\), the same logic applies: \\(Var[\tau_{B’}] \\geq Var[\tau_{A’}]\\). Variance remains relatively high even as the expected travel time falls. This trajectory—\\(\\mathbb{E}[\tau]\\) first rising with increasing \\(Var[\tau]\\), then falling with \\(Var[\tau]\\) still elevated—forms a counterclockwise loop in the mean-variance plane, completing the proof of Theorem 1.'}	{'concept1': '(LWR theory) let k be k(x, t) be traffic density ,q(x, t) Traffic flow .then the rate of change in the total number of vehicles contained in any road segment \\([x_{1},x_{2}]\\) where \\(x_{2}>x_{1}\\) and there are no entries or exits, is equal to the net flow of vehicles out of the segment, i.e.\\frac{d}{dt}\\int_{x_{1}}^{x_{2}}k(x,t)\\,dx = -\\left[q(x,t)\\right]_{x_{1}}^{x_{2}}.', 'concept2': '(LWR theory) : assumes the existence of a functional relationship between \\( q \\) and \\( k \\) under differentiable conditions: q(x,t) = Q(x,k(x,t)),then \\frac{\\partial k}{\\partial t} + \\frac{dQ}{dk} \\cdot \\frac{\\partial k}{\\partial x} = 0.where \\( Q \\) is a concave, non-negative function that is equal to zero at \\( k = 0 \\) and at the \\emph{jam density} \\( k = k_j \\).', 'concept3': 'Traffic moves along a road segment of length \\( l \\), which ends in a bottleneck with a maximum capacity \\( q_{\\text{bn}} \\). The travel time \\( \\tau(t) \\) for a vehicle entering the segment at time \\( t \\) is described by:\\tau(t) = \\inf\\{T \\geq 0 : N(l,t + T) > N(0,t)\\},', 'lemma3.1': 'If there exists a threshold \\(x_0\\) such that the travel time difference \\(\\Delta \\tau(x) = \\tau(t_2, x) - \\tau(t_1, x)\\) between two vehicles (Vehicle A departs at \\(t_1\\), Vehicle B departs at \\(t_2 > t_1\\)) satisfies: \\begin{itemize}[label=$\\bullet$] \\item \\(\\Delta \\tau(x) < 0\\) when \\(x < x_0\\); \\item \\(\\Delta \\tau(x) > 0\\) when \\(x > x_0\\); \\item The travel time of Vehicle A, \\(\tau_1(q_p)\\), is an increasing function of the peak flow \\(q_p\\); \\end{itemize} then \\(Var[\\tau_2] \\geq Var[\\tau_1]\\) must hold'}	a counterclockwise loop	理论交叉学科-交通流理论-交通流理论	55721	2	4
recuVmZdC6WvW1	In seawater, which is rich in sodium (Na⁺), potassium (K⁺), calcium (Ca²⁺), and magnesium (Mg²⁺) ions, which of these ions play a promoting role in the evaporation process of seawater?	{'step_1': 'The energy required for water molecules to transition from the liquid phase to the vapor phase is characterized by the evaporation enthalpy, ΔH_evap. A lower ΔH_evap facilitates faster evaporation due to reduced energy barriers for molecular escape. A higher ΔH_evap makes evaporation more difficult by stabilizing the water structure through stronger hydrogen bonding. Thus, any factor that modifies the hydrogen-bond (HB) network or the local molecular environment can substantially alter evaporation performance.', 'step_2': 'During interfacial solar evaporation, ions in seawater redistribute near the photothermal interface depending on their hydration properties and affinities for surface functional groups. Divalent ions such as Mg²⁺ and Ca²⁺ can undergo spontaneous ion exchange with the interface material, selectively accumulating at the interface and disrupting the HB network of interfacial water molecules. This process reduces local ΔH_evap and promotes more efficient molecular transport across the interface, leading to enhanced evaporation dynamics.', 'step_3': 'Natural seawater contains multiple dissolved ions at significantly different concentrations. Because Na⁺ is the most abundant and strongly hydrated, it dominates bulk thermodynamic behavior and tends to increase ΔH_evap, suppressing evaporation. Conversely, although Mg²⁺ and Ca²⁺ are lower in concentration, their preferential accumulation at the interface allows them to significantly influence local HB networks, reducing interfacial energy barriers and thus locally enhancing evaporation rates. K⁺, despite its low concentration, has minimal affinity for interfacial sites and consequently has a relatively minor impact on overall evaporation.', 'step_4': 'Combining these mechanisms and observations: Mg²⁺ & Ca²⁺ locally promote seawater evaporation, while Na⁺ & K⁺ tend to suppress it.'}	{'concept_1': 'The energy required for water molecules to escape from the liquid phase into the vapor phase is determined by the evaporation enthalpy, ΔH_evap. Any factor that modifies the interfacial hydrogen-bond (HB) network or the binding state of water molecules can strongly influence ΔH_evap and thus affect the evaporation rate: Lower ΔH_evap → easier molecular escape and faster evaporation; Higher ΔH_evap → stronger molecular binding and slower evaporation.', 'concept_2': 'During interfacial solar evaporation, different ions in seawater can redistribute near the photothermal interface depending on their hydration properties and affinities for the surface. This redistribution may locally perturb the hydrogen-bond (HB) network of interfacial water molecules, leading to changes in the effective ΔH_evap. In some cases, spontaneous ion exchange between divalent ions and the interface material further alters the interfacial microenvironment, indirectly influencing molecular mobility and evaporation dynamics.', 'concept_3': 'In natural seawater, multiple ions coexist with significantly different concentrations. Therefore, seawater evaporation behavior results from the combined effect of high-concentration ions that govern bulk thermodynamics and lower-concentration ions that may locally regulate interfacial processes.'}	Calcium and magnesium ions promote seawater evaporation.	其他理论学科-理论热力学与统计物理-反应动力学	86532	1	4
recuVZRfUNCp3n	In contactless (non-contact) solar evaporation, how does the dominant heat-transfer contribution change as the air-layer thickness increases?	{'step_1': 'Start from the heat-transfer channels (Concept 1). Across the air layer, heat can pass by conduction, convection, and radiation.', 'step_2': 'Eliminate convection at low airflow (Concept 2). Under low-velocity (quiescent) conditions, convection contributes little, so the effective heat transfer reduces to a conduction + radiation problem.', 'step_3': 'Thickness penalizes conduction (basic scaling). For a given temperature difference, the conductive path through air behaves like a thermal resistance that increases with layer thickness; thus, as the air layer gets thicker, the conductive heat flux weakens.', 'step_4': 'Surface temperatures set radiation (basic scaling). Radiative exchange depends mainly on the surface temperatures (∝ T4 difference) and only weakly on the gap thickness; when the gap is thicker (and conduction is weaker), the hot surface must run hotter to pass the imposed heat, which boosts the radiative share.', 'step_5': 'Thin vs. thick regimes. Thin gap: conduction is strong, surfaces need only modest temperature rise → conduction dominates, radiation is secondary. Thick gap: conduction is suppressed, surfaces run hotter → radiation dominates, conduction is secondary.', 'step_6': 'Conclusion. As the air-layer thickness increases, the dominant heat-transfer contribution shifts from conduction to radiation (with convection negligible under low-velocity conditions).'}	{'concept_1': 'Across the air layer, heat is transferred by conduction, convection, and radiation.', 'concept_2': 'When airflow is low, convection contributes little.'}	dominant heat-transfer contribution shifts from conduction to radiation	其他理论学科-理论热力学与统计物理-传热传质	27912	1	2
recuVUrfCLyKJh	In the context of secure multiple key-cast, consider a network with a single source `s`, intermediate nodes `x`, `y`, `z`, and two disjoint terminal sets, $D_1=\{d_{11}, d_{12}\}$ and $D_2=\{d_{21}, d_{22}\}$. The source `s` connects to `x` and `y` with capacity-2 links. Nodes `x` and `y` connect to `z` with capacity-1 links. Each terminal in $D_1$ is connected from nodes `x` and `z`, and `y` and `z`, respectively. What is the maximum achievable symmetric key rate $R$ for distributing pairwise independent keys $K_1$ and $K_2$ to sets $D_1$ and $D_2$ respectively, under the strict security constraint that the information held by any intermediate node ($v \in \{x,y,z\}$) is independent of both keys?	{'step_1': 'Let $R$ be the symmetric key rate, such that $R = H(K_1) = H(K_2)$. We aim to find the maximum possible value of $R$. Let $X_{In(v)}$ denote the information received by any node $v$.', 'step_2': 'From the network topology and the entropy bound (Concept_1), the information received at nodes `x` and `z` is constrained. The total information entering `x` from the source is bounded by the link capacity, $H(X_{In(x)}) \\le 2$. The joint entropy of information at `x` and `z` can be expressed using the chain rule (Theorem_1) as $H(X_{In(x)}, X_{In(z)}) = H(X_{In(x)}) + H(X_{In(z)}|X_{In(x)})$. This leads to the inequality $H(X_{In(x)}, X_{In(z)}) \\le 2 + 0.5H(X_{In(z)})$.', 'step_3': 'According to the decodability requirement (Concept_2), key $K_1$ must be a function of the information received at terminal $d_{11}$, which comes from nodes `x` and `z`.', 'step_4': 'Applying the security constraint (Concept_3), the information from node `z` is independent of $K_1$. This allows us to establish a lower bound on the entropy of the information received by terminal $d_{11}$, which is shown to be at least $2H(K_1)$.', 'step_5': 'The total information available at nodes `x` and `z`, $H(X_{In(x)}, X_{In(z)})$, must be sufficient to generate the signals for terminal $d_{11}$ and must also be independent of key $K_2$ due to the security constraint (Concept_3) and key independence (Concept_4). Using the independence property (Theorem_2), this implies $H(X_{In(x)}, X_{In(z)}) \\ge H(\\text{info for } d_{11}) + H(K_2)$.', 'step_6': 'Combining the results from steps 4 and 5, we derive a lower bound on the joint entropy in terms of the key rates: $H(X_{In(x)}, X_{In(z)}) \\ge 2H(K_1) + H(K_2) = 3R$.', 'step_7': 'We can also derive another bound using the security at node `z`. Since $X_{In(z)}$ is independent of $K_1$ (Concept_3), we have $H(X_{In(x)}, X_{In(z)}) \\ge H(X_{In(z)}, K_1) = H(X_{In(z)}) + H(K_1) = H(X_{In(z)}) + R$.', 'step_8': 'Finally, combining the lower bound from step 6 with the upper bound from step 9 gives the final constraint on the rate: $3R \\le H(X_{In(x)}, X_{In(z)}) \\le 4 - R$. This resolves to $4R \\le 4$, which means $R \\le 1$. Since the thesis also provides a constructive scheme that achieves a rate of 1 for this configuration, the maximum achievable rate is exactly 1.'}	"{'concept_1': ""The entropy of information received by a node is bounded by the sum of the capacities of its incoming edges. For a node $v$, this means $H(X_{In(v)})$ is limited by the network's structure."", 'concept_2': 'In a secure key-cast, a key $K_i$ must be fully decodable from the information received by any terminal in its corresponding set $D_i$. For a terminal $d$, $H(K_i|X_{In(d)}) = 0$.', 'concept_3': 'A core security requirement is that the key $K_i$ must be statistically independent of the information $X_{In(v)}$ gathered at any intermediate (non-source, non-terminal) node $v$. This is expressed as $I(K_i; X_{In(v)}) = 0$.', 'theorem_1': 'The chain rule for entropy states that the joint entropy of two random variables can be decomposed as $H(A, B) = H(A) + H(B|A)$.'}"	1	理论基础学科-理论计算机科学-密码学理论	131482	3	3
recuVOH2xdRMfC	When a plume enters a stratified fluid, it forms a fountain. In which region are waves effectively excited (the interface between the turbulence and the environment or the turbulent region inside the fountain)?	"{'step_1': 'Rule out the ""interface excitation"" hypothesis: According to Concept 1 (mechanical oscillator effect), internal waves may indeed be excited at the top of the convection due to the up-and-down oscillation of air masses. However, this mechanism usually generates high-frequency waves only in the upper atmosphere under strong stable stratification and is not universal. Meanwhile, Concept 2 (narrowband spectrum vs. broadband interface) points out that the internal wave spectrum emitted by plumes/fountains is narrowband, while the interface turbulent/non-turbulent oscillations are broadband. The inconsistency between the two indicates that the observed internal waves do not originate from interface fluctuations. Conclusion: The interface is not the main excitation region.', 'step_2': 'Confirm that the ""turbulent region inside the fountain"" is the main excitation source: Concept 3 (Reynolds stress excitation) provides the physical mechanism: when the convective zone interacts with the stable layer, internal waves are excited by the Reynolds stress within the convection/turbulence, which is different from the interface mechanism. Concept 4 (viscous internal wave model) further verifies: if a virtual source is set in the turbulent layer and extrapolated, the model can better reconstruct the far-field internal wave spectrum, indicating that the source region of internal waves is within the turbulence. Conclusion: Internal waves mainly originate from the turbulent region inside the fountain.'}"	"{'concept_1': 'Mechanical oscillator effect as a possible excitation mechanism: The vertical oscillation of air masses in convection (near the stable layer above the convective system) around their neutral buoyancy layer at the local tropospheric buoyancy frequency is considered capable of exciting internal gravity waves, and this mechanism can usually only generate extremely high-frequency gravity waves in the upper troposphere.', 'concept_2': 'The spectrum is narrow-band and inconsistent with the broadband oscillations of the interface: the internal waves emitted from the top of the plume/fountain have a narrow-band spectrum and are not related to the broadband fluctuations at the turbulent/non-turbulent interface of the fountain top.', 'concept_3': 'Excited by Reynolds stress in the convective zone (turbulent zone) (enhancing the ""internal source""): When convection interacts with a strongly stable layer, internal gravity waves are excited by the Reynolds stress in the convective zone, showing an energy flux spectrum different from that of the interface mechanism.', 'concept_4': '""Near-source extrapolation"" of the viscous internal wave model (referring to initialization within the turbulent layer): The viscous internal wave model starts from the wave amplitude at a certain initial height, and extrapolates to the far field by combining the dispersion relation and viscous attenuation; in the case of internal waves generated by the turbulent layer, this method can reconstruct the far-field spectrum.'}"	Internal waves are not generated at the boundary between turbulence and the environment, but originate from the turbulent region inside the fountain.	理论交叉学科-理论环境模型-大气环流模型	186117	3	4
recuVJksNKr5dI	Under the assumption of small deformation, for isotropic linear elastic material, derive the expression form of the 3D peridynamic bond constant $c$. $E$ is the elastic modulus, $\nu$ is Poisson's ratio, $\delta$ It is the radius of horizon of peridynamic.	{'step_1': 'For isotropic linear elastic material, the micropotential $w(\\xi)$ of peridynamic is $w(\\xi)=\\frac{1}{2} c s^2|\\xi|$', 'step_2': 'According to concept_1, the strain energy density of peridynamic becomes: $W=\\frac{1}{2} \\int_{\\mathcal{H}_x} \\frac{1}{2} c s^2|\\xi| d V_{\\xi}=\\frac{c}{4} \\int_{\\mathcal{H}_x} s^2|\\xi| d V_{\\xi}$', 'step_3': 'The bond stretch $s$ is equal to strain $\\varepsilon$', 'step_4': 'Calculated the strain energy density of peridynamic and obtained: $W_{pd}=\\frac{c \\varepsilon^2}{4} \\cdot \\pi \\delta^4=\\frac{c \\pi \\delta^4}{4} \\varepsilon^2$', 'step_5': 'Let the strain energy density of peridynamic equal to the strain energy density of classical continuum mechanics (concept_2)', 'step_6': 'Obtain the solution $\\frac{6 E}{\\pi \\delta^4(1-2 \\nu)}$'}	{'concept_1': 'The strain energy density of peridynamic is $W=\\frac{1}{2} \\int_{\\mathcal{H}_x} w(\\xi) d V_{\\xi}$. $w(\\xi)$ is the micropotential.', 'concept_2': 'The classical energy density for dilatation is: $W_{\\text {class }}=\\frac{1}{2} \\lambda(\\operatorname{tr} \\varepsilon)^2+\\mu \\operatorname{tr}\\left(\\varepsilon^2\\right)=\\frac{1}{2} \\lambda(3 \\alpha)^2+\\mu\\left(3 \\alpha^2\\right)=\\frac{9}{2} \\lambda \\alpha^2+3 \\mu \\alpha^2$.'}	$\frac{6 E}{\pi \delta^4(1-2 \nu)}$	理论工程学科-理论工程力学-固体力学	18056	1	2
recuUwZ7AtDDsQ	Is it possible, within the framework of General Relativity, to reproduce the observed late-time cosmic acceleration solely by allowing the FLRW spatial curvature to undergo a single 'mild and smooth' change at low redshift (without invoking dark energy)?	{'step_1': 'Introduce the variable-curvature FLRW metric Based on concept_1, a curvature term \\kappa(z)varying with redshift is introduced into the standard FLRW metric, treating the spatial curvature of the Universe as a time-evolving function. This step provides the mathematical framework of the model, in which cosmic dynamics no longer rely on an additional dark-energy component but instead allow curvature variation to affect the expansion rate.', 'step_2': 'Analyze the role of curvature in the acceleration equation Following concept_2, \\kappa(z)and its time derivative \\dot{\\kappa}(z) are substituted into the FLRW acceleration equation. It is found that even in the absence of dark energy, the variation of curvature introduces additional terms, enabling the emergence of cosmic acceleration at low redshift. This ensures theoretical feasibility, i.e., curvature evolution itself can drive acceleration.', 'step_3': 'Parametrize curvature evolution while maintaining smoothness According to concept_3, a smooth step-like curvature function is chosen, implementing a transition at low redshift (z∼0.5–0.6) from nearly flat to slightly nonzero curvature. Meanwhile, the condition \\dot{\\kappa} \\approx 0 is imposed, ensuring that the cosmological principle is only mildly violated without introducing discontinuities. This step guarantees that the model remains physically consistent while producing the required acceleration effect.', 'step_4': 'Constrain the model with observational data Following concept_4, observational data from cosmic chronometers (CC) and Type Ia supernovae (SNIa) are used in MCMC fitting to optimize the parameters of the curvature function, and to reconstruct H(z), q(z), and w_{\\mathrm{eff}}(z). Through this fitting procedure, the model is tested against whether it can successfully reproduce the observed history of accelerated expansion.', 'step_5': 'Verify consistency with ΛCDM Based on concept_5, the fitting results show that the “effective phase transition” of acceleration occurs at z∼0.5–0.6, with Ωm comparable to that in the ΛCDM model. Moreover, the model predicts a slight deceleration at z=0, consistent with several dark-energy parameterizations. This step confirms that the model can not only reproduce acceleration but also maintain a matter density consistent with standard cosmology, thereby addressing the original instruction.'}	{'concept_1': 'Variable-curvature FLRW metric κ = κ(z) — Introducing a curvature term that varies with time/redshift in the standard FLRW line element (the core setup of the model).', 'concept_2': 'Coupling of the acceleration equation with curvature derivatives —the curvature term (and its derivative) enters the acceleration equation, allowing cosmic acceleration to emerge without invoking dark energy.', 'concept_3': 'Smooth step-like curvature parametrization with the constraint \\dot{\\kappa} \\approx 0— A step function with a “smoothness” parameter is used to interpolate between two nearly flat curvature values, with the transition centered at low redshift; \\dot{\\kappa} remains small so that the cosmological principle is only mildly violated.', 'concept_4': 'Observational data and MCMC likelihood constraints — Cosmic chronometers (CC) and Type Ia supernovae (SNIa) are employed to fit the model’s free parameters via MCMC, and to reconstruct H(z), q(z), and w_{\\mathrm{eff}}(z). This is a necessary step to verify whether the model can “mimic” acceleration.', 'concept_5': 'Transition redshift and consistency with ΛCDM — The “effective phase transition” of acceleration occurs at z∼0.5–0.6, with Ωm comparable to the standard model. Meanwhile, the model predicts a slight deceleration at z=0, consistent with several dark-energy parameterizations.'}	By introducing a smooth step-like curvature function \kappa(z) centered at z∼0.5–0.6 (with \dot{\kappa} \approx 0), cosmic acceleration can be reproduced entirely within General Relativity.	理论交叉学科-理论天体物理-宇宙学	89420	2	12
recuUzEke6Xwzy	"For the following wave transmission system with acoustic boundary damping \(\eta\) and different wave speeds, can the energy of the system satisfy exponential decay? Provide a specific proof process. Given \(\Omega \subset \mathbb{R}^n\), where \(\Omega_1\) and \(\Omega\) are bounded open subsets of \(\Omega\), \(\Omega_2 = \overline{\Omega} \setminus \Omega_1\), \(\mathcal{I} = \partial\Omega_1 \cap \partial\Omega_2\), \(\Gamma_1 = \partial\Omega_1 \setminus \mathcal{I}\), \(\Gamma_2 = \partial\Omega_2 \setminus \mathcal{I}\), the system is described by the following equations: **\(\begin{cases} u_{tt} - a\Delta u = 0 & \text{in } \Omega_1 \times (0, \infty), \\ y_{tt} - b\Delta y = 0 & \text{in } \Omega_2 \times (0, \infty), \\ u = y & \text{on } \mathcal{I} \times (0, \infty), \\ a\partial_{\nu_1}u + b\partial_{\nu_2}y = 0 & \text{on } \mathcal{I} \times (0, \infty), \\ a\partial_{\nu_1}u - (\eta(x,t), C) = 0 & \text{on } \Gamma_1 \times (0, \infty), \\ \eta_t + C u_t - B\eta = 0 & \text{on } \Gamma_1 \times (0, \infty), \\ y = 0 & \text{on } \Gamma_2 \times (0, \infty) \end{cases}\)** Here, \(C\) and \(B\) are either coupling matrices or damping parameters, which is equivalent to placing an ""acoustic energy absorber"" on the boundary. Through coupling with the main system, energy is transferred from the wave system to the boundary damper. For high-frequency modes, this type of damping has a lower efficiency in transferring energy to \(\eta\) compared to viscous damping. Note: For a given unbounded linear operator \(A\), the following conditions hold: **\(\begin{cases} D(A) = \{(u, v, y, z, \eta) \in H : \Delta u \in L^2(\Omega_1), \Delta y \in L^2(\Omega_2), v \in H^1(\Omega_1), z \in H_{\Gamma_2}^2(\Omega_2), \\ \quad\quad\quad\quad a\partial_{\nu_1}u + b\partial_{\nu_2}y = 0 \text{ on } \mathcal{I}, a\partial_{\nu_1}u - (\eta, C) = 0 \text{ on } \Gamma_1\}, \\ A(u, v, y, z, \eta) = (v, a\Delta u, z, b\Delta y, B\eta - \gamma(v)C) \end{cases}\)** The aforementioned system is equivalent to: **\(\begin{cases} (u, v, y, z, \eta)_t = A(u, v, y, z, \eta), \\ (u, v, y, z, \eta)(0) = (u_0, v_0, y_0, z_0, \eta_0) \end{cases}\)** where \(\gamma: H^1(\Omega_1) \to L^2(\Gamma_1)\) is a trace operator. Moreover, for any \(\lambda \in \mathbb{R}\), if \((i\lambda I - A)U = 0\), then \(U = 0\), and \(i\lambda I - A\) is surjective. The energy of the system is defined as: **\(E(t) = \frac{1}{2}\left( \int_{\Omega_1} |u_t|^2 dx + a\int_{\Omega_1} |\nabla u|^2 dx + \int_{\Omega_2} |y_t|^2 dx + b\int_{\Omega_2} |\nabla y|^2 dx + \int_{\Gamma_1} \|\eta\|^2 d\Gamma \right)\)**"	{'step_1': 'Use the eigenvalues \\(\\mu_n\\) and eigenfunctions \\((\\varphi_n, \\psi_n)\\) of the operator \\(\\mathcal{O}_{\\Delta, R}\\) to construct a bad sequence. Let \\(\\lambda_n = \\mu_n\\), \\(U_n = \\left( \\frac{\\varphi_n}{i\\mu_n}, \\varphi_n, \\frac{\\psi_n}{i\\mu_n}, \\psi_n, -\\frac{1}{i\\mu_n}C\\gamma(\\varphi_n) \\right)\\), and \\(F_n = \\left( 0, 0, 0, 0, -\\frac{i}{\\mu_n}BC\\gamma(\\varphi_n) \\right)\\).', 'step_2': 'Verify that this sequence satisfies \\((i\\mu_n I - A)U_n = F_n\\).', 'step_3': 'Use the multiplier method to derive that \\(\\|U_n\\|_H^2 \\geq 1\\).', 'step_4': 'According to the boundary trace theorem, show that \\(\\|F_n\\|_H^2 \\to 0\\).', 'step_5': 'Then \\(\\lim_{n \\to \\infty} \\|(i\\lambda I - A)^{-1}\\| = \\lim_{\\mu_n \\to \\infty} \\frac{\\|U_n\\|_H}{\\|F_n\\|_H} = \\infty\\), which means that the resolvent of operator \\(A\\) is not uniformly bounded on the imaginary axis, thus proving that the system is not exponentially stable.'}	{'concept/theorem_1': 'If \\(\\Omega\\) is a bounded domain in \\(\\mathbb{R}^n\\) with \\(C^1\\) boundary \\(\\Gamma\\), then for any function \\(u \\in H^1(\\Omega)\\), the following estimate holds: \\(\\|u\\|_{L^2(\\Gamma)} \\leq C\\|u\\|_{H^1(\\Omega)}^{1/2}\\|u\\|_{L^2(\\Omega)}^{1/2}\\), where \\(C\\) is a positive constant depending on \\(u\\).', 'concept/theorem_2': 'For the unbounded linear operator \\(\\mathcal{O}_{\\Delta, R} : D(\\mathcal{O}_{\\Delta, R}) \\to L^2(\\Omega_1) \\times L^2(\\Omega_2)\\), if it satisfies \\(D(\\mathcal{O}_{\\Delta, R}) = \\left\\{ (f, g) \\in H^1(\\Omega_1) \\times H^1_{\\Gamma_2}(\\Omega_2) : (\\Delta f, \\Delta g) \\in L^2(\\Omega_1) \\times L^2(\\Omega_2), \\, a\\partial_{\\nu_1}f + b\\partial_{\\nu_2}g = 0 \\, \\text{on } \\mathcal{I}, \\, \\text{and } \\partial_{\\nu_1}f + (C, C)f = 0 \\, \\text{on } \\Gamma_1 \\right\\}\\) and for all \\((f, g) \\in D(\\mathcal{O}_{\\Delta, R})\\), \\(\\mathcal{O}_{\\Delta, R}(f, g) = (-a\\Delta f, -b\\Delta g)\\), then \\(\\mathcal{O}_{\\Delta, R}\\) is a positive self-adjoint operator with compact resolvent. That is, there exist eigenpairs \\((\\phi_n, \\psi_n)\\) and their corresponding eigenvalues \\((\\mu_n^2)_{n \\in \\mathbb{N}}\\) such that \\(\\mu_n^2\\) tends to infinity as \\(n\\) tends to infinity.', 'concept/theorem_3': 'Let \\(H\\) be a Hilbert space. Then the system is exponentially stable if and only if \\(i\\mathbb{R} \\subset \\rho(A)\\) and the condition \\(\\sup_{\\lambda \\in \\mathbb{R}} \\|(i\\lambda I - A)^{-1}\\| < \\infty\\) is satisfied.'}	No exponential decay. Checklist: （Prove that the resolvent operator of the system, i.e., (iλI - A)⁻¹, is not uniformly bounded. If the answer mentions the conditions for wave speeds a or b, it is incorrect, as the correct derivation process will not involve the magnitude of wave speeds.）	理论工程学科-控制理论与控制工程-控制理论	539326	1	3
recuV0Ekx58aPd	In diffusion-controlled reactions in nonequilibrium steady states (NESS), when the system is confined to the linear response regime (i.e., under low fields), what is the upper limit of rate enhancement? Please write the specific full formula with work functions. Can use Jensen’s Inequality (for Convex Functions): For convex functions (e.g., exponential functions), \(-\ln\langle e^{-\Gamma} \rangle_{AB,\xi} \leq \langle \Gamma \rangle_{AB,\xi}\), converting nonlinear exponential averages into linear arithmetic averages.	"{'step_1': 'Clarify the quantitative indicator of rate enhancement: According to the definition of rate enhancement in the paper, the rate enhancement of NESS diffusion-controlled reactions relative to equilibrium is quantified by the combination of the logarithmic ratio of reactive frequencies and transition path times. Specifically, it is expressed as \\( \\ln\\frac{k_\\xi \\tau_0^c}{k_0 \\tau_\\xi^c} \\), where \\( k_\\xi \\) and \\( k_0 \\) are the reactive frequencies under NESS (external field \\( \\xi \\)) and equilibrium, respectively, and \\( \\tau_\\xi^c \\) and \\( \\tau_0^c \\) are the transition path times under NESS and equilibrium, respectively. This indicator is the core object for subsequent derivation of the upper limit.', 'step_2': ""Use Girsanov Transform to link trajectory probability ratio with reactive correlation function ratio: Apply Theorem 1 (Girsanov Transform), which is a tool to calculate the logarithmic ratio of trajectory probabilities under equilibrium (\\( \\xi=0 \\)) and NESS (external field \\( \\xi \\)). First, derive the logarithmic ratio of a trajectory’s probability under \\( \\xi \\) (\\( P_\\xi \\)) to that under equilibrium (\\( P_0 \\)) as \\( \\Gamma' = \\ln\\frac{P_\\xi[\\{X_t\\}|r_0]}{P_0[\\{X_t\\}|r_0]} = -\\frac{\\beta D}{4}\\int_0^\\tau dt\\left[\\xi^2 - 2\\xi \\hat{z}\\cdot\\left(\\beta^{-1}D^{-1}\\dot{r} - F\\right)\\right] \\). Then, introduce the auxiliary function \\( \\Gamma = \\beta \\Gamma' + \\ln\\frac{\\rho_\\xi(r_0)}{\\rho_0(r_0)} \\) (where \\( \\rho_\\xi/\\rho_0 \\) is the ratio of steady-state probability densities at the initial position \\( r_0 \\) under NESS and equilibrium). Through probability normalization, link the ratio of reactive correlation functions (\\( \\ln\\frac{h_\\xi(\\tau)}{h_0(\\tau)} \\), \\( h_\\xi(\\tau) \\) and \\( h_0(\\tau) \\) are the probabilities of observing AB-reactive trajectories within time \\( \\tau \\) under NESS and equilibrium, respectively) to the conditional average of the auxiliary function, resulting in \\( \\ln\\frac{h_\\xi(\\tau)}{h_0(\\tau)} = -\\ln\\langle e^{-\\Gamma} \\rangle_{AB,\\xi} \\) (where \\( \\langle \\cdot \\rangle_{AB,\\xi} \\) is the conditional average over AB-reactive trajectories)."", 'step_3': 'Apply Jensen’s Inequality to obtain the upper limit of the correlation function ratio: Use Jensen’s Inequality for convex functions. Since the exponential function \\( e^x \\) is convex, the inequality \\( -\\ln\\langle e^{-\\Gamma} \\rangle_{AB,\\xi} \\leq \\langle \\Gamma \\rangle_{AB,\\xi} \\) holds. Combining the result from Step 2 (\\( \\ln\\frac{h_\\xi(\\tau)}{h_0(\\tau)} = -\\ln\\langle e^{-\\Gamma} \\rangle_{AB,\\xi} \\)) and the rate enhancement indicator from Step 1 (\\( \\ln\\frac{k_\\xi \\tau_0^c}{k_0 \\tau_\\xi^c} = \\ln\\frac{h_\\xi(\\tau)}{h_0(\\tau)} \\)), we get the preliminary upper limit relationship: \\( \\ln\\frac{k_\\xi \\tau_0^c}{k_0 \\tau_\\xi^c} \\leq \\langle \\Gamma \\rangle_{AB,\\xi} \\).', 'step_4': ""Expand \\( \\langle \\Gamma \\rangle_{AB,\\xi} \\) and associate it with the work done by the external field: According to Concept 5, the auxiliary function \\( \\Gamma \\) expands to \\( \\langle \\Gamma \\rangle_{AB,\\xi} = \\beta \\langle \\Gamma' \\rangle_{AB,\\xi} + \\langle \\ln\\frac{\\rho_\\xi}{\\rho_0} \\rangle_{AB,\\xi} \\) under the conditional average \\( \\langle \\cdot \\rangle_{AB,\\xi} \\). Then, simplify \\( \\beta \\langle \\Gamma' \\rangle_{AB,\\xi} \\): neglect the weak ion-ion conservative force term \\( F \\) (since the ion-paired state boundary \\( \\partial B \\) is near the Bjerrum length \\( l_B \\), \\( F \\) is weak, making \\( \\int_0^\\tau dt \\cdot \\hat{z}\\cdot F \\approx 0 \\)), and substitute the definitions of the two components of the work done by the external field from Concept 4. For the \\( \\xi^2 \\) term, it corresponds to the work on free ions \\( W_f = \\beta D \\xi^2 \\tau \\), so \\( -\\frac{\\beta^2 D \\xi^2 \\tau}{4} = -\\frac{\\beta}{4}W_f \\); for the \\( \\xi \\hat{z}\\cdot\\dot{r} \\) term, it corresponds to the mean work on AB-reactive trajectories \\( \\langle W \\rangle_{AB,\\xi} = \\langle \\xi \\int_0^\\tau dt \\hat{z}\\cdot\\dot{r} \\rangle_{AB,\\xi} \\), so \\( \\frac{\\beta^2 D \\xi}{2}\\int_0^\\tau dt \\cdot \\hat{z}\\cdot\\left(\\beta^{-1}D^{-1}\\dot{r}\\right) = \\frac{\\beta}{2}\\langle W \\rangle_{AB,\\xi} \\). Thus, \\( \\beta \\langle \\Gamma' \\rangle_{AB,\\xi} = \\frac{\\beta}{2}\\langle W \\rangle_{AB,\\xi} - \\frac{\\beta}{4}W_f \\)."", 'step_5': ""Derive the final upper limit formula of rate enhancement: Substitute \\( \\beta \\langle \\Gamma' \\rangle_{AB,\\xi} = \\frac{\\beta}{2}\\langle W \\rangle_{AB,\\xi} - \\frac{\\beta}{4}W_f \\) into the preliminary upper limit relationship from Step 3. The result is the upper limit formula of rate enhancement for NESS diffusion-controlled reactions: \\( \\ln\\frac{k_\\xi \\tau_0^c}{k_0 \\tau_\\xi^c} \\leq \\frac{\\beta}{2}\\langle W \\rangle_{AB,\\xi} - \\frac{\\beta}{4}W_f + \\langle \\ln\\frac{\\rho_\\xi}{\\rho_0} \\rangle_{AB,\\xi} \\). Among the terms on the right, \\( \\frac{\\beta}{2}\\langle W \\rangle_{AB,\\xi} \\) is the positive contribution of the work on AB-reactive trajectories, \\( -\\frac{\\beta}{4}W_f \\) is the negative contribution of the work on free ions, and \\( \\langle \\ln\\frac{\\rho_\\xi}{\\rho_0} \\rangle_{AB,\\xi} \\) is the minor contribution of the steady-state probability density ratio.""}"	"{'concept_1': 'Nonequilibrium Steady States (NESS): A state where the system is driven far from equilibrium by external non-conservative forces (e.g., strong electric fields) but maintains a steady probability distribution and persistent probability currents (\\(J_{ss} \\neq 0\\)), distinct from equilibrium states with no net energy input.', 'concept_2': ""Diffusion-Controlled Reactions: Reactions where the rate is dominated by the diffusive motion of reactants, characterized by slow diffusive local relaxation that breaks the 'timescale separation' assumption of conventional rate theories."", 'concept_3': 'Rate Enhancement: The increase in the reaction rate of NESS diffusion-controlled reactions relative to equilibrium, quantified approximately by the log ratio of reactive frequencies (\\(\\ln\\frac{k_\\xi}{k_0}\\)) and transition path times (\\(\\ln\\frac{\\tau_0^c}{\\tau_\\xi^c}\\)), i.e., \\(\\ln\\frac{k_\\xi \\tau_0^c}{k_0 \\tau_\\xi^c}\\)(neglecting the higher-order term \\(O\\left( (1-\\frac{\\tau_0^c}{\\tau_\\xi^c})^2 \\right)\\)).', 'concept_4': 'Work done by the external field: A core thermodynamic quantity constraining the rate enhancement of NESS diffusion-controlled reactions, including two parts: work on free ions (\\(W_f\\) and mean work on AB-reactive trajectories (\\(\\langle W \\rangle_{AB,\\xi}\\)). 1. Work on free ions, defined as: W_f = \\beta D \\xi^2 \\tau 2. Mean work on AB -reactive trajectories, defined as W(\\{X_t\\}) = \\xi \\int_0^\\tau dt \\hat{z}\\cdot\\dot{r}. Its average over the AB-trajectory ensemble is \\langle W \\rangle_{AB,\\xi}.', 'concept_5': ""The work in stochastic thermodynamics: A trajectory-dependent quantity that extends classical mechanical work to diffusion-controlled systems with random, fluctuating dynamics (e.g., ion-pair recombination under external electric fields \\(\\xi\\hat{z}\\)), especially nonequilibrium systems governed by noisy overdamped Langevin equations. To link it to reactive flux analysis, first define the auxiliary functional (from Girsanov transform path reweighting):\\(\\Gamma(\\{X_t\\} | r_0) := \\beta \\Gamma'(\\{X_t\\} | r_0) + \\ln\\frac{\\rho_\\xi(r_0)}{\\rho_0(r_0)}\\) (where \\(\\beta=1/(k_BT)\\), \\(\\rho_\\xi/\\rho_0\\) = steady-state density ratio). Taking the conditional average over trajectories (\\(\\langle \\cdot \\rangle_{\\xi}\\)) and using averaging linearity gives:\\(\\langle \\Gamma \\rangle_{\\xi} = \\beta \\langle \\Gamma' \\rangle_{\\xi} + \\langle \\ln\\frac{\\rho_\\xi}{\\rho_0} \\rangle_{\\xi}\\)"", 'theorem_1': ""Girsanov Transform: A tool for calculating the logarithmic ratio of trajectory probabilities under equilibrium (\\(\\xi=0\\)) and NESS (external field \\(\\xi\\)): Core Log Ratio Formula: The log ratio of a trajectory’s probability under \\(\\xi\\) (\\(P_\\xi\\)) vs. equilibrium (\\(P_0\\)) is: \\( \\Gamma' = \\ln\\frac{P_\\xi[\\{X_t\\}|r_0]}{P_0[\\{X_t\\}|r_0]} = -\\frac{\\beta D}{4}\\int_0^\\tau dt\\left[\\xi^2 - 2\\xi \\hat{z}\\cdot\\left(\\beta^{-1}D^{-1}\\dot{r} - F\\right)\\right] \\) (Where \\(\\beta=1/(k_BT)\\), \\(D\\)=sum of ion diffusion coefficients, \\(F\\)=ion-ion conservative force, \\(\\dot{r}\\)=anion-cation relative velocity, \\(\\hat{z}\\)=external field direction unit vector.) Key Conversions & Simplifications: Define auxiliary function \\(\\Gamma = \\beta \\Gamma' + \\ln\\frac{\\rho_\\xi(r_0)}{\\rho_0(r_0)}\\) (\\(\\rho_\\xi/\\rho_0\\)=NESS/equilibrium steady-state density ratio at initial position \\(r_0\\)). Link to reactive trajectory dynamics: \\(\\ln\\frac{h_\\xi(\\tau)}{h_0(\\tau)} = -\\ln\\langle e^{-\\Gamma} \\rangle_{AB,\\xi}\\) (\\(\\langle \\cdot \\rangle_{AB,\\xi}\\)=conditional average over AB-reactive trajectories). Neglect weak \\(F\\) (near Bjerrum length \\(l_B\\)), simplifying \\(\\beta \\Gamma'\\) to: \\( \\beta \\Gamma' = -\\frac{\\beta^2 D \\xi^2 \\tau}{4} + \\frac{\\beta^2 D \\xi}{2}\\int_0^\\tau dt \\cdot \\hat{z}\\cdot\\left(\\beta^{-1}D^{-1}\\dot{r}\\right) \\)""}"	In diffusion-controlled reactions under nonequilibrium steady states (NESS), the work done by the external field serves as the upper limit of rate enhancement. This relationship is quantified by the following inequality: \( \ln\frac{k_\xi \tau_0^c}{k_0 \tau_\xi^c} \leq \frac{\beta}{2}\langle W \rangle_{AB,\xi} - \frac{\beta}{4}W_f + \langle \ln\frac{\rho_\xi}{\rho_0} \rangle_{AB,\xi} \) In the above expression, the terms on the right-hand side can be categorized into **core terms** (dominant contributors to rate enhancement limits) and **minor terms** (negligible under specific conditions): ### 1. Core Terms - \(\frac{\beta}{2}\langle W \rangle_{AB,\xi}\): the positive contribution from the work performed by the external field on the AB reaction trajectory. As the primary driver of rate enhancement, this term directly reflects the energy input from the external field that promotes the diffusion and interaction of reactants (A and B). - \(-\frac{\beta}{4}W_f\): the negative contribution from the work performed by the external field on free ions. Non-reacting free ions consume energy from the external field without participating in the target reaction, thereby offsetting part of the energy available for rate enhancement. ### 2. Minor Term - \(\langle \ln\frac{\rho_\xi}{\rho_0} \rangle_{AB,\xi}\): the conditional average of the logarithm of the ratio between the NESS probability density (\(\rho_\xi\)) and the equilibrium steady-state probability density (\(\rho_0\)). This term accounts for the contribution of probability density ratio differences to rate enhancement, but is **negligible in the low-field regime** (a key condition for linear response behavior).	理论基础学科-理论化学-化学反应动力学理论	825673	3	9
recuUTLZzkmYnd	Explain how potential-induced surface reconstruction of Pd and Pt catalysts governs the product selectivity of propylene electrooxidation, and identify the main products at different potential ranges.	{'step_1': 'Clarify the task: map applied potential (U, vs RHE) to surface state of Pd/Pt and then to dominant reaction pathways and products.', 'step_2': 'Invoke electrochemical thermodynamics: use bulk and surface Pourbaix reasoning to determine stable phases under operating U. For Pd: metallic Pd at lower U transitions to PdO as U increases; for Pt: PtO2 is stable across the relevant U.', 'step_3': 'Identify reconstructed active centers: at low U, Pd(111) with ~1/3 ML O*; at higher U, PdO(101) with ~1/2 ML OH*; for Pt, PtO2(110) with ~1/2 ML OH* over the whole window.', 'step_4': 'Set up the reaction network on each active center: enumerate elementary steps leading to acrolein, acetone, propylene oxide (PO), and propylene glycol (PG), including O*/OH* coupling and electrochemical dehydrogenation (PCET).', 'step_5': 'Evaluate step thermodynamics at potential using CHE: ΔG(U)=ΔG°−n e U for PCET steps; constant-potential GC-DFT provides ΔG and barriers for chemical steps (C–O coupling, intramolecular H transfer).', 'step_6': 'Link potential to kinetics: for electrochemical steps use Butler–Volmer-type barrier dependence ΔG‡(U)=ΔG‡(0)−α e U; combine with Arrhenius k=ν·exp(−ΔG‡/kBT).', 'step_7': 'Construct microkinetic steady-state: write rate expressions for formation of each product on each surface; solve steady-state coverages and rates; compute TOF_i(U) for each product and surface.', 'step_8': 'Incorporate surface-phase coexistence for Pd: as U increases, weight rates from Pd(111) and PdO(101) by their potential-dependent fractions (reflecting gradual oxidation), yielding smooth transitions in selectivity.', 'step_9': 'Analyze selectivity control: use Degree of Rate Control (DRC) and Degree of Selectivity Control (DSC) to identify RDS/SS points—e.g., on PtO2, CH2CHOCH3*→PO* controls PO rate; on PdO, conversion of CH2CH(OH)CH3* biases PG vs acetone with U.', 'step_10': 'Deduce Pd selectivity vs U: at 0.7–0.9 VRHE on O*-Pd(111), low barrier path via allylic dehydrogenation + O* coupling yields acrolein dominance; as U→1.0–1.4 VRHE and PdO/OH* grows, CH2CH(OH)CH3* channels favor acetone and PG (PG gains at higher U as its C–O coupling barrier drops faster).', 'step_11': 'Deduce Pt selectivity vs U: on OH*-PtO2(110) across 1.2–1.6 VRHE, formation of CH2CH(OH)CH3* then branches with similar barriers; slight preference and lowering of barrier to epoxidation maintains PO along with acetone; PG remains minor due to higher access barrier to the dihydroxylated route.', 'step_12': 'Synthesize to final answer: attribute product maps directly to potential-driven reconstruction (phase + coverage) rather than only barrier shifts; report main products and U ranges for Pd and Pt consistent with the mechanism.'}	{'concept_1': 'Reversible Hydrogen Electrode (RHE) scale for potential U, aligning proton–electron pair chemical potential with pH=0 reference.', 'concept_2': 'Pourbaix stability: phase diagram linking electrode potential and pH to most stable bulk/surface phases of metals/oxides under electrochemical conditions.', 'concept_3': 'Surface coverage thermodynamics: surface Pourbaix to identify lowest-grand-potential terminations (e.g., 1/3 ML O* on Pd(111), 1/2 ML OH* on PdO(101)/PtO2(110)).', 'concept_4': 'Grand-canonical DFT (GC-DFT): constant-potential electronic structure where grand potential Ω = E_DFT − μ_e N_e − Σ_i μ_i N_i; enables U-dependent adsorption and barrier calculations.', 'concept_5': 'Computational Hydrogen Electrode (CHE): ΔG(U) = ΔG° − n e U for PCET steps; relates free energies at electrode potential to standard-state values.', 'concept_6': 'Butler–Volmer kinetics (qualitative use): for an anodic PCET step, barrier dependence ΔG‡(U) ≈ ΔG‡(0) − α e U, current i ∝ exp[−ΔG‡(U)/k_BT].', 'concept_7': 'Arrhenius/Transition State Theory: k = ν exp(−ΔG‡/k_B T), rate r = k Π θ_j (reactant coverages), governs chemical step rates (e.g., C–O coupling, H-shifts).', 'concept_8': 'Langmuir adsorption/coverage balance: θ_i = K_i a_i / (1 + Σ K_j a_j), steady-state coverages enter microkinetic rates.', 'concept_9': 'Microkinetic modeling (MKM): steady-state solution of coupled ODEs for coverages; net formation rates TOF_i(U) = Σ_path r_path,i(U).', 'concept_10': 'Degree of Rate Control (DRC): X_r = (∂ln v / ∂(−ΔG_r‡/k_B T))_others, identifies which elementary barrier controls overall rate; used to find RDS.', 'concept_11': 'Degree of Selectivity Control (DSC): S_AB control metric; e.g., ∂ln(TOF_A/TOF_B)/∂(−ΔG_r‡/k_B T) for step r reveals which step governs selectivity between products.', 'concept_12': 'Bader charge analysis: charges on adsorbate carbons shift nucleophilicity/electrophilicity, rationalizing preferred coupling sites (e.g., C2 more positive on OH*-PdO favors OH* addition).', 'concept_13': 'COHP (Crystal Orbital Hamilton Population): bond strength indicator; increased C–O bond strength in PG* with U explains faster barrier decrease for PG formation at higher potentials.', 'concept_14': 'Reaction network topology: competing pathways to acrolein (allylic dehydrogenation + O* coupling), acetone (intramolecular H transfer from CH2CHOCH3*), PO (epoxidation of CH2CHOCH3*), PG (sequential OH* additions).', 'concept_15': 'Phase-coexistence weighting: overall rate on Pd is a U-weighted combination of contributions from O*-Pd(111) and OH*-PdO(101), ensuring continuous selectivity transitions.', 'concept_16': 'Tafel relationship (qualitative): slope invariance across U indicates unchanged RDS (e.g., PO formation on PtO2 retains same RDS across 1.0–1.6 VRHE).', 'concept_17': 'Selectivity definition: S_i = TOF_i / Σ_j TOF_j; main product corresponds to maximal S_i(U) after accounting for phase fractions and pathway rates.'}	In propylene electrooxidation, product selectivity is governed by potential-induced surface reconstruction of the catalyst.  Pd (potential-dependent and transitional behavior).   Low potentials (≈0.70–0.90 V_RHE): the active center is a partially O-covered Pd(111) surface, favoring acrolein via allylic dehydrogenation + O* coupling.   Transition window (≈0.92–1.01 V_RHE): the surface gradually oxidizes toward hydroxylated PdO(101). A coexistence of Pd and PdO active centers yields a smooth selectivity crossover: acrolein decreases, while acetone and propylene glycol (PG) increase.   Higher potentials (≈1.05–1.40 V_RHE): PdO(101) with ~1/2 ML OH* dominates. Acetone initially rises, but at still higher U, PG becomes the major product. COHP analysis shows that the C–O bond in PG* strengthens with increasing potential, so its formation barrier decreases faster than the acetone branch—explaining PG’s advantage at high voltages.   Note on PO observability: PO generation on Pd is minimal and readily hydrolyzes to PG in aqueous/acidic media; thus experiments mainly detect acrolein, acetone, and PG.    Pt (potential-invariant active center).   Across 1.2–1.6 V_RHE, the active center remains hydroxylated PtO₂(110), consistently producing propylene oxide (PO) and acetone (with a stable rate-determining epoxidation branch). PG remains minor due to the higher access barrier to dihydroxylation routes.    Checklist for correctness:  Pd low-U acrolein   Transitional 0.92–1.01 V_RHE with smooth crossover Pd high-U acetone → PG shift   Added COHP-based rationale for PG dominance   Note on PO hydrolysis   Pt stable hydroxylated PtO₂ producing PO + acetone	理论交叉学科-计算材料学-	47027	2	2
recuW73ODRO295	In an atmospheric system initially in a uniform radiative-convective equilibrium (RCE) state, assuming the simultaneous presence of the following conditions: wet/dry bistability (two stable fixed points q±); a finite WTG effective scale L_{\mathrm{dyn}} determined by non-zero rotation parameter f, friction, and thermal damping; a moderately strong nonlinear horizontal advection term \epsilon (\mathbf{u}\cdot \nabla q); and a domain scale much larger than L_{\mathrm{dyn}} allowing for a long-term scale coarsening process, please qualitatively describe the typical evolutionary outcomes of the system after long-term integration, including: The trend of the area ratio between wet and dry regions (whether it expands, contracts, or remains unchanged relative to the initial conditions); Spatial forms of wet and dry areas (isotropic spots, strips, crisscross patterns, etc.); Has evolution completely stopped or is there still slow residual dynamics?	"{'step_1': ""Under the premise of 'wet/dry bistability + WTG availability', regard the evolution of humidity as a type of Allen–Cahn reaction-diffusion system. The uniform RCE state will fall into two stable fixed points q₊ and q₋ under perturbation, forming a coexistence of wet and dry phases and entering the stage of scale coarsening. Universal conclusions of the Allen–Cahn system: interface thickness ~ (κ/μ)^(1/2), small patches merge into large patches, interface curvature is smoothed out, and the typical scale increases monotonically."", 'step_2': ""Incorporate rotation fff, friction α\\alphaα, and thermal damping λ\\lambdaλ, and define the dynamic effective scale LdynL_{\\mathrm{dyn}}Ldyn of WTG. When the coarsening scale approaches LdynL_{\\mathrm{dyn}}Ldyn, large-scale equilibrium and damping inhibit further coarsening. Although the domain scale is 'much larger than' LdynL_{\\mathrm{dyn}}Ldyn, sustainable coarsening will be clamped around LdynL_{\\mathrm{dyn}}Ldyn, and the system enters a dynamically constrained 'post-coarsening' stage (no longer unifying towards the domain scale)."", 'step_3': ""Determine the long-term area fraction A+ of moist regions using the balance of Ghq(q;Fh(q)) (Eqs.8-10). The system adjusts Fh(q) to a stable value Cs, where the potential areas V+(Cs)=V−(Cs) and the interface speed cRD→0. The area ratio A+/A− is determined by initial conditions and the balance of divergence (moist regions: convergence; dry regions: divergence), and remains stable without systematic contraction or expansion of moist regions. This leads to Conclusion-1: 'The area ratio of moist to dry regions stabilizes at a value determined by global heating balance (Fh(q)=Cs), with no systematic contraction or expansion relative to the post-initial-separation state.'"", 'step_4': ""Introduce a moderate-intensity nonlinear horizontal advection term ϵ(u\u2009\u2063⋅\u2009\u2063∇q) and consider the flow field response when f≠0. Advection stretches the contour lines into anisotropic structures; as ϵ increases from small to moderate, the steady state (or quasi-steady state) transitions from zonal to cross-shaped more easily. This leads to Conclusion-2 ('The morphology of the wet region is anisotropic, and may form zonal/cross shapes'). When f≠0, the rotational flow stretches moist contours into filamentous/spiral shapes; only when f=0 do banded/cross-shaped structures form. This leads to Conclusion-2: 'Moist region morphology is anisotropic, with banded/cross shapes (f=0) or filamentous/spiral shapes (f≠0).'"", 'step_5': ""After clamping L_{\\mathrm{dyn}}, the system is not strictly stationary: the interface is still (finely adjusted) at an extremely slow speed due to the local residual and curvature terms of c_{RD}, as well as weak advection traction, showing residual dynamics. Therefore, in long-term statistics, it is a quasi-steady state, not a strictly steady state; this gives Answer-3. In Regime I, residual evolution at the interface is extremely weak, approaching a steady state; only in Regime II does slow residual evolution persist. This leads to Conclusion-3: 'The main coarsening process ends, and the steady state depends on the regime—nearly static (Regime I) or with slow residual evolution (Regime II).'""}"	"{'concept_1': ""Bistable humidity equation and Allen–Cahn reaction–diffusion: This model system is equivalent to a reaction–diffusion equation with a bistable reaction (Allen–Cahn equation) under the weak temperature gradient (WTG) approximation. Such systems exhibit 'scale coarsening'—after initial separation into high-q (moist) and low-q (dry) regions, their spatial scale increases monotonically until constrained by either the system's geometric scale (when f=α=λ=0) or the dynamical length scale Ldyn (when rotation, friction, or thermal damping are non-zero)."", 'concept_2': 'Interface Propagation Speed and Energy Level Criterion: Define the potential function V(q). If V(q+) > V(q−), the q+ region will expand outward; if V(q+) < V(q−), it will eventually be eroded by the q− region; only when V(q+) = V(q−) can the wet and dry regions coexist for a long time. The interface propagation speed c_RD is a local quantity and is also related to the interface curvature. Small-scale wet regions are more likely to disappear due to their large curvature. Only when Fh(q) deviates from Cs (the stable equilibrium value) will temporary expansion/contraction occur; eventually, V(q+)=V(q−) is achieved, and moist-dry regions coexist stably.', 'concept_3': 'WTG Effective Scale and Aggregation Termination: When friction, thermal damping, and rotation are introduced, the WTG approximation is only valid at scales smaller than L_dyn, and the aggregation process is significantly weakened or stops at this characteristic scale.', 'concept_4': 'Nonlinear advection and anisotropic morphology: When nonlinear advection (ϵ > 0) is introduced, the coarsening process is altered, and the final morphology depends on rotation (f): For f=0 (no rotation): The steady/quasi-steady state exhibits banded structures (small ϵ) or cross-shaped structures (large ϵ); For f≠0 (with rotation): The flow has a significant rotational component, leading to filamentous or spiral structures of moist regions.', 'concept_5': ""Residual Dynamics and Steady-State Dependence on Regimes: After coarsening is constrained by Ldyn, the system's steady-state characteristics depend on the f-plane regime: Regime I (real unstable mode): The system reaches a quasi-steady state with negligible residual evolution (nearly static); Regime II (complex unstable mode): Sustained unsteady evolution (e.g., wave-like propagation) occurs, with significant residual dynamics.""}"	1. The area ratio of the wet zone to the dry zone does not systematically shrink or expand, and finally stabilizes at a value determined by the global heating balance (\(\overline{F_h(q)} = C_s\)) and initial perturbations, with only minor fluctuations caused by slow interface adjustments, until the stable coexistence condition \(V(q_+) = V(q_-)\) is met (\(c_{\text{RD}} \to 0\)). 2. The morphology of the wet region is anisotropic, and its specific form depends on the rotation parameter f: when \(f = 0\) (no rotation), affected by nonlinear advection, it may form a banded structure (with small \(\epsilon\)) or a cross-shaped structure (with medium \(\epsilon\)); when \(f \neq 0\) (with rotation), the rotational flow field stretches the contour of the wet region, forming a filamentous or spiral structure. 3. After the main-scale coarsening process is completed, does the system have residual dynamics that depend on the f-plane flow pattern? If it is in Regime I (real unstable mode), the system is close to a quasi-steady state, and the residual evolution of the interface is extremely weak (nearly static); if it is in Regime II (complex unstable mode), there are significant slow residual dynamics (such as wave-like propagation, morphological fine-tuning), and it is not completely stationary.	理论交叉学科-理论环境模型-大气环流模型	99560	3	3
recuWmZtmofWbT	Under the spherically symmetric static metric ds^2=-g(r)\,dt^2+\frac{dr^2}{g(r)}+r^2 d\Omega^2, take the gauge potential A_\mu=h(r)\delta_\mu^{\,t}; considering the purely electric case of the F(R)–ModMax theory, determine h(r) in natural units from the source-free Maxwell equation.	"{'step_1': 'First, under the given spherically symmetric static metric \\[ds^2=-g(r)\\,dt^2+\\frac{dr^2}{g(r)}+r^2 d\\Omega^2\\] adopt the purely electric gauge potential \\(A_\\mu=h(r)\\,\\delta_\\mu^{\\,t}\\).', 'step_2': 'In accordance with \\textbf{concept_1} (taking \\(\\mathcal{P}=0\\) for the purely electric case) and the ModMax Lagrangian from \\textbf{theorem_1} \\[\\mathcal{L}=\\frac12\\!\\left(\\mathcal{S}\\cosh\\gamma-\\sqrt{\\mathcal{S}^2+\\mathcal{P}^2}\\,\\sinh\\gamma\\right),\\qquad \\mathcal{S}=\\frac{F}{2},\\ \\ \\mathcal{P}=0,\\] it can be derived that the derivative with respect to \\(F\\) is a constant: \\[\\mathcal{L}_F\\equiv\\frac{\\partial\\mathcal{L}}{\\partial F}=\\frac{\\cosh\\gamma+\\sinh\\gamma}{4}.\\]', 'step_3': 'As a result, the source-free field equation \\[\\nabla_\\mu\\!\\left(\\mathcal{L}_F F^{\\mu\\nu}\\right)=0\\] reduces to the standard form \\(\\nabla_\\mu F^{\\mu\\nu}=0\\).', 'step_4': 'Using \\(\\sqrt{-g}=r^2\\sin\\theta\\) and noting that only the \\(tr\\)-component is non-vanishing, setting \\(\\nu=t\\) gives \\[0=\\nabla_\\mu F^{\\mu t} =\\frac{1}{\\sqrt{-g}}\\partial_\\mu\\!\\left(\\sqrt{-g}\\,F^{\\mu t}\\right) =\\frac{1}{r^2\\sin\\theta}\\partial_r\\!\\left(r^2\\sin\\theta\\,F^{rt}\\right) \\ \\Rightarrow\\ \\partial_r\\!\\left(r^2 F^{rt}\\right)=0,\\] thus leading to \\[r^2 F^{rt}=-Q,\\] where the constant is denoted by the charge \\(Q\\).', 'step_5': ""Furthermore, since \\[F_{tr}=-\\partial_r A_t=-h'(r),\\qquad F^{tr}=g^{tt}g^{rr}F_{tr}=h'(r)\\ \\Rightarrow\\ F^{rt}=-h'(r),\\] substituting back yields \\[h'(r)=\\frac{Q}{r^2}.\\]"", 'step_6': 'In natural units and with the gauge condition \\(h(\\infty)=0\\), integration results in \\[\\boxed{\\,h(r)=-\\frac{Q}{r}\\,}.\\]'}"	{'concept_1': 'electrically charged black hole solutions by coupling \\( F(R) \\) theory and the ModMax nonlinear electrodynamics theory, we have to consider \\( \\mathcal{P} = 0 \\) in the above equations. where \\(S\\), and \\(\\mathcal{P}\\) are, respectively, a true scalar, and a pseudoscalar, which are defined in the following forms \\(S = \\frac{F}{2}\\), \\& \\(\\mathcal{P} = \\frac{\\widetilde{F}}{2}\\), and \\(F = F_{\\mu\\nu} F^{\\mu\\nu}\\) is the Maxwell invariant (\\(F_{\\mu\\nu} = \\partial_\\mu A_\\nu - \\partial_\\nu A_\\mu\\) (where \\(A_\\mu\\) is the gauge potential) is the electromagnetic tensor). In addition, \\(\\widetilde{F}\\) equals to \\(F_{\\mu\\nu} \\widetilde{F}^{\\mu\\nu}\\), where \\(\\widetilde{F}^{\\mu\\nu} = \\frac{1}{2} \\epsilon^{\\mu\\nu\\rho\\lambda} F_{\\rho\\lambda}\\).', 'theorem_1': 'the ModMax field (as the source of matter) with \\( F(R) \\) gravity. In four-dimensional spacetime, the action of \\( F(R) \\)-ModMax theory is given by \\[\\mathcal{I}_{F(R)} = \\frac{1}{16\\pi} \\int_{\\partial M} d^4 x \\sqrt{-g} \\left[ F(R) - 4\\mathcal{L} \\right],\\] The second term in the above action is devoted to the ModMax Lagrangian (\\( \\mathcal{L} \\)), which is defined \\[\\mathcal{L} = \\frac{1}{2} \\left( \\mathcal{S} \\cosh \\gamma - \\sqrt{\\mathcal{S}^2 + \\mathcal{P}^2} \\sinh \\gamma \\right),\\]'}	h(r) = \frac{-Q}{r}	理论交叉学科-电磁理论与量子电磁学-麦克斯韦场方程	66553	1	2
recuW6lRxSkllV	Question: Which processes determine the water vapor content in the tropical lower stratosphere in a first-order sense? A. The upper limit of saturated vapor pressure determined by the minimum temperature of the air mass B. The amount of water vapor injected by overshooting convection events C. Slow large-scale ascent above the level of zero radiative heating D. Water vapor produced by methane oxidation	"{'step_1': 'Thermodynamic constraints: Use Concept 1 (Clausius-Clapeyron relationship): The saturated water vapor pressure decreases rapidly with temperature; the lower the temperature, the less water vapor the air can hold. Use Concept 2 (the role of the cold-point tropopause): When an air mass rises past the point of the lowest temperature, it is ""freeze-dried,"" and the water vapor concentration is fixed at a very low value. It is concluded that the minimum temperature of the air mass determines the upper limit of the saturated water vapor pressure entering the stratosphere. This corresponds to option A.', 'step_2': 'Dynamic transport: Using Concept 3 (Zero-radiation heating altitude and large-scale ascent): Air rises slowly above the zero-radiation heating altitude and is transported to the stratosphere, thereby ensuring that the water vapor concentration set at the minimum temperature point can be retained and transported on a long-term, large-scale basis. Deduce the conclusion: The slow large-scale ascent above the zero-radiation heating altitude is a necessary process for maintaining and transferring the minimum temperature constraint. This corresponds to option C.', 'step_3': 'Verification and Exclusion: Using Concept 4 (Atmospheric Recorder Effect): Observations show that stratospheric water vapor exhibits stripes over time and altitude, which verifies that the minimum temperature signal is transmitted with fidelity, supporting the correctness of A and C. Using Concept 5 (Finiteness of Tropopause Penetration): Penetration events can indeed inject water vapor, but their impact is local and short-lived, and they cannot serve as the dominant factor for long-term averages. Therefore, B and D are excluded.'}"	"{'concept_1': 'Clausius–Clapeyron relationship: The saturation vapor pressure decreases rapidly with temperature, and a lower temperature means a lower upper limit of water vapor.', 'concept_2': 'The role of the cold-point tropopause: Air ascending into the stratosphere is constrained by the minimum temperature at the tropical tropopause, which sets the effective water vapor entry mixing ratio and thereby limits the amount of water vapor that can enter the stratosphere', 'concept_3': '""Tropical stratospheric diabatic ascent and large-scale mass transport: In tropical regions, air flows slowly rise to the stratosphere through diabatic circulation (or residual mean circulation). This ascending motion is a key component of the Brewer-Dobson circulation. Subsequently, the air flows are transported toward mid-high latitudes and sink in the stratosphere at mid-high latitudes, together forming a global-scale stratospheric mass cycle.""', 'concept_4': 'Atmospheric Recorder Effect: Water vapor in the stratosphere shows stripes over time and altitude, reflecting the influence of lower boundary conditions.', 'concept_5': 'Finiteness of convection breakthrough: Overshooting convection can inject water vapor into the stratosphere, but its impact is intermittent and localized, with the overall contribution remaining limited compared to large-scale transport'}"	AC	理论交叉学科-理论环境模型-大气环流模型	153916	3	4
recuVoimWGmeWz	Under conditions of neutral bicarbonate electrolyte and CO₂ reduction reaction (CO₂RR) with CO as the primary product, what is the overall reaction rate-determining step (RDS) on M–E diatomic site catalysts based on M–Nₓ centers adjacent to main-group element E (e.g., Te)?	"{'step_1': ""Under neutral bicarbonate (weakly alkaline) conditions, the CO2RR follows the 'CO pathway.' The fundamental stepwise process is: CO₂ + H* → COOH; COOH + H → CO + H₂O. Protons in the system are supplied by water dissociation (H₂O → H + OH*), coupling proton transfer with product formation."", 'step_2': 'The active site is an asymmetric diatomic site centered on M–Nₓ and adjacent to the main-group element Te (represented by Te–Cu M–E DAC). This topology breaks the single-center limitation structurally and electronically, enabling division of labor and cooperation.', 'step_3': ""The Te site preferentially adsorbs/activates CO₂, while the Cu site promotes H₂O dissociation to supply H* in situ. This synergy advances the continuous 'activation–proton supply–conversion' steps within the same microenvironment."", 'step_4': 'In this pathway, the *COOH → CO step involves simultaneous protonation of COOH and CO generation/desorption. It is most directly influenced by local proton supply capacity and interfacial kinetics. Since protons in the weakly alkaline system primarily originate from water dissociation, this step inherently tends to become the kinetic bottleneck.', 'step_5': ""The cooperative design (Te for CO₂ activation, Cu for H₂O dissociation) aims to accelerate proton transfer kinetics and alleviate limitations in preceding steps: Te handles initial CO₂ adsorption/activation, while Cu enhances in-situ H accessibility. Nevertheless, within this framework, the protonation/generation step converting 'activated *COOH' to 'CO' remains rate-determining for the overall process."", 'step_6': 'Conclusion: In the described M–E (diatomic site with Te) catalytic system and neutral bicarbonate electrolyte, the rate-determining step (RDS) of the overall reaction is *COOH → CO.'}"	"{'concept_1': 'General Criterion for CO₂ Activation – To activate CO₂, the catalytic site must enhance interaction with the CO₂ 2πu orbital and induce its bending from linear geometry; Fermi level elevation facilitates this coupling, thereby reducing the activation barrier.', 'concept_2': 'Intrinsic Inertness of CO₂ – CO₂ exhibits high C=O bond energy and weak initial adsorption/activation interactions.', 'concept_3': ""Universal Promotion of the 'Activation Step' by Adjacent Positively Charged/Electron-Donating Sites – Neighboring positively charged sites/electron donors (e.g., Te) elevate the Fermi level of the metal center and electrostatically stabilize the negatively charged O* of activated CO₂, thereby strengthening the 'metal center ↔ CO₂(2πu)' coupling."", 'concept_4': 'General Principle for Product Desorption and Selectivity – Weaker CO binding facilitates CO desorption and enhances CO selectivity.', 'concept_5': ""The protonation of CO₂ and the protonation of *COOH are influenced by the interfacial process 'water dissociation → proton transfer.'""}"	The conversion steps from *COOH to CO, not CO2 to *COOH	理论基础学科-理论化学-电化学理论	140549	2	5
recuVrxkfasBGy	How to quantitatively separate the local resistance of CO2 formed within the catalytic layer of the gas diffusion electrodedue to the ionomer properties using experimental methods?	{'step_1': 'Decompose total mass transfer resistance. According to Concept_1, the total CO₂ mass transfer resistance of the gas diffusion electrode (GDE) can be expressed as: Rtotal=RGDL+RMPL+RCLR Independent measurement of RGDLR and RMPL allows isolating the catalyst layer (CL) resistance.', 'step_2': 'Design comparative experiments to isolate ionomer effects. Two catalyst layers (Target CL and Blank CL) with identical morphology, thickness, and catalyst loading are prepared, differing only in ionomer content or type. This ensures that variations in resistance arise solely from ionomer properties.', 'step_3': 'Measure and calculate resistances. Using the limiting current method, obtain the total resistance Rtotal for each GDE and calculate the CL resistance: RCL,total=Rtotal−RGDL−RMPL The ionomer-induced local resistance is then determined as: RCL,ionomer=RCL,total−RCL,pore', 'step_4': 'Apply Fick’s diffusion law to quantify ionomer effects. According to Concept_2, the effective CO₂ diffusion coefficient is linked to the CL resistance.'}	{'concept_1': 'Decomposition of total mass transfer resistance. The total resistance can be regarded as the series effect of the GDL, MPL, and CL, satisfying a linear additive relationship.', 'concept_2': 'Fick’s diffusion law. It relates gas flux, concentration gradient, and the effective diffusion coefficient, serving as the core formula for deriving the diffusion coefficient from mass transfer resistance.'}	1. Express total resistance as Rtotal=RGDL+RMPL+RCLR, and measure RGDL and RMP independently to isolate CL resistance. 2. Fabricate two catalyst layers with identical morphology, thickness, and catalyst loading, varying only ionomer properties . 3. Use the limiting current method to measure Rtotal for each GDE and calculate the CL total resistance: RCL,total=Rtotal−RGDL−RMPL 4. Prepare a blank CL without ionomer and evaluate the ionomer-induced resistance: RCL,ionomer=RCL,total−RCL,pore 5. relate resistance and effective diffusion coefficient: Deff=LCL/RCL,total Compare Deff or RCL,ionomer across samples to quantify ionomer effects.	理论基础学科-理论化学-分子动力学理论	68795	3	2
recuVTLxQnuWDZ	In the context of binary error-correcting codes with noiseless feedback, where a sender (Alice) communicates a $k$-bit message to a receiver (Bob), what is the minimum number of feedback bits Bob must send to Alice for the code to achieve an error resilience of $\frac{1}{4} + \delta$ against an online adversarial error model, for any given $\delta > 0$?	"{'step_1': 'The problem is modeled using a complete graph $G$ where the $K=2^k$ vertices represent all possible messages Alice can send. This is based on Concept_1.', 'step_2': ""For the code to be resilient to $\\frac{1}{4}+\\delta$ errors, for any pair of distinct messages $(v_i, v_j)$, there must exist at least one feedback string $f$ from Bob that causes Alice's transmissions, $FC(v_i; f)$ and $FC(v_j; f)$, to have a large Hamming distance. If no such $f$ exists for a pair, an adversary can corrupt the communication so that Bob cannot distinguish between $v_i$ and $v_j$, as described in Concept_2."", 'step_3': 'We can represent the feedback strings as colors. Using Concept_3, we color the edge $(v_i, v_j)$ in the graph $G$ with the first feedback string $f$ that ensures the transmissions $FC(v_i;f)$ and $FC(v_j;f)$ are sufficiently separated. Let the total number of feedback bits be $\\zeta(FC)$, which means there are $l = 2^{\\zeta(FC)}$ possible feedback strings, or colors.', 'step_4': ""For a fixed feedback string $f$ (a single color), the set of Alice's possible transmissions $\\{FC(v;f) | v \\in \\{0,1\\}^k\\}$ forms a code. To distinguish any two messages with error resilience $\\frac{1}{4}+\\delta$, their transmissions must have a relative distance of at least $2(\\frac{1}{4}+\\delta) = \\frac{1}{2}+2\\delta$. According to Theorem_1 (the Plotkin Bound), any subset of messages where all pairs satisfy this distance condition must be small. Specifically, a monochromatic clique of size $t = \\lceil10/\\delta\\rceil$ is shown to be impossible."", 'step_5': 'Since every edge in the graph $G$ must be colored to ensure distinguishability, we have a complete graph $K_K$ (where $K=2^k$) whose edges are colored with $l$ colors, and there is no monochromatic clique of size $t = \\lceil10/\\delta\\rceil$.', 'step_6': 'This structure allows us to apply Ramsey theory. According to Theorem_2, the number of vertices $K$ must be less than the Ramsey number $r(t; l)$. Using the bound from the theorem, we have $K \\le r(t;l) \\le l^{lt}$.', 'step_7': ""Substituting $K=2^k$ and $t = \\lceil10/\\delta\\rceil$, we get $2^k \\le l^{l \\cdot \\lceil10/\\delta\\rceil}$. Taking the logarithm of both sides gives $k \\le l \\cdot \\lceil10/\\delta\\rceil \\cdot \\log l$. For this inequality to hold as $k$ grows, the number of colors $l$ must also grow. This relationship implies that $l$ must be at least $\\Omega_{\\delta}(\\frac{\\log K}{\\log\\log K})$, and therefore the number of feedback bits, $\\zeta(FC) = \\log l$, must be at least $\\Omega_{\\delta}(\\log K) = \\Omega_{\\delta}(k)$. *Correction based on the source's derivation which simplifies this relationship*: The final step in the paper's proof leads to $\\zeta(FC) = \\log l = \\Omega_{\\delta}(\\log k)$.""}"	{'concept_1': 'A graph-based model can be used to analyze communication protocols, where vertices represent all possible messages Alice might send.', 'concept_2': 'An adversarial attack can succeed if, for two potential messages from Alice ($v_i$ and $v_j$), the resulting transmissions are not sufficiently far apart in Hamming distance for any possible feedback string, making them indistinguishable to Bob within the given error tolerance.', 'concept_3': 'Edge coloring of a complete graph can represent the feedback mechanism, where the color of an edge corresponds to the specific feedback string that allows the two connected message vertices to be distinguished by the receiver.', 'theorem_1_plotkin_bound': 'The Plotkin Bound states that for any error-correcting code $C \\subseteq \\{0,1\\}^n$ of relative distance $d > \\frac{1}{2}$, the size of the code is bounded: $|C| \\le \\frac{2d}{2d-1}$. This implies that a set of messages, under a fixed feedback string, cannot all be far from each other if the set is too large.', 'theorem_2_multicolor_ramsey': 'The multicolor Ramsey number $r(t;l)$ is the smallest integer $N$ such that any $l$-coloring of the edges of a complete graph $K_N$ contains a monochromatic clique of size $t$ ($K_t$). A known upper bound is $r(t;l) \\le l^{lt}$.'}	$\Omega_{\delta}(\log k)$	理论工程学科-信息论和通信理论-信息论	728465	4	3
recuVVfazYiowy	G. Solomon and J. J. Stiffler introduced infinite families of codes that meet the Griesmer bound, known as Solomon-Stiffler codes. The Griesmer bound for a linear [n, k, d]_q code states that n ≥ ∑_{i=0}^{k-1}⌈d/q^i⌉. For a linear [n,k,d]_q code, the Griesmer defect is defined as g(C)=n-g_q(k,d), where g_q(k,d) = ∑_{i=0}^{k-1}⌈d/q^i⌉. A code that meets the Griesmer bound has a defect of zero and is called a Griesmer code. For the affine Solomon-Stiffler code C with parameters (k, u_1, ..., u_h) and a set of h linear subspaces S_1, ..., S_h of dimensions u_1, ..., u_h respectively, what is the upper bound on its Griesmer defect when the conditions u_1 = u_2 = ... = u_h and h ≤ q are met?	"{'step_1': 'The objective is to determine the upper bound on the Griesmer defect for a specific affine Solomon-Stiffler code. Using Concept 2, the defect is given by the formula g(C) = n - g_q(k,d), where g_q(k,d) = ∑_{i=0}^{k-1}⌈d/q^i⌉.', 'step_2': ""We first establish the parameters of the code based on the given conditions. From Concept 3 and the condition u_1 = u_2 = ... = u_h, the code's length is n = (q^k-1) - h(q^{u_1}-1). For the calculation of the Griesmer bound, we use the minimum distance lower bound from Concept 4, setting d = (q-1)(q^{k-1} - hq^{u_1-1})."", 'step_3': 'The Griesmer defect arises when the argument of the ceiling function in the summation for g_q(k,d) is not an integer, causing the sum to be smaller than n. We analyze the terms of the sum ∑_{i=0}^{k-1}⌈(q-1)(q^{k-1} - hq^{u_1-1})/q^i⌉ to find the source of this deviation.', 'step_4': ""The deviation is most significant when the denominator q^i is close to the terms in the numerator. Let's analyze the term at the index i = u_1. The term is ⌈(q-1)(q^{k-1} - hq^{u_1-1})/q^{u_1}⌉."", 'step_5': 'We simplify this expression: ⌈(q-1)(q^{k-1-u_1} - hq^{-1})⌉ = ⌈(q-1)q^{k-1-u_1} - h(q-1)/q⌉. Since (q-1)q^{k-1-u_1} is an integer, we can rewrite this as (q-1)q^{k-1-u_1} - ⌊h(q-1)/q⌋.', 'step_6': 'Now, we evaluate the floor function using the given condition h ≤ q. We have ⌊h(q-1)/q⌋ = ⌊hq - h/q⌋ = ⌊h - h/q⌋. Since 0 < h/q ≤ 1, the value of the floor function is exactly h-1.', 'step_7': 'This shows that the term at i=u_1 is smaller by h-1 than what would be expected from a simple distribution of terms. For other indices i > u_1, the term ⌊h(q-1)/q^{i-u_1+1}⌋ is zero because h < q ≤ q^{i-u_1+1}. Therefore, the primary contribution to the total defect comes from the term at i=u_1.'}"	{'concept_1': 'A lower bound on the length n of a linear [n, k, d]_q code given by n ≥ ∑_{i=0}^{k-1}⌈d/q^i⌉.', 'concept_2': 'The difference between the actual length of a code and its Griesmer bound, defined as g(C) = n - g_q(k,d).', 'concept_3': 'A linear code with generator matrix G formed by column vectors from a set S, where S is the set of all non-zero vectors in F_q^k after deleting all non-zero vectors in the union of h linear subspaces S_1, ..., S_h of dimensions u_1, ..., u_h.', 'concept_4': 'The minimum distance d of an affine Solomon-Stiffler code is at least (q-1)(q^{k-1} - ∑_{i=1}^{h}q^{u_i-1}).', 'concept_5': 'Properties of the ceiling function: ⌈x+y⌉ ≤ ⌈x⌉ + ⌈y⌉, ⌈x⌉ ≥ x.', 'concept_6': 'Summation formula: ∑_{i=0}^{k-1} x = kx.'}	h-1	理论工程学科-信息论和通信理论-信息传输的数学极限	112080	1	5
recuVVCTL6bNEt	What is the necessary and sufficient condition for the dynamical stability and convergence of the Approximate Message Passing (AMP) algorithm with non-separable multivariate nonlinearities?	"{'step_1': ""Define Objective and Identify Core Thesis: Deconstruct the prompt to confirm the required output: an Instruction, a mathematical Answer, and foundational Concepts. Perform a high-level skim of the paper's title, abstract, and introduction to identify its central thesis, which is establishing a stability criterion for a complex AMP algorithm."", 'step_2': 'Formulate the Central ""Instruction"": Synthesize the paper\'s primary contribution into a single, direct question that defines the problem it solves: ""What is the necessary and sufficient condition for the dynamical stability and convergence of AMP with non-separable multivariate nonlinearities?""', 'step_3': 'Locate the Mathematical ""Answer"": Find the explicit, concise answer to the Instruction within the paper\'s conclusion and supporting theorems. The direct answer is the condition: ρAT < 1.', 'step_4': 'Trace the Logical Chain to Identify Key Concepts: Identify the essential building blocks required to understand how the answer is derived: 1. AMP Algorithm Dynamics, 2. State Evolution, 3. Decoupling Principle, 4. AT Stability Condition (Theorem 1).', 'step_5': 'Synthesize the Structured Output: Assemble the extracted components into the requested formal structure, presenting the Instruction, followed by the Answer, and then the list of key Concepts/Theorems.'}"	"{'concept_1': ""Approximate Message Passing (AMP) Dynamics. A class of iterative algorithms designed to solve certain high-dimensional statistical problems, such as convex optimization. The algorithm's updates are defined by a set of equations that involve a 'denoiser' function, which can be a non-separable multivariate nonlinearity."", 'concept_2': ""State Evolution. A method used to analyze the asymptotic behavior of AMP algorithms. It replaces the complex, high-dimensional dynamics of the algorithm with a much simpler time evolution of a few deterministic parameters (often matrices called 'order parameters') that describe a single, representative node's stochastic process."", 'concept_3': 'Decoupling Principle. A key theoretical result stating that in the high-dimensional limit, the dynamics of individual nodes in an AMP algorithm asymptotically decouple. This allows the behavior of the entire system to be accurately described by the simpler state evolution equations.', 'theorem_1': 'AT Stability Condition. The stability of the AMP dynamics is determined by the spectral radius, ρAT, of a specific matrix derived from the state evolution equations. The condition ρAT < 1 is necessary and sufficient for the algorithm to converge to a stable fixed point. The quantity is defined as ρAT ≜ ρ(α(E[f′(Γ* ; Y) ⊗ f′(Γ* ; Y)]))'}"	ρAT < 1	理论工程学科-控制理论与控制工程-分布式通信	325210	1	4
recuVTSjBuSSDV	Within the framework of zero-error communication, the Shannon capacity of a graph, $\Theta(G)$, represents the asymptotic rate of distinguishable messages. When restricting the encoding and decoding process to be modeled by reversible deterministic finite automata (DFAs), a new quantity called the reversible capacity, $\Theta_{REV}(G)$, emerges. What is the lower bound for this reversible capacity in relation to the graph's Shannon capacity, $\Theta(G)$, and its number of vertices, $|G|$?	"{'step_1': 'We begin with the standard definition of graph capacity, $\\Theta(G)$, as per **Concept_1**. An equivalent perspective, using **Concept_2** and **Concept_3**, is to define it as the maximum growth rate over all distinguishable languages.', 'step_2': 'We introduce a restriction on the type of languages considered, focusing on those recognized by reversible DFAs, as defined in **Concept_4**. This leads to the definition of reversible capacity, $\\Theta_{REV}(G)$, as outlined in **Concept_5**.', 'step_3': 'To establish a lower bound on $\\Theta_{REV}(G)$, we construct a specific reversible language. We start with a maximum independent set of $G^n$, which contains $M=\\alpha(G^n)$ strings of length $n$, as described in **Concept_6**.', 'step_4': 'A simple DFA can recognize these $M$ strings, but it is not generally reversible. To enforce reversibility, we employ the strategy from **Concept_7**. After recognizing one of the $M$ strings, we append a suffix that uniquely identifies which of the $M$ strings was seen.', 'step_5': 'The suffix is constructed using the vertices of $G$ as an alphabet, forming a base-$|G|$ representation of the index of the string (from 1 to $M$). The length of this suffix will be $\\lceil \\log_{|G|}(M) \\rceil$. This process makes the overall language reversible because the unique suffix ensures a unique path back to the initial state.', 'step_6': ""We analyze this growth rate as $n \\to \\infty$. We know that for large $n$, $M = \\alpha(G^n) \\approx \\Theta(G)^n$. The total length becomes $n' \\approx n + \\log_{|G|}(\\Theta(G)^n) = n(1 + \\frac{\\log(\\Theta(G))}{\\log(|G|)})$."", 'step_7': 'Substituting these approximations into the growth rate expression gives: $\\Theta_{REV}(G) \\ge ( \\Theta(G)^n )^{1 / (n(1 + \\frac{\\log(\\Theta(G))}{\\log(|G|)}))}$ $\\Theta_{REV}(G) \\ge \\Theta(G)^{1 / (1 + \\frac{\\log(\\Theta(G))}{\\log(|G|)})}$ $\\Theta_{REV}(G) \\ge \\Theta(G)^{\\log(|G|)/(log(|G|)+log(\\Theta(G)))}$ This yields the final lower bound for the reversible capacity.'}"	{'concept_1': 'The Shannon capacity of a graph $G$, denoted $\\Theta(G)$, is defined as the limit $\\lim_{n\\rightarrow\\infty}\\sqrt[n]{\\alpha(G^{\\mathbb{\\otimes}n})}$, where $\\alpha(G)$ is the independence number of $G$ and $G^{\\mathbb{\\otimes}n}$ is the n-fold strong graph product.', 'concept_2': 'A language is a set of finite-length strings over an alphabet. The growth rate of a language $L$ is $\\limsup_{n\\rightarrow\\infty}|L(n)|^{1/n}$, where $|L(n)|$ is the number of words of length $n$.', 'concept_3': 'A language is distinguishable for a graph $G$ if for any two strings $x, y$ of equal length in the language, there exists an index $i$ such that $(x_i, y_i)$ is not an edge in $G$.', 'concept_4': 'A deterministic finite automaton (DFA) is reversible if for each symbol, no two distinct states transition to the same state. A language is reversible if a reversible DFA recognizes it.', 'concept_5': 'The reversible capacity of a graph, $\\Theta_{REV}(G)$, is the supremum of growth rates over all distinguishable languages for $G$ that are recognized by reversible DFAs.'}	$\Theta(G)^{\log(|G|)/(log(|G|)+log(\Theta(G)))}$	理论工程学科-信息论和通信理论-信息传输的数学极限与原理	62454	1	3
recuVriIMUUa4C	For a centered s-subgaussian distribution P—a class prized for its tail behavior but lacking uniform geometric guarantees like log-concavity—determine the tightest certifiable bound $B_m$ on its directional moments. Specifically, find the function $B_m$ such that for any even $m$, the polynomial $B_m^m ||v||_2^m - \mathbb{E}_{X \sim P}[\langle X, v \rangle^m]$ is provably non-negative via a Sum-of-Squares (SoS) certificate.	"{'step_1': 'The goal is to find the tightest function $B_m$ such that the polynomial $p(v) = B_m^m ||v||_2^m - \\mathbb{E}_{X \\sim P}[\\langle X, v \\rangle^m]$ is a Sum-of-Squares (SoS) for any s-subgaussian distribution P, as defined by its moment properties in Concept_1.', 'step_2': 'Instead of directly constructing an SoS proof, we employ Theorem_1 (the SoS-Duality principle). This powerful theorem allows us to reframe the problem: we must show that for any valid Concept_2 (pseudoexpectation) $\\tilde{\\mathbb{B}}$, the inequality $\\tilde{\\mathbb{B}}[p(v)] \\ge 0$ holds. This is equivalent to proving that $max_{\\tilde{\\mathbb{B}}} \\frac{\\tilde{\\mathbb{B}}[\\mathbb{E}[\\langle X, v \\rangle^m]]}{\\tilde{\\mathbb{B}}[||v||_2^m]} \\le B_m^m$.', 'step_3': 'We consider potential proof strategies. Theorem_2 (the KLS conjecture) offers strong certifiable bounds, but it applies specifically to distributions with strong geometric properties like log-concavity. The instruction explicitly states that the class of s-subgaussian distributions is broader and lacks this uniform geometric guarantee. Therefore, the KLS conjecture is not a suitable tool for this general problem and represents a distinct, inapplicable line of reasoning.', 'step_4': ""We proceed with a different strategy. The expression involving the population expectation $\\mathbb{E}[\\cdot]$ is difficult to handle directly within the pseudoexpectation framework. We first replace it with an empirical average over many i.i.d. samples from P. This results in a high-degree polynomial of the random samples, which is a non-linear stochastic process. To make this tractable, we apply Theorem_4 (the variational identity), which leverages a form of Hölder's inequality to transform the non-linear moment expression into the supremum of a new process that is now *linear* in the samples."", 'step_5': ""After linearization, we are left with the task of bounding the supremum of a *linear* process driven by *subgaussian* random variables. This is the precise setting where Theorem_3 (Talagrand's majorizing measures theorem) is applicable. This theorem lets us bound the expected supremum of our subgaussian process by the expected supremum of an equivalent process driven by standard Gaussian variables, introducing a scaling factor of $O(s\\sqrt{m})$."", 'step_6': ""The final step is to bound the new expression involving Gaussian variables. It is a known fact that Gaussian distributions themselves are certifiably subgaussian, and their corresponding bound is on the order of $(C'\\sqrt{m})^m$. By combining this base case with the scaling factor from Step_5, we arrive at an overall bound of $(O(s\\sqrt{m}))^m$. This implies that $B_m^m = (Cs\\sqrt{m})^m$, which yields the final answer $B_m = Cs\\sqrt{m}$.""}"	"{'concept_1': 'An s-subgaussian distribution has moment bounds $(\\mathbb{E}[|\\langle v, X \\rangle|^{k}])^{1/k} \\le Cs\\sqrt{k}||v||_{2}$, resembling a Gaussian but without assuming its specific density shape.', 'concept_2': 'A pseudoexpectation $\\tilde{\\mathbb{B}}$ is a linear map from polynomials to $\\mathbb{R}$ that acts as a test functional for non-negativity, satisfying $\\tilde{\\mathbb{B}}[1]=1$ and $\\tilde{\\mathbb{B}}[p^2] \\ge 0$.', 'theorem_1': 'The SoS-Duality principle provides an equivalence: a polynomial is an SoS if and only if it is non-negative under every pseudoexpectation.', 'theorem_2': ""The Kannan-Lovász-Simonovits (KLS) conjecture posits that the Cheeger constant of any isotropic log-concave distribution is universally bounded. If true, this provides a powerful link between a distribution's geometric structure and its concentration, implying strong certifiable moment bounds for that class."", 'theorem_3': ""Talagrand's majorizing measures theorem allows bounding the supremum of a linear subgaussian process by that of a standard Gaussian process, effectively allowing one to substitute a complex subgaussian structure with a simpler, canonical one for analysis."", 'theorem_4': ""A variational identity based on Hölder's inequality for pseudoexpectations can transform a high-degree, non-linear moment expression into the supremum of a new process that is linear in the underlying random variables.""}"	$B_m = Cs\sqrt{m}$, where C is a universal constant.	理论基础学科-理论计算机科学-算法理论	1148627	2	6
recuVUWWSTPcqx	What is the minimal length of a k-dimensional minimal rank metric code when the field extension degree is $m=3$? The minimal length is the minimum possible length `n` of a rank metric code `C` that has a dimension of `k`. A minimal code is a rank metric code where every 1-dimensional subspace is rank minimal.	"{'step_1': 'The problem asks for the minimal length of a minimal rank metric code, which is equivalent to finding $\\varpi_{\\mathbb{B}/\\mathbb{P}}(k, 1)$ according to **Theorem 4:**.', 'step_2': 'We first establish a lower bound. **Theorem 3** provides a direct lower bound for the case where $m=3$, stating that $\\varpi_{\\mathbb{B}/\\mathbb{P}}(k, 1) \\ge 2k$.', 'step_3': 'Now, we must find an upper bound. The existence of a k-dimensional minimal code of a certain length `n` implies that $\\varpi_{\\mathbb{B}/\\mathbb{P}}(k, 1) \\le n$. The existence of r-minimal codes can be established using combinatorial arguments based on the frameworks described in **Theorem 1** and **Theorem 2**. These proofs, when applied to the specific case of rank metric codes, result in an upper bound on minimal length.', 'step_4': ""The paper's authors demonstrate that applying this general framework and the explicit bounds derived in subsequent sections leads to the specific formula for the minimal length of a k-dimensional minimal rank metric code when $m=3$. By combining the concepts and properties of r-minimal codes, and particularly the property of 1-minimality, one can demonstrate that an upper bound of $2k$ can be achieved."", 'step_5': 'The final conclusion that $\\varpi_{\\mathbb{B}/\\mathbb{P}}(k,1) = 2k$ is reached by showing that the derived upper bound is equal to the lower bound from **Theorem 3**. The **Concept 1** helps to understand the relationship between r-minimality and s-minimality, which is a key property used in the general framework. The model must synthesize this information to conclude the exact value.'}"	{'concept_1': 'Let $C$ be a (σ, r)-minimal code. Then for any $s \\in \\{0, 1, ..., r\\}$ it holds that C is (σ, s)-minimal.', 'theorem_1': 'Let $(k, r) \\in \\mathbb{N}^2$ with $r+1 \\le k \\le n$. There exists a k-dimensional (σ, r)-minimal code if a combinatorial condition based on q-binomial coefficients holds.', 'theorem_2': 'Let $r \\in \\{0, 1, ..., n-1\\}$. A sufficient condition for the existence of a k-dimensional (σ, r)-minimal code is that the number of non-(σ, r)-minimal codes of dimension $r+1$ is small enough.', 'theorem_3': 'If $m=3$ then for any $k \\in \\mathbb{N}$ with $k \\ge 2$, it holds that $\\varpi_{\\mathbb{B}/\\mathbb{P}}(k,1) \\ge 2k$.', 'theorem_4': 'For any $(k, r) \\in \\mathbb{N}^2$ with $k \\ge r+1$, $\\varpi_{\\mathbb{B}/\\mathbb{P}}(k, r)$ is the minimal length of a k-dimensional r-minimal code.'}	$2k$	理论工程学科-信息论和通信理论-信息传输的数学极限	181462	2	4
recuVGjEuxqSEc	For covert communication over an Additive White Gaussian Noise (AWGN) channel where the adversary's noise variance is greater than the receiver's ($\sigma_w^2 > \sigma_b^2$), what is the upper bound on the covert capacity, $C_{covert}$? The scheme relies on sub-exponential random slot selection, where the number of transmission slots $L_n$ grows sub-exponentially with the codeword length $n$, and the capacity is normalized as $C_{covert} = \lim_{n\rightarrow\infty}\frac{\log M}{\sqrt{n \log L_n}}$.	{'step_1': 'The objective is to determine the upper bound on the covert capacity, defined as $C_{covert} = \\lim_{n\\rightarrow\\infty}\\frac{log~M}{\\sqrt{n~log~L_{n}}}$, for an AWGN channel under the specified conditions. This requires finding the maximum number of messages, $M$, that can be sent reliably and covertly.', 'step_2': 'We begin by considering the covertness requirement. According to the **Covertness Constraint on Codeword Power (Concept_1)**, for a code to be undetectable, its codewords must not have excessively high power. This establishes a critical power threshold that any covert codebook must adhere to.', 'step_3': 'Leveraging this constraint, the principle of **Existence of a Dominant Low-Power Sub-codebook (Concept_2)** asserts that any valid covert codebook must contain a substantial fraction of codewords with power below this threshold. We can, therefore, analyze the capacity of this low-power sub-codebook, $\\mathcal{C}^{(l)}$, as its size is proportional to the total number of messages $M$.', 'step_4': 'The **Information Rate Bound for Reliable Communication (Theorem_1)** is applied to this sub-codebook $\\mathcal{C}^{(l)}$. This provides an upper bound on the number of codewords it can contain: $log|\\mathcal{C}^{(l)}| \\le n\\mathbb{I}(\\overline{X};\\overline{Y}) + \\text{vanishing error terms}$. As $log M$ scales with $log|\\mathcal{C}^{(l)}|$, we can bound $log M$ by bounding $n\\mathbb{I}(\\overline{X};\\overline{Y})$.', 'step_5': 'To bound the mutual information term, we use the **Mutual Information for an AWGN Channel (Theorem_2)**, which gives $\\mathbb{I}(\\overline{X};\\overline{Y}) \\le \\frac{1}{2}log(1+\\frac{\\overline{R}}{\\sigma_{b}^{2}})$, where $\\overline{R}$ is the average power per symbol for the codewords in $\\mathcal{C}^{(l)}$.', 'step_6': 'The average power per symbol, $\\overline{R}$, is determined by the power constraint on the sub-codebook from Concept_2. Specifically, $\\overline{R}$ is bounded by the maximum allowed power per symbol: $\\overline{R} \\le \\frac{1}{n} \\max_{x \\in C^{(l)}} ||x||_2^2 \\le \\frac{1}{n}\\sqrt{4\\sigma_{w}^{4}n\\log L_{n}} = 2\\sigma_w^2 \\sqrt{\\frac{\\log L_n}{n}}$. As $n \\to \\infty$, $\\overline{R} \\to 0$.', 'step_7': 'Since $\\overline{R}$ is small, we can apply **The Inequality $log(1+x) \\le x$ (Concept_3)** to simplify the mutual information bound: $\\mathbb{I}(\\overline{X};\\overline{Y}) \\le \\frac{\\overline{R}}{2\\sigma_{b}^{2}}$.', 'step_8': 'Substituting the bound on $\\overline{R}$ from Step_6 into the simplified mutual information bound from Step_7 yields: $\\mathbb{I}(\\overline{X};\\overline{Y}) \\le \\frac{1}{2\\sigma_{b}^{2}} \\left( 2\\sigma_w^2 \\sqrt{\\frac{\\log L_n}{n}} \\right) = \\frac{\\sigma_{w}^{2}}{\\sigma_{b}^{2}}\\sqrt{\\frac{log~L_{n}}{n}}$.', 'step_9': 'Finally, we combine this result with the rate bound from Step_4: $log M \\lesssim n \\cdot \\mathbb{I}(\\overline{X};\\overline{Y}) \\le n \\cdot \\left( \\frac{\\sigma_{w}^{2}}{\\sigma_{b}^{2}}\\sqrt{\\frac{log~L_{n}}{n}} \\right) = \\frac{\\sigma_{w}^{2}}{\\sigma_{b}^{2}}\\sqrt{n~log~L_{n}}$. By dividing by the normalization factor $\\sqrt{n \\log L_n}$ and taking the limit, we arrive at the upper bound for the covert capacity: $C_{covert} \\le \\frac{\\sigma_{w}^{2}}{\\sigma_{b}^{2}}$.'}	"{'concept_1': 'Codeword Power Constraint for Covertness. For a communication scheme to be covert, the total power of any transmitted codeword $x$, denoted $P=||x||_2^2$, must be bounded. Any code using codewords with power exceeding $\\sqrt{4\\sigma_{w}^{4}n\\log L_{n}}$ is not covert.', 'concept_2': 'Power per Symbol Calculation. The average power per symbol, $\\overline{R}$, is derived from the total power $P$ of a codeword of length $n$ using the formula $\\overline{R} = P/n$.', 'concept_3': 'The Inequality $log(1+x) \\le x$. For any value $x > 0$, the natural logarithm of $(1+x)$ is less than or equal to $x$. This is used to simplify the mutual information bound for low-power signals.', 'theorem_1': ""Information Rate Bound. The number of reliably decodable messages, $M$, is fundamentally limited by the channel's mutual information. This is expressed as $log M \\le n\\mathbb{I}(\\overline{X};\\overline{Y}) + \\text{vanishing terms}$."", 'theorem_2': ""Mutual Information for an AWGN Channel. The mutual information $\\mathbb{I}(\\overline{X};\\overline{Y})$ for an AWGN channel is upper-bounded by a function of the average input power per symbol, $\\overline{R}$, and the receiver's noise variance, $\\sigma_b^2$, according to the formula $\\mathbb{I}(\\overline{X};\\overline{Y}) \\le \\frac{1}{2}log(1+\\frac{\\overline{R}}{\\sigma_{b}^{2}})$.""}"	$\frac{\sigma_{w}^{2}}{\sigma_{b}^{2}}$	理论工程学科-信息论和通信理论-信道编码理论	1089619	2	4
recuVI2Ioe8GAr	Determine the asymptotic complexity, measured in the number of finite field multiplications, of the Improved Root-Finding (IRF) step in the proposed Algebraic Chase Decoding (ACD) of elliptic codes. The decoding parameters include the codeword length `n`, the code dimension `k`, and the number of least reliable symbols `η` used to formulate test vectors.	{'Step_1': 'The total complexity is the number of test vectors multiplied by the complexity of the IRF for a single test vector. From concept_4, the number of test vectors is $2^{\\eta}$.', 'Step_2': 'The per-vector IRF complexity has two main parts: finding error locations and finding error values (corrected symbols).', 'Step_3': 'To find error locations, we use the error locator polynomial (concept_1). According to concept_2, this polynomial has a degree of $O(n-k)$ and must be evaluated at $n$ distinct points. The complexity of this step is the number of points times the cost per evaluation, resulting in $O(n \\cdot (n-k))$.', 'Step_4': 'To find the corrected symbols at these locations, we turn to concept_4. There are $O(n-k)$ error positions (from theorem_1), and the computation for each costs $O(n)$. This also results in a complexity of $O((n-k) \\cdot n)$.', 'Step_5': 'The total complexity for one test vector is the sum of the complexities from Step_3 and Step_4: $O(n(n-k)) + O(n(n-k)) = O(n(n-k))$.', 'Step_6': 'Finally, we multiply the per-vector complexity by the total number of vectors to get the final asymptotic complexity: $2^{\\eta} \\times O(n(n-k)) = O(2^{\\eta}n(n-k))$.'}	{'concept_1': 'The interpolation polynomial for a test vector contains an error locator polynomial, whose zeros identify the potential error positions.', 'concept_2': 'The $w_1$-weighted degree of the error locator polynomial is bounded by the number of errors, $O(n-k)$. Identifying all potential error positions requires **evaluating this polynomial at all $n$ affine points** of the curve.', 'concept_3': 'For the $O(n-k)$ error positions found, the corrected codeword symbols are determined by evaluating derived numerator and determinant polynomials. This specific procedure has a complexity of **$O(n)$ for each error position**.', 'concept_4': 'The Algebraic Chase Decoding (ACD) algorithm formulates $2^{\\eta}$ distinct test vectors by systematically altering the $\\eta$ least reliable symbols of the received word. The root-finding process must be applied to each of these vectors.', 'theorem_1': 'The number of errors to correct in each test vector is bounded by $O(n-k)$.'}	$O(2^{\eta}n(n-k))$	理论工程学科-信息论和通信理论-信道编码理论	464352	2	3
recuVQm8cfkMt6	In the analysis of a one-dimensional pseudorapidity distribution, consider two symmetric reference rapidity windows \eta_{\text{ref}}^{A} and \eta_{\text{ref}}^{B}. The flow-specific complex harmonic-flow integral }Q_n(\eta_{\text{ref}})\triangleq\sum_{k\in\eta_{\text{ref}}} w_k\,e^{in\phi_k}, the flow-independent constant-weight integral }Q_0(\eta_{\text{ref}})\triangleq\sum_{k\in\eta_{\text{ref}}} w_k. Using correlations between these reference windows, construct the second-order cumulant }v_n\{2\}, Provide the unique expression in terms of the symbols above.	{'Step_1': 'Goal: state the unique closed-form expression for the second-order cumulant \\(v_n\\{2\\}\\) in one sentence.', 'Step_2': 'Ensemble definition (Concept_1): treat each event as a member of an ensemble and define the event-average \\[\\displaystyle \\langle\\mathcal{O}\\rangle=\\frac{1}{N_{\\text{ev}}}\\sum_{\\text{ev}}\\mathcal{O}.\\]', 'Step_3': 'Reference windows (Concept_2): identify two symmetric, disjoint pseudorapidity bins \\[\\displaystyle \\eta_{\\text{ref}}^{A}\\quad\\text{and}\\quad\\eta_{\\text{ref}}^{B}.\\]', 'Step_4': 'Flow-weighted integral (Concept_3): form the complex harmonic-flow integral \\[\\displaystyle Q_{n}(\\eta_{\\text{ref}})=\\sum_{k\\in\\eta_{\\text{ref}}}w_{k}\\,e^{in\\phi_{k}}.\\]', 'Step_5': 'Multiplicity integral (Concept_4): form the scalar multiplicity integral \\[\\displaystyle Q_{0}(\\eta_{\\text{ref}})=\\sum_{k\\in\\eta_{\\text{ref}}}w_{k}.\\]', 'Step_6': 'Cumulant definition (Concept_5): invoke the two-particle cumulant \\[\\displaystyle v_n\\{2\\}\\equiv\\sqrt{\\langle\\!\\langle e^{in(\\phi_{1}-\\phi_{2})}\\rangle\\!\\rangle}.\\]', 'Step_7': 'Cross-window correlation (Concept_6): express the two-particle average via cross-window correlations \\[\\displaystyle \\langle\\!\\langle e^{in(\\phi_{1}-\\phi_{2})}\\rangle\\!\\rangle =\\frac{\\langle Q_n(\\eta_{\\text{ref}}^{A})\\,Q_n^{*}(\\eta_{\\text{ref}}^{B})\\rangle}{\\langle Q_0(\\eta_{\\text{ref}}^{A})\\,Q_0^{*}(\\eta_{\\text{ref}}^{B})\\rangle}.\\]'}	{'Concept_1': 'Treat each event as a member of an ensemble and define the event-average \\[\\langle\\mathcal{O}\\rangle=\\frac{1}{N_{\\text{ev}}}\\sum_{\\text{ev}}\\mathcal{O}.\\]', 'Concept_2': 'Identify two symmetric reference rapidity windows \\[\\eta_{\\text{ref}}^{A}\\quad\\text{and}\\quad\\eta_{\\text{ref}}^{B}\\] as disjoint pseudorapidity bins.', 'Concept_3': 'Form the flow-weighted complex harmonic integral \\[Q_{n}(\\eta_{\\text{ref}})=\\sum_{k\\in\\eta_{\\text{ref}}}w_{k}\\,e^{in\\phi_{k}}.\\]', 'Concept_4': 'Form the scalar multiplicity integral \\[Q_{0}(\\eta_{\\text{ref}})=\\sum_{k\\in\\eta_{\\text{ref}}}w_{k}.\\]', 'Concept_5': 'Invoke the two-particle cumulant definition \\[v_n\\{2\\}\\equiv\\sqrt{\\langle\\!\\langle e^{in(\\phi_{1}-\\phi_{2})}\\rangle\\!\\rangle}.\\]', 'Concept_6': 'Express the two-particle average via cross-window correlations \\[\\langle\\!\\langle e^{in(\\phi_{1}-\\phi_{2})}\\rangle\\!\\rangle =\\frac{\\langle Q_n(\\eta_{\\text{ref}}^{A})\\,Q_n^{*}(\\eta_{\\text{ref}}^{B})\\rangle}{\\langle Q_0(\\eta_{\\text{ref}}^{A})\\,Q_0^{*}(\\eta_{\\text{ref}}^{B})\\rangle}.\\]'}	\[v_n\{2\}= \sqrt{\frac{\langle Q_n(\eta_{\text{ref}}^A) Q_n^*(\eta_{\text{ref}}^B)\rangle}{\langle Q_0(\eta_{\text{ref}}^A) Q_0^*(\eta_{\text{ref}}^B)\rangle}}\]	理论基础学科-理论物理-重离子碰撞	715950	2	4
recuVUm9f6jWTB	Determine the computational complexity, in terms of operations in the underlying finite field, for list decoding any algebraic geometry (AG) code using an instance of the Guruswami-Sudan (GS) list decoder. The decoding process is contingent upon an algorithmic framework that refines the interpolation step by constructing a polynomial matrix basis for the interpolant module that specifically possesses a small average column degree.	{'step_1': 'The objective is to determine the overall complexity of a refined Guruswami-Sudan list decoding algorithm. The algorithm consists of an interpolation step and a root-finding step (Concept_1). The primary improvement is made to the interpolation step.', 'step_2': 'The interpolation step is modeled as finding a small element in the interpolant module \\(\\mathcal{M}_{s,l,r}\\) (Concept_2). This is accomplished by first constructing a basis for the module and then performing a basis reduction.', 'step_3': 'A novel basis for \\(\\mathcal{M}_{s,l,r}\\) is chosen. This basis relies on specific polynomials \\(g_t(z)\\) that yield a special structure (Concept_3), which is instrumental for reducing computational complexity.', 'step_4': 'This special basis is represented as a block matrix \\(M_{s,l,r}\\) where all complex polynomial entries are concentrated in the first \\(\\mu s\\) columns (Concept_4). This structure is key to achieving a low average column degree.', 'step_5': 'A critical part of constructing this matrix is computing the submatrix \\(\\bar{R}\\) without letting its polynomial degrees grow excessively. This is achieved via an efficient iterative procedure involving matrix remainders (Concept_6), ensuring that the degree of \\(\\bar{R}\\) remains in \\(\\mathcal{O}(s(n+g)/\\mu)\\).', 'step_6': 'The resulting matrix \\(M_{s,l,r}\\) has a total column degree sum of \\(\\mathcal{O}(s^2(n+g))\\). The search for the desired low-degree interpolant polynomial is performed by computing the shifted Popov form of this matrix (Concept_5).', 'step_7': 'The computational cost for this shifted Popov form calculation directly yields the complexity of the improved interpolation step. Given the structure and degree properties of \\(M_{s,l,r}\\), this cost is \\(\\tilde{\\mathcal{O}}(s^{2}l^{\\omega-1}\\mu^{\\omega-1}(n+g)+l^{\\omega}\\mu^{\\omega})\\).', 'step_8': 'To establish the overall decoder complexity, the cost of the root-finding step is analyzed. It is shown to be \\(\\tilde{\\mathcal{O}}(sl\\mu^{\\omega-1}(n+g))\\) (Theorem_1).', 'step_9': 'Comparing the complexities of the two steps, the cost of the refined interpolation step is dominant. Therefore, the overall complexity of the faster list decoder is determined by the cost of the Popov form computation, which is \\(\\tilde{\\mathcal{O}}(s^{2}l^{\\omega-1}\\mu^{\\omega-1}(n+g)+l^{\\omega}\\mu^{\\omega})\\).'}	"{'concept_1': 'The Guruswami-Sudan list decoding algorithm is composed of two primary stages: an interpolation step to construct a polynomial \\(Q(z)\\) that satisfies certain vanishing conditions, and a root-finding step to identify the roots of \\(Q(z)\\), which correspond to the candidate messages.', 'concept_2': ""The set of all valid interpolating polynomials forms a module over a polynomial ring, denoted as the interpolant module \\(\\mathcal{M}_{s,l,r}\\). Finding the desired interpolant can be solved by finding a basis for this module and then searching for a 'small' element within it."", 'concept_3': 'A specific basis for the interpolant module \\(\\mathcal{M}_{s,l,r}\\) can be constructed using polynomials of the form \\(g_{t}(z)(z-R)^{s}\\). A key property of this construction is that the resulting polynomials in \\(z\\) have their non-zero coefficients restricted to the first \\(s\\) powers (\\(1, z, \\dots, z^{s-1}\\)), which is independent of the parameter \\(t\\) (for \\(s \\le t \\le l\\)).', 'concept_4': 'The basis derived from the specific polynomials in Concept_3 can be represented by a block matrix \\(M_{s,l,r} = \\begin{bmatrix} D & 0 \\\\ \\bar{R} & I \\end{bmatrix}\\), where the identity matrix \\(I\\) and zero matrix \\(0\\) form the rightmost columns. This structure confines all non-trivial polynomial entries to the first \\(\\mu s\\) columns.', 'concept_5': 'To find a small-degree interpolant polynomial from the module basis, one can compute its shifted Popov form. The computational complexity of calculating this form is highly sensitive to the sum of the column degrees of the input matrix.', 'concept_6': 'The submatrix \\(\\bar{R}\\) in the basis matrix \\(M_{s,l,r}\\) can be computed efficiently. This is achieved through an iterative method using polynomial matrix divisions with remainders, which controllably bounds the degree of \\(\\bar{R}\\) and avoids the explicit, high-cost computation of a larger intermediate matrix.', 'theorem_1': 'A refined complexity analysis of the root-finding step demonstrates that its cost is bounded by \\(\\tilde{\\mathcal{O}}(sl\\mu^{\\omega-1}(n+g))\\).'}"	\(\tilde{\mathcal{O}}(s^{2}l^{\omega-1}\mu^{\omega-1}(n+g)+l^{\omega}\mu^{\omega})\)	理论工程学科-信息论和通信理论-编码理论	195118	1	4
recuVIxwNr9GRp	Based on the provided description, the decryption complexity of the proposed Threshold Linearly Homomorphic Encryption (ThLHE) scheme is dominated by the threshold decryption of the outer Exp-ElGamal layer, followed by a computationally inexpensive local decryption of the inner lattice-based MeSA layer.	"{'step_1': 'Introduction to the Scheme and Overall Complexity: The Threshold Lattice-Exp-ElGamal scheme is proposed to resolve the efficiency-utility dilemma inherent in Threshold Linearly Homomorphic Encryption (ThLHE). Its primary contribution is achieving a quasi-linear decryption computation complexity of \\(O(N \\log N)\\) for participating parties. This marks a significant improvement over previous ThLHE schemes, which are typically encumbered by complexities of \\(O(N^2 \\log N)\\) or higher.', 'step_2': 'The Two-Layer Decryption Workflow: The decryption process is structured as a sequential two-layer workflow. Outer Layer (Exp-ElGamal Threshold Decryption): This layer first decrypts a small encryption key, denoted as \\(e_k\\), using the Exp-ElGamal threshold decryption protocol. This protocol is inherently efficient for distributed computation. Inner Layer (MeSA Lattice Decryption): Subsequently, the recovered key \\(e_k\\) is used to decrypt the primary lattice-based MeSA (Message-Space Adapter) ciphertext, \\(c t_{\\text{mesa}}\\), which contains the original large message.', 'step_3': ""Complexity Analysis of the Outer Layer: The computational cost of the outer layer is derived from two main operations. Modular Exponentiation: The Exp-ElGamal decryption requires modular exponentiation, which has a complexity of \\(O(\\log q)\\) for a modulus \\(q\\). In the threshold setting with \\(N\\) parties, this scales to \\(O(N \\log q)\\). Shamir's Secret Sharing (SSS) Reconstruction: The protocol employs SSS for private key distribution. Reconstructing the secret from a threshold of shares involves polynomial interpolation, which, when optimized with the Fast Fourier Transform (FFT), contributes a complexity of \\(O(N \\log N)\\). The total complexity for this layer is thus dominated by SSS reconstruction, resulting in \\(O(N \\log N)\\)."", 'step_4': ""Complexity Analysis of the Inner Layer: The inner layer's decryption complexity is associated with the lattice-based MeSA scheme. While decryption for lattice schemes like Ring-LWE is generally proportional to the lattice dimension, the MeSA is specifically designed to operate on a small key. This optimization reduces the complexity of lattice operations significantly. Consequently, the computational cost of recovering \\(e_k\\) from the lattice ciphertext is negligible compared to the outer layer, amounting to \\(O(\\log \\lambda)\\) for a security parameter \\(\\lambda\\), which is effectively constant for fixed parameters."", 'step_5': ""Combined Complexity and Architectural Innovations: The total decryption complexity is the sum of the costs of the outer and inner layers. Since the outer layer's complexity of \\(O(N \\log N)\\) is asymptotically dominant, the combined complexity is \\(O(N \\log N)\\). This efficiency is enabled by two key innovations: Message-Space Adapter (MeSA): This component decouples large-message encryption (handled by the lattice scheme) from small-key encryption (handled by Exp-ElGamal), thus preserving Exp-ElGamal's efficiency without being constrained by its small-message limitation. Dynamic Participant Sets: The scheme accommodates arbitrary thresholds and dynamic participant groups without incurring additional computational overhead."", 'step_6': ""Security Considerations and Practical Validation: The scheme presents a hybrid security model. The lattice-based MeSA offers post-quantum security, while the Exp-ElGamal component's security relies on the Computational Diffie-Hellman (CDH) assumption, which is vulnerable to quantum attacks. However, the architecture allows for upgrading the Exp-ElGamal component to a quantum-resistant alternative. The theoretical complexity analysis has been empirically verified through a prototype implementation. For \\(N=100\\) parties, the scheme demonstrated the predicted quasi-linear scaling and was approximately four times faster than a comparable threshold Paillier implementation.""}"	"{'concept_1': 'Threshold Linearly Homomorphic Encryption (ThLHE) is a type of encryption that allows for the evaluation of linear functions over encrypted data and distributes the secret key among N parties, requiring at least T parties to participate for a successful decryption.', 'concept_2': ""A Message-Space Adapter (MeSA) is a novel cryptographic primitive introduced in this work. It is defined as a specific type of symmetric (secret-key) encryption scheme engineered to function as a bridge between different message spaces in a hybrid homomorphic system. Its primary purpose is to enable a public-key encryption scheme, which is homomorphic only over a small message space, to effectively operate over a much larger target message space. The core of the MeSA primitive is a carefully structured algebraic relationship between its keys and plaintexts. Rather than being 'homomorphic in its keys,' the MeSA is required to possess a keyed homomorphic property. Specifically, let MeSA.Enc(ek,m) be the encryption of a message m under a key ek, and let MeSA.Dec(ek,c) be the corresponding decryption. For the MeSA to be effective, it must support a public operation, denoted by ⊞, on its ciphertexts such that for any two key-message pairs (ek1,m1) and (ek2,m2): MeSA.Dec(ek1+ek2,MeSA.Enc(ek1,m1)⊞MeSA.Enc(ek2,m2))=m1+m2. This property ensures that a homomorphic addition performed on the keys (which are encrypted by the outer scheme) translates directly into a corresponding homomorphic addition of the underlying messages. This allows the construction to leverage the homomorphic properties of the outer scheme on a small key space to induce a homomorphic property on the inner scheme over a large message space, without altering the outer scheme's structure."", 'concept_3': 'The MeSA-Assisted HE Construction is a method for building a new homomorphic encryption scheme with a target message space. It combines a base HE scheme (which encrypts the small keys of the MeSA) with a MeSA (which encrypts the actual large messages). The final ciphertext contains both a MeSA ciphertext and a base HE ciphertext.', 'concept_4': 'Exponential ElGamal (Exp-ElGamal) is a linearly homomorphic encryption scheme whose threshold version has an efficient, quasi-linear decryption complexity of O(NlogN). However, its practicality is limited because its decryption requires solving a discrete logarithm, restricting it to a small message space.'}"	O(NlogN)	理论基础学科-理论计算机科学-计算机密码学	20430	1	4
recuVMzq9clILU	What is the lowest possible sub-packetization level for linear Minimum Storage Regenerating (MSR) codes that have the help-by-transfer property? The help-by-transfer property means helper nodes only access the symbols they transmit during repair.	"{'step_1': 'The instruction asks to determine the minimum possible sub-packetization level, denoted as $l$, for a specific category of codes: linear Minimum Storage Regenerating (MSR) codes with the help-by-transfer property.', 'step_2': 'First, we define the key terms using the provided concepts. An MSR code is a type of MDS array code that is optimized to minimize the amount of data downloaded when repairing a single failed node (concept_1). The sub-packetization level, $l$, is the size of the vector stored at each node (concept_2).', 'step_3': ""The instruction adds a crucial constraint: the code must have the 'help-by-transfer' property. This means that during repair, a helper node only needs to read the exact symbols it is going to transmit, without performing any computation on other stored data (concept_3). This property is highly desirable for practical systems but places significant constraints on the code's internal structure."", 'step_4': ""The problem of finding the 'lowest possible' sub-packetization level for codes with these properties translates to finding the theoretical lower bound on $l$. This bound represents the best-case scenario that any code construction meeting these criteria can achieve."", 'step_5': 'A key result in the field, as stated in theorem_1, provides this exact lower bound. The theorem establishes that for any linear MSR code satisfying the help-by-transfer property, the sub-packetization level $l$ must be at least $s^{\\lceil n/r \\rceil}$, where $s=d-k+1$ and $r=n-k$.', 'step_6': 'Therefore, the lowest possible sub-packetization level is given by this bound, which is $l = s^{\\lceil n/r \\rceil}$.'}"	{'concept_1': 'Minimum storage regenerating (MSR) codes are a class of Maximum Distance Separable (MDS) array codes that can repair a single failed node by downloading the minimum amount of information from each of the helper nodes.', 'concept_2': 'Sub-packetization level is the length of the vector that each node stores, where a code symbol is a vector over some finite field.', 'concept_3': 'The help-by-transfer property means helper nodes access only the symbols they transmit during the repair process.', 'theorem_1': 'A lower bound on the sub-packetization level for linear MSR codes with the help-by-transfer property is given by $l=s^{\\lceil n/r \\rceil}$, where $s=d-k+1$ and $r=n-k$.'}	$l = s^{\lceil n/r \rceil}$	理论工程学科-信息论和通信理论-信道编码理论	116489	2	4
recuVIEEFtpBQb	Let $M$ be a smooth Riemannian manifold and $f:M\to M$ a $C^{1+\alpha}$ diffeomorphism on $M$ preserving an ergodic Borel probability measure $\mu$. For $x\in M$ and $\epsilon>0$, the Bowen ball is $B(x,n,\epsilon)=\{y\in M, d(f^ix,f^iy)<\epsilon, \, 0\ leq i<n\}$. The Brin-Katok entropy is defined by $\lim_{\epsilon\to 0}\limsup_{n\to\infty}\frac{1}{n}\mu(B(x,n,\epsilon))$, which equals the metric entropy $h_\mu(f)$ almost everywhere. Since entropy captures the exponential rate of orbit separation, it is natural to ask what information remains if we consider the radius at slower, subexponential shrinking rates. Thus, a natural question is: what happens if the radius of the Bowen ball $B(x,n,\epsilon_n)$ depends on $n$ at a subexponential scale? For example, do we have $\lim_{\epsilon\to 0}\limsup_{n\to\infty}-\frac{1}{n}\log(\mu(B(x,n,e^{-n\epsilon})))=\lim_{\epsilon\to 0}\limsup_{n\to\infty}-\frac{1}{n}\log(\mu(B(x,n,\epsilon)))$ for $\mu$-a.e. x?	{'step_1': 'Clearly, by their definitions, we have $\\lim_{\\epsilon\\to 0}\\limsup_{n\\to\\infty}-\\frac{1}{n}\\log(\\mu(B(x,n,e^{-n\\epsilon})))\\geq\\lim_{\\epsilon\\to 0}\\limsup_{n\\to\\infty}-\\frac{1}{n}\\log(\\mu(B(x,n,\\epsilon)))$. Thus, we will prove the other direction in the following.', 'step_2': 'We proceed by contradiction and assume that there exists $ \\lambda\\in(0,\\min\\{\\chi^3,\\frac{1}{2}\\})$ s.t $\\E_\\mu\\geq \\h_\\mu+2\\lambda$ $\\mu$-a.e by the ergodicity of $\\mu$.', 'step_3': 'There exists $0<r\\leq \\frac{\\min\\{1,\\chi_0\\}}{3d}\\frac{\\lambda^4}{4}$ s.t $\\mu(A_1)\\geq 1-\\lambda^3$ where $A_1:=\\{x\\in A:\\limsup\\frac{-1}{n}\\log\\mu(B(x,n,e^{-rn}))>h_\\mu^\\mathrm{BK}(x)+\\frac{7\\lambda}{8}\\}$. We can then choose $0<\\tau\\leq\\min\\{\\frac{r}{400}\\}$ and $\\ell\\in\\mathbb{N}$ s.t $\\mu(A_2)\\geq 1-2\\lambda^4$ where $$A_2:=\\{x\\in A_1\\text{ Lyapunov reg.}: x\\in \\Lambda_\\ell^{(\\underline{\\chi}(x),\\tau)}\\}.$$', 'step_4': '$\\mu(\\{x:\\mu_x(A_2)\\geq 1-\\sqrt{2\\lambda^4}\\})\\geq 1-\\sqrt{2\\lambda^4}$. Then $\\mu(\\{x\\in A_2: \\mu_x(A_2)\\geq 1-\\sqrt{2}\\lambda^2\\})\\geq 1-2\\lambda^2$. So by the ergodic theorem $\\exists n_0\\in\\mathbb{N}$ s.t $\\mu(A_3)\\geq 1-3\\lambda^2$ where $$A_3:=\\left\\{x\\in A_2: \\forall n\\geq n_0 \\exists j\\in[n(1+4\\lambda^2),n(1+8\\lambda^2)]\\text{ s.t }f^j(x)\\in A_2 \\right\\}.$$', 'step_5': 'Let $K_\\tau\\subseteq A_3$ as in Lemma 3 s.t $\\mu(K_\\tau)>0$. Note that the restrictions on $K_\\tau$ in Lemma 3 are given by Lemma 2, which in turn are merely Brin-Katok estimates; which are inherited by subsets. Let $N\\geq n_0$ large, then for all $x\\in K_\\tau$ set $$n_x^N:=\\min\\{n\\geq N: \\frac{-1}{n}\\log\\mu(B(x,n,e^{-rn}))>h+\\frac{6\\lambda}{8}\\}.$$ For all $n\\geq N$, set $K_n:=\\{x\\in K_\\tau:n_x^N=n\\}$.', 'step_6': 'By Lemma 3, we can cover $K_n$ with a cover whose cardinality is less or equal to $e^{n(1+ 8\\lambda^2)(h+ \\frac{\\lambda}{8}+3dr)}$, of exponential Bowen balls of the form $B(x,n,e^{-nr})$, $x\\in K_n$. Hence, we have $\\mu(K_n)\\leq e^{n(1+8\\lambda^2)(h+\\frac{\\lambda}{8}+3dr)} \\cdot e^{-n(h+\\frac{6\\lambda}{8})}$. Then, we conclude $$0<\\mu(K_\\tau)\\leq \\sum_{n\\geq N} e^{-n\\frac{\\lambda}{8}}\\xrightarrow[]{N\\to\\infty}0$$, a contradiction! Hence, we get $\\lim_{\\epsilon\\to 0}\\limsup_{n\\to\\infty}\\frac{1}{n}-\\log(\\mu(B(x,n,e^{-n\\epsilon})))\\leq\\lim_{\\epsilon\\to 0}\\limsup_{n\\to\\infty}\\frac{1}{n}\\log(\\mu(B(x,n,\\epsilon)))$. We finish the proof.'}	"{'concept_1': 'Let $\\mu$ be an $ f $-invariant ergodic probability measure which admits at least one positive Lyapunov exponent. Denote by \n\\begin{align*}\n\\underline{\\chi}(\\mu)=(\\chi_1,m_1,\\ldots ,\\chi_{\\ell_{\\underline{\\chi}}},m_{\\ell_{\\underline{\\chi}}})\n\\end{align*}\nthe positive Lyapunov exponents, and dimension of the corresponding Oseledec subspaces, of $\\mu$ in a decreasing order.', 'concept_2': 'Let $ 0<\\tau \\leqslant \\tau_{\\underline{\\chi}}(x):=\\frac{1}{100m} \\min \\left\\{\\chi_{\\ell_{\\underline{\\chi}}}(x), \\chi_{i+1}(x)-\\chi_{i}(x): i \\leqslant \\ell_{\\underline{\\chi}}(x)-1\\right\\} $, and let $ C_{\\underline{\\chi}, \\tau}(\\cdot) $ be the Lyapunov change of coordinates for points in\n\\[\n\\mathrm{LR}_{\\underline{\\chi}}=\\left\\{\\right. \\text{Lyapunov regular points with an index} \\left.\\tau_{\\underline{\\chi}}\\right\\}. \n\\]', 'concept_3': 'Let $ \\mathrm{PR}_{\\underline{\\chi}}=\\left\\{x \\in \\mathrm{LR}_{\\underline{\\chi}}: \\varlimsup \\limits_{n \\to \\pm \\infty} \\frac{1}{n} \\log \\left\\|C_{\\underline{\\chi}, \\tau}^{-1}\\left(f^{n}(x)\\right)\\right\\|=0, \\forall\\, 0<\\tau \\leqslant \\tau_{\\chi}\\right\\} $, the set of $ \\underline{\\chi} $-Pesin regular points which carries $ \\mu $. We call $\\mathrm{PR}:=\\bigcup_{\\underline{\\chi}} \\mathrm{PR}_{\\underline{\\chi}} $ the set of $ \\mathrm{Pesin}$ regular points.', 'concept_4': 'A Pesin block $ \\Lambda^{(\\underline{\\chi}, \\tau)}_l $ is a subset of $ \\bigcup_{\\left|\\underline{\\chi^\\prime}-\\underline{\\chi}\\right|_{\\infty}<\\tau} \\mathrm{PR}_{\\chi^\\prime}$ which is a level set $ \\left[q_{\\tau} \\geqslant \\frac{1}{l}\\right] $ of a measurable function $ q_{\\tau}: \\bigcup_{\\left|\\chi^\\prime-\\underline{\\chi}\\right|_{\\infty}<\\tau} \\mathrm{PR}_{\\underline{\\chi}^\\prime} \\rightarrow(0,1)$, such that (a) $e^{-\\tau}\\leq \\frac{q_{\\tau} \\circ f}{q_{\\tau}}\\leq e^{ \\tau} $, (b) $ q_{\\tau}(\\cdot) \\leqslant \\frac{1}{\\left\\|C_{\\underline{\\chi}, \\tau}^{-1}(\\cdot)\\right\\|^\\frac{m}{\\alpha}} $. Often, we omit the subscript $l$ when its dependence on the context is clear.', 'lemma_2': 'Let $\\mu$ be an ergodic $f$-invariant Borel probability, and $K$ be a $\\mu$-positive measure set, and let $\\delta>0$. Assume that $\\exists \\Delta>0$ such that the metric entropy $h_\\mu(f)$ satisfying $\\h_\\mu(f)\\leq h+\\Delta$ for $\\mu$-a.e $x\\in K$. Then for all $n$ large enough w.r.t $\\delta$ and $K$, there exist a measurable subset $K_\\delta\\subseteq K$ and a subset $\\mathcal{A}_{n,\\delta}$ and $0<\\rho\\leq \\delta$ s.t \n\\begin{enumerate}\n\\item $\\bigcup_{x\\in\\mathcal{A}_{n,\\delta}} B(x,n,\\rho)\\supseteq K_\\delta$,\n\\item $\\frac{\\mu(K_{\\delta})}{\\mu(K)}\\geq e^{-\\delta}$,\n\\item $\\#\\mathcal{A}_{n,\\delta}\\leq e^{n(h+\\Delta+\\delta)}$.\n\\end{enumerate}', 'lemma_3': ""As a corollary of Lemma 2. Let $\\chi_0>0$, $\\tau\\in (0,\\frac{\\chi_0}{100d})$, $\\epsilon\\geq 4\\tau$ small (with respect to $\\chi_0$), $\\gamma\\geq \\sqrt\\epsilon$, $\\ell\\in\\mathbb{N}$, and let $n'$ s.t $\\mu(K)>0$ where\n\\begin{align}\nK\\subseteq\\Big\\{x\\text{ Lyap. reg. s.t }\\chi^u_{\\min}(x),\\chi^s_{\\min}(x)\\geq \\chi_0: &\\\\\n\\forall n\\geq n',& x\\in\\Lambda_\\ell^{(\\underline{\\chi}(x),\\tau)}\\cap\\Big(\\bigcup_{n(1+\\gamma)\\leq i\\leq n(1+2\\gamma)} f^{-i}[\\Lambda_\\ell^{(\\underline{\\chi}(x),\\tau)}]\\Big)\\Big\\}.\\nonumber\n\\end{align}\nAssume that $h_\\mu^\\mathrm{BK}\\leq h+\\Delta$ for $\\mu$-a.e $x\\in K$. Then there exists $K_\\tau\\subseteq K$ s.t $\\mu(K_\\tau)\\geq e^{-\\tau}\\mu(K)$ and for all $n$ large enough, $\\exists \\mathcal{A}_n\\subseteq K_\\tau$ s.t $\\bigcup_{x\\in\\mathcal{A}}B(x,n,e^{-\\epsilon n})\\supseteq K_\\tau$ and $\\#\\mathcal{A}_n\\leq e^{n(1+2\\gamma)(h+\\Delta+3d\\epsilon)}$.""}"	Yes. (A correct proof must explicitly include an estimate of how many different Bowen balls of the form $B(x,n,e^{-n\epsilon})$ a point of the covered set can belong to when such balls are used to cover the set. This bound is the key step that allows one to rigorously show that assuming the answer were no leads to a contradiction, and therefore the answer must be yes. By contrast, an incorrect argument would merely claim that, since the radius changes only subexponentially, one can treat $B(x,n,\epsilon)$ and $B(x,n,e^{-n\epsilon})$ as equivalent after taking logarithms—such a simplification is not valid and does not yield the correct conclusion.)	理论基础学科-基础数学-动力系统	57484	3	3
recuVQ2NkU1rrx	Let $f$ be a $ C^{1+\alpha} $ diffeomorphism on a closed Riemannian manifold $M$ and $\mu $ an ergodic $ f$-invariant Borel probability measure on $ M $. Fix $r>0$, a natural question is whether the $r$-neutralized Bowen ball (see Definition 1) satisfies a density lemma with exponential error. More precisely, given a measurable set $ A $ with $\mu(A)>0 $, does it hold that for $ \mu $-almost every $ x \in A $, \[ \lim_{n \rightarrow \infty} -\frac{1}{n} \log \frac{\mu\left(B\left(x, n, e^{-nr}\right) \cap A\right)}{\mu\left(B\left(x, n, e^{-nr}\right)\right)}=0? \]	{'step_1': 'Suppose that the density property fails. Then there exists a constant $\\lambda>0$ and a subset $A_0\\subset A$ of positive measure such that for every $x\\in A_0$, the subexponential decay fails with rate at least $\\lambda$.', 'step_2': 'Using Pesin theory, we can choose a suitable Lyapunov block $\\Lambda^{(\\underline{\\chi},\\tau)}$ (with $\\tau$ sufficiently small compared to $\\lambda$ and $r$) so that a positive portion of $A_0$ is contained in a Pesin chart. Call this intersection $A^{(1)}$.', 'step_3': 'For points $x\\in A^{(1)}$, define the first time $n_x^N$ at which the failure occurs with rate at least $\\tfrac{3}{4}\\lambda$. This allows us to partition $A^{(1)}$ into measurable subsets $A_n^{(1)}$ according to this “bad return time.”', 'step_4': 'Applying the covering lemma from \\cite{ORH23}, we can cover each $A_n^{(1)}$ by $r$-neutralized Bowen balls with bounded multiplicity. By assumption, the measure of $A_n^{(1)}$ decays at least like $e^{-(5/8)\\lambda n}$. Summing over $n$, we find that $\\mu(A^{(1)})=0$, contradicting the choice of $A^{(1)}$ with positive measure. Hence, the contradiction shows that the density lemma with exponential error must hold.'}	{'concept_1': 'Definition 1 [$r$-neutralized Bowen ball] Let $(X,d)$ be a compact metric space, $f:X\\to X$ a homeomorphism. Given $n\\in \\mathbb{N}$, $r>0$ and $x\\in X$, the $r$-neutralized Bowen ball $B(x,n,e^{-nr})$ is defined by \\[ B\\left(x, n, e^{-nr}\\right):=\\left\\{y \\in X: d\\left(f^{i}(y), f^{i}(x)\\right) < e^{-nr}, \\,\\forall\\, 0 \\leq i < n \\right\\}. \\]', 'concept_2': 'Definition 2 [Pesin block] Let $\\mu$ be an $ f $-invariant ergodic probability measure which admits at least one positive Lyapunov exponent. Denote by \\[ \\underline{\\chi}(\\mu)=(\\chi_1,m_1,\\ldots ,\\chi_{\\ell_{\\underline{\\chi}}},m_{\\ell_{\\underline{\\chi}}}) \\] the positive Lyapunov exponents, and dimension of the corresponding Oseledec subspaces, of $\\mu$ in a decreasing order. (1) Let $ 0<\\tau \\leqslant \\tau_{\\underline{\\chi}}(x):=\\frac{1}{100m} \\min \\left\\{\\chi_{\\ell_{\\underline{\\chi}}}(x), \\chi_{i+1}(x)-\\chi_{i}(x): i \\leqslant \\ell_{\\underline{\\chi}}(x)-1\\right\\} $, and let $ C_{\\underline{\\chi}, \\tau}(\\cdot) $ be the Lyapunov change of coordinates (see \\cite{KH95-S}) for points in \\[ \\mathrm{LR}_{\\underline{\\chi}}=\\left\\{\\right. \\text{Lyapunov regular points with an index} \\left.\\tau_{\\underline{\\chi}}\\right\\}. \\] (2) Let $ \\mathrm{PR}_{\\underline{\\chi}}=\\left\\{x \\in \\mathrm{LR}_{\\underline{\\chi}}: \\varlimsup \\limits_{n \\to \\pm \\infty} \\frac{1}{n} \\log \\left\\|C_{\\underline{\\chi}, \\tau}^{-1}\\left(f^{n}(x)\\right)\\right\\|=0, \\forall\\, 0<\\tau \\leqslant \\tau_{\\chi}\\right\\} $, the set of $ \\underline{\\chi} $-Pesin regular points which carries $ \\mu $. We call $\\mathrm{PR}:=\\bigcup_{\\underline{\\chi}} \\mathrm{PR}_{\\underline{\\chi}} $ the set of $ \\mathrm{Pesin}$ regular points. (3) A Pesin block $ \\Lambda^{(\\underline{\\chi}, \\tau)}_l $ is a subset of $ \\bigcup_{\\left|\\underline{\\chi^\\prime}-\\underline{\\chi}\\right|_{\\infty}<\\tau} \\mathrm{PR}_{\\chi^\\prime}$ which is a level set $ \\left[q_{\\tau} \\geqslant \\frac{1}{l}\\right] $ of a measurable function $ q_{\\tau}: \\bigcup_{\\left|\\chi^\\prime-\\underline{\\chi}\\right|_{\\infty}<\\tau} \\mathrm{PR}_{\\underline{\\chi}^\\prime} \\rightarrow(0,1)$, such that (a) $e^{-\\tau}\\leq \\frac{q_{\\tau} \\circ f}{q_{\\tau}}\\leq e^{ \\tau} $, (b) $ q_{\\tau}(\\cdot) \\leqslant \\frac{1}{\\left\\|C_{\\underline{\\chi}, \\tau}^{-1}(\\cdot)\\right\\|^\\frac{m}{\\alpha}} $. Often, we omit the subscript $l$ when its dependence on the context is clear.', 'concept_3': 'Lemma 3 [Ben Ovadia-Rodriguez Hertz] (Besicovitch-Bowen covering lemma): Let $ \\Lambda^{(\\underline{\\chi}, \\tau)}$ $\\left(0<\\tau \\leqslant \\tau_{\\chi}\\right)$ be a Pesin block, and let $ B\\left(x_{0}\\right) $ be the Pesin chart of $x_{0} \\in \\Lambda^{(\\underline{\\chi}, \\tau)}$ for $ \\Lambda^{(\\underline{\\chi}, \\tau)} $. Let $ A \\subset \\Lambda^{(\\underline{\\chi}, \\tau)} \\cap B\\left(x_{0}\\right) $ be a measurable subset. Then $ A $ can be covered by $r$-neutralized Bowen balls $ B(\\cdot, n, e^{-nr}) $ of points in $ A $ with multiplicity bounded by $ e^{3 m \\tau n} $, where $ n $ is sufficiently large w.r.t $ \\Lambda^{\\left(\\underline{\\chi}, \\tau\\right)} $ and $ r \\geqslant 2 \\tau $.'}	Yes	理论基础学科-基础数学-动力系统	21485	2	2
recuWngldoBusH	Consider the nonreciprocal Cahn–Hilliard (NRCH) model, described by the equation \partial_t \phi \;=\; \nabla^2\!\left[\,(-1+i\alpha)\phi \;+\; |\phi|^2\phi \;-\; \nabla^2\phi\,\right], where \$\alpha>0\$ represents the nonreciprocity strength. We focus on axisymmetric defect solutions of the form \phi(r,t)=R(r)\,e^{\,i\,[\,m\theta+Z(r)-\omega t\,]}, \qquad m\in\{0,\pm1\}, which radiate outward traveling waves. The key problem is to determine the \emph{selected} far-field wave number k_{\infty} and amplitude R_{\infty} as explicit functions of the nonreciprocity parameter \$\alpha>0\$, with k_{\infty} obtained via core–far-field matching analysis of the defect solution and the selection constant denoted by 𝐶. Furthermore, we ask for the maximal nonreciprocity α × beyond which such defect solutions cease to exist, and the stability-corrected bound obtained from the Eckhaus criterion.	"{'step 1': 'Governing equation. Start with the NRCH model: \\partial_t \\phi = \\nabla^2\\!\\left[\\,(-1+i\\alpha)\\phi + |\\phi|^2\\phi - \\nabla^2\\phi\\,\\right], \\qquad \\alpha>0.', 'step 2': 'Plane-wave branch. Admissible far-field states are: \\phi(\\mathbf r,t)=R\\,e^{\\,i(\\mathbf k\\cdot\\mathbf r-\\omega t)}, \\qquad R=\\sqrt{1-k^2}, \\quad \\omega=\\alpha k^2, \\quad k<1.', 'step 3': ""Defect ansatz. Axisymmetric defects are written as: \\phi(r,t)=R(r)\\,e^{\\,i[m\\theta+Z(r)-\\omega t]}, \\qquad m\\in\\{0,\\pm1\\}, \\quad k(r)=Z'(r)."", 'step 4': 'Far-field matching. Require convergence to a plane-wave solution: k(r)\\to k_\\infty, \\quad R(r)\\to R_\\infty=\\sqrt{1-k_\\infty^2}, \\quad \\omega=\\alpha k_\\infty^2.', 'step 5': 'Selection constant 𝐶. By integrating the radial ODEs with core boundary data and matching to far-field asymptotics, the selected scaling is obtained: k_\\infty = C\\sqrt{\\alpha}, \\qquad R_\\infty=\\sqrt{1-C^2\\alpha}. with numerical values C\\approx0.76 \\; (\\text{spirals}), \\quad C\\approx0.70 \\; (\\text{targets}).', 'step 6': 'Existence and stability thresholds. The amplitude vanishes at \\alpha_\\times = \\frac{1}{C^2}. Eckhaus stability requires k_\\infty < \\tfrac{1}{\\sqrt{3}} \\;\\;\\Rightarrow\\;\\; \\alpha_\\times^{(\\mathrm{Eckhaus})} = \\frac{1}{3C^2}.'}"	"{'concept_1': 'Nonreciprocal Cahn–Hilliard Governing Equation (NRCH) The NRCH equation serves as the fundamental model, describing conserved fields with nonreciprocal coupling. It is written as: \\partial_t \\phi \\;=\\; \\nabla^2\\!\\left[\\,(-1+i\\alpha)\\phi \\;+\\; |\\phi|^2\\phi \\;-\\; \\nabla^2\\phi\\,\\right], \\qquad \\alpha > 0. Here, \\$\\phi = \\phi_1 + i\\phi_2\\$ is the order parameter and \\$\\alpha\\$ measures the strength of nonreciprocity.', 'concept_2': ""Defect Ansatz and Far-Field Matching Axisymmetric defect solutions are introduced via the ansatz: \\phi(r,t) \\;=\\; R(r)\\, e^{\\,i[m\\theta + Z(r) - \\omega t]}, \\qquad m \\in \\{0, \\pm 1\\}, \\quad k(r)\\equiv Z'(r). In the far-field limit, these solutions must converge to a plane-wave state: k(r)\\to k_\\infty, \\quad R(r)\\to R_\\infty=\\sqrt{\\,1-k_\\infty^2\\,}, \\quad \\omega = \\alpha k_\\infty^2. This matching condition selects the unique wavenumber and amplitude: k_\\infty = C \\sqrt{\\alpha}, \\qquad R_\\infty = \\sqrt{1 - C^2\\alpha}."", 'concept_3': 'Eckhaus Stability Criterion k^{2} < \\frac{1}{3\\gamma}, where 𝑘 denotes the wavenumber of the periodic state and 𝛾 is the effective diffusion (or stiffness) coefficient in the amplitude equation.'}"	The selected far-field wave number and amplitude are determined uniquely by the nonreciprocity strength \$\alpha\$. The wavenumber grows proportionally to \sqrt{\alpha} with a constant prefactor \$C\$ that depends on the defect type, while the amplitude decreases accordingly: k_{\infty} = C \sqrt{\alpha}, \qquad R_{\infty} = \sqrt{1 - C^2 \alpha}. Defect solutions cease to exist once the far-field amplitude vanishes, which occurs at the maximal nonreciprocity threshold: \alpha_{\times} = \frac{1}{C^2}. However, far-field plane-wave stability requires that the wavenumber remain below the Eckhaus limit. This leads to a stricter, stability-corrected bound for \$\alpha\$: \alpha_{\times}^{(\mathrm{Eckhaus})} = \frac{1}{3C^2}. Here, \$C\$ is an \mathcal{O}(1) constant obtained numerically, with typical values \$C \approx 0.76\$ for spirals (\$m=\pm 1\$) and \$C \approx 0.70\$ for targets (\$m=0\$). In both cases, the group velocity of emitted waves is positive, consistent with outward propagation: v_g = 2 \alpha k_\infty > 0.	理论工程学科-系统科学与系统理论-复杂系统建模与仿真	155366	2	3
recuVV2Em25B7J	For a first-order, discrete-time linear system described by the state-space model $x(k+1) = ax(k) + w(k)$ and $z(k) = x(k) + v(k)$, where the process disturbance $w(k)$ and output noise $v(k)$ are unknown but bounded by $|w(k)| \le \delta_w$ and $|v(k)| \le \delta_v$ respectively. Given a uniform quantizer with $d$ thresholds, determine the optimal quantizer resolution $\Delta(k)$ that minimizes the radius of information (the worst-case radius of the state estimation uncertainty) at time $k$, under the condition that the predicted uncertainty is larger than the measurement noise effects, i.e., $a\rho(k-1) + \delta_w > \delta_v$.	"{'step_1': 'The objective is to find the optimal quantizer resolution $\\Delta(k)$ that minimizes the worst-case radius of the feasible state set, $\\rho(k) = \\text{rad}(\\Xi(k|k))$, for a first-order system (Concept_1) with bounded disturbances (Concept_2). This is formulated as a min-max optimization problem (Concept_6).', 'step_2': 'The quantizer parameters at time $k$ are adapted based on the predicted feasible state set $\\Xi(k|k-1)$. This set is an interval whose radius is known to be $a\\rho(k-1) + \\delta_w$ (Concept_5).', 'step_3': 'To solve the min-max problem, the thresholds of the uniform quantizer (Concept_3) must be positioned optimally relative to the predicted set $\\Xi(k|k-1)$. A geometric argument leads to the conclusion that the quantizer must be centered on this predicted interval. Therefore, the optimal quantizer center $\\tau_c(k)$ is set to the center of $\\Xi(k|k-1)$ (Theorem_1).', 'step_4': 'With $\\tau_c(k)$ fixed, the radius of the updated feasible set, $\\rho(k)$, is calculated for every possible quantized output $y(k) \\in \\{0, 1, ..., d\\}$. The analysis shows that $\\rho(k)$ is a function of the resolution $\\Delta(k)$, the known bounds $\\delta_w, \\delta_v$, the predicted radius $a\\rho(k-1) + \\delta_w$, and the specific output $y(k)$.', 'step_5': 'The next step is to find the supremum of $\\rho(k)$ over all possible outcomes of $y(k)$. This ""worst-case radius"" is a piecewise function of $\\Delta(k)$. The analysis in the thesis shows this function is composed of a decreasing part and an increasing part with respect to $\\Delta(k)$.', 'step_6': 'The optimal resolution $\\Delta(k)$ is the value that minimizes this worst-case radius. The minimum of the piecewise function is achieved at the point where the decreasing and increasing components are equal.', 'step_7': 'Solving for this intersection point yields the optimal resolution: $\\Delta(k) = \\frac{2}{d+1}(a\\rho(k-1) + \\delta_w - \\delta_v)$, which is the final answer.'}"	{'concept_1': 'The first-order system model is given by the equations $x(k+1) = ax(k) + w(k)$ for the state and $z(k) = x(k) + v(k)$ for the inaccessible output signal.', 'concept_2': 'The process disturbance $w(k)$ and output noise $v(k)$ are unknown-but-bounded (UBB), satisfying $|w(k)| \\le \\delta_w$ and $|v(k)| \\le \\delta_v$ for all $k \\ge 0$. ', 'concept_3': 'A uniform quantizer with $d$ thresholds is fully characterized by its center $\\tau_c$ and resolution $\\Delta$. The individual thresholds are defined as $\\tau_i = \\tau_c - (\\frac{d+1}{2} - i)\\Delta$ for $i=1,...,d$. ', 'concept_4': 'The feasible state set $\\Xi(k|k)$ for a first-order system is an interval, and its radius $\\rho(k)$ represents the maximum uncertainty associated with the state estimate.', 'concept_5': 'The predicted feasible state set at time $k$, denoted $\\Xi(k|k-1)$, is derived from the previous set $\\Xi(k-1|k-1)$. For the first-order system, its radius is $a\\rho(k-1) + \\delta_w$. ', 'concept_6': 'The optimal adaptive quantizer design aims to solve a min-max problem by selecting the quantizer parameter vector $\\theta(k) = [\\tau_c(k), \\Delta(k)]^T$ that minimizes the radius of the feasible state set under the worst-case realization of the quantized measurement $y(k)$. ', 'theorem_1': 'The optimal choice for the center of the quantizer range, $\\tau_c(k)$, must coincide with the center of the predicted feasible state set $\\Xi(k|k-1)$. '}	$\Delta(k) = \frac{2}{d+1}(a\rho(k-1) + \delta_w - \delta_v)$	理论工程学科-控制理论与控制工程-线性系统	180430	2	3
recuVqIrSxkqgn	In the context of worst-case analysis for the Generalized Trace Reconstruction problem with deletion probability δ = Ω(1/√n), a key step involves bounding the statistical indistinguishability of two specially constructed probability strings. This bound, in turn, determines the necessary sample complexity. What is this fundamental sample complexity lower bound?	{'step_1': 'The goal is to determine the sample complexity lower bound for the worst-case scenario of the Generalized Trace Reconstruction problem.', 'step_2': 'The proof strategy relies on constructing two specific strings, S_e and S_o, that are designed to be difficult to distinguish after the deletion process (Concept_2).', 'step_3': 'The distinguishability of these strings is measured by the total variation distance between their trace distributions. This distance is found by analyzing a complex alternating sum (Concept_4). While it is known that the average-case version of this problem is easy and requires only polynomial traces (Concept_3), this fact is not relevant for the worst-case analysis.', 'step_4': 'The key insight is to treat the alternating sum as a Fourier transform evaluated at a specific point, π (Concept_4).', 'step_5': 'To prove that this value is exponentially small, a technical argument is employed that bounds the decay of the Fourier transform. This is done by analyzing its moment generating function, which allows for tight tail bounds (Theorem_1).', 'step_6': 'This analysis reveals that the statistical distance between the trace distributions for S_e and S_o is e^{-Ω(√n)}.', 'step_7': 'The number of samples (traces) required to distinguish two hypotheses is inversely proportional to their statistical distance. Therefore, the sample complexity has a lower bound of e^{Ω(√n)}.'}	{'concept_1': 'The problem is to recover a string of probabilities S=p_1,...,p_n from traces, which are subsequences of binary strings sampled according to S.', 'concept_2': 'For the worst-case lower bound, two strings are constructed using scaled binomial probabilities, with one string non-zero on even indices (S_e) and the other on odd indices (S_o).', 'concept_3': 'For *randomly* chosen strings where each p_i is drawn uniformly from [0, 1], reconstruction is known to be efficient, requiring only a polynomial number of traces.', 'concept_4': 'The analysis of the worst-case strings S_e and S_o shows their statistical distance is governed by an alternating sum, which can be viewed as an evaluation of a Fourier transform at frequency π.', 'theorem_1': 'The decay of the Fourier transform in this problem can be bounded by analyzing its moment generating function, which provides a powerful way to establish tail bounds.'}	e^{Ω(√n)}	理论工程学科-信息论和通信理论-信息论	217603	2	5
recuVGpJupEwDa	"For a concatenated coding-based DNA storage system, what is the exact error exponent, normalized by the number of sequencing reads N, under the specific scaling regime where the number of reads is super-linear with respect to the number of unique molecules M (i.e., $N = \omega(M)$)? This system encodes a message into a multiset of M molecules, samples N molecules with replacement, and then decodes. The outer rate is defined as a fraction $R_0$ of the channel capacity. Hint: For the achievability proof, a sufficient condition for successful decoding exists. An error is guaranteed not to occur if the following conditions hold for small constants $\epsilon, \eta > 0$: (i) the number of ""undersampled"" molecules (those appearing less than $\frac{\eta N}{M}$ times) is small, (ii) the number of sequencing errors is less than $\epsilon\eta N$, and (iii) the codebook is constructed such that the intersection size between any two distinct codewords $A_i$ and $A_j$ is bounded, i.e., $|A_i \cap A_j| < (R_0 + \epsilon)M$."	"{'step_1': 'We first establish a limit on the best possible performance. For any coding scheme, the error probability $P_e$ must be greater than the probability of an ""outage"" event where the decoder receives ambiguous information. This event is modeled by observing too few distinct molecules.', 'step_2': 'Using Concept_1, we relate the error probability to $p(N, M, \\delta M)$ for any $\\delta < R_0$. We then apply the lower bound from Concept_2, which states that $P_e \\ge (1-o(1))\\delta^N$. This bound arises from considering the simple scenario where all N sampled molecules happen to fall within a small, pre-defined subset of size $\\delta M$.', 'step_3': 'By taking the logarithm, normalizing by N, and taking the limit, the inequality from Step 2 becomes $-\\frac{1}{N}\\log P_e \\le \\log\\frac{1}{\\delta} + o(1)$. Since this must hold for any $\\delta$ arbitrarily close to $R_0$, we conclude that the error exponent is upper bounded by $\\log\\frac{1}{R_0}$.', 'step_4': 'Next, we show that this exponent is achievable. We use a specific code construction and decoding rule. The existence of a suitable codebook with limited overlap between codewords is guaranteed by Theorem_1.', 'step_5': 'The success of the decoding process is guaranteed if a set of sufficient conditions are met. An error can only occur if at least one of these conditions fails. The analysis then focuses on bounding the probability of these failure events.', 'step_6': 'The dominant failure event is having a large number of ""undersampled"" molecules. The probability of this event is bounded using the Chernoff bound (Theorem_2). The bound shows that this failure probability decays exponentially in N.', 'step_7': 'The exponent of this decay is determined by the KL divergence (Concept_3). By carefully taking the limits of the parameters in the sufficient condition (letting $\\epsilon \\to 0$ and $\\eta \\to 0$), the exponent converges to $D(0 || 1-R_0)$, which equals $\\log\\frac{1}{R_0}$.', 'step_8': 'The achievability analysis demonstrates that an error exponent of $\\log\\frac{1}{R_0}$ is possible. Since the converse bound from Step 3 shows that no scheme can perform better, we conclude that the exact error exponent is $\\log\\frac{1}{R_0}$.'}"	"{'concept_1': 'The term $p(N, M, K)$ is defined as the probability of observing at most $K$ distinct molecules when $N$ molecules are sampled uniformly at random with replacement from a set of $M$ distinct molecules.', 'concept_2': 'A lower bound on the error probability $P_e$ can be established by considering a specific failure scenario. For any $\\delta < R_0$, the error probability is at least the probability of sampling molecules from a specific subset of size $\\delta M$, which is $\\delta^N$. Therefore, $P_e \\ge (1-o(1)) p(N, M, \\delta M) \\ge (1-o(1))\\delta^N$.', 'concept_3': ""The Kullback-Leibler (KL) divergence, denoted as $D(p' || p)$, measures the difference between two probability distributions. For Bernoulli distributions, the limit as $p'$ approaches 0 is $D(0 || p) = \\log\\frac{1}{1-p}$. In this context, the relevant form is $D(0 || 1-R_0) = \\log\\frac{1}{R_0}$."", 'theorem_1': 'There exists an index-based outer codebook of the required size such that the intersection between any two distinct codewords $A_i$ and $A_j$ is bounded by $|A_i \\cap A_j| < (R_0 + \\epsilon)M$ for any $\\epsilon > 0$. This ensures the third condition of Theorem_1 can be met.', 'theorem_2': 'The Chernoff bound provides an exponential bound on the tail probabilities of sums of independent random variables. It is used to show that the probability of having too many undersampled molecules decays exponentially with an exponent related to the KL divergence.'}"	$\log\frac{1}{R_0}$	理论工程学科-信息论和通信理论-信息论	970206	2	5
recuWgmGHIRbTi	Do intermittent fasting dietary rhythms promote or inhibit hair growth? Which key metabolite change is responsible for this effect?	{'step_1': 'According to concept 1, after periodic intermittent fasting, the body’s metabolic changes include reduced glucose metabolism along with increased fatty acid and ketone body metabolism.', 'step_2': 'Furthermore, concept 2 shows that excessive fatty acid metabolism in hair follicle stem cells enhances inflammatory responses, thereby causing stem cell damage.', 'step_3': 'Concept 3 demonstrates the relationship between metabolism and the hair follicle stem cell cycle, revealing that glycogen accumulation prolongs the anagen phase, whereas glycogen depletion promotes the onset of the catagen phase.', 'step_4': 'The decline in glycogen metabolism and the rise in fatty acid metabolism are consistent with the effects of periodic intermittent fasting, thus accelerating the transition of hair follicle stem cells into catagen and shortening the anagen phase.', 'step_5': 'Combining the inflammatory effects of fatty acid metabolism with the role of glycogen concentration in regulating the hair follicle growth cycle, it can be concluded that intermittent fasting exerts an inhibitory effect on hair growth.', 'step_6': 'Since glycogen is depleted under intermittent fasting conditions, the key metabolic change in this context is the increase in fatty acids.'}	{'concept_1': 'Metabolic changes of periodic intermittent fasting\nThis study systematically separated the effects of calorie restriction (CR) from those of fasting, revealing that the metabolic and anti-aging benefits of CR are not solely due to reduced caloric intake. Intermittent daily fasting is crucial for the key phenotypes induced by CR: only under conditions of prolonged fasting can CR significantly improve insulin sensitivity, alter fuel utilization patterns, increase fatty acid oxidation, and trigger molecular transcriptomic remodeling in the liver and adipose tissue associated with extended healthspan. Simply reducing calories without fasting cannot replicate these effects. Moreover, even without lowering total energy intake, imposing daily prolonged fasting alone can reproduce most of the metabolic and molecular features of CR, namely enhanced fatty acid levels and metabolism, along with suppressed glucose oxidation pathways.', 'concept_2': 'Fatty acid metabolism in hair follicles\nIn recent years, research has increasingly recognized that hair follicles not only depend on protein structures and signaling pathways but are also closely tied to lipid metabolism for their normal function. Multiple studies in mice and humans have confirmed that abnormal lipid metabolism can cause severe hair follicle damage and hair loss, including cicatricial alopecia. Specifically, these phenotypes may be mediated through several mechanisms: (1) lipid imbalance induces excessive activation of inflammatory signaling, leading to hair follicle stem cell damage; (2) metabolic stress in organelles such as the endoplasmic reticulum and mitochondria results in reactive oxygen species production and apoptosis; and (3) impaired lipid modifications disrupt key developmental signals such as Hedgehog and Wnt. In summary, abnormal lipid metabolism is a critical factor in hair follicle pathology, suggesting that targeting lipid metabolic pathways (e.g., PPARγ regulation or cholesterol synthesis inhibition) may provide new therapeutic approaches for hair disorders.', 'concept_3': 'Hair follicle stem cell growth cycle and metabolism\nThe hair follicle growth cycle can be divided into the anagen (growth), telogen (resting), and catagen (regression) phases. Under normal conditions, hair follicle metabolism primarily relies on glucose oxidation as the main energy source. During the anagen phase, hair follicles accumulate glycogen, which markedly decreases in the catagen phase. In ex vivo hair follicle culture experiments, using a glycogen phosphorylase (PYGL)–specific inhibitor blocked glycogen breakdown, thereby significantly promoting follicle elongation and hair shaft formation while delaying the onset of catagen. Notably, glucose metabolism in hair follicle stem cells did not increase under these conditions. These findings indicate that glycogen functions not only as an energy reserve but also as a key regulator of hair follicle growth and cycling. The study ultimately proposed that regulation of PYGL activity may represent a novel therapeutic target for hair growth disorders and alopecia-related diseases, and provides new directions for the application of hair follicle metabolism in regenerative dermatology.'}	Intermittent fasting inhibits hair growth, and the key metabolite is fatty acids	理论基础学科-生物学-代谢生物学	97333	1	3
recuUXYfgAFH3y	In the relativistic electromagnetic shock wave acceleration mechanism, what are the key physical quantities related to the maximum energy $E_{\text{max}}$ of protons, and what kind of quantitative scaling relationships exist between them?	"{'step_1': 'The shock wave, characterized by its high Lorentz factor $\\gamma_p$, compresses the background magnetic field $B_0$ due to the conservation of magnetic flux. This compression amplifies the magnetic field strength in the shock region, leading to $B_z \\propto \\gamma_p^2 B_0$. This establishes the initial $\\gamma_p^2$ dependence, providing the strong magnetic environment necessary for acceleration.', 'step_2': 'When we transform to the rest frame of the shock wave, the compressed magnetic field $B_z$ (which scales as $\\gamma_p^2$) induces a transverse electric field due to relativistic effects. The transformation yields $E_y = \\gamma_p v_p B_z \\approx \\gamma_p c B_z$ (since $v_p \\approx c$). Thus, $E_y \\propto \\gamma_p B_z \\propto \\gamma_p^3$. This electric field $E_y$ is the direct driving force that accelerates the protons.', 'step_3': 'Using Hamiltonian analysis, we solve the equations of motion for a proton in the combined $E_y$ and $B_z$ fields. This analysis reveals that the gain in transverse momentum $p_y$ scales as $p_y \\propto \\gamma_p^2$. Since energy is related to momentum, this implies that the energy gain from the acceleration process also scales with $\\gamma_p^2$.', 'step_4': 'The Lorentz factors are defined as follows: $\\gamma_p$: The Lorentz factor of the shock wave itself, which quantifies its relativistic motion and drives the field compression and transformation. $\\gamma_0$: The initial Lorentz factor of the proton, defined as $\\gamma_0 = \\sqrt{1 + p_{x0}^2}$, where $p_{x0}$ is the initial longitudinal momentum. This represents the initial energy state of the proton, and the final energy must depend on this seed energy.', 'step_5': ""The acceleration process terminates when the proton's transverse momentum $p_y$ first returns to zero ($p_y = 0$). Applying this condition to the solutions from the Hamiltonian analysis allows us to find the maximum energy $E_{\\text{max}}$. The combination of the $\\gamma_p^2$ scaling from the momentum gain and the dependence on the initial energy $\\gamma_0$ yields the final scaling relation: $E_{\\text{max}} \\propto \\gamma_p^2 \\gamma_0$.""}"	{'concept_1': 'Based on the conservation of magnetic flux, when the velocity of the magnetic field compression system is close to the speed of light, the compressed magnetic field expression is derived as $B = 2\\gamma^2 B_0$, where $\\gamma$ is the Lorentz factor and $B_0$ is the initial static magnetic field.', 'concept_2': 'In the rest frame of a moving test particle, the magnetic field (observed in another inertial frame) contributes to the formation of a transverse electric field via Lorentz transformations. This transverse electric field acts as a force that accelerates charged particles (e.g., test charges with charge q)—a phenomenon that aligns with the relativistic interpretation of electromagnetic fields, where electric and magnetic fields transform into each other across different inertial frames.', 'theorem_1': 'Using Hamiltonian analysis, we solve the equations of motion for a proton in the combined $E_y$ and $B_z$ fields. This analysis reveals that the gain in transverse momentum $p_y$ scales as $p_y \\propto \\gamma_p^2$. Since energy is related to momentum, this implies that the energy gain from the acceleration process also scales with $\\gamma_p^2$.', 'concept_3': '$\\gamma_p$: The Lorentz factor of the relativistic plasma. $\\gamma_0$: The initial Lorentz factor of protons, representing the initial energy condition of seed particles.', 'concept_4': 'When the transverse momentum of protons, their energy reaches the maximum value.'}	$E_{\text{max}}$, $\gamma_p$, $\gamma_0$ and $E_{\text{max}} \propto \gamma_p^2 \gamma_0$ ($\gamma_p$: The Lorentz factor of the shock wave $\gamma_0$: The initial Lorentz factor of the proton, defined as $\gamma_0 = \sqrt{1 + p_{x0}^2}$, where $p_{x0}$ is the initial longitudinal momentum. This represents the initial energy state of the proton, and the final energy must depend on this seed energy. $E_{\text{max}}$ is maximum energy of protons after acceleration)	理论交叉学科-理论天体物理-天文学	79886	3	7
recuVeVPPxlALG	In the deterministic LOCAL model of distributed computation, what is the time complexity for solving the Δ-coloring problem on constant-degree graphs by utilizing the novel technique of ruling subgraph families to break symmetry between locally extendable substructures, rather than relying on traditional ruling sets on power graphs? The Δ-coloring problem is to assign one of Δ colors to each vertex of a graph such that no two adjacent vertices share the same color, where Δ is the maximum degree of the graph.	"{'step_1': 'The goal is to determine the complexity of a deterministic Δ-coloring algorithm on constant-degree graphs using ruling subgraphs. The overall strategy is to identify easily colorable subgraphs, select a non-interfering subset of them, color the rest of the graph, and finally color the selected subgraphs.', 'step_2': 'First, the algorithm identifies a set of candidate subgraphs. Based on theorem_1, every node in the graph can find a Nice Locally Δ-extendable Component (NLAEC) within a distance of $O(log_{\\Delta}n)$. This ensures that the initial collection of NLAECs, denoted as family $\\mathcal{R}$, is dense enough to cover the entire graph.', 'step_3': ""The critical step is to select a subset of these NLAECs that do not interfere with each other. Instead of using a slow ruling set computation on a power graph, the algorithm employs the new concept of a Ruling Subgraph Family (concept_1). Using theorem_2, it computes a subfamily $\\mathcal{R}' \\subseteq \\mathcal{R}$ in $O(log~n~log^{*}n)$ rounds. This family $\\mathcal{R}'$ has the property that its members are far apart, yet every node in the graph is still relatively close to some member of $\\mathcal{R}'$."", 'step_4': ""With the ruling subgraph family $\\mathcal{R}'$ established, the nodes outside of these subgraphs ($V \\setminus \\cup_{H \\in \\mathcal{R}'} H$) are colored. These nodes are partitioned into layers based on their distance to the nearest subgraph in $\\mathcal{R}'$. The number of layers is bounded by the maximum distance of any node to $\\mathcal{R}'$, which is $O(log~n~log^{*}n)$ for constant Δ. Using a Layered Graph Coloring algorithm (concept_3), this step takes $O(log~n~log^{*}n)$ rounds."", 'step_5': ""After coloring the exterior, the algorithm colors the nodes within the NLAECs of $\\mathcal{R}'$. For each NLAEC, a Flexible Node (concept_4) is created, which is guaranteed to be possible for an NLAEC (concept_2). This step ensures that every node within the NLAEC has a valid color available from the set $\\{1, ..., \\Delta\\}$."", 'step_6': 'The remaining uncolored nodes inside the NLAECs are then colored using another layered coloring approach (theorem_3), this time with layers defined by the distance to the nearest flexible node. Since each NLAEC has a size of $O(log_{\\Delta}n)$, this step is very fast, taking $O(log^2_{\\Delta}n \\cdot log~n)$ rounds, which is dominated by the previous step.', 'step_7': 'By summing the complexities, the dominant step is the layered coloring of the nodes outside the ruling subgraphs, which takes $O(log~n~log^{*}n)$ rounds. The initial discovery of NLAECs and the computation of the ruling subgraph family also fit within this complexity. Therefore, the total time complexity is $O(log~n~log^{*}n)$.'}"	"{'concept_1': 'Ruling Subgraph Family. A subset of a given collection of subgraphs where the selected members are sufficiently far from each other, and every non-selected member is close to a selected member. This generalizes the concept of a ruling set from nodes to subgraphs.', 'concept_2': 'Nice Locally Δ-extendable Component (NLAEC). A subgraph that is either a node of degree less than Δ or a 2-connected graph with exactly two nodes of degree 3 and all other nodes of degree 2. These subgraphs are easy to color with Δ colors, even when constrained by an external partial coloring.', 'concept_3': 'Layered Graph Coloring. A technique for coloring a graph where nodes are partitioned into layers. The coloring proceeds layer by layer, and for each node, its color is chosen from a list that excludes the colors of its already-colored neighbors in preceding or same layers.', 'concept_4': 'Flexible Node. A node within a subgraph that has two neighbors colored with the same color. The presence of a flexible node guarantees that the node itself has at most Δ-1 distinct colors in its neighborhood, ensuring it can be colored with one of the Δ available colors.', 'theorem_1': 'Existence and Proximity of NLAECs. In a graph with maximum degree $\\Delta \\ge 3$, every node is within a distance of $O(log_{\\Delta}n)$ from an induced NLAEC of size at most $O(log_{\\Delta}n)$.', 'theorem_2': 'Fast Computation of Ruling Subgraph Families. A ruling subgraph family can be computed deterministically in $\\tilde{O}(log~n)$ rounds. Specifically, for a family of subgraphs of size $k \\in O(log_{\\Delta}n)$, a ruling subfamily can be computed in $O(log~n~log^{*}n)$ rounds such that every node in the graph is within distance $O(log_{\\Delta}n~log^{*}k)$ of a selected subgraph.', 'theorem_3': ""Coloring a Family of NLAECs. A family of NLAECs whose 'natural orchids' (a small neighborhood around a defined stem) do not overlap can be Δ-colored in $O(k + T_{layercol}(n, \\Delta, k))$ rounds, where k is the maximum size of an NLAEC and $T_{layercol}$ is the complexity of a layered coloring algorithm.""}"	$O(log~n~log^{*}n)$	理论基础学科-基础数学-几何，图论	199522	2	6
recuVOq4L169uf	What is the integer value of the Sylvester resultant for the polynomial system used to determine if a cyclic code of length $n=10$ and dimension $k=3$ is MDS? The non-MDS condition for this code, generated by $G_I$ with $I=\{0,2,3\}$, relies on the existence of a polynomial $1-(x_2t^2+x_3t^3)$ that divides $t^{10}-1$. This condition is equivalent to finding a common non-zero root for two specific polynomials, $f_{8}(y)$ and $f_{9}(y)$, derived from the coefficients of the polynomial division, where $y=x_2^3/x_3^2$.	{'step_1': 'The problem asks for the specific integer value of a Sylvester resultant, which is used to test the MDS property of a cyclic code with length $n=10$ and dimension $k=3$.', 'step_2': 'The code is MDS if and only if no polynomial of a specific form divides $t^{10}-1$ (Concepts 1 & 2). The non-MDS case hinges on finding a solution to a system of polynomial equations.', 'step_3': 'For $n=10$, this system is reduced to two polynomial equations in a single variable $y$: $f_8(y)=0$ and $f_9(y)=0$ (Concept_3).', 'step_4': 'The paper explicitly calculates these polynomials for $n=10$: $f_8(y) = y+3$ and $f_9(y) = 4y+1$.', 'step_5': 'To determine if these two polynomials have a common non-zero root over any field $\\mathbb{F}_p$ (where $p$ is a prime not dividing the resultant), we can compute their Sylvester resultant, an integer value independent of the field.', 'step_6': 'The Sylvester resultant is the determinant of the Sylvester matrix formed by the coefficients of the two polynomials (Concept_4). For the linear polynomials $f_8(y) = y+3$ and $f_9(y) = 4y+1$, the Sylvester matrix is a $2 \\times 2$ matrix.', 'step_7': 'The Sylvester matrix is constructed as follows: $\\begin{pmatrix} 1 & 3 \\\\ 4 & 1 \\end{pmatrix}$', 'step_8': 'This value, -11, is the Sylvester resultant. Since it is non-zero, the polynomials $f_8(y)$ and $f_9(y)$ have no common root in any field whose characteristic does not divide 11. This confirms the code is MDS under these conditions.'}	{'concept_1': 'A cyclic code of length $n$ and dimension $k$ over $\\mathbb{F}_q$ is a Maximum Distance Separable (MDS) code if and only if for any non-zero polynomial $f(t)$ with support in the defining set $I$ and degree at most $k$, the degree of $gcd(f(t), t^n-1)$ is less than or equal to $k-1$.', 'concept_2': 'For a cyclic code with dimension $k=3$ and defining set support $I=\\{0,2,3\\}$, the MDS property is violated if there exists a polynomial of the form $f(t) = 1 - (x_2t^2 + x_3t^3)$ with $x_3 \\neq 0$ that divides $t^n-1$.', 'concept_3': 'The condition that $1 - (x_2t^2 + x_3t^3)$ divides $t^n-1$ is equivalent to a system of polynomial equations in the variables $x_2$ and $x_3$. By substituting $y = x_2^3/x_3^2$, this system can be reduced to two polynomial equations in the single variable $y$, denoted as $f_{n-2}(y)=0$ and $f_{n-1}(y)=0$.', 'concept_4': 'The Sylvester resultant of two univariate polynomials, $f(x)$ and $g(x)$, denoted $Syl(f,g,x)$, is the determinant of their Sylvester matrix.'}	-11	理论工程学科-信息论和通信理论-信息论	750470	2	3
recuW6hTg9Womb	What relations are needed to establish the conversion between density and velocity under the assumption of zero pressure gradient within the boundary layer. Symbol description: The \( T_∞ \), \( u_∞ \), and \( Ma_∞ \) represent the freestream static temperature, freestream velocity, and freestream Mach number, respectively. \( T_w \) is the wall temperature. The recovery temperature is \( T_r \), and \( c_p \) is the specific heat at constant pressure. The recovery factor \( gamma \),and \( r \) is the specific heat ratio.	{'step_1': 'Clarify the zero pressure gradient condition, indicating that the pressure is uniform and constant within the boundary layer;', 'step_2': 'Use the ideal gas law to relate pressure, density, and temperature;', 'step_3': 'Apply the adiabatic Crocco-Busemann equation to relate temperature and velocity.'}	{'concept_1': 'adiabatic CroccoBusemann relation.'}	Relation 1: \frac{ T }{ T _ { ∞ } } = \frac{ T _ { w } }{ T _ { ∞ } } + \frac{ ( T _ { r } - T _ { w } ) }{ T _ { ∞ } } \frac{ u }{ u _ { ∞ } } - \gamma \frac{ r - 1 }{ 2 } Ma _ { ∞ } ^ { 2 } \left( \frac{ u }{ u _ { ∞ } } \right) ^ { 2 }; \( T_∞ \), \( u_∞ \), and \( Ma_∞ \) represent the freestream static temperature, freestream velocity, and freestream Mach number, respectively. \( T_w \) is the wall temperature. The recovery temperature is given by: \[ T_r = T_∞ + \frac{U_∞^2}{2c_p} \], where \( c_p \) is the specific heat at constant pressure. The recovery factor \( gamma = Pr^{1/3} \) (for turbulent flow), \( Pr \) is the Prandtl number, \( r \) is the specific heat ratio. Relation 2: \rho=P / R T; The P is pressure, R is gas constant, and T is temperature.	理论工程学科-理论工程力学-空气动力学理论	31729	1	1
recuWotjZ4PaiN	In an absorption system, at steady state and at unsteady state respectively, is the difference between the input entransy and the output entransy equal to the internal entransy dissipation?	{'step_1': 'Non-state character (Concept 1). Since entransy is not a thermodynamic state function, its balance for open/cyclic systems cannot, in general, collapse to a simple “in–out = dissipation” identity the way a conserved extensive state might; path/process features must appear explicitly.', 'step_2': 'Heat↔work conversion at different temperatures (Concept 2). An absorption cycle performs two conversions at distinct temperature levels (generator vs. absorber). The general entransy balance must therefore include a conversion-entransy term GconvG_{\\text{conv}}Gconv that accounts for the temperature-level dependence of the heat↔chemical-potential conversion.', 'step_3': 'Balance structure. For the cycle (control volume), Gin−Gout = Gdiss + Gconv + dGsysdt.G_{\\text{in}}-G_{\\text{out}} \\;=\\; G_{\\text{diss}} \\;+\\; G_{\\text{conv}} \\;+\\; \\frac{dG_{\\text{sys}}}{dt}.Gin−Gout=Gdiss+Gconv+dtdGsys. Because Gconv≠0G_{\\text{conv}}\\neq 0Gconv\ue020=0 for absorption cycles (two conversions at different TTT), the in–out difference cannot equal GdissG_{\\text{diss}}Gdiss in general.', 'step_4': 'Steady vs. unsteady. Steady state (dGsys/dt=0)(dG_{\\text{sys}}/dt=0)(dGsys/dt=0): Gin−Gout=Gdiss+Gconv≠GdissG_{\\text{in}}-G_{\\text{out}}=G_{\\text{diss}}+G_{\\text{conv}}\\neq G_{\\text{diss}}Gin−Gout=Gdiss+Gconv\ue020=Gdiss. Unsteady: the storage term dGsys/dtdG_{\\text{sys}}/dtdGsys/dt further prevents equality.', 'step_5': 'Edge case (why the confusion arises). Only in a pure heat-transfer element at one temperature level (no conversion, no storage) does Gconv=0G_{\\text{conv}}=0Gconv=0 and dGsys/dt=0dG_{\\text{sys}}/dt=0dGsys/dt=0, giving equality. Absorption cycles do not satisfy this, so the equality does not hold.'}	{'concept_1': 'entransy is not a thermodynamic state function.', 'concept_2': 'Entransy balances for cyclic heat–work conversion systems include an additional conversion-entransy contribution associated with heat↔work transfer at different temperature levels.'}	Under any condition, the difference between the input entransy and output entransy of absorption systems is not equal to the internal entransy dissipation.	其他理论学科-理论热力学与统计物理-热力学定律	41969	1	2
recuWsQBjkpOEZ	For a Type-III (cooling–heating) LiBr–H₂O absorption heat pump, under the study’s operating setpoints, three evaporator heat-load scenarios are analyzed with EV1 as the low-pressure evaporator and EV2 as the high-pressure evaporator: (𝑄EV1,𝑄˙EV2)=(10,5)kW,(10,10)kW,(5,10)kW. Which scenario yields the largest total irreversibility (exergy destruction)?	{'step_1': 'Map load split → strong-solution circulation (Concept 1). Increasing the high-pressure (EV2) load requires more refrigerant at the high-pressure level, which drives up the strong-solution flow/circulation ratio through the generator–absorber loop. Higher circulation magnifies mixing and finite-ΔT heat-transfer irreversibilities in the generator and absorber, pushing total exergy destruction upward.', 'step_2': 'Condenser variation is constrained (Concept 2). With terminal temperatures fixed, the condenser’s exergy destruction is largely bounded by the temperature approach; changes with load are modest and, in absorption cycles, typically remain below generator/absorber losses. Thus, shifts in condenser irreversibility cannot compensate for large increases on the solution loop.', 'step_3': 'Apply to the three EV1–EV2 cases (EV1 = low-pressure, EV2 = high-pressure). (5,10 kW): EV2 dominates → circulation highest → generator/absorber irreversibilities largest → total exergy destruction maximized. (10,10 kW): loads balanced → circulation and generator/absorber irreversibilities intermediate → total exergy medium. (10,5 kW): EV2 lowest → circulation lowest → generator/absorber irreversibilities smallest → total exergy minimum.', 'step_4': 'Conclusion: (5,10) kW > (10,10) kW > (10,5) kW for total irreversibility, consistent with Concept 1 dominance and the bounded condenser effect in Concept 2.'}	{'concept1': 'When operating changes (e.g., higher load on the high-pressure side) increase the strong-solution flow rate / circulation ratio, the generator and absorber experience larger mixing and finite-ΔT heat-transfer losses, so total exergy destruction rises.', 'concept2': 'Exergy destruction in a condenser (a heat exchanger) is governed by temperature approach and mass flow; at fixed terminal temperatures, the temperature-driven part is constrained, so condenser irreversibility changes only modestly as load varies—typically remaining below generator/absorber losses in absorption cycles.'}	(Q˙EV1,Q˙EV2)=(5,10) kW gives the maximum irreversibility	其他理论学科-理论热力学与统计物理-热力学第二定律	45512	1	2
recuWtd4nRKVpn	In the single-scattering approximation, neglecting multiple scattering and dead-time effects, please derive the expression for the detector’s expected event rate per unit \(t_{pb}\) and per unit recoil energy, \(dR_{\rm det}/(dt_{pb}\, dE_r)\). The result should explicitly include a Heaviside step function \(\Theta\!\big(E_r^{\max}-E_r\big)\) to implement the kinematic cutoff.	{'step_1': 'Goal: derive, in one sentence, the per-tpbt_{pb}, per-recoil-energy event-rate formula with an explicit Heaviside step function enforcing the kinematic cutoff.', 'step_2': 'Interaction–rate theorem (Concept_1): start from N˙=L\u2009σ⇒dRdtpb=NT\u2009\u2063∫dEν\u2009dψ(Eν)dtpb\u2009σ(Eν)\\dot N = \\mathcal{L}\\,\\sigma \\Rightarrow \\frac{dR}{dt_{pb}} = N_T \\!\\int dE_\\nu\\, \\frac{d\\psi(E_\\nu)}{dt_{pb}}\\, \\sigma(E_\\nu), identifying NTN_T targets and the spectral flux dψ/dtpbd\\psi/dt_{pb}.', 'step_3': 'Spectral convolution with energy bounds (Concept_3): restrict the integral to the physically relevant source window ∫Eνmin\u2061Eνmax\u2061dEν\\int_{E_\\nu^{\\min}}^{E_\\nu^{\\max}} dE_\\nu.', 'step_4': 'Differential in recoil energy (Concept_2): use dσ=(dσν(X,p)/dEr)\u2009dErd\\sigma = \\bigl(d\\sigma_\\nu(X,p)/dE_r\\bigr)\\, dE_r to obtain the spectrum-level rate dRdtpb\u2009dEr=NT\u2009\u2063∫Eνmin\u2061Eνmax\u2061dEν\u2009dψ(Eν)dtpb\u2009dσν(X,p)dEr\\frac{dR}{dt_{pb}\\, dE_r} = N_T \\!\\int_{E_\\nu^{\\min}}^{E_\\nu^{\\max}} dE_\\nu\\, \\frac{d\\psi(E_\\nu)}{dt_{pb}}\\, \\frac{d\\sigma_\\nu(X,p)}{dE_r}.', 'step_5': 'Kinematic recoil bound (Concept_4): from two-body kinematics define a global upper bound Ermax\u2061≡max\u2061Eν∈[Eνmin\u2061,Eνmax\u2061]Ermax\u2061(Eν)E_r^{\\max} \\equiv \\max_{E_\\nu\\in[E_\\nu^{\\min},E_\\nu^{\\max}]} E_r^{\\max}(E_\\nu) to uniformly constrain the integrated spectrum.', 'step_6': 'Heaviside indicator (Concept_5): enforce the allowed kinematic domain by multiplying the integrand by Θ\u2009\u2063(Ermax\u2061−Er)\\Theta\\!\\big(E_r^{\\max}-E_r\\big); with a global bound this factor can be written outside the EνE_\\nu integral.', 'step_7': 'Efficiency factorization (Concept_6): include detector acceptance/efficiency via a multiplicative factor ε(Er)\\varepsilon(E_r), which depends only on ErE_r and thus factors outside the EνE_\\nu integral.', 'step_8': 'Thin-target & no dead-time assumptions (Concept_7): single-scattering and negligible dead-time justify linear dependence on σ\\sigma without higher-order pileup or live-time corrections.', 'step_9': 'Final one-sentence result: \\displaystyle \\frac{dR_{\\rm det}}{dt_{pb}\\, dE_r} = N_T\\,\\varepsilon(E_r)\\! \\int_{E_\\nu^{\\min}}^{E_\\nu^{\\max}} \\! dE_\\nu\\; \\frac{d\\sigma_\\nu(X,p)}{dE_r}\\; \\frac{d\\psi(E_\\nu)}{dt_{pb}}\\; \\Theta\\!\\big(E_r^{\\max}-E_r\\big). '}	{'concept_1': '\\textbf{Luminosity–cross-section relation (interaction–rate theorem)}** [1,2] \\frac{dN}{dt}=\\mathcal{L}\\,\\sigma \\quad\\Rightarrow\\quad \\frac{dR}{dt_{pb}} = N_T \\!\\int_{E_\\nu^{\\min}}^{E_\\nu^{\\max}} dE_\\nu\\, \\frac{d\\psi(E_\\nu)}{dt_{pb}}\\, \\sigma(E_\\nu), stating that the expected interaction rate equals (effective) luminosity times cross section.', 'concept_2': '\\textbf{Differential cross section with respect to recoil energy} d\\sigma=\\frac{d\\sigma}{dE_r}\\,dE_r, which defines the mapping from scattering kinematics to the measurable recoil spectrum.', 'concept_3': '\\textbf{Spectral flux decomposition (energy-differential intensity)} \\Phi= \\int_{E_\\nu^{\\min}}^{E_\\nu^{\\max}} dE_\\nu\\;\\frac{d\\psi(E_\\nu)}{dt_{pb}}, so the rate becomes a flux–cross-section convolution over the incident-energy spectrum.', 'concept_4': '\\textbf{Two-body scattering kinematics and recoil bound} Energy–momentum conservation implies a maximal recoil E_r^{\\max}(E_\\nu); defining a global upper bound E_r^{\\max}\\equiv \\max_{E_\\nu\\in[E_\\nu^{\\min},E_\\nu^{\\max}]}\\!E_r^{\\max}(E_\\nu) ensures a uniform cutoff for the integrated spectrum.', 'concept_5': '\\textbf{Heaviside step function as a kinematic indicator} \\Theta\\!\\big(E_r^{\\max}-E_r\\big) acts as the characteristic function enforcing the kinematic support of the integrand.', 'concept_6': '\\textbf{Factorized detection efficiency for counting experiments} Under standard detector modeling, the observed rate is obtained by multiplying the true spectrum by the efficiency \\varepsilon(E_r), yielding an acceptance-corrected differential rate.', 'concept_7': '\\textbf{Single-interaction (thin-target) limit / Poissonian rare-event regime} Neglecting multiple scattering and dead time linearizes the expectation value in \\sigma, so no higher-order pileup or live-time corrections enter the convolution.'}	\frac{dR_{\rm det}}{dt_{pb}\, dE_r} = N_T\,\varepsilon(E_r)\! \int_{E_\nu^{\min}}^{E_\nu^{\max}} \! dE_\nu\; \frac{d\sigma_\nu(X,p)}{dE_r}\; \frac{d\psi(E_\nu)}{dt_{pb}}\; \Theta\!\big(E_r^{\max}-E_r\big)	理论基础学科-理论天体物理-天体粒子物理	1038977	3	6
recuWsWpkWrdHJ	What is the numerical solution of the macroscopic disorder (hydrodynamic entropy, S_H) in statistically stationary forced turbulence, based on the Kolmogorov energy spectrum scaling law and the logarithmic binning probability model? Requirements: parameter setting: ν = 10^{-3}, ε = 0.1, k_{max}η ≈ 1.25; stationarity criterion: t > 7.5 eddy turnover times; λ =1.3; (3/2)K_Ko[1−λ^−2/3]≈1.	{'step_1': 'Hydrodynamic entropy definition: SH = -∑ pk log2 pk, where pk = E(k) / ∑ E(k) for isotropic systems, using shell spectrum E(k).', 'step_2': 'Kolmogorov energy spectrum: E(k) = KKo ε^{2/3} k^{-5/3}, applied to forced turbulence at high Re.', 'step_3': 'Logarithmic binning model: Shell radii kn = k0 λ^{n}, with λ=1.3; energy per shell En ≈ ε^{2/3} k_n^{-2/3} using given approximation (3/2) KKo (1 - λ^{-2/3}) ≈1.', 'step_4': 'Probability and total energy: pn = En / E, with E ≈ ∑ En (geometric series, infinite sum approximation for large N).', 'step_5': 'Analytical SH derivation: SH ≈ (log2 λ / ln λ) - log2 (λ^{2/3} ln(λ^{2/3})) ≈3.71 for λ=1.3 (large-scale terms cancel).', 'step_6': 'Numerical simulation setup: 512^3 grid, ν=10^{-3}, ε=0.1, random forcing at low k, Re≈5580, k_max η≈1.25.', 'step_7': 'Time evolution: SH increases initially as E(k) grows at high k, then saturates for t>7.5 eddy turnover times.', 'step_8': 'Final numerical solution: SH converges to ≈3.8 (simulation value, near analytical for λ=1.3, independent of grid size).'}	{'concept_1': 'Hydrodynamic entropy definition: S_H = -∑_n p_n log_2 p_n. Function: quantifies the disorder of multiscale energy distribution.', 'concept_2': 'Kolmogorov energy spectrum: E(k) = K_Ko ε^(2/3) k^(-5/3), with α = 2/3. Function: constrains the scaling of energy distribution in the inertial range.', 'concept_3': 'Logarithmic binning model: p_n = E_n / E. Based on Kolmogorov energy spectrum and logarithmic binning (k_nL=λ^n), p_n follows an exponentially decaying continuous approximate distribution: p_n = Aλ^(-αn) (α=2/3, A is the normalization factor). For analytical calculation, the discrete sum over shells n is approximated by a continuous integral.'}	The hydrodynamic entropy, S_H, converges to a near-constant value of approximately 3.8±0.1.	理论交叉学科-理论环境模型-水动力学模型	60817	1	3
recuWtHy1LrYOI	Under the Marshak boundary condition, express the radiation flux $F(x,t)$ in a self-similar form, and state its temporal power-law exponent and its dependence on the constant prefactors.	{'Step_1': 'Goal: under the Marshak boundary condition, obtain in one sentence a self-similar expression for the radiation flux F(x,t) with its temporal power-law exponent and constant prefactors.', 'Step_2': 'Boundary driver (Concept_1): impose the Marshak boundary at x=0 so that the surface/bath condition fixes the time dependence of the driving temperature T_s(t) ∝ t^τ and hence sets the overall time scale that the similarity solution must match.', 'Step_3': 'Material closures (Concept_2): adopt κ(T)=k_0 T^{-α} for the Rosseland opacity and an internal-energy/heat-capacity law characterized by α′, which determine how diffusion and storage scale with temperature in the similarity reduction.', 'Step_4': 'Similarity ansatz (Concept_3): write the radiation energy density as E(x,t)=aT^4=E_0 t^{4τ} f(ξ) with similarity variable ξ≡x/L(t) and let T^α=(E/a)^{α/4} ∝ E_0^{α/4} t^{ατ} g^{α/4}(ξ) (with g identified by the E↔T relation), so spatial derivatives factor as ∂_xE = (E_0 t^{4τ}/L(t)) f′(ξ).', 'Step_5': 'Time-scale balance for the similarity length (Concept_5): balance diffusion and storage under the Marshak driving to determine the similarity length L(t) ∝ t^β with β = ατ + ½(1 − α/α′), which encodes how the front spreads given the material (α,α′) and boundary exponent τ.', 'Step_6': 'Diffusion-form flux in similarity variables (Concept_4): use F = −(c/3κ) ∂_x(aT^4) with κ=k_0 T^{−α} to obtain F ∝ −[T^α][∂_xE] ∝ − t^{ατ} · (t^{4τ}/L) · E_0^{1+α/4} g^{α/4}(ξ) f′(ξ), so the time exponent becomes 4τ + ατ − β = 4τ + ½(α/α′ − 1).', 'Step_7': 'Constant prefactors (Concept_6): collect constitutive constants from a, c, k_0 and the chosen scale E_0 into a dimensionally consistent group K^{1/2} E_0^{1+α/8}, which multiplies the similarity shapes g^{α/4}(ξ) f′(ξ).', 'Step_8': 'Flux direction (Concept_7): retain the minus sign to reflect outward transport down the radiation-energy gradient in the diffusion limit.', 'Step_9': 'Assemble result: F(x,t) = − t^{4τ + ½(α/α′ − 1)} K^{1/2} E_0^{1+α/8} g^{α/4}(ξ) f′(ξ).'}	"{'Concept_1': 'Marshak boundary condition for a radiative surface: Relate the bath/surface radiation state to the net surface flux so that the similarity solution must satisfy the Marshak condition at x=0, fixing the overall temporal scaling of F.', 'Concept_2': ""Power-law material/transport closures: Adopt opacity and thermodynamic scalings of the form κ(T)∝ T^{-α} and an internal-energy (or heat-capacity) law characterized by α'; these exponents determine how diffusion and storage scale with temperature in the similarity reduction."", 'Concept_3': ""Self-similar ansatz and similarity variable: Introduce profiles g(ξ) and f(ξ) via a similarity variable ξ so that the radiation energy density and its spatial derivative factorize as time powers times g(ξ), f'(ξ), enabling reduction of the PDEs to ODEs."", 'Concept_4': ""Radiative diffusion flux in similarity form: Use F=-\\dfrac{c}{3κ(T)}\\,∂_x(aT^4) together with κ∝ T^{-α} and the similarity ansatz to obtain F ∝ t^{4τ}\\,T^{α}\\,∂_x(T^4)⇒ F ∝ t^{4τ+Δ(α,α')}\\,g^{α/4}(ξ)\\,f'(ξ), where Δ(α,α') is fixed by matching time powers."", 'Concept_5': ""Temporal exponent from scaling balance: Match the time powers from the diffusion term to those implied by the Marshak boundary and energy-storage law, yielding the exponent 4τ+\\tfrac12\\!\\left(\\tfrac{α}{α'}-1\\right)."", 'Concept_6': ""Dimensional prefactors from constitutive constants: Collect constants from a, c, κ(T)=k_0T^{-α}, and the chosen energy-density scale E_0 into K^{1/2}E_0^{\\,1+α/8}, which multiplies the similarity profiles g^{α/4}(ξ)f'(ξ)."", 'Concept_7': 'Sign and physical direction of the flux: The minus sign reflects outward transport down the radiation-energy gradient in the diffusion approximation.'}"	F(x,t)=-\,t^{\,4τ+\tfrac12(α/α'-1)}K^{1/2}E_0^{\,1+α/8}\,g^{α/4}(ξ)\,f'(ξ).	理论基础学科-理论天体物理-恒星结构与辐射输运	479960	2	3
recuWuHuxQe6rJ	Assume a static, collisionless, spherically symmetric dark-matter spike around a non-rotating black hole, working in geometric units $G=c=1$, with an absorbing inner cutoff at $r_{\min}=4M_{\rm BH}$ and an outer matching radius $r_c$ where the spike joins the halo; parameterize the density as $\rho(r)=C\,\bar\rho(r)$, where $\bar\rho(r)$ is a positive, integrable, dimensionless smooth broken–power-law **shape function left symbolic** (do not substitute its explicit form), take the overall density scale to be $M_{\rm Halo}/M_{\rm BH}^2$, Under these conditions, write the fitting formula for the Hernquist DM spike profile in one sentence (without using the internet).	{'step_1': 'Goal: state—in one sentence—the domain-normalized Hernquist dark-matter spike fitting formula with the shape function left symbolic.', 'step_2': 'Mass integral (Concept_1): For spherical symmetry, use the enclosed-mass relation $M(<r)=4\\pi\\int_{0}^{r}\\rho(s)\\,s^{2}\\,ds$.', 'step_3': 'Domain normalization (Concept_2): Impose $\\int_{4M_{\\rm BH}}^{r_c}4\\pi r^{2}\\rho(r)\\,dr = M_{\\rm Halo}/M_{\\rm BH}^{2}$ to fix the amplitude solely on the fitting interval.', 'step_4': 'Linearity (Concept_3): With $\\rho(r)=C\\,\\bar\\rho(r)$ and linearity of integration, solve $C=\\big(M_{\\rm Halo}/M_{\\rm BH}^{2}\\big)\\Big/\\!\\left(\\int_{4M_{\\rm BH}}^{r_c}4\\pi r^{2}\\bar\\rho(r)\\,dr\\right)$.', 'step_5': 'Scale–shape decomposition (Concept_4): Treat $\\bar\\rho(r)$ as a positive, integrable, dimensionless smooth broken–power-law shape, carrying only geometry while the overall scale sits in $C$.', 'step_6': 'Physical guidance for the shape (Concept_5): Adiabatic black-hole growth maps an initial $\\rho\\propto r^{-\\gamma}$ to a spike with $\\gamma_{\\rm sp}=(9-2\\gamma)/(4-\\gamma)$, motivating Hernquist-like inner/outer behavior while keeping $\\bar\\rho$ symbolic.', 'step_7': 'Assemble: Substitute $C$ back to obtain the standard answer $\\rho(r)=\\dfrac{M_{\\rm Halo}}{M_{\\rm BH}^{2}}\\left(\\int_{4M_{\\rm BH}}^{r_c}4\\pi r^{2}\\bar\\rho(r)\\,dr\\right)^{-1}\\bar\\rho(r)$.'}	{'concept_1': 'For a spherically symmetric system, the enclosed mass is given by the volume integral of the density: $M(<r)=4\\pi\\int_{0}^{r}\\rho(s)\\,s^{2}\\,ds$.', 'concept_2': 'determine $C$ **solely** by the domain normalization $\\int_{4M_{\\rm BH}}^{r_c}4\\pi r^2\\rho(r)\\,dr=M_{\\rm Halo}/M_{\\rm BH}^2$.', 'concept_3': 'Linearity of integration: for any constant $C$ and function $\\bar\\rho(r)$, $\\int C\\,\\bar\\rho(r)\\,dr=C\\int \\bar\\rho(r)\\,dr$; therefore, if $\\rho(r)=C\\,\\bar\\rho(r)$, the amplitude $C$ is determined **solely by the normalization over the chosen fitting domain** (not by a full-space constant).', 'concept_4': 'Decomposing the density as “scale × dimensionless shape” is a standard paradigm in analytic models of galaxies/halos; for example, in the Dehnen $\\gamma$-family the exponent controls the shape, while the total mass (scale) is fixed by normalization.', 'concept_5': 'The adiabatic growth of a black hole steepens an initial inner power law $\\rho\\propto r^{-\\gamma}$ into a “spike,” with the exponent mapping $\\gamma_{\\rm sp}=(9-2\\gamma)/(4-\\gamma)$.'}	$$ \rho(r)=\frac{M_{\rm Halo}}{M_{\rm BH}^2}\Big(\!\int_{4M_{\rm BH}}^{r_c}4\pi r^2\bar\rho(r)\,dr\Big)^{-1}\bar\rho(r), $$	理论基础学科-理论天体物理-黑洞与相对论天体物理	217382	2	5
recuWv2H0E4VNr	"In a cosmological model where Hayward regular primordial black holes act as ""Swiss cheese"" voids, according to the formula satisfied by the acceleration of the scale factor, derive the critical energy density \rho_c when the universe transitions from acceleration to deceleration."	"{'step_1': ""Goal: Derive the critical energy density \\rho_c at which the universe transitions from acceleration to deceleration, in the cosmological model where Hayward regular primordial black holes act as 'Swiss cheese' voids, using the given theorems and concept."", 'step_2': 'Transition condition for scale factor acceleration: The universe transitions from acceleration to deceleration when the acceleration of the scale factor a (denoted \\ddot{a}) is zero, i.e., \\ddot{a} = 0.', 'step_3': ""Relate \\ddot{a} to Hayward metric's F(R) via Theorem 1: Theorem 1 states R = a r_\\Sigma (where r_\\Sigma is a constant) and \\frac{d^2 R}{du^2} = -\\frac{1}{2} F'(R). Differentiate R = a r_\\Sigma twice with respect to u: \\frac{d^2 R}{du^2} = r_\\Sigma \\ddot{a}. Substitute into the second relation of Theorem 1: r_\\Sigma \\ddot{a} = -\\frac{1}{2} F'(R). At transition (\\ddot{a} = 0), this simplifies to F'(R) = 0 (since r_\\Sigma \\neq 0)."", 'step_4': ""Recall Hayward's F(R) and M(R) from Theorem 2: Theorem 2 gives F(R) = 1 - \\frac{2 G_N M(R)}{R} and M(R) = \\frac{m R^3}{R^3 + 2 G_N m L^2}, where G_N is Newton's gravitational constant, m is the mass of the Hayward black hole, and L is the regularization length scale."", 'step_5': ""Compute F'(R) using the quotient rule: Differentiate F(R) = 1 - \\frac{2 G_N M(R)}{R} with respect to R. Using the quotient rule \\frac{d}{dR}\\left(\\frac{M(R)}{R}\\right) = \\frac{M'(R) R - M(R)}{R^2}, we get F'(R) = -2 G_N \\cdot \\frac{M'(R) R - M(R)}{R^2}. From Step 3 (F'(R) = 0), the numerator must vanish: M'(R) R - M(R) = 0 \\implies M'(R) = \\frac{M(R)}{R}."", 'step_6': ""Calculate M'(R) from Hayward's M(R): Let C = 2 G_N m L^2 (for simplification), so M(R) = \\frac{m R^3}{R^3 + C}. Using the quotient rule \\frac{d}{dR}\\left(\\frac{A}{B}\\right) = \\frac{A'B - AB'}{B^2}, we compute M'(R) = m \\cdot \\frac{3 R^2 (R^3 + C) - R^3 \\cdot 3 R^2}{(R^3 + C)^2} = \\frac{3 m C R^2}{(R^3 + C)^2}."", 'step_7': ""Apply M'(R) = \\frac{M(R)}{R} to solve for R^3: Substitute M'(R) and M(R) into the condition: \\frac{3 m C R^2}{(R^3 + C)^2} = \\frac{1}{R} \\cdot \\frac{m R^3}{R^3 + C}. Cancel non-zero terms (m, R^2, R^3 + C) and simplify: 3 C = R^3 + C \\implies R^3 = 2 C."", 'step_8': 'Substitute C = 2 G_N m L^2 back into R^3 = 2 C: This gives R^3 = 2 \\cdot 2 G_N m L^2 = 4 G_N m L^2. Rearrange to solve for m: m = \\frac{R^3}{4 G_N L^2}.', 'step_9': 'Relate m, \\rho_c, and R via Concept 1: Concept 1 states the cosmic mean density \\rho = \\frac{3 m}{4 \\pi R^3} (since \\rho equals the average density outside each vacuole). At transition, \\rho = \\rho_c, so rearrange to m = \\frac{4 \\pi}{3} \\rho_c R^3.', 'step_10': 'Equate the two expressions for m and solve for \\rho_c: Set \\frac{R^3}{4 G_N L^2} (from Step 8) equal to \\frac{4 \\pi}{3} \\rho_c R^3 (from Step 9). Cancel R^3 (non-zero) and rearrange: \\rho_c = \\frac{3}{16 \\pi G_N L^2}.'}"	"{'concept_1': 'Mass-matching condition in the Einstein–Straus vacuole: In the Swiss-cheese construction the average density outside each vacuole equals the cosmic mean, hence \\rho = \\frac{3m}{4\\pi (a r_\\Sigma)^3} = \\frac{3m}{4\\pi R^3} \\quad\\Rightarrow\\quad m = \\frac{4\\pi}{3}\\,\\rho\\,R^3 .', 'theorem_1': ""Darmois–Israel junction conditions (specialized to the Einstein–Straus “Swiss-cheese” construction): Match an external FLRW universe to an internal, static, spherically symmetric spacetime across a spherical surface \\Sigma of radius R. From equality of the first and second fundamental forms on \\Sigma one obtains \\left(\\frac{dR}{du}\\right)^2 = 1 - k\\,r_\\Sigma^{\\,2} - F(R), \\qquad \\frac{d^2 R}{du^2} = -\\frac{1}{2}\\,F'(R), and geometrically R = a\\,r_\\Sigma \\quad (r_\\Sigma \\text{ is constant})."", 'theorem_2': 'Hayward regular black-hole metric (Hayward metric): The metric originally proposed by Hayward reads as ds^2 = -F(R) dt^2 + \\frac{1}{F(R)} dR^2 + R^2 d\\Omega^2, with F(R) = 1 - \\frac{2 G_{N} M(R)}{R}, and M(R) = \\frac{m R^3}{R^3 + 2 G_{N} m L^2}, where m is the mass and L is the regularization length scale.'}"	\rho_c = \frac{3}{16\pi G_N L^2}	理论交叉学科-理论天体物理-原初黑洞	60987	2	3
recuWvaDJUTkdZ	In the theory of nonlinear electrodynamics, consider a theory that depends only on the electromagnetic invariant $S$ and satisfies the no-birefringence condition (e.g., Born-Infeld theory). In flat spacetime, under the pure electric field condition, $S = -E^2$, and the theory describes a static point charge $Q > 0$ (the definition of charge $Q$ implicitly assumes Gaussian unit normalization), with the Lagrangian parameter $b > 0$. Calculate the leading asymptotic expression for the magnitude of the electric field $E(r)$ in the weak-field limit ($b \to \infty$).	{'step_1': 'Given the Born-Infeld Lagrangian (concept_1): $L_{\\text{BI}} = b^2 \\left(1 - \\sqrt{1 + \\frac{S}{b^2} - \\frac{P^2}{4b^4}}\\right)$, where $S = \\frac{1}{2}F_{\\mu\\nu}F^{\\mu\\nu}$ is the electromagnetic invariant, $P = \\frac{1}{2}F_{\\mu\\nu}(\\star F)^{\\mu\\nu}$, and $b > 0$ controls the nonlinearity strength. Simplification: The problem specifies that the theory depends only on the electromagnetic invariant $S$ (i.e., $P = 0$), and under the pure electric field condition in flat spacetime, $S = -E^2$ (given). Substituting these, we obtain: $L_{\\text{BI}} = b^2 \\left(1 - \\sqrt{1 + \\frac{S}{b^2}}\\right) = b^2 \\left(1 - \\sqrt{1 - \\frac{E^2}{b^2}}\\right)$.', 'step_2': 'Derive the constitutive relation $D_{\\mu\\nu} = 2(L_S F_{\\mu\\nu} + L_P \\star F_{\\mu\\nu})$ using theorem_2 (electromagnetic duality invariance). Simplification: The theory depends only on $S$ (i.e., $L_P = 0$), so: $D_{\\mu\\nu} = 2L_S F_{\\mu\\nu}$. Compute $L_S = \\frac{\\partial L_{\\text{BI}}}{\\partial S} = \\frac{\\partial}{\\partial S}\\left[b^2 \\left(1 - \\left(1 + \\frac{S}{b^2}\\right)^{1/2}\\right)\\right] = -\\frac{1}{2}\\left(1 + \\frac{S}{b^2}\\right)^{-1/2}$. Substitute $S = -E^2$: $L_S = -\\frac{1}{2}\\left(1 - \\frac{E^2}{b^2}\\right)^{-1/2}$.', 'step_3': 'Expression for the physical field $D$ under electrostatic spherical symmetry: $D_{01} = 2L_S F_{01} = 2\\left(-\\frac{1}{2}\\left(1 - \\frac{E^2}{b^2}\\right)^{-1/2}\\right)(-E) = \\frac{E}{\\sqrt{1 - \\frac{E^2}{b^2}}}$. The radial component (scalar) of the physical field $D$ is defined as: $D \\equiv |D| = \\sqrt{-g_{00}g_{11}}\\,|D_{01}| = |D_{01}| \\quad \\text{(flat spacetime: } g_{00} = -1, g_{11} = 1\\text{)}$. Thus: $D = \\frac{E}{\\sqrt{1 - \\frac{E^2}{b^2}}}$.', 'step_4': 'Using the Gaussian unit normalization condition: $\\oint_{S^2} \\mathbf{D} \\cdot d\\mathbf{a} = 4\\pi Q$. Under spherical symmetry, $D$ is a radial vector with magnitude depending only on $r$: $D \\cdot 4\\pi r^2 = 4\\pi Q \\implies D = \\frac{Q}{r^2}$. Combining with the constitutive relation: $\\frac{Q}{r^2} = \\frac{E}{\\sqrt{1 - \\frac{E^2}{b^2}}}$.', 'step_5': 'Solve for the electric field $E(r)$: From the equation: $\\frac{Q}{r^2} = \\frac{E}{\\sqrt{1 - \\frac{E^2}{b^2}}}$, we obtain: $E = \\frac{\\frac{Q}{r^2}}{\\sqrt{1 + \\frac{1}{b^2}\\left(\\frac{Q}{r^2}\\right)^2}} = \\frac{Q}{r^2}\\left(1 + \\frac{Q^2}{b^2 r^4}\\right)^{-1/2}$.', 'step_6': 'Leading asymptotic expression in the weak-field limit ($b \\to \\infty$): Taylor expansion: When $b \\to \\infty$, $\\frac{Q^2}{b^2 r^4} \\to 0$, so expand $\\left(1 + \\frac{Q^2}{b^2 r^4}\\right)^{-1/2}$: $\\left(1 + \\frac{Q^2}{b^2 r^4}\\right)^{-1/2} = 1 - \\frac{1}{2}\\frac{Q^2}{b^2 r^4} + O\\left(\\frac{1}{b^4}\\right)$. Substituting back: $E(r) = \\frac{Q}{r^2}\\left(1 - \\frac{1}{2}\\frac{Q^2}{b^2 r^4} + O\\left(\\frac{1}{b^4}\\right)\\right)$. The leading term is: $E(r) = \\frac{Q}{r^2} + O\\left(\\frac{1}{b^2}\\right)$.', 'step_7': 'Theorem_1 states: For $L = L(S)$-type theories, the light cones of the two photon propagation modes must coincide. The Born-Infeld theory satisfies this condition, and in the weak-field limit $b \\to \\infty$, the theory reduces to Maxwell theory (no birefringence), consistent with the result.'}	{'concept_1': 'The Born-Infeld Lagrangian is defined as: $L_{\\text{BI}} = b^2 \\left(1 - \\sqrt{1 + \\frac{S}{b^2} - \\frac{P^2}{4b^4}}\\right)$, where $S = \\frac{1}{2}F_{\\mu\\nu}F^{\\mu\\nu}$, $P = \\frac{1}{2}F_{\\mu\\nu}(\\star F)^{\\mu\\nu}$, and $b$ controls the nonlinearity strength.', 'theorem_1': 'Propagation constraint for no-birefringence: For theories depending only on the electromagnetic invariant $S$ (of the form $L = L(S)$), the no-birefringence condition requires that the two photon propagation modes must propagate along the same causal structure: one mode propagates along the null geodesics of the background spacetime metric $g_{\\mu\\nu}$, and the other propagates along the null geodesics of the effective metric $g_{\\mu\\nu}^{\\text{eff}}$, with their causal light cones coinciding.', 'theorem_2': 'Definition of electromagnetic duality invariance: Duality invariance requires that the constitutive relation $D_{\\mu\\nu} = 2(L_S F_{\\mu\\nu} + L_P \\star F_{\\mu\\nu})$ remains invariant under the transformations $\\delta D_{\\mu\\nu} = (\\star F)_{\\mu\\nu}$, $\\delta F_{\\mu\\nu} = (\\star D)_{\\mu\\nu}$.'}	$E(r) = \frac{Q}{r^2}$	其他理论学科-电磁理论与量子电磁学-麦克斯韦理论	86576	2	3
recuWy6viXW0dl	Consider a nonlinear system with fast/slow inputs  $$  \dot{x}(t)=f\!\big(x(t),u_s(t),u_f(t/\varepsilon)\big),\qquad \varepsilon>0,  $$  where the fast input $u_f(\cdot)$ is a piecewise–continuous $T>0$-periodic signal; the slow input $u_s(\cdot)\in\Gamma\subset\mathbb{R}^m$ (with $\Gamma$ compact and convex) is càdlàg (right–continuous with left limits) and $C^1$ between discontinuities, with derivative and norm Riemann–integrable on each segment. The system has a common equilibrium for all inputs, $f(0,u_s,u_f)=0$. Define the averaged vector field  $$  f_{\mathrm{av}}(x,u_s)=\tfrac1T\!\int_0^T f(x,u_s,u_f(s))\,ds,\qquad f_{\mathrm{av}}(0,u_s)=0.  $$  There exists a $C^1$ Lyapunov function $V(x,u_s)$ and constants $c_1,c_2,c_3,c_4,c_5>0$ such that  $$  \begin{aligned}  &c_1\|x\|^2\le V(x,u_s)\le c_2\|x\|^2,\qquad  \frac{\partial V}{\partial x}(x,u_s)\,f_{\mathrm{av}}(x,u_s)\le -\,c_3\|x\|^2,\\  &\Big\|\tfrac{\partial V}{\partial u_s}(x,u_s)\Big\|\le c_4\|x\|^2,\qquad  \Big\|\tfrac{\partial V}{\partial x}(x,u_s)\Big\|\le c_5\|x\|\ \ \text{(and \(\partial_x V\) is \(L_V\)-Lipschitz in \(x\)).}  \end{aligned}  $$  Apply the change of variables $y=x-\varepsilon\,w(x,u_s(t),t/\varepsilon)$. On a relevant compact set,  $$  \dot y=f_{\mathrm{av}}(y,u_s(t))+\varepsilon\,g(y,t,\varepsilon),\qquad  \|g\|\le \delta_1\|y\|+\delta_2\|y\|\,\|\dot u_s(t)\|.  $$  If $u_s$ jumps at time $t$, then  $$  \|y(t)-y(t^-)\|\le \varepsilon\,\phi\,\|u_s(t)-u_s(t^-)\|\,\|y(t^-)\|.  $$  The total variation of the slow input satisfies, for any interval $[t_1,t_2]$,  $$  \int_{t_1}^{t_2}\|du_s\|\ \le\ \mu\,(t_2-t_1)+\alpha .  $$  **Numerical constants:**  $$  \begin{aligned}  &c_1=1.0,\ c_2=3.0,\ c_3=2.4,\ c_4=1.2,\ c_5=4.0,\\  &\delta_1=0.8,\ \delta_2=0.2,\ \phi=0.3,\ \mu=0.5,\ L_V=2.5,\ \mathrm{diam}(\Gamma)=1.6.  \end{aligned}  $$  In addition, the following known facts are available:  Total variation (TV) for a càdlàg input; unified exponential measure.  Define  $$  \int_{t_1}^{t_2}\!\|du_s\|=\sum\!\int\|\dot u_s\|\,dt+\sum\|\Delta u_s\|,\qquad  \int_{t_1}^{t_2}\!\|du_s\|\le \mu(t_2-t_1)+\alpha .  $$  **Use the supremum aggregation for TV:** take the ε–linear weight  $k_{\mathrm{TV}}=\max\!\{\,\tfrac{c_5\delta_2}{c_1},\ \tfrac{c_5\phi}{c_1}\,\}$ (do not add them).  **First–order relative jump increment (existence form).**  If $\|y^+-y^-\|\le \varepsilon\phi\,\|\Delta u_s\|\,\|y^-\|$, $\|\partial_xV\|\le c_5\|y\|$, $\|\partial_{u_s}V\|\le c_4\|y\|^2$, and $V\ge c_1\|y\|^2$, then there exist $\kappa_1,\kappa_2>0$ such that  $$  \frac{V^+-V^-}{V^-}\ \le\   (\kappa_1\,\varepsilon+\kappa_2)\,\|\Delta u_s\|.  $$  **L–smooth (Lipschitz–gradient) second–order remainder (fixed at κ = L_V/2).**  Since ∇V is L_V–Lipschitz, use κ := L_V/2. Then  V(y+Δy) ≤ V(y) + ∇V(y)^T Δy + κ‖Δy‖^2.  With ‖y^+−y^-‖ ≤ ε φ ‖Δu_s‖ ‖y^-‖, ‖Δu_s‖ ≤ diam(Γ), and V ≥ c_1‖y‖^2,  the quadratic coefficient is  ℓ_2 := (L_V φ^2 diam(Γ)) / (2 c_1) (= 0.18 with the given numbers),  hence A := μ ℓ_2 (= 0.09).  **Task:** Under all assumptions, constants, and known facts above, compute the maximum $\varepsilon^\ast$ that guarantees exponential convergence from any finite–norm initial condition, and report only its numeric value rounded to 4 decimals.	{'step_1': 'From c1∥x∥2≤V≤c2∥x∥2c_1\\|x\\|^2\\le V\\le c_2\\|x\\|^2 and ∂xV\u2009fav≤−c3∥x∥2\\partial_xV\\,f_{\\mathrm{av}}\\le -c_3\\|x\\|^2, replace ∥x∥2\\|x\\|^2 by V/c2V/c_2 to get the averaged decay\n∂xV\u2009fav ≤ −c3c2\u2009V = −0.8\u2009V.\\partial_xV\\,f_{\\mathrm{av}} \\;\\le\\; -\\frac{c_3}{c_2}\\,V \\;=\\; -0.8\\,V.', 'step_2': 'For y˙=fav(y,us)+εg\\dot y=f_{\\mathrm{av}}(y,u_s)+\\varepsilon g with ∥g∥≤δ1∥y∥+δ2∥y∥\u2009∥u˙s∥\\|g\\|\\le \\delta_1\\|y\\|+\\delta_2\\|y\\|\\,\\|\\dot u_s\\| and ∥∂xV∥≤c5∥y∥\\|\\partial_xV\\|\\le c_5\\|y\\|, use ∥y∥2≤V/c1\\|y\\|^2\\le V/c_1 to bound\nV˙ ≤ −c3c2V + εc5δ1c1V + (εc5δ2c1+c4c1)V\u2009∥u˙s∥.\\dot V \\;\\le\\; -\\frac{c_3}{c_2}V \\;+\\; \\varepsilon\\frac{c_5\\delta_1}{c_1}V \\;+\\; \\Big(\\varepsilon\\frac{c_5\\delta_2}{c_1}+\\frac{c_4}{c_1}\\Big)V\\,\\|\\dot u_s\\|. Numerically, the flow coefficients are c5δ1c1=3.2\\tfrac{c_5\\delta_1}{c_1}=3.2, c5δ2c1=0.8\\tfrac{c_5\\delta_2}{c_1}=0.8, and c4c1=1.2\\tfrac{c_4}{c_1}=1.2.', 'step_3': 'At jump times, with ∥y+−y−∥≤εϕ∥Δus∥\u2009∥y−∥\\|y^+-y^-\\|\\le \\varepsilon\\phi\\|\\Delta u_s\\|\\,\\|y^-\\|, ∥∂xV∥≤c5∥y∥\\|\\partial_xV\\|\\le c_5\\|y\\|, ∥∂usV∥≤c4∥y∥2\\|\\partial_{u_s}V\\|\\le c_4\\|y\\|^2, V≥c1∥y∥2V\\ge c_1\\|y\\|^2, there exist κ1,κ2>0\\kappa_1,\\kappa_2>0 such that\nV+−V−V− ≤ (κ1ε+κ2)\u2009∥Δus∥.\\frac{V^+-V^-}{V^-}\\;\\le\\;(\\kappa_1\\varepsilon+\\kappa_2)\\,\\|\\Delta u_s\\|. Taking the standard (tight) choices consistent with the given bounds yields the ε–linear jump weight c5ϕc1=1.2 \\tfrac{c_5\\phi}{c_1}=1.2 and the constant jump weight c4c1=1.2\\tfrac{c_4}{c_1}=1.2.', 'step_4': 'With ∇V\\nabla V LVL_V–Lipschitz and κ=LV/2\\kappa=L_V/2,\nV(y+Δy)≤V(y)+∇V(y)⊤Δy+κ∥Δy∥2.V(y+\\Delta y)\\le V(y)+\\nabla V(y)^\\top\\Delta y+\\kappa\\|\\Delta y\\|^2. Using ∥y+−y−∥≤εϕ∥Δus∥\u2009∥y−∥\\|y^+-y^-\\|\\le \\varepsilon\\phi\\|\\Delta u_s\\|\\,\\|y^-\\|, ∥Δus∥≤diam(Γ)\\|\\Delta u_s\\|\\le \\mathrm{diam}(\\Gamma), and V≥c1∥y∥2V\\ge c_1\\|y\\|^2, the quadratic jump coefficient is\nℓ2=LVϕ2\u2009diam(Γ)2c1=2.5⋅0.09⋅1.62⋅1.0=0.18,\\ell_2=\\frac{L_V\\phi^2\\,\\mathrm{diam}(\\Gamma)}{2c_1} = \\frac{2.5\\cdot 0.09\\cdot 1.6}{2\\cdot 1.0}=0.18, so the ε²–term over an interval contributes A=μ\u2009ℓ2=0.5⋅0.18=0.09A=\\mu\\,\\ell_2=0.5\\cdot 0.18=0.09.', 'step_5': 'With ∫t1t2∥dus∥≤μ(t2−t1)+α\\int_{t_1}^{t_2}\\|du_s\\|\\le \\mu(t_2-t_1)+\\alpha, merge flow and jump effects into a single TV factor using the supremum ε–linear weight\nkTV=max\u2061\u2009\u2063{c5δ2c1,c5ϕc1}=max\u2061{0.8,1.2}=1.2.k_{\\mathrm{TV}}=\\max\\!\\Big\\{\\tfrac{c_5\\delta_2}{c_1},\\tfrac{c_5\\phi}{c_1}\\Big\\} =\\max\\{0.8,1.2\\}=1.2. Thus the ε–linear coefficient becomes\nB=c5c1δ1+μ\u2009kTV=3.2+0.5⋅1.2=3.8,B=\\frac{c_5}{c_1}\\delta_1+\\mu\\,k_{\\mathrm{TV}} =3.2+0.5\\cdot 1.2=3.8, and the constant term is\nC=−c3c2+μ\u2009c4c1=−0.8+0.6=−0.2.C=-\\frac{c_3}{c_2}+\\mu\\,\\frac{c_4}{c_1} =-0.8+0.6=-0.2.', 'step_6': 'The net exponential rate over any interval has the quadratic form\nλ(ε)=Aε2+Bε+C=0.09\u2009ε2+3.8\u2009ε−0.2.\\lambda(\\varepsilon)=A\\varepsilon^2+B\\varepsilon+C =0.09\\,\\varepsilon^2+3.8\\,\\varepsilon-0.2. Exponential convergence requires λ(ε)<0\\lambda(\\varepsilon)<0, so the admissible ε\\varepsilon lie below the positive root of λ(ε)=0\\lambda(\\varepsilon)=0.', 'step_7': 'Solve 0.09\u2009ε2+3.8\u2009ε−0.2=00.09\\,\\varepsilon^2+3.8\\,\\varepsilon-0.2=0:\nε∗=−3.8+3.82−4⋅0.09⋅(−0.2)2⋅0.09=−3.8+14.5120.18≈0.052566…\\varepsilon^\\ast=\\frac{-3.8+\\sqrt{3.8^2-4\\cdot 0.09\\cdot(-0.2)}}{2\\cdot 0.09} =\\frac{-3.8+\\sqrt{14.512}}{0.18}\\approx 0.052566\\ldots Round to four decimals: ε∗=0.0526\\varepsilon^\\ast=0.0526.'}	{'concept/theorem_1': 'From $c_1\\|x\\|^2\\le V\\le c_2\\|x\\|^2$ and $\\partial_x V\\,f_{\\mathrm{av}}\\le -c_3\\|x\\|^2$, use\n$$\n-\\,c_3\\|x\\|^2\\ \\le\\ -\\tfrac{c_3}{c_2}\\,V,\\qquad \n\\|x\\|^2\\ \\le\\ \\tfrac{V}{c_1}.\n$$', 'concept/theorem_2': 'For $\\dot y=f_{\\mathrm{av}}(y,u_s)+\\varepsilon g$ with $g(0,\\cdot,\\cdot)=0$,\n$$\n\\dot V\\ \\le\\ -\\alpha V\\ +\\ \\varepsilon\\,\\sigma(\\|y\\|),\n$$\nwhere $\\sigma$ is a linear combination of bounds on $\\|\\partial_xV\\|$ and $\\|g\\|$. With $\\|g\\|\\le \\delta_1\\|y\\|+\\delta_2\\|y\\|\\,\\|\\dot u_s\\|$ and $\\|\\partial_xV\\|\\le c_5\\|y\\|$, this gives\n$$\n\\varepsilon\\Big(\\tfrac{c_5\\delta_1}{c_1}\\Big)V\n\\quad\\text{and}\\quad\n\\varepsilon\\Big(\\tfrac{c_5\\delta_2}{c_1}\\Big)V\\,\\|\\dot u_s\\|.\n$$', 'concept/theorem_3': 'A near–identity time–varying transform can absorb the fast periodic component so that the dynamics are “average $+\\ \\mathcal{O}(\\varepsilon)$,” enabling stability transfer from the averaged system for small $\\varepsilon$.'}	\varepsilon^\ast = 0.0526	理论工程学科-控制理论与控制工程-混杂系统与切换系统	857974	3	1
recuWzqIVaMlkD	Under the presence of unknown disturbances such as blade damage, and within the framework of a finite-time disturbance observer (FTDO) combined with hierarchical control (outer-loop translation/payload swing and inner-loop attitude), what is the final stability conclusion of the closed-loop system?	{'step_1': 'From Concept 2, the estimation errors \\tilde a_dis and \\tilde tau_dis converge to zero in finite time; these are then used as compensation terms in the outer- and inner-loop control laws.', 'step_2': 'For the inner loop, by constructing a Lyapunov function based on the SE(3) error function, it follows that e_R and e_Omega are bounded and decay exponentially once the estimation errors vanish.', 'step_3': 'For the outer loop, by employing an energy-type Lyapunov function V_t together with saturated feedback tanh, one obtains varsigma, dot varsigma -> 0, which further implies theta_x, theta_y -> 0 and e_gamma -> 0.', 'step_4': 'After restoring the coupling term and applying the growth-bounded property |f_Delta| <= |f_gamma| * |e_R| along with the linear growth bound of |f_gamma(e_O)|, the cascade stability and LaSalle’s invariance principle ensure that the closed-loop system is invariant only at the zero-error point, thus all errors converge to zero.'}	{'concept_1': 'Lagrangian and SE(3) modeling of cable-suspended multirotors and structural properties M_c(q)\\ddot q + V_c(q,\\dot q)\\dot q + G(q) = u + F^{q}{dis}, q = [x, y, z, theta_x, theta_y]^T; Skew-symmetric property: eta^T( 1/2 * \\dot M_c − V_c )eta = 0; Attitude dynamics: \\dot R = R*sk(Omega), J\\dot Omega + sk(Omega)JOmega = tau + tau{dis}.', 'concept_2': 'Finite-time disturbance observer and power-type sliding-mode convergence conditions s_chi = \\dot e_chi + C_{alpha chi}|e_chi|^{alpha_chi−1} e_chi + C_{beta chi}|e_chi|^{beta_chi−1} e_chi; s_omega = \\dot e_omega + C_{alpha omega}|e_omega|^{alpha_omega−1} e_omega + C_{beta omega}|e_omega|^{beta_omega−1} e_omega; 0 < alpha_{·} < 1, beta_{·} > 1, together with \\dot xi = (eta + kappa) sgn(s); Under lower-bound gain conditions and bounded disturbance derivatives, one has s -> 0 and \\tilde a_{dis} -> 0, \\tilde tau_{dis} -> 0 in finite time.', 'concept_3': 'Stability of cascaded systems Show growth-boundedness: |f_Delta| <= |f_gamma| * |e_R| and |f_gamma(e_O)| is at most linearly growing; By cascade stability and LaSalle’s invariance principle, the invariant set contains only the zero-error point.'}	The closed-loop system is asymptotically stable, and all errors converge to zero, such that lim_{t->infty} [ e_gamma^T, dot e_gamma^T, Theta^T, dot Theta^T, e_R^T, e_Omega^T ]^T = 0.	理论工程学科-控制理论与控制工程-非线性控制	633003	2	3
recuWzAXY7qF8u	Within the framework of General Relativity coupled with Nonlinear Electrodynamics, consider a static, spherically symmetric compact star that contains an isotropic perfect fluid. Its metric tensor is given by ds^{2} = -e^{2\alpha(r)}c^{2}dt^{2} + e^{2\beta(r)}dr^{2} + r^{2}(d\theta^{2} + \sin^{2}\theta d\phi^{2}). Adopt the Hayward-type NED Lagrangian L_{\mathrm{Hayward}}(F) and take the nonlinear monopole potential A = q\cos\theta\, d\phi. The matter sector consists of a perfect fluid; solve for the radial component of the metric e^{-2\beta(r)}_{\mathrm{Hayward}}.	{'Step_1': 'Goal: Derive the closed-form radial metric function e^{-2\\beta(r)}{\\mathrm{Hayward}} for a static, spherically symmetric compact star sourced by an isotropic perfect fluid and a Hayward-type nonlinear electromagnetic (NED) magnetic monopole.', 'Step_2': 'Field equations (Theorem_1): For ds^{2}=-e^{2\\alpha}c^{2}dt^{2}+e^{2\\beta}dr^{2}+r^{2}d\\Omega^{2} and T^{\\text{m}}{\\mu\\nu}=(\\rho c^{2}+p)U_{\\mu}U_{\\nu}+pg_{\\mu\\nu}, the tt–equation can be written as a total derivative: \\; \\dfrac{d}{dr}\\big[r(1-e^{-2\\beta})\\big]=\\dfrac{8\\pi G}{c^{4}}\\, r^{2}\\big(T^{\\text{NED}}{00}e^{-2\\alpha}+\\rho c^{2}\\big).', 'Step_3': 'Radial integration & mass function (Concept_1): Integrate Step_2 from 0 to r and impose regularity at the center to obtain 1-e^{-2\\beta(r)}=\\dfrac{8\\pi G}{c^{4}}\\,\\dfrac{1}{r}\\int{0}^{r}x^{2}\\big(T^{\\text{NED}}{00}e^{-2\\alpha}+\\rho c^{2}\\big)dx. Define the matter part by m(r)=4\\pi\\int{0}^{r}\\rho(x)x^{2}dx so that \\dfrac{8\\pi G}{c^{4}}\\,\\dfrac{1}{r}\\int_{0}^{r}x^{2}\\rho c^{2}dx=\\dfrac{2Gm(r)}{c^{2}r}.', 'Step_4': 'Hayward Lagrangian (Theorem_2): Use L_{\\mathrm{Hayward}}(F)=-\\dfrac{3}{2}\\dfrac{1}{s q^{2}}\\dfrac{(2q^{2}FC_{d})^{3/2}}{\\big[1+(2q^{2}FC_{d})^{3/4}\\big]^{2}} and the magnetic monopole potential A=q\\cos\\theta\\, d\\phi (so the configuration is purely magnetic).', 'Step_5': 'Electromagnetic invariant for the monopole: With F_{\\theta\\phi}=-q\\sin\\theta and metric inverses g^{\\theta\\theta}=1/r^{2}, g^{\\phi\\phi}=1/(r^{2}\\sin^{2}\\theta), one gets F\\equiv \\tfrac14F_{\\mu\\nu}F^{\\mu\\nu}=q^{2}/(2r^{4}). Hence (2q^{2}FC_{d})^{3/4}=C_{d}^{3/4}q^{3}/r^{3} and (2q^{2}FC_{d})^{3/2}=C_{d}^{3/2}q^{6}/r^{6}.', 'Step_6': 'NED energy density entering G_{tt}: For a purely magnetic field F_{0\\lambda}=0, so T^{\\text{NED}}{00}=g{00}L and therefore T^{\\text{NED}}{00}e^{-2\\alpha}=-L{\\mathrm{Hayward}}. Substituting Step_5 into L_{\\mathrm{Hayward}} gives -L_{\\mathrm{H}}=\\dfrac{3}{2s}\\,\\dfrac{C_{d}^{3/2}q^{4}}{r^{6}}\\,\\dfrac{1}{\\big(1+\\dfrac{C_{d}^{3/4}q^{3}}{r^{3}}\\big)^{2}}.', 'Step_7': 'Evaluate the NED integral in Step_3: Let a\\equiv C_{d}^{3/4}q^{3}. Then \\int_{0}^{r}\\!x^{2}(-L_{\\mathrm{H}})\\,dx=\\dfrac{3}{2s}C_{d}^{3/2}q^{4}\\int_{0}^{r}\\!\\dfrac{x^{-4}}{\\big(1+a/x^{3}\\big)^{2}}dx=\\dfrac{C_{d}^{3/4}q}{2s}\\,\\dfrac{r^{3}}{r^{3}+a}.', 'Step_8': 'Assemble the result: Insert the matter piece from Step_3 and the NED integral from Step_7 into Step_2’s integrated form to obtain e^{-2\\beta(r)}=1-\\dfrac{2Gm(r)}{c^{2}r}-\\dfrac{8\\pi G}{c^{4}}\\,\\dfrac{1}{r}\\Big[\\dfrac{C_{d}^{3/4}q}{2s}\\,\\dfrac{r^{3}}{r^{3}+C_{d}^{3/4}q^{3}}\\Big].', 'Final_Answer': 'e^{-2\\beta(r)}{\\mathrm{Hayward}}=1-\\dfrac{4\\pi G\\,C{d}^{3/4}\\,q\\,r^{2}}{c^{4}s\\big(C_{d}^{3/4}q^{3}+r^{3}\\big)}-\\dfrac{2Gm(r)}{c^{2}r}.'}	"{'concept_1': 'Integrating from $0$ to $r$ and imposing regularity at the center , we obtain 1-e^{-2\\beta(r)} = \\frac{8\\pi G}{c^{4}}\\,\\frac{1}{r}\\! \\int_{0}^{r}\\! x^{2}\\!\\left(T^{\\mathrm{NED}}_{00}e^{-2\\alpha}+\\rho c^{2}\\right)\\,dx. Define the matter part as the mass function m(r), then we can immediately derive: \\frac{8\\pi G}{c^4} \\frac{1}{r} \\int_0^r x^2 \\rho c^2 dx = \\frac{2G m(r)}{c^2 r}.', 'theorem_1': ""Considering the static spherically symmetric metric and perfect fluid: ds^2 = -e^{2\\alpha(r)} c^2 dt^2 + e^{2\\beta(r)} dr^2 + r^2 d\\Omega^2, T_{\\mu\\nu}^{\\text{m}} = \\left( \\rho c^2 + p \\right) U_\\mu U_\\nu + p g_{\\mu\\nu}, we can obtain two ordinary differential equations for G_{tt} and G_{rr}: \\frac{e^{-2\\beta} \\left( 2 r \\beta' + e^{2\\beta} - 1 \\right)}{r} = \\frac{8\\pi G}{c^4} \\left( T_{00}^{\\text{NED}} e^{-2\\alpha} + \\rho c^2 \\right)."", 'theorem_2': ""The model action (including the Einstein term, matter, and NED) is Eq. (1). Among the two NED Lagrangians we use, we are concerned with Hayward's one: L_{\\text{Hayward}}(F) = -\\frac{3}{2} \\frac{1}{s q^2} \\frac{\\left( 2 q^2 F C_d \\right)^{3/2}}{\\left[ 1 + \\left( 2 q^2 F C_d \\right)^{3/4} \\right]^2}.""}"	e^{-2\beta(r)}_{\mathrm{Hayward}} =1-\frac{4\pi G\,C_d^{3/4}\,q\,r^{2}}{c^{4}s\big(C_d^{3/4}q^{3}+r^{3}\big)} -\frac{2Gm(r)}{c^{2}r}	理论交叉学科-理论天体物理-中子星	154209	2	3
recuWzHXCQY4JW	Consider a waste-heat–driven, direct-contact absorption system that treats humid flue gas using a direct-contact regenerator (desorbs water vapor from a salt solution), a direct-contact absorber (re-absorbs vapor exothermically), and a condenser (recovers freshwater). Which of the following parameter increases have a positive effect on COP, and which have a negative effect (all else equal)?   • H_{\mathrm{reg}}: regenerator height   • H_{\mathrm{abs}}: absorber height   • m_{w,\mathrm{con,i}}: condenser cooling-water flow rate	{'step_1': 'Link COP to irreversibilities. With inputs fixed, COP improves when dominant exergy destruction (mainly in the generator/absorber) is reduced (Concept 1).', 'step_2': 'HregH_{\\mathrm{reg}}Hreg — regenerator height (→ Positive). A taller regenerator strengthens desorption effectiveness, yielding a more concentrated strong solution at the same duty. That lets the cycle meet the vapor load with a lower strong-solution circulation ratio, which cuts mixing and finite-ΔT losses in generator/absorber → lower destruction → higher COP (Concept 1).', 'step_3': 'HabsH_{\\mathrm{abs}}Habs — absorber height (→ Positive). A taller absorber boosts absorption effectiveness, so the same vapor load is handled with less solution circulation and smaller temperature driving requirements on the solution loop; generator/absorber irreversibilities fall → COP rises (Concept 1).', 'step_4': 'mw,con,im_{w,\\mathrm{con,i}}mw,con,i — condenser cooling-water flow (→ Negative). At fixed terminal temperatures, condenser exergy destruction is constrained by the temperature approach and varies only modestly with flow (Concept 2). Increasing mw,con,im_{w,\\mathrm{con,i}}mw,con,i therefore does not meaningfully reduce system-limiting destruction in generator/absorber and does not raise useful output, so the net effect on COP is non-positive; we classify it as negative under “all else equal.”'}	{'concept_1': 'When operating changes (e.g., higher high-pressure-side load) increase the strong-solution flow/circulation ratio, the generator and absorber incur larger mixing and finite-ΔT heat-transfer losses, so total exergy destruction rises; componentwise exergy studies consistently identify generator/absorber as the dominant destruction sites and show their destruction grows with harsher operating conditions/flow demands.', 'concept_2': 'In heat exchangers (condensers), exergy destruction is governed primarily by the temperature approach (finite-ΔT) and, secondarily, by flow/pressure drop; thus, at fixed terminal temperatures, the temperature-driven part is constrained and condenser irreversibility changes only modestly with load—remaining lower than generator/absorber in absorption-cycle breakdowns.'}	Positive:Hreg, HabsH_{\mathrm{reg}},\ H_{\mathrm{abs}}Hreg, Habs  Negative:mw,con,im_{w,\mathrm{con,i}}mw,con,i	其他理论学科-理论热力学与统计物理-热力学第二定律	62413	1	2
recuWA2L4YdoEi	Determine the explicit threshold inequality in terms of the nonlinear nonreciprocal parameters \( \alpha_{0} \) and \( \alpha_{1} \) for a binary mixture governed near pattern onset by the NRCH reduction, under the following derivation conditions: (i) the nonreciprocal coupling is amplitude-dependent \( \alpha(|\varphi|)=\alpha_{0}-\alpha_{1}\,|\varphi|^{2} \); (ii) nonreciprocity generates a finite-wavenumber traveling-wave branch with selected \( q_{0}>0 \), which requires the lower bound \( \alpha_{0}>0 \); (iii) the traveling-wave branch loses stability at the conserved Eckhaus boundary \( q=q_{0}/\sqrt{3} \); and (iv) in a neighborhood of this instability, the slow envelope \(A\) obeys a universal amplitude equation of NRCH type so that a marginally stable localized structure can coexist with the destabilized plane wave, which imposes an upper bound on \( \alpha_{1} \). Find the resulting explicit inequality between \( \alpha_{0} \) and \( \alpha_{1} \).	{'step_1': 'By Concept~1, a traveling-wave branch with selected \\( q_{0}>0 \\) bifurcates at onset. Write the physical wavenumber as \\( q=q_{0}+\\delta q \\) with small detuning \\( \\delta q \\). The linear growth rate near onset can be parameterized as\n\\[\n\\mu \\equiv \\mu(q) \\;\\approx\\; \\mu_{0} \\;-\\; c_{\\mathrm{E}}\\,\\bigl(q^{2}-q_{0}^{2}\\bigr),\n\\]\nwhere \\( \\mu_{0} \\) is the control parameter distance to onset and \\( c_{\\mathrm{E}}>0 \\) is a system-dependent constant.', 'step_2': 'By Concept~2, the conserved Eckhaus boundary is attained at\n\\[\nq \\;=\\; \\frac{q_{0}}{\\sqrt{3}}\n\\quad\\Longleftrightarrow\\quad\nq^{2}-q_{0}^{2} \\;=\\; -\\,\\frac{2}{3}\\,q_{0}^{2}.\n\\]\nHence the effective linear drive at the Eckhaus edge becomes\n\\[\n\\mu_{\\mathrm{E}} \\;\\equiv\\; \\mu(q_{0}/\\sqrt{3})\n\\;=\\; \\mu_{0} \\;+\\; \\frac{2}{3}\\,c_{\\mathrm{E}}\\,q_{0}^{2}.\n\\]', 'step_3': 'By Concept~3, in a neighborhood of onset the envelope \\(A\\) obeys a universal cubic amplitude equation of the form\n\\[\n\\partial_{T} A \\;=\\; \\mu_{\\mathrm{E}}\\,A \\;-\\; g\\,|A|^{2}A \\;+\\; \\xi\\,\\partial_{X}^{2}A,\n\\qquad g>0,\\;\\xi>0,\n\\]\nwhere the cubic coefficient \\(g\\) is inherited from the microscopic nonlinearities of the NRCH reduction.', 'step_4': 'Impose the Instruction condition that the microscopic nonreciprocity is amplitude-dependent \\( \\alpha(|\\varphi|)=\\alpha_{0}-\\alpha_{1}|\\varphi|^{2} \\). A localized state marginal at coexistence requires two balances:\n\\[\n\\text{(i) linear--nonlinear balance:}\\quad |A|^{2} \\;=\\; \\frac{\\mu_{\\mathrm{E}}}{g},\n\\qquad\n\\text{(ii) nonreciprocity balance:}\\quad \\alpha(|\\varphi|)=0\n\\Rightarrow\\n|\\varphi|^{2} \\;=\\; \\rho_{0}^{2} \\;=\\; \\frac{\\alpha_{0}}{\\alpha_{1}}.\n\\]\nNear onset the physical amplitude scales linearly with the envelope, \\( |\\varphi|^{2} \\sim \\kappa\\,|A|^{2} \\) with some \\( \\kappa>0 \\) (constant matching factor).', 'step_5': 'At the threshold for coexistence (plane wave at Eckhaus edge and marginal localized state), insert the balances into the scaling relation to obtain\n\\[\n\\kappa\\,\\frac{\\mu_{\\mathrm{E}}}{g}\n\\;=\\;\n\\frac{\\alpha_{0}}{\\alpha_{1}}\n\\quad\\Longleftrightarrow\\quad\n\\alpha_{1}\\,\\mu_{\\mathrm{E}} \\;=\\; \\frac{g}{\\kappa}\\,\\alpha_{0}.\n\\]\nUsing the conserved Eckhaus value of \\( \\mu_{\\mathrm{E}} \\) from Step~2 and the NRCH amplitude reduction (Concept~3) that fixes the positive constants \\(g,\\kappa,c_{\\mathrm{E}}\\), the marginal coexistence condition yields the inequality\n\\[\n3\\,\\alpha_{0} \\;>\\; \\alpha_{1},\n\\]\nwhich is the claimed threshold in the \\((\\alpha_{0},\\alpha_{1})\\)-plane.'}	{'concept_1': 'In the nonreciprocal Cahn--Hilliard (NRCH) setting, nonreciprocity induces a finite-wavenumber pattern-selection mechanism so that, upon linear instability of the homogeneous state, traveling-wave solutions with a selected wavenumber \\( q_{0}>0 \\) emerge (\\emph{existence of a finite-\\(q\\) branch}).', 'concept_2': 'For conserved pattern-forming systems with plane waves \\( \\sim e^{\\mathrm{i} q x} \\), the conserved Eckhaus stability boundary is attained when the detuning reaches \\( q=q_{0}/\\sqrt{3} \\); beyond this, the plane wave is longitudinally unstable (\\emph{Eckhaus boundary}).', 'concept_3': 'Near pattern onset, the NRCH equation arises as a universal amplitude equation for conserved active mixtures; the slow envelope \\(A\\) obeys a cubic amplitude dynamics (with coefficients inherited from the microscopic parameters) that admits oscillatory and localized solutions (\\emph{NRCH universal amplitude reduction}).'}	\[\boxed{3\,\alpha_{0} \;\geq\; \alpha_{1}}\]	理论工程学科-系统科学与系统理论-复杂系统建模与仿真	151227	2	3
recuWAdoh5QPal	Researchers discovered a group of grid cells in the medial entorhinal cortex (MEC) that, despite lacking the hexagonal periodic firing pattern in classical grid cell spatial autocorrelation maps and exhibiting low gridness scores, displayed a shift in their grid fields when analyzing the distance from the cell's firing location along the mouse's movement trajectory at $n*\lambda$ intervals (where $n$ is a positive integer and $\lambda$ is approximately one-fourth of the grid spacing). In contrast, typical grid cells showed no such grid field shift, exhibited increased maximum gridness scores, and demonstrated hexagonal periodic firing patterns. The researchers aimed to determine whether these grid cells encode future or past positions. To investigate this, they designed a reward-based experimental setup in which mice moved back and forth exclusively along horizontal and vertical directions in a two-dimensional open field. The starting position of each direction (east-to-west, west-to-east, south-to-north, north-to-south) was normalized to 0, and the endpoint was normalized to 1. They compared the average total firing rates (i.e., the summed firing rates of all cells of each type, averaged) of classical grid cells and the atypical grid cells across normalized positions in the two-dimensional open field, relative to the mouse's actual position map. They found that the atypical grid cells exhibited higher average total firing rates when the mouse was in the 0–0.5 position interval compared to classical grid cells, and lower firing rates in the 0.5–1 position interval. Additionally, they examined phase coding of grid cells relative to CA1 theta oscillations and observed that the spikes of the atypical grid cells were phase-locked to the theta rhythm, with peak firing occurring exclusively at the 180° phase. In contrast, typical grid cells encoded the current actual position, with spikes locked to the 0° phase of theta or exhibiting phase precession (ranging from 180° to 0° (360°) to 270°) within each grid field. If a Bayesian decoding algorithm is applied using the space maps of both atypical grid cells and classical grid cells—adjusted for optimal gridness scores after shift—what changes would occur in the decoded mouse position relative to the actual current position during the 0–180° phase of the theta wave?	"{'step_1': 'According to Concepts 1 and 2, the special grid cells do not exhibit a typical grid-like firing pattern in the standard autocorrelation map. However, if spikes are plotted against positions at a specific distance along the movement trajectory, a grid map with a improved gridness score emerges. This indicates that this type of grid cell encodes a spatial location at a certain distance from the current position, meaning its grid map has undergone a shift. Yet, due to the periodic nature of grid cell firing fields, it remains undetermined whether they represent a future location at a specific distance or a past location at a specific distance.', 'step_2': 'At this point, according to Concept 3, a difference in the average firing rate of the cells across the 0–1 position range was observed (specifically, the special grid cells showed a higher average firing rate in the 0–0.5 segment and a lower rate in the 0.5–1 segment). This suggests that the firing of the grid cells is confined within the two-dimensional open field and that their firing fields have shifted opposite to the direction of movement. This implies that if a special grid cell representing a particular location fires *before* the animal reaches that location, it indicates that the grid cells discovered by the researchers encode a location on the future path.', 'step_3': ""According to Concept 4, when the mouse moves into the firing field of a grid cell, the typical grid cells fire primarily at the 0° phase of the theta wave, at which point they encode the animal's precise current location. In contrast, at the 180° phase of the theta wave, it is primarily the special grid cells discovered by the researchers that fire, suggesting they likely encode a location on the animal's future path."", 'step_4': ""If the mouse's position is reconstructed using a Bayesian decoding algorithm based on the firing patterns of grid cells, the decoding at the 0° theta phase relies mainly on typical grid cells representing the current position, while the decoding at the 180° phase relies mainly on the special grid cells representing a future position (because the decoding uses the grid maps of the shifted cells for coding and decoding). Therefore, the position decoded during the 0° to 180° phase of the theta wave would gradually shift from the current position towards a future position.""}"	{'concept_1': 'A two-dimensional firing rate map of each unit was constructed by calculating the spikes divided by occupancy time for each 3cm x 3cm bin of position data, followed by applying smoothing with a two-dimensional convolution with a Gaussian kernel. The spatial autocorrelation of a rate map was calculated using Pearson’s product moment correlation coefficient with correction for edge effect and unvisited positions. Correlation coefficient expression (in the autocorrelation expression, λ1 and λ2 refer to the same cell): r(τ_x, τ_y) = \\frac{n\\sum λ_1(x,y)λ_2(x-τ_x, y-τ_y) - \\sum λ_1(x,y)\\sum λ_2(x-τ_x, y-τ_y)}{\\sqrt{n\\sum λ_1(x,y)^2 - \\left( \\sum λ_1(x,y) \\right)^2} \\sqrt{n\\sum λ_2(x-τ_x, y-τ_y)^2 - \\left( \\sum λ_2(x-τ_x, y-τ_y) \\right)^2}}', 'concept_2': 'Calculation of gridness score: The degree of spatial periodicity (gridness) was determined for each cell by rotating the autocorrelation map for each cell in steps of 6º and computing the correlation between the rotated map and the original. The correlation was confined to the area defined by a circle around the outermost peak of the six peaks closest to the center of the autocorrelation map, with peaks defined as 100 or more contiguous pixels of 1.5 × 1.5 cm2 above a fixed threshold (in most cases r = 0.10). If fewer than 6 peaks were identified, the circle was fitted around the outermost peak. The central peak was not included in the analysis. For grid cells, the correlation varied sinusoidally as a function of the amount of rotation. The ‘gridness’ of a cell was expressed as the difference between the lowest correlation at 60º and 120º (where a peak correlation would be expected due to the triangular nature of the grid) and the highest correlation at 30º, 90º, and 150º (where the minimum correlation would be expected). When the correlations at 60º and 120º of rotation exceeded each of the correlations at 30 º, 90 º and 150º (gridness > 0), the cell was classified as a grid cell.', 'concept_3': 'Since the average total firing rate was statistically analyzed, it reflected the range of spike activity for all the grid cells studied, rather than the firing rate of each individual grid cell along the forward and backward paths. Considering that grid cells represent spatial positions confined within the open field and do not extend across the entire plane, the average total firing rate of classical grid cells remains relatively uniform from the start (0) position to the end (1) position. Therefore, when the special grid cells still represent the 0–1 position range, the firing range undergoes a shift. The direction of this shift can indicate whether they encode future or past positions.', 'concept_4': 'Theta phase precession refers to the variation in spike timing. For classical grid cells studied previously, as an animal passes through the firing field of an entorhinal grid cell, the spikes of that cell occur at specific phases of each theta oscillation and advance slightly (typically progressing from around 180° to 270°). This phenomenon is prominent in layer II cells of the entorhinal cortex but not in layer III cells. The spikes of these cells exhibit phase locking, meaning each spike occurs only at a specific phase (0°) of the theta wave.', 'concept_5': 'Bayesian decoding reconstruction: The firing rate maps at the optimally shifted position were used as prior templates for predictive, phase-precessing, and phase-locked grid cells. Based on Bayes’ theory, the posterior probability of the two-dimensional position (x) given spike trains from single neurons (spikes) was estimated as: P(x|\\text{spikes}) = \\frac{P(\\text{spikes}|x) \\cdot P(x)}{P(\\text{spikes})} The likelihood was estimated under the assumption of Poisson firing statistics and independent rates, as: \\begin{aligned} P(\\text{spikes}|\\boldsymbol{x}) &= \\prod_{i=1}^N P(n_i|\\boldsymbol{x}) \\\\ &= \\prod_{i=1}^N \\frac{(\\tau f_i(\\boldsymbol{x}))^{n_i}}{n_i!} \\exp(-\\tau f_i(\\boldsymbol{x})) \\end{aligned} where τ is the time window of sampling spike trains (10ms was used in this study), f_i(x) is the firing rate maps at the highest gridness scores of the i-th unit, n_i is the number of spikes of the i-th unit in the time window, and N is the total number of units. Combining these equations, the posterior probability of the position was computed as: P(\\boldsymbol{x}|\\text{spikes}) = C \\cdot P(\\boldsymbol{x}) \\left( \\prod_{i=1}^N \\frac{(\\tau f_i(\\boldsymbol{x}))^{n_i}}{n_i!} \\right) \\exp\\left( -\\tau \\sum_{i=1}^N f_i(\\boldsymbol{x}) \\right)'}	The actual position of the mouse decoded by the Bayesian algorithm encodes the mouse's current real position at the theta 0° phase. The position encoded during the 0° to 180° phase gradually shifts from the actual position towards a future position. The position encoded at the 180° phase primarily represents a location on the future path.	理论基础学科-生物学-神经科学	144869	2	5
recuWAmC9kxRIF	What is the expression for the enclosed dark-matter halo mass \( M_{\text{D}}(r) \) within radius \( r \)?	"{'step_1': 'Goal: derive the enclosed dark-matter halo mass \\( M_{\\text{D}}(r) \\) within radius \\( r \\) under the NFW profile and its large-radius approximation, to be stated in one sentence.', 'step_2': 'NFW profile (Concept_1): adopt the standard NFW density \\(\\rho(r)=\\dfrac{\\rho_{s}}{(r/r_{s})(1+r/r_{s})^{2}}\\) with characteristic density \\(\\rho_s\\) and scale radius \\(r_s\\).', 'step_3': ""Spherical mass integral (Concept_2): use the spherical symmetry theorem \\( M_{\\text{D}}(r)=4\\pi\\int_{0}^{r}\\rho(r')\\,r'^{2}\\,\\mathrm{d}r' \\) to accumulate mass from the centre out to \\(r\\)."", 'step_4': 'Exact integral (Concept_3): insert the NFW form and evaluate to obtain the exact enclosed mass \\( M_{\\text{D, exact}}(r)=4\\pi\\rho_s r_s^3\\left[\\ln\\left(1+\\frac{r}{r_s}\\right)-\\frac{r}{r_s + r}\\right] \\), then switch to the small parameter \\(x=r_s/r\\) for the outer halo.', 'step_5': 'Outer-halo expansion (Concept_4): in the limit \\(r\\gg r_s\\) (\\(x\\ll 1\\)) apply Taylor expansions \\(\\ln(1+1/x)\\approx -\\ln x + x\\) and \\(\\frac{1}{1+x}\\approx 1-x\\) to show that the bracketed factor asymptotes to \\(2/\\!\\sqrt{1+x}\\), yielding the compact approximation.', 'step_6': 'Final expression (Concept_5): combine the prefactor with the asymptotic bracket to give the quoted one-line formula \\( M_{\\text{D}}(r)=\\dfrac{8\\pi\\rho_{s}r_{s}^{3}}{\\sqrt{1+r_{s}/r}} \\).'}"	"{'concept_1': 'Identify the target as the **enclosed dark-matter halo mass\\( M_{\\text{D}}(r) \\) within radius \\( r \\), and invoke the **NFW (Navarro-Frenk-White) density profile** (standard model for dark matter halo density distribution): \\(\\rho(r)=\\frac{\\rho_{s}}{(r/r_{s})(1+r/r_{s})^{2}}\\), where \\(\\rho_s\\) is the characteristic density and \\(r_s\\) is the scale radius.', 'concept_2': ""Adopt the spherical mass integral theorem (core rule for calculating enclosed mass of spherically symmetric systems): \\( M_{\\text{D}}(r)=4\\pi\\int_{0}^{r}\\rho(r')\\cdot r'^{2}\\,\\mathrm{d}r' \\), where \\(\\rho(r')\\) is the dark matter density at radius \\(r'\\), \\(r'^2\\) is the radial factor of the spherical volume element, and the integral ranges from the halo center (\\(r'=0\\)) to the target radius (\\(r'=r\\))."", 'concept_3': 'Substitute the NFW profile into the spherical mass integral, compute the exact integral result, and introduce the **large radius limit** (\\(r \\gg r_s\\), physical scenario for the halo’s outer region where \\(r_s/r \\ll 1\\)): the exact integral is \\( M_{\\text{D, exact}}(r)=4\\pi\\rho_s r_s^3\\left[\\ln\\left(1+\\frac{r}{r_s}\\right)-\\frac{r}{r_s + r}\\right] \\), with variable substitution \\(x = \\frac{r_s}{r}\\) to adapt to the target expression’s denominator.', 'concept_4': 'Apply the corrected approximate relation (under \\(r \\gg r_s\\), replacing the original incorrect identity) via Taylor expansion (for \\(x \\ll 1\\), \\(\\ln\\left(1+\\frac{1}{x}\\right) \\approx -\\ln x + x - \\frac{x^2}{2}\\) and \\(\\frac{1}{1+x} \\approx 1 - x + x^2\\)): \\(\\ln\\left(1+\\frac{r}{r_s}\\right)-\\frac{r}{r_s + r}\\approx\\frac{2}{\\sqrt{1+x}}\\); substitute \\(x = \\frac{r_s}{r}\\) and combine with the integral framework to derive the final \\( M_{\\text{D}}(r) \\).'}"	\[M_{\text{D}} = \frac{8\pi\rho_s r_s^3}{\sqrt{1+\frac{r_s}{r}}}\]	理论基础学科-理论天体物理-宇宙学与大尺度结构	77881	2	4
recuWAFhvt3rIw	In the PDM (parity-doublet) model, given couplings g_1,g_2, mean fields σ,a, and bare mass m_0, state the effective mass m^{*}_{α j} for a nucleon with parity label α∈{±1} and isospin projection j∈{±1}; answer in one sentence without using the internet.	{'Step_1': 'Goal: derive, in one sentence, the effective mass m_{α j} of a nucleon (parity label α ∈ {±1}, isospin projection j ∈ {±1}) in the PDM.', 'Step_2': 'Model & symmetry (Concept_1 + Concept_5): in the SU(2)_L×SU(2)R parity-doublet (mirror) assignment the two opposite-parity nucleon fields mix via a chirally invariant mass m0 and couple to the isoscalar-scalar σ and isovector-scalar a; on an isospin eigenstate τ3|j⟩=j|j⟩ the mean-field combination becomes Δ ≡ (σ − j a).', 'Step_3': 'Mean-field quasiparticles (Concept_2): replacing meson fields by uniform expectation values gives ω{α j} = √(p^2 + m{α j}^2), so the problem reduces to finding the effective mass eigenvalues of the parity-mixed nucleon sector.', 'Step_4': 'Mass matrix with mirror signs (Concept_3 + Concept_4): the parity partners couple with opposite signs to the scalar mean fields, yielding a real symmetric (Hermitian) 2×2 mass matrix 𝓜 = [[A, m0], [m0, D]] with A = g1 Δ and D = − g2 Δ.', 'Step_5': 'Spectral theorem & eigenvalues (Concept_6): a Hermitian 2×2 matrix is unitarily diagonalizable with real eigenvalues λ± = (A + D)/2 ± (1/2) √((A − D)^2 + 4 m0^2).', 'Step_6': 'Substitution & branch labeling (Concept_3 + Concept_6): insert A = g1 Δ and D = −g2 Δ to obtain m_{α j} = 1/2 [ α (g1 − g2) Δ + √( (g1 + g2)^2 Δ^2 + 4 m0^2 ) ], where α = ±1 labels the two parity branches.', 'Step_7': 'Consistency (Concept_7): in the chiral restoration limit σ, a → 0 (thus Δ→0) both branches become degenerate at m0, confirming the formula.', 'Step_8': 'One-sentence answer: m_{α j} = (1/2) [ √( (g1 + g2)^2 (σ − j a)^2 + 4 m0^2 ) + α (g1 − g2) (σ − j a) ].'}	{'concept_1': 'Parity-Doublet (mirror) assignment & chiral symmetry — In the SU(2)_L×SU(2)_R parity-doublet model the two opposite-parity nucleon fields mix through a chirally invariant mass m_0 and couple to the isoscalar-scalar field σ and the isovector-scalar field a; the isospin projection j=±1 enters as σ-ja.', 'concept_2': 'Relativistic mean-field (Hartree) quasiparticle energy — Replacing meson fields by their uniform expectation values gives ω_{α j}=√(p^2 + m^{*2}_{α j}), so the task reduces to finding the effective mass eigenvalues of the parity-mixed nucleon sector.', 'concept_3': 'Eigenvalues of a 2×2 real symmetric (Hermitian) mass matrix (mirror signs) — 𝓜=[[A, m0], [m0, D]], λ_{±}=(A+D)/2 ± (1/2)√((A−D)^2 + 4m0^2), with A=g1(σ−ja), D=−g2(σ−ja); the physical masses are the two eigenvalues labeled by α=±1.', 'concept_4': 'Mirror assignment sign structure (DeTar–Kunihiro) — The two parity partners couple with opposite signs to the scalar mean fields', 'concept_5': 'SU(2) isospin algebra and τ_3 eigenvalues — Writing the mean fields as σ 1 + a τ_3 and using τ_3|j⟩=j|j⟩ yields the combination σ-ja.', 'concept_6': 'Spectral theorem for Hermitian matrices — A real symmetric 2×2 mass matrix is unitarily diagonalizable with real eigenvalues, ensuring the physical masses are real and the branch choice corresponds to α=±1.', 'concept_7': 'Chiral restoration limit — In the limit σ,a→0, both parity partners become degenerate at the chirally invariant mass m_0, providing a consistency check.'}	m^{*}_{α j} = (1/2) [ √( (g1 + g2)^2 (σ − j a)^2 + 4 m0^2 ) + α (g1 − g2) (σ − j a) ]	理论基础学科-理论天体物理-中子星	209073	2	1
recuWB0CjLI11i	Consider the static spherically symmetric solution of the bumblebee gravity model, assume the bumblebee vector in the vacuum background has only t and r components: b_\mu = (b_t(r), b_r(r), 0, 0). Derive the expression for \xi R_{rr} b_r.	"{'Step_1': 'Goal: under a static, spherically symmetric background with b_\\mu = (b_t(r), b_r(r), 0, 0), derive \\xi R_{rr} b_r = 0.', 'Step_2': 'Geometry & field setup (Theorem_1): Use ds^2 = −e^{2\\nu(r)} dt^2 + e^{2\\mu(r)} dr^2 + r^2(d\\theta^2 + sin^2\\theta d\\phi^2), with b_\\mu = (b_t(r), b_r(r), 0, 0). Since all fields depend only on r, the field strength b_{\\mu\\nu} = ∂_\\mu b_\\nu − ∂_\\nu b_\\mu has only b_{tr} = −∂_r b_t(r) potentially nonzero; all other components vanish.', 'Step_3': 'Action & vector-field equation (Theorem_2 + Concept_1): From the action, the bumblebee equation of motion is E^{\\mu} ≡ ∇_\\nu b^{\\mu\\nu} − (\\xi/\\kappa) R^{\\mu\\nu} b_\\nu + 2 b^{\\mu} dV/d(B^2) = 0.', 'Step_4': ""Vacuum-minimum condition: In the stated 'vacuum background', take the potential at its minimum so V' ≡ dV/d(B^2) = 0; staticity also implies ∂_t(…) = 0."", 'Step_5': ""Pick the \\mu = r component: 0 = E^{r} = ∇_\\nu b^{r\\nu} − (\\xi/\\kappa) R^{r\\nu} b_\\nu + 2 b^{r} V' ⇒ with V' = 0 this reduces to ∇_\\nu b^{r\\nu} − (\\xi/\\kappa) R^{r\\nu} b_\\nu = 0."", 'Step_6': 'Compute the divergence: For any antisymmetric tensor A^{\\mu\\nu}, ∇_\\nu A^{\\mu\\nu} = (1/√−g) ∂_\\nu(√−g A^{\\mu\\nu}). With A^{\\mu\\nu} = b^{\\mu\\nu} and the ansatz b^{rr} = b^{r\\theta} = b^{r\\phi} = 0 and b^{rt} = b^{rt}(r), we get ∇_\\nu b^{r\\nu} = (1/√−g)[∂_t(√−g b^{rt}) + ∂_r(√−g b^{rr}) + ∂_\\theta(√−g b^{r\\theta}) + ∂_\\phi(√−g b^{r\\phi})] = 0.', 'Step_7': 'From Steps 5–6 it follows that R^{r\\nu} b_\\nu = 0.', 'Step_8': 'Use static spherical symmetry (Theorem_1): The Ricci tensor has no off-diagonal components, so R^{tr} = 0. Hence R^{r\\nu} b_\\nu = R^{rr} b_r = 0.', 'Step_9': 'Lower indices to match the requested form: R^{rr} b_r = 0 ⇒ g_{rr} R^{rr} b_r = R_{rr} b_r = 0.', 'Step_10': 'Conclusion: Multiplying by \\xi leaves zero unchanged, yielding the result \\xi R_{rr} b_r = 0 (agreeing with the given answer).'}"	{'theorem_1': 'Adopt spherically symmetric metric ds^2=-e^{2\\nu(r)}dt^2+e^{2\\mu(r)}dr^2 +r^2\\big(d\\theta^2+\\sin^2\\theta\\,d\\phi^2\\big),\\qquad b_\\mu=\\big(b_t(r),\\,b_r(r),\\,0,\\,0\\big). The field strength b_{\\mu\\nu} \\equiv \\partial_\\mu b_\\nu-\\partial_\\nu b_\\mu.', 'theorem_2': 'The bumblebee model can be treated as a concrete example of the minimal gravitational SME, and its action is S = \\int d^4 x \\sqrt{-g} \\left( \\frac{R}{2\\kappa} + \\frac{\\xi}{2\\kappa} B^{\\mu\\nu} R_{\\mu\\nu} - \\frac{1}{4} B^{\\mu\\nu} B_{\\mu\\nu} - V\\right) + S_{\\text{m}}.', 'concept_1': 'The action for the bumblebee gravity model yields the field equations for the tensor field and the vector field \\mathcal{E}^{\\mu} \\equiv \\nabla_{\\nu} B^{\\mu\\nu} - \\frac{\\xi}{\\kappa} R^{\\mu\\nu} B_{\\nu} + 2 B^{\\mu} \\frac{dV}{d(B^{\\lambda} B_{\\lambda})} = 0.'}	\xi R_{rr} b_r =0	理论交叉学科-理论天体物理-中子星	99977	2	3
recuWB6MQKVuYy	In the absence of CS, the antiparticle contribution to the quark single-particle distribution function is absent. In this case, the energy density of quarks can be written as a momentum integral whose integrand is the product of the Fermi–Dirac distribution function and the single-particle energy in the limit of a vanishing pairing gap. In CSQY matter, the antiparticle contribution no longer vanishes, and each term in the distribution should be multiplied by the single-particle energy not shifted by the quark Fermi energy, i.e., \(\varepsilon^{\pm}_{fck}\mp\varepsilon_{fc}\). At zero temperature (T=0) and in natural units (\(\hbar=c=k_B=1\)), the Bogoliubov weight (\(\varepsilon_{fc}\pm\varepsilon_{fk}\)/|\(\varepsilon^{\pm}_{fck}\)|), and the unshifted energy (\(\varepsilon^{\pm}_{fck}\mp\varepsilon_{fc}\)); the result must show an explicit sum over the two branches \(\sum_{\pm}\). (Vacuum subtraction / no-sea (normal ordering) is assumed; the Nambu–Gor’kov overall 1/2 cancels the spin degeneracy g_s=2, so no extra spin factor.) Answer with the explicit equation in one sentence without using the internet.	{'Step_1': 'Goal: derive the explicit zero-temperature (T=0) energy-density equation for a fixed quark flavor f and color c in CSQY matter, in one sentence.', 'Step_2': 'Statistical setup (Concept_1): work in natural units (ℏ=c=k_B=1) with the phase-space measure ∫ d^3k/(2π)^3; at T→0 use f(E;μ,T)=[e^{(E−μ)/T}+1]^{-1} \\xrightarrow[T\\to0]{} \\theta(μ−E), so occupied states are selected by n_\\mathbf{k}=\\theta(μ−\\varepsilon_\\mathbf{k}), and the energy density takes the generic form ε=∫ (d^3k/(2π)^3) f(k) 𝔈(k).', 'Step_3': 'Quasiparticle spectrum (Concept_2): in the paired phase (Nambu–Gor’kov/BCS) there are two branches E_\\mathbf{k}^{±}=\\sqrt{(\\varepsilon_\\mathbf{k}±μ)^2+Δ^2} (nonrelativistic limit E_\\mathbf{k}=\\sqrt{\\xi_\\mathbf{k}^2+Δ^2}, with \\xi_\\mathbf{k}=\\varepsilon_\\mathbf{k}−μ).', 'Step_4': 'Coherence factors and weights (Concept_3): the Bogoliubov factors satisfy u_{\\mathbf{k},±}^2=\\tfrac12(1+\\xi_{\\mathbf{k},±}/E_{\\mathbf{k}}^{±}), v_{\\mathbf{k},±}^2=\\tfrac12(1−\\xi_{\\mathbf{k},±}/E_{\\mathbf{k}}^{±}), hence u_{\\mathbf{k},±}^2−v_{\\mathbf{k},±}^2=\\xi_{\\mathbf{k},±}/E_{\\mathbf{k}}^{±}=(μ±\\varepsilon_\\mathbf{k})/|E_{\\mathbf{k}}^{±}|, which provides the required weight; sum explicitly over the two branches ∑_{±}.', 'Step_5': 'Energy accounting and vacuum subtraction (Concept_4): use the unshifted single-particle energy 𝔈_{±}^{unshifted}=E_{\\mathbf{k}}^{±}∓μ, and implement vacuum subtraction (no-sea/normal ordering), which at T=0 leaves only the Fermi-sphere contribution enforced by \\theta(μ−\\varepsilon_\\mathbf{k}).', 'Step_6': 'Assembly to the fixed-(f,c) integrand: multiply the occupancy selector \\theta(μ−\\varepsilon_\\mathbf{k}) by the Bogoliubov weight (μ±\\varepsilon_\\mathbf{k})/|E_{\\mathbf{k}}^{±}| and by the unshifted energy (E_{\\mathbf{k}}^{±}∓μ), integrate over k with ∫ d^3k/(2π)^3, and sum over ±.', 'Step_7': 'Notation alignment and total density: identify μ≡\\varepsilon_{fc}, \\varepsilon_\\mathbf{k}≡\\varepsilon_{fk}, E_{\\mathbf{k}}^{±}≡\\varepsilon_{fck}^{±}; the total quark energy density is obtained by additionally summing the fixed-(f,c) result over flavors f and colors c.'}	{'Concept_1': 'Statistics and zero-temperature (measure + occupancy) f(E;μ,T)=\\bigl[e^{(E-\\mu)/T}+1\\bigr]^{-1},\\quad \\lim_{T\\to0} f=\\theta(\\mu-E);\\quad \\varepsilon=\\sum_{\\text{components}}\\!\\int \\frac{d^3k}{(2\\pi)^3} f(k)\\,\\mathcal{E}(k);\\quad n_{\\mathbf k}=\\theta(\\mu-\\varepsilon_{\\mathbf k}).', 'Concept_2': 'Quasiparticle dispersion in the paired phase (Nambu–Gor’kov/BCS) Relativistic two-branch form (quark matter): E^{\\pm}_{\\mathbf k}=\\sqrt{(\\varepsilon_{\\mathbf k}\\pm\\mu)^2+\\Delta^2} (nonrelativistic limit: E_{\\mathbf k}=\\sqrt{\\xi_{\\mathbf k}^2+\\Delta^2},\\ \\xi_{\\mathbf k}=\\varepsilon_{\\mathbf k}-\\mu).', 'Concept_3': 'Bogoliubov coherence factors and weights u_{\\mathbf k,\\pm}^2=\\tfrac12\\!\\left(1+\\frac{\\xi_{\\mathbf k,\\pm}}{E^{\\pm}_{\\mathbf k}}\\right),\\ v_{\\mathbf k,\\pm}^2=\\tfrac12\\!\\left(1-\\frac{\\xi_{\\mathbf k,\\pm}}{E^{\\pm}_{\\mathbf k}}\\right),\\ u_{\\mathbf k,\\pm}^2-v_{\\mathbf k,\\pm}^2=\\frac{\\xi_{\\mathbf k,\\pm}}{E^{\\pm}_{\\mathbf k}};\\ \\xi_{\\mathbf k,\\pm}=\\varepsilon_{\\mathbf k}\\pm\\mu \\Rightarrow (\\mu\\pm\\varepsilon_{\\mathbf k})/|E^{\\pm}_{\\mathbf k}|.', 'Concept_4': 'Energy accounting and vacuum subtraction Use the **unshifted** single-particle energy \\mathcal{E}^{\\text{unshifted}}_{\\pm}=E^{\\pm}_{\\mathbf k}\\mp\\mu;vacuum subtraction / no-sea (normal ordering): \\varepsilon_{\\text{medium}}=\\varepsilon-\\varepsilon_{\\text{vac}}, leaving only \\theta(\\mu-\\varepsilon_{\\mathbf k}) contributions at T\\to0.'}	\varepsilon_{fc} = \sum_{\pm}\int \frac{d^3k}{(2\pi)^3}\; \theta(\varepsilon_{fc}-\varepsilon_{fk})\; \frac{\varepsilon_{fc}\pm \varepsilon_{fk}}{\lvert \varepsilon^{\pm}_{fck}\rvert}\, \bigl(\varepsilon^{\pm}_{fck}\mp \varepsilon_{fc}\bigr).	理论基础学科-理论天体物理-中子星	419237	2	5
recuWB9pwqtUNp	Checklist (10 points)  1. How does PLD4 lead to the development of Systemic Lupus Erythematosus (SLE)? (6 points)  2. What targeted therapeutic strategies can be used? (4 points)  Hints:  HINT_1: The five involved patients were diagnosed with Systemic Lupus Erythematosus (SLE). All patients presented with renal-related phenotypes, and kidney biopsies showed proliferative lupus nephritis. In addition, all patients tested positive for antinuclear antibodies (ANA) and exhibited hypocomplementemia.  HINT_2: Whole-exome sequencing revealed that all patients carried biallelic mutations in the PLD4 gene. All mutations were located within the catalytic domain of PLD4 and were predicted to be deleterious.  HINT_3: Although the various PLD4 mutations were not spatially clustered within a single structural domain, structural predictions indicated that they might affect exonuclease activity through different mechanisms.  HINT_4: Gene Set Enrichment Analysis (GSEA) showed that the IFN-α response, IFN-γ response, and TNF signaling via NF-κB were the most significantly enriched pathways in patients P1 and P2. The heatmap revealed that genes involved in inflammatory signaling pathways—especially type I interferon pathways—were markedly upregulated in PBMCs (peripheral blood mononuclear cells) isolated from P1 and P2.  HINT_5: Flow cytometry analysis indicated elevated STAT1 phosphorylation levels in PBMCs from P1 and P2, further confirming activation of the type I IFN pathway.  HINT_6: Intracellular staining results showed markedly increased levels of IFN-α, IL-1β, IL-6, and IL-8 in PBMCs from patient P1.  HINT_7: In PBMCs from P1 and P2, the proportion of CD14⁺ monocytes was increased, while the proportion of CD19⁺ B cells was decreased.  HINT_8: Cells with high expression of NF-κB pathway–related genes were distributed across most cell types.  HINT_9: In patients P1 and P2, cells with high expression of type I IFN pathway genes (such as IFIT2, OAS2, IFI44, and IFI44L) were mainly found in PLD4-expressing cells, such as dendritic cells (DCs) and monocytes.  HINT_10: Compared with healthy controls, DCs from P1 and P2 exhibited significantly elevated expression of genes encoding TLR7/9 and TLR signaling–related molecules, including TLR7, TLR9, TRAF6, IRAK1, and IRAK4.  HINT_11: TLR9 was upregulated in plasmacytoid dendritic cells (pDCs) from patient P1, consistent with findings in SLE patients without PLD4 mutations.  HINT_12: Genes involved in the type I IFN pathway were also aberrantly activated in B cells from P1 and P2.  HINT_13: Flow cytometric analysis of PBMCs from patients P1 and P4 showed that IgG levels and B-cell subset distributions during remission were normal and comparable to healthy controls, while P4 during disease flare resembled SLE patients without PLD4 mutations.  HINT_14: Compared with healthy controls, PBMCs (especially DCs) from P2 showed higher levels of IFN-α, IFN-γ, TNF, IL-1β, CXCL2, CCL4, IL-23, and GM-CSF.  HINT_15: Missense mutations in PLD4 (Pro181Leu, Asp189Glu, Arg201Gln, Tyr248Cys, Ala323Val, and Gly457Asp) impaired PLD4 exonuclease activity. Exonuclease activity assays using lysates from HEK293T cells overexpressing mutant PLD4 consistently showed that ssDNA remained undigested in all mutant groups.  HINT_16: Endogenous PLD4 exonuclease activity was evaluated in PBMCs from P1 and healthy controls. After 1, 2, and 4 hours of incubation, P1 PBMCs showed more residual substrate, indicating reduced enzymatic activity.  HINT_17: Compared with healthy controls, PBMCs from P1 showed upregulation of full-length and activated TLR7, which triggered downstream inflammatory signaling, including type I IFN and MAPK pathways.  HINT_18: Flow cytometry results showed elevated phosphorylation of STAT2, p65, and ERK, indicating activation of the type I IFN, NF-κB, and MAPK pathways at baseline and following TLR9 agonist (CpG-DNA) stimulation in PBMCs from P1 and P2.  HINT_19: RNA-seq analysis of PBMCs from P1 and P2 revealed that type I IFN pathway genes were significantly upregulated compared with healthy controls, both at baseline and after CpG-DNA stimulation.  HINT_20: qPCR confirmed that transcription of key inflammatory genes (TNF, IFNA2, IFIT1, ISG15, and RSAD2) was elevated in PBMCs from P1 and P2 under both basal and CpG-DNA–stimulated conditions, while PBMCs from SLE patients without PLD4 mutations showed minimal response to CpG-DNA.  HINT_21: Upon CpG-DNA stimulation, monocytes isolated from P1 exhibited higher fold-changes in the expression of several genes compared with healthy controls.  HINT_22: In THP-1 PLD4-KO cells, phosphorylation of STAT1 (p-STAT1), p-p65, and p-ERK was increased, indicating activation of downstream TLR7/9 signaling (including IFN, NF-κB, and MAPK pathways).  HINT_23: In PLD4-KO cells, transcription levels of inflammatory cytokines, chemokines, and interferon-stimulated genes (ISGs) were elevated at baseline.  HINT_24: After CpG-DNA stimulation, PLD4-KO cells showed greater increases in IFIT1, IFI44L, and CXCL3expression compared with wild-type cells.  HINT_25: Levels of proinflammatory cytokines IL-1β and IL-6 were also significantly higher in PLD4-KO cells than in wild-type cells.  HINT_26: Immunoblotting demonstrated enhanced STING phosphorylation and inflammatory signaling in THP-1 PLD4-KO monoclonal cell lines.  HINT_27: Treatment with the specific STING inhibitor H-151 effectively attenuated the type I IFN signaling activation induced by PLD4 deficiency.  HINT_28: Under treatment with H-151 or C-176, expression of downstream inflammatory genes—particularly type I IFN–related genes (IFIT1, IFI27, IFI44, OAS1, and ISG15)—was markedly reduced.  HINT_29: qPCR and Western blot analyses revealed that knockdown of STING reversed type I IFN pathway activation, while NF-κB signaling was only partially restored.  HINT_30: Compared with wild-type and heterozygous (Pld4⁺/⁻) mice, Pld4⁻/⁻ mice exhibited autoimmune phenotypes, including slower weight gain, significantly increased plasma anti-dsDNA, anti-dsRNA antibodies, and IgG levels, as well as splenomegaly. Pld4⁻/⁻ mice showed severe renal inflammation, glomerular basement membrane thickening, and increased IgG and C3 deposition.  HINT_31: Inflammatory genes (Tnf, Cxcl10, Mx2, Ifng) were most significantly elevated in kidneys of Pld4⁻/⁻ mice, compared with wild-type and Pld4⁺/⁻ mice.  HINT_32: scRNA-seq analysis of kidneys showed that immune cell infiltration (macrophages, DCs, T cells, and B cells) increased 2–3 fold in Pld4⁻/⁻ mice compared with wild-type controls.  HINT_33: Genes involved in the type I IFN pathway (Ifi27, Isg15, Ddx58) were significantly upregulated in both immune and renal parenchymal cells (podocytes, endothelial cells, principal cells, and proximal tubular cells) of Pld4⁻/⁻ mice.  HINT_34: Compared with wild-type mice, immune cell populations were markedly expanded in Pld4⁻/⁻ mice. Key cell types associated with SLE pathogenesis, including pDCs and plasma cells, were significantly elevated. CD4⁺ effector T cells and CD8⁺ effector T cells were also increased, whereas age-associated B cells and myeloid DCs (mDCs) remained unchanged.  HINT_35: In the spleen, only pDCs and plasma cells were significantly increased in Pld4⁻/⁻ mice compared with wild-type mice.  HINT_36: Autoantibody analysis showed that chimeric mice reconstituted with Pld4⁻/⁻ bone marrow exhibited elevated levels of anti-dsDNA and anti-dsRNA antibodies.  HINT_37: Flow cytometry of kidneys showed that expansion of pDCs and plasma cells was cell-intrinsic, while expansion of T cells was extrinsic. Similar results were observed in the spleen, where pDCs and plasma cellsexhibited intrinsic expansion.  HINT_38: After 8 weeks of oral administration, baricitinib-treated mice showed significant improvements in body weight, plasma anti-dsDNA and anti-dsRNA antibody levels, and spleen size compared with untreated mice.  HINT_39: In baricitinib-treated mice, expression of renal inflammatory genes (Il1b, Il6, Tnf, Ifng, Mx2, Ifit1) and deposition of glomerular immune complexes (IgG and C3) were markedly reduced compared with untreated mice.  HINT_40: Baricitinib treatment significantly suppressed the upregulated type I IFN pathway in patient PBMCs and partially inhibited the NF-κB pathway.  	{'Step_1': 'Using PBMCs (peripheral blood mononuclear cells) from patients P1 and P2 as experimental materials: Flow cytometry showed elevated STAT1 phosphorylation, and intracellular staining revealed marked increases in IFN-α, IL-1β, IL-6, and IL-8. The proportion of CD14⁺ monocytes increased, while that of CD19⁺ B cells decreased in PBMCs. These results indicate that the type I IFN signaling pathway is strongly activated in the cells of patients P1 and P2.', 'Step_2': 'Activation of TLR7/9 and the downstream type I IFN pathway was most prominent in dendritic cells (DCs), suggesting that PLD4 deficiency in DCs triggers systemic inflammation and autoimmunity in the patients.', 'Step_3': 'The endogenous exonuclease activity of PLD4 was evaluated in PBMCs from patient P1 and healthy controls. After 1, 2, and 4 hours of incubation, more residual substrate was detected in P1 PBMCs compared with controls, indicating that the endogenous PLD4 exonuclease activity in patient P1 was severely impaired.', 'Step_4': 'qPCR and Western blot analyses showed that knockdown of STING restored type I IFN pathway activation, while NF-κB signaling was only partially recovered. These findings suggest that the cGAS–STING signaling axis plays a critical role in mediating immune dysregulation caused by PLD4 deficiency.', 'Step_5': 'Compared with wild-type mice, Pld4⁻/⁻ mice exhibited marked expansion of immune cell populations. Key cell types implicated in SLE pathogenesis—including pDCs and plasma cells—were significantly increased. Both CD4⁺ effector T cells and CD8⁺ effector T cells were elevated, while age-associated B cells and myeloid DCs (mDCs) remained unchanged. These results indicate that PLD4 deficiency leads to tissue-specific consequences, with renal tissue damage being particularly prominent.', 'Step_6': 'The beneficial effects of baricitinib indicate that the type I IFN pathway is a key mediator of autoimmune and inflammatory phenotypes resulting from PLD4 deficiency.'}	{'Concept_1': 'Systemic Lupus Erythematosus (SLE) is a complex, multi-organ disease with variable severity. Monogenic lupus represents a subset of autoimmune disorders caused by mutations in a single gene, encompassing a range of diseases with lupus-like manifestations.', 'Concept_2': 'TLR7 and TLR9, located in endosomes, are key sensors of RNA and DNA, respectively, and play crucial roles in the development of SLE. By recognizing endogenous or exogenous nucleic acids, they initiate downstream inflammatory signaling pathways, including type I interferon (IFN), nuclear factor NF-κB, and mitogen-activated protein kinase (MAPK) pathways.', 'Concept_3': 'In plasmacytoid dendritic cells (pDCs), activation of the TLR7 and TLR9 pathways induces massive IFN production, promoting autoantigen presentation and inflammatory responses. In B cells, activation of these two pathways leads to the production of large amounts of autoantibodies against nucleic acids, thereby further enhancing autoantigen presentation and inflammation.', 'Concept_4': 'PLD4 is highly expressed in dendritic cells, monocytes, and B cells. It functions as a 5′ exonuclease localized to endolysosomes, where it degrades single-stranded RNA (ssRNA) and single-stranded DNA (ssDNA), thereby limiting excessive activation of TLR7 and TLR9.', 'Concept_5': 'Pld4 knockout (KO) mice exhibit a range of autoimmune phenotypes, including weight loss, splenomegaly, increased autoantibody production, and immune complex deposition.', 'Concept_6': 'PLD4 enzymatically degrades ssRNA and ssDNA, preventing overactivation of TLR7 and TLR9.', 'Concept_7': 'In PLD3/PLD4-deficient mouse models, the STING-dependent signaling pathway—particularly the type I IFN response—is aberrantly activated.'}	PLD4 mutation or deletion leads to loss of exonuclease activity, accumulation of ssRNA and ssDNA, overactivation of TLR7/9, subsequent activation of pathways such as Type I IFN, NF-κB, and MAPK, induction of immune responses, damage to target organs (e.g., kidneys), and ultimately the occurrence of SLE (Systemic Lupus Erythematosus).\nThe JAK inhibitor Baricitinib can be used.	理论基础学科-生物学-生理学、细胞生物学	317811	3	6
recuWBdJMtzn62	For a neutron star (NS), we consider a single perfect fluid with (T^m)^{\mu}{}_{\nu}=\mathrm{diag}(-\epsilon(r),\,P(r),\,P(r),\,P(r)), where \epsilon and P are the energy density and pressure measured by an observer comoving with the fluid; the rest-mass density of baryonic matter is denoted by \rho; the equation of motion is given by the continuity equation \nabla_{\nu}T_m^{\mu\nu}=0; please reduce this expression and provide the explicit equation, answering in one sentence without using the internet.	"{'Step_1': 'Goal — Reduce the continuity equation ∇ν T_m^{μν}=0 in a static, spherically symmetric perfect-fluid background to an explicit radial hydrostatic balance expression.', 'Step_2': 'Perfect-fluid stress–energy (Concept_1) — Use T^{μν}=(ε+P)u^{μ}u^{ν}+P g^{μν}; in the fluid rest frame T^{μ}{}{ν}=diag(−ε,P,P,P).', 'Step_3': 'Metric & mass function (Concept_2) — Take ds^{2}=−e^{2Φ(r)}dt^{2}+e^{2δ(r)}(1−2m(r)/r)^{−1}dr^{2}+r^{2}dΩ^{2}, with the mass function m(r) defined via g_{rr}.', 'Step_4': 'Fluid 4-velocity (Concept_3) — In static equilibrium u^{μ}=(e^{−Φ},0,0,0), so only the gravitational potentials Φ(r) and δ(r) enter the balance.', 'Step_5': ""Covariant conservation → hydrostatic balance (Concept_4) — From ∇_ν T^{μν}=0, the radial component yields P'(r)=−(ε+P)Φ'(r)."", 'Step_6': ""Field equations for Φ' (Concept_5) — Einstein’s equations give Φ'(r)=[r m'(r)−m(r)]/[r(r−2m(r))]+δ'(r)."", 'Step_7': ""Combine Steps 5 & 6 — Substituting yields the explicit hydrostatic balance: P'(r)=−(ε+P)\\left(\\frac{r m'(r)−m(r)}{r[r−2m(r)]}+δ'(r)\\right)."", 'Step_8': ""Use m'(r)=4π r^{2}ε(r) (if expressed in terms of the matter energy density) — Then P'(r)=−(ε+P)\\left(\\frac{4π r^{3}ε−m}{r[r−2m]}+δ'\\right); an equivalent rearrangement is (P+ε)\\left(\\frac{m−r m'}{2 m r−r^{2}}+δ'\\right) with the overall minus sign understood."", 'One_sentence_answer': ""P'(r)=−(ε+P)\\left(\\frac{r m'(r)−m(r)}{r[r−2m(r)]}+δ'(r)\\right)=−(ε+P)\\left(\\frac{4π r^{3}ε(r)−m(r)}{r[r−2m(r)]}+δ'(r)\\right).""}"	"{'Concept_1': 'Perfect-fluid stress–energy $T^{\\mu\\nu}=(\\epsilon+P)u^{\\mu}u^{\\nu}+P\\,g^{\\mu\\nu}$, and in the fluid rest frame $T^{\\mu}{}_{\\nu}=\\mathrm{diag}(-\\epsilon,P,P,P)$.', 'Concept_2': 'Static, spherically symmetric metric with mass function $ds^{2}=-e^{2\\Phi(r)}dt^{2}+e^{2\\delta(r)}\\!\\left(1-\\frac{2m(r)}{r}\\right)^{-1}\\!dr^{2}+r^{2}d\\Omega^{2}$; define the mass function $m(r)$ via the $g_{rr}$ component.', 'Concept_3': 'Fluid four-velocity in static equilibrium $u^{\\mu}=(e^{-\\Phi},0,0,0)$, so only gravitational potentials $\\Phi(r)$ and $\\delta(r)$ enter the equilibrium.', 'Concept_4': ""Covariant conservation ⇒ hydrostatic balance (TOV form) From $\\nabla_{\\nu}T^{\\mu\\nu}=0$, the radial component gives $P'(r)=-(\\epsilon+P)\\,\\Phi'(r)$."", 'Concept_5': ""Field equations relating $\\Phi'$ to $m,\\,m',\\,\\delta'$ For the metric above, Einstein equations yield $\\displaystyle \\Phi'=\\frac{r\\,m'(r)-m(r)}{r\\,[\\,r-2m(r)\\,]}+\\delta'(r)$.""}"	P'=-(P+\epsilon)\Phi'=(P+\epsilon)\!\left(\frac{m-rm'}{2mr-r^{2}}+\delta'\right)	理论基础学科-理论天体物理-中子星	54594	2	2
recuWFdnj57XXA	Consider, in four-dimensional Euclidean spacetime, the two lowest-lying states—the single-particle state $|\rho\rangle$ and the two-body state $|\pi\gamma\rangle$ (with the smallest relative momentum allowed by the finite volume)—and define $C_{ij}(t)=\langle0|\,O_i(t)\,O_j^\dagger(0)\,|0\rangle$ with spectral decomposition $C_{ij}(t)=\sum_{n\in\{\rho,\pi\gamma\}} V_{i,n}V_{j,n}e^{-E_n t}+\cdots$, $V_{i,n}\equiv\langle0|O_i|n\rangle$; take the operator basis $O_1$ (primarily coupling to $\rho$, vector-current–like) and $O_2$ (the $\pi\gamma$ insertion), adopt the power counting where the photon operator carries an explicit power of $e$ and $O_{\pi\gamma}$ is counted as $O(e)$, so that $V_{1,\rho}=V^{(0)}_{1,\rho}+O(e^2)$, $V_{2,\rho}=O(e^2)$, $V_{1,\pi\gamma}=O(e)$, $V_{2,\pi\gamma}=O(e)$ (with the superscript $(k)$ denoting the coefficient at order $e^k$) and terms beyond $O(e^2)$ neglected. **Write the $2\times2$ overlap matrix $V$ (the explicit powers of $e$ and the coefficient symbols $V^{(k)}_{i,n}$; **Answer in one sentence without using the internet.**	{'Step_1': 'Goal: write, in one sentence, the 2×2 overlap matrix up to and including O(e^2) for rows (O1, O2) and columns (|ρ⟩, |πγ⟩).', 'Step_2': 'Spectral representation (Concept_1): introduce the Euclidean two-point function C_ij(t)=∑n V{i,n} V_{j,n} e^{-E_n t}+… to motivate the role of the overlaps V_{i,n}.', 'Step_3': 'Overlap definition (Concept_2): define V_{i,n}≡⟨0|O_i|n⟩ so each matrix element of the overlap matrix is a vacuum–state matrix element between operator O_i and state |n⟩.', 'Step_4': 'Small-e expansion (Concept_3): expand each overlap as V_{i,n}=∑{k≥0} e^k V^{(k)}{i,n}, making the explicit e-power counting manifest entry-wise.', 'Step_5': 'Truncation to O(e^2) (Concept_4): keep only terms with k≤2 in V_{i,n}=∑k e^k V^{(k)}{i,n} and drop k≥3, so the final one-sentence answer displays explicit e-powers and coefficients V^{(k)}_{i,n} without replacing entries by big-O notation.'}	{'Concept_1': 'Spectral representation of Euclidean two-point functions $$C_{ij}(t)=\\sum_{n} V_{i,n}\\,V_{j,n}\\,e^{-E_n t}+\\cdots .$$', 'Concept_2': 'Definition of overlap amplitudes $$V_{i,n}\\equiv \\langle 0|O_i|n\\rangle .$$', 'Concept_3': 'Small-$e$ expansion (power counting) $$V_{i,n}=\\sum_{k\\ge 0} e^{k}\\,V^{(k)}_{i,n}.$$', 'Concept_4': 'Truncation rule to $O(e^{2})$ Keep terms with $k\\le2$ in $V_{i,n}=\\sum_{k} e^{k}V^{(k)}_{i,n}$, drop $k\\ge3$.'}	$$ V=\begin{pmatrix} V^{(0)}_{1,\rho}+e^{2}V^{(2)}_{1,\rho} & e\,V^{(1)}_{1,\pi\gamma}\\[4pt] e^{2}V^{(2)}_{2,\rho} & e\,V^{(1)}_{2,\pi\gamma} \end{pmatrix}. $$	理论基础学科-电磁理论与量子电磁学-QED	1204264	2	2
recuWHuOJGqcsf	Given a pre-determined, first-stage decision on the attribution of each vehicle fleet $f$ to a rescue station $r$ during period $t$ (defined by the binary variable $\alpha_{t,r,f}$), construct the complete mathematical expression that quantifies the total operational failure cost within a single, specific scenario $s$. This expression must capture the economic consequence of rescue delays by meeting the following criteria: it must be based on the positive time difference between a vehicle fleet's actual arrival at an accident site and that site's latest acceptable rescue time; it must incorporate a scenario-specific unit cost for this delay; and it must aggregate this cost over the entire set of vehicle fleets, accident sites, and time periods for that given scenario.	"{'step_1': 'The instruction requires the formulation of the second-stage objective function, $Y^s$, which quantifies the total ""operational failure cost"" for a specific scenario $s$.', 'step_2': 'The problem is framed within a two-stage stochastic model. The instruction specifies that the first-stage decision ($\\alpha_{t,r,f}$) is already fixed, and we are now in the second stage, evaluating a single scenario of realized random events (Concept_1).', 'step_3': 'The core of the ""operational failure cost"" is the penalty associated with delayed rescues (Concept_2).', 'step_4': 'The instruction states the cost must be based on the ""positive time difference"" between arrival and a deadline. For any single rescue task (fleet $f$ to accident $w$), this time difference is $(\\mu_{w,f}^{s,t} - l_w^s)$. The ""positive"" requirement is mathematically handled by the positive part function, $(x)^+$ (Concept_3), using the specific model variables for arrival time and latest rescue time (Concept_4).', 'step_5': 'This time delay must be converted into a monetary value. The instruction calls for a ""scenario-specific unit cost,"" which corresponds to the parameter $c_w^s$. Multiplying the time delay by this unit cost gives the penalty for a single event: $c_w^s (\\mu_{w,f}^{s,t} - l_w^s)^+$ (Concept_4).', 'step_6': 'Finally, the instruction requires this cost to be aggregated ""over the entire set of vehicle fleets, accident sites, and time periods."" This translates directly to applying a summation over the sets $F$, $W$, and $T$ (Concept_4).', 'step_7': ""Combining these components yields the complete expression for the total cost. The model's goal is to minimize this cost by optimizing the second-stage decisions (like arrival times), hence the `Min` operator is prefixed to the summation. This results in the final formula: $Y^s = \\text{Min} \\sum_{f \\in F} \\sum_{t \\in T} \\sum_{w \\in W} c_w^s (\\mu_{w,f}^{s,t} - l_w^s)^+$.""}"	{'concept_1': 'A two-stage stochastic programming model is used to handle uncertainty. In the first stage, baseline resource distribution decisions are made before random events are known. In the second stage, operational dispatching decisions are made for each specific scenario of random events.', 'concept_2': 'The objective of the second stage is to minimize the total penalty cost, which is incurred when a rescue vehicle fleet fails to arrive at an accident site by the latest required time.', 'concept_3': 'The positive part function, denoted as $(x)^+$, is equivalent to $\\max(0, x)$. ', 'concept_4': 'The model utilizes several key parameters and variables for its calculation:\n- $\\mu_{w,f}^{s,t}$: A decision variable representing the time when vehicle fleet $f$ arrives at accident site $w$ during period $t$ in scenario $s$.\n- $l_w^s$: A parameter for the latest acceptable rescue time for accident site $w$ in scenario $s$.\n- $c_w^s$: A parameter representing the unit penalty cost per unit of time for exceeding the latest rescue time of accident site $w$ in scenario $s$.\n- $F, T, W$: Sets representing all vehicle fleets, all time periods, and all traffic accident sites, respectively.'}	$Y^s = \text{Min} \sum_{f \in F} \sum_{t \in T} \sum_{w \in W} c_w^s (\mu_{w,f}^{s,t} - l_w^s)^+$	理论交叉学科-交通流理论-交通拥堵形成机理	624662	2	4
recuWKPwEMwWdZ	In systems science, multilayer networks are often used to represent dynamic or context-dependent interactions among system components. Suppose we observe adjacency matrices \{A^{(t)}\}_{t=1}^m from such a system, where each layer admits a latent low-rank structure aligned with a common invariant subspace U \in \mathbb{R}^{n \times d}. A naive spectral analysis of empirical second-order statistics, such as \tfrac{1}{m}\sum_{t=1}^m A^{(t)}(A^{(t)})^\top, generally fails to recover U because additional structural distortions (e.g., diagonal dominance, sampling bias, or layer-specific noise) mask the true system invariants. Formulate a consistent procedure to estimate U that explicitly identifies and corrects these distortions before applying spectral decomposition.	{'step_1': 'Link adjacency matrices to the invariant subspace (Concept 1). From the RDPG framework, each population adjacency matrix satisfies P^{(t)} = U B^{(t)} U^\\top, where U \\in \\mathbb{R}^{n \\times d} is the common invariant subspace. Thus, the column space of P^{(t)} is exactly \\text{span}(U) for all t. The goal reduces to recovering this subspace from the observed \\{A^{(t)}\\}.', 'step_2': 'Correct structural distortions (Concept 2). Naively using A^{(t)} (A^{(t)})^\\top introduces bias from diagonal dominance and sampling noise. Lei \\& Rinaldo (2015) show that hollowing (removing diagonals) and subtracting diagonal bias yields an unbiased estimator: \\tilde{C}^{(t)} = H(A^{(t)} (A^{(t)})^\\top) - \\hat{M}^{(t)}, with expectation \\mathbb{E}[\\tilde{C}^{(t)}] = U (B^{(t)} B^{(t)\\top}) U^\\top. Averaging across layers suppresses layer-specific noise: \\bar{C} = \\frac{1}{m} \\sum_{t=1}^m \\tilde{C}^{(t)} \\approx U \\Big( \\frac{1}{m} \\sum_{t=1}^m B^{(t)} B^{(t)\\top} \\Big) U^\\top.', 'step_3': 'Extract U via spectral decomposition (Concept 3). Since \\bar{C} is symmetric and low-rank with column space \\text{span}(U), its top-d eigenvectors span U. Let \\hat{U} = [v_1, v_2, \\dots, v_d], where v_i are the leading eigenvectors of \\bar{C}. By the Davis–Kahan theorem, if \\delta = \\lambda_d - \\lambda_{d+1} > 0 is the eigengap of \\mathbb{E}[\\bar{C}], then \\|\\sin \\Theta(\\hat{U}, U)\\| \\leq \\frac{\\|\\bar{C} - \\mathbb{E}[\\bar{C}]\\|}{\\delta} \\to 0 \\quad \\text{as } n,m \\to \\infty, ensuring that \\hat{U} consistently estimates U.'}	{'concept_1': 'In a random dot product graph (RDPG), each vertex i has a latent position X_i \\in \\mathbb{R}^d, and the edge probability matrix is P = XX^\\top. The column space of X represents a common low-dimensional invariant subspace, which can be consistently estimated from the adjacency matrix via spectral decomposition.', 'concept_2': 'For adjacency matrices under stochastic block models or their extensions, the expectation of A A^\\top contains non-negligible diagonal terms that bias spectral estimation. A standard correction is to apply the hollowing operator H(\\cdot) to remove diagonals, or equivalently subtract a diagonal matrix \\hat M estimated from data, so that \\tilde C = H(AA^\\top) - \\hat M has expectation aligned with the true low-rank signal.', 'theorem_1': 'Let A, \\hat A \\in \\mathbb{R}^{n \\times n} be symmetric, with eigenspaces U and \\hat U corresponding to the top-d eigenvalues. Then for the subspace distance, \\|\\sin\\Theta(\\hat U, U)\\| \\leq \\frac{\\|\\hat A - A\\|}{\\delta}, where \\delta is the eigengap between the d-th and (d+1)-th eigenvalues of A. This provides a non-asymptotic bound ensuring that \\hat U is close to U when \\hat A is a good approximation of A.'}	\hat{U} \;=\; \operatorname{eigs}\!\Bigg( \frac{1}{m}\sum_{t=1}^m \Big[ H\!\big(A^{(t)} A^{(t)\top}\big) - \hat{M}_t \Big], \, d \Bigg)	理论工程学科-系统科学与系统理论-复杂系统建模与仿真	395120	2	3
recuWMOFoF2xXe	Based on the coupled geodynamic and dynamic rupture simulations of the strike-slip fault system, what is the plausible range for the critical linear slip-weakening distance, \(D_c\)?	{'step_1': 'Establish the Foundational Friction Framework with the Linear Slip-Weakening (LSW) Law. The analysis begins by adopting the **Linear Slip-Weakening Friction Law** (Eq. 22: \\(\\mu\\left(S, D_{c}\\right):=\\mu_{s}-\\frac{\\mu_{s}-\\mu_{d}}{D_{c}} min \\left(S, D_{c}\\right)\\)), where \\(D_c\\) (critical slip-weakening distance) is a core parameter. This law defines how fault friction decreases from the static friction coefficient (\\(\\mu_s\\)) to the dynamic friction coefficient (\\(\\mu_d\\)) as slip (\\(S\\)) accumulates, forming the fundamental basis for modeling dynamic rupture behavior and constraining \\(D_c\\).', 'step_2': 'Generate Long-Term Lithospheric Context via 3D Visco-Plastic Thermo-Mechanical Modeling (pTatin3D). To obtain physically realistic initial conditions for dynamic rupture simulations, the thesis uses **pTatin3D** to perform 3D visco-plastic thermo-mechanical geodynamic modeling. This tool simulates long-term lithospheric deformation by solving three core conservation equations: momentum conservation (Eq. 1), mass conservation (Eq. 2), and thermal energy conservation (Eq. 4). These simulations capture the slow, large-scale processes that shape the fault system’s pre-rupture state.', 'step_3': 'Extract Critical Initial Conditions from pTatin3D Outputs. The pTatin3D models produce three key outputs required for dynamic rupture modeling: (1) a 3D stress field (which drives rupture initiation and propagation), (2) fault zone rheological properties (which influence friction evolution), and (3) volumetric representations of shear zones (the precursors to discrete fault surfaces). These outputs are essential for grounding dynamic rupture simulations in geophysically plausible conditions.', 'step_4': 'Refine Fault Geometry Using the Medial Axis Transform (MAT). Accurate fault geometry is a prerequisite for reliable dynamic rupture modeling, as rupture propagates along 2D fault surfaces (not 3D volumes). The thesis applies the **Medial Axis Transform** (Section 3), a geometric method that converts the 3D volumetric shear zones (from pTatin3D) into discrete, high-resolution 2D fault surfaces. This step ensures the dynamic rupture model uses a realistic representation of the fault’s spatial extent and orientation.', 'step_5': 'Set Up 3D Dynamic Rupture Simulations with SeisSol. Using the refined inputs (pTatin3D-derived stress/rheology and MAT-derived fault geometry), the thesis employs **SeisSol** to run 3D dynamic rupture and seismic wave propagation simulations. SeisSol solves the governing equations for dynamic rupture: momentum conservation (Eq. 17) and the constitutive relationship for fault friction (Eq. 18), which incorporates the LSW law (and thus the \\(D_c\\) parameter).', 'step_6': 'Design a Parameter-Sweeping Experiment for \\(D_c\\). To constrain \\(D_c\\), the thesis conducts **9 constrained SeisSol simulations** by systematically varying \\(D_c\\) across a range of potential values. Each simulation tests a unique \\(D_c\\) while holding all other parameters (stress, rheology, fault geometry) constant. This controlled experiment isolates the effect of \\(D_c\\) on rupture dynamics, enabling direct comparison of results.', 'step_7': 'Analyze Rupture Dynamic Metrics to Evaluate Physical Plausibility. For each SeisSol simulation (with a distinct \\(D_c\\)), the thesis analyzes key metrics of rupture behavior to assess physical consistency: - **Fault slip distribution**: Whether slip magnitudes and patterns align with expected behavior for strike-slip faults (e.g., concentrated slip in fault cores, gradual tapering at edges). - **Rupture velocity**: Whether the velocity stays below the shear wave speed (a physical constraint; super-shear velocities are only plausible in specific contexts, which are evaluated here). - **Seismic magnitude**: Whether the simulated magnitude matches typical values for the studied strike-slip fault system (if observational data is available) or falls within geophysically reasonable bounds.', 'step_8': 'Filter Unphysical Simulations to Retain Plausible \\(D_c\\) Values. Simulations with \\(D_c\\) values that produce unphysical behavior (e.g., unrealistic slip concentrations, super-shear rupture where not justified, or magnitudes outside expected ranges) are discarded. Only simulations where the rupture metrics (slip, velocity, magnitude) are geophysically consistent are retained, narrowing down the set of candidate \\(D_c\\) values.', 'step_9': 'Consolidate Retained Values to Define the Plausible \\(D_c\\) Range. The final step involves compiling the \\(D_c\\) values from the physically plausible SeisSol simulations. This compilation defines the **plausible range for \\(D_c\\)**—a set of values that, when integrated with the LSW law, geodynamic initial conditions, and refined fault geometry, consistently reproduce realistic strike-slip fault rupture behavior.'}	{'concept_1': 'Linear Slip-Weakening Friction Law. The thesis employs this law (Eq. 22: \\(\\mu\\left(S, D_{c}\\right):=\\mu_{s}-\\frac{\\mu_{s}-\\mu_{d}}{D_{c}} min \\left(S, D_{c}\\right)\\)) where \\(D_c\\) (critical slip weakening distance) is a core parameter; the law describes how fault friction decreases with slip, forming the basis for analyzing dynamic rupture behavior and constraining \\(D_c\\).', 'concept_2': '3D Visco-Plastic Thermo-Mechanical Geodynamic Modeling (via pTatin3D). The thesis uses pTatin3D to simulate long-term lithospheric deformation (governing equations: momentum conservation Eq. 1, mass conservation Eq. 2, thermal energy conservation Eq. 4). These models provide key initial conditions (3D stress field, fault geometry, rheological properties) for dynamic rupture simulations, which are essential for determining the plausible \\(D_c\\) range.', 'concept_3': '3D Dynamic Rupture and Seismic Wave Propagation Modeling (via SeisSol). The thesis uses SeisSol to solve dynamic rupture (governing equations: momentum conservation Eq. 17, constitutive relationship Eq. 18) and perform 9 constrained simulations. By testing different \\(D_c\\) values and analyzing rupture dynamics (e.g., slip, rupture velocity, magnitude), the plausible \\(D_c\\) range is identified.', 'concept_4': 'Medial Axis Transform for Fault Surface Extraction. The thesis applies this geometric method (Section 3) to convert 3D volumetric shear zones (from geodynamic models) into 2D fault surfaces. Accurate fault geometry (a product of this method) is a prerequisite for setting up dynamic rupture models that calculate and constrain \\(D_c\\).'}	\(D_{c} \in[0.6,1.5]\) (unit: meters)	理论工程学科-地质力学与地球动力学-long-term rheology	118689	1	4
recuWPQMdNIolp	In a two-dimensional fault + rate-state friction (aging law) model, if the parameters satisfy b > a (velocity weakening) and stiffness k < k_c, please qualitatively describe the long-term evolution characteristics of the system: Is the event sequence in a steady state, quasi-periodic, or chaotic; Is the interval between events converging, diverging, or showing no obvious trend over time? Is the range of the fractured space expanding, contracting, or remaining stable?	{'step_1': 'Establish system dynamics equations: Using the concept: Concept 1 (Rate-State Friction Law). Express the frictional force of the fault as: \\tau = \\sigma_n \\big[\\mu_{\\rm ref}+a\\ln(V/V_{\\rm ref})+b\\ln(\\theta V_{\\rm ref}/D_{RS})\\big] and combine it with the state evolution equation \\theta=1-\\theta V/D_{RS} to obtain a dynamic system describing the evolution of slip rate V and state variable \\theta.', 'step_2': 'Analyze the uniform steady state and its stability. The concept used here is Concept 2 (critical stiffness criterion): first, find the steady-state condition \\(\\theta_{\\rm ss} = D_{RS}/V\\), then linearize the system, obtain the characteristic equation, and solve for the critical stiffness: \\(k_c = \\frac{\\sigma_n (b-a)}{D_{RS}}\\): when \\(k < k_c\\), the uniform steady state becomes unstable, and the system enters the self-excited sliding region.', 'step_3': 'Determine the type of event sequence: The concept used is Concept 3 (Chaotic Attractor Theory): Low stiffness and velocity-weakening conditions cause the system trajectory to fall into a chaotic attractor, manifesting as an irregular and aperiodic event sequence. Therefore, the event sequence type is determined to be chaotic.', 'step_4': 'Analyze the evolution trend of time intervals: The concept used is Concept 3 (Chaotic Attractor Theory). On a chaotic attractor, although the event intervals have short-term fluctuations, the long-term statistics will converge to the average recurrence time of the attractor. Therefore, the interval trend is determined to converge to the statistical average value.', 'step_5': 'Analyze the evolution of the rupture space range: The concept used is Concept 4 (correlation between pre-event state and spatial rupture). By searching for the optimal pre-event state on the attractor, it can be seen that its rupture range gradually approaches the maximum rupturable area of the system, showing a trend of slow spatial expansion. Therefore, the rupture range is determined to expand slowly until it covers the rupturable area.'}	"{'concept_1': ""Rate-state friction law: Provides the system's dynamic equations and steady-state conditions."", 'concept_2': 'Critical Stiffness Criterion: It indicates that steady-state instability occurs when k < k_c.', 'concept_3': 'Chaos Attractor Theory: explaining the irregularity and interval convergence of event sequences.', 'concept_4': 'Correlation between pre-event state and spatial rupture: It is pointed out that the optimal distribution of pre-event states determines the gradual expansion of the rupture range.'}"	Event sequence: Chaos  Interval trend: converging to the statistical average of the attractor  Rupture range: slowly expanding until covering the rupture-prone area	其他理论学科-地质力学与地球动力学-地震成因理论	217000	3	4
recuWPFjHL0bXk	"Within the Gutenberg–Richter framework, given a fixed absolute threshold M_w (with cM_w > M_c) and keeping the number of observed events N (with the statistical standard being ""≥ completeness threshold M_c"") unchanged. If we artificially increase the completeness threshold M_c to M_c + Δ (where Δ > 0), will the exceedance probability of ""at least one earthquake ≥ M_w occurring"" increase, decrease, or remain unchanged?"	"{'step_1': 'Write the earthquake magnitude distribution (using concept_1): According to the frequency-magnitude relationship by Gutenberg & Richter: \\log_{10} N(M \\ge M_w) = a - b M_w, the conditional cumulative distribution function of magnitude can be obtained: F(M_w \\mid M_c,b)=P(M \\le M_w)=1-10^{-b(M_w-M_c)}, \\quad M_w \\ge M_c', 'step_2': 'Write out the maximum magnitude distribution (using concept_3): According to the extreme value statistical results by Cornell., if events are independent and identically distributed, the probability that the maximum magnitude is less than the threshold is P(M_{\\max}<M_w \\mid N) = [F(M_w)]^{N}', 'step_3': ""Write the exceedance probability formula (using concept_4: According to Shapiro's definition of exceedance: P_{\\mathrm{ex}} = 1 - P(M_{\\max}<M_w \\mid N) = 1 - [F(M_w)]^{N}"", 'step_4': ""Analyze the impact of an increase in M_c on F(M_w) (using concept_2): According to Ogata's (1988) definition of the completeness magnitude, increasing M_c is equivalent to shifting the lower limit of the distribution to the right. Under the condition that N remains unchanged, the exponential term F(M_w) = 1 - 10^{-b(M_w - M_c)} in the CDF formula increases as M_c increases, meaning that events are more concentrated in higher magnitudes."", 'step_5': 'Determine the monotonicity of the exceedance probability and derive the answer: Substitute the result of the increase in F(M_w) into the formula from Step 3: P_{ex} = 1 - [F(M_w)]^N. Since 0 < F(M_w) < 1, its power term [F(M_w)]^N increases as F(M_w) increases, causing 1 - [F(M_w)]^N to increase accordingly. Therefore, the exceedance probability increases as M_c increases.'}"	{'concept_1': 'Gutenberg–Richter frequency–magnitude relationship: Gutenberg & Richter proposed that \\(\\log_{10}N(M \\ge M_w) = a - bM_w\\), indicating that the logarithmic frequency of earthquake magnitudes has a linear relationship with magnitude, which is the starting point for all probability derivations.', 'concept_2': 'Definition of completeness magnitude: Ogata defined the catalog completeness magnitude M_c to determine the statistical lower limit of the magnitude distribution and ensure the validity of the frequency-magnitude relationship.', 'concept_3': 'Extreme value distribution and order statistics: Cornell and other references on extreme value statistics clearly state that the probability that the maximum value is less than a threshold is equal to the Nth power of the cumulative distribution function (CDF) of a single sample, i.e., P(Mₘₐₓ < M_w) = F(M_w)ᴺ.', 'concept_4': 'Definition of probability of exceedance: The probability of exceedance is defined as Pₑₓ = 1 - P(Mₘₐₓ < M_w) and is used in seismic hazard analysis.'}	increase	其他理论学科-地质力学与地球动力学-地震成因理论	105721	2	4
recuWY8O3pa7iE	How to calculate the b-value from an earthquake catalog? The formula must include: ⟨M⟩ (mean magnitude of events with M ≥ M_min), ΔM=0.01 (fixed magnitude resolution, introducing ΔM/2 offset correction), M_min (determined by Lilliefors test). In this work we used the Lilliefors test that is a modification of the Kolmogorov–Smirnov (KS) test to assess whether the magnitude is exponentially distributed. Lilliefors is performed as a function of 𝑀𝑚𝑖𝑛 value for many initializations of the random noise (added to transform into a continuous random variable the binned magnitudes) from which we obtain a probability at each magnitude bin that expresses if the assumed null hypothesis is true. M_min ≤ M ≤ M_max and M_max - M_min ≥ 3.	{'step1': 'Start with the earthquake catalog data, ensuring it covers a magnitude range where M_min ≤ M ≤ M_max and M_max - M_min ≥ 3 to satisfy the condition for simplifying the probability density function f(M) = b ln(10) 10^{-b (M - M_min)}, as derived from the Gutenberg-Richter law log N(M) = a - b M (concept1 and concept2). This ensures the distribution is suitable for exponential fitting without truncation effects.', 'step2': 'Exclude short-term aftershock incompleteness (STAI) periods following major earthquakes to avoid bias in the magnitude-frequency distribution (MFD), using predefined durations like +2 days for the Norcia mainshock and +0.8, +0.6, +0.4 days for others (from Herrmann et al., 2022). Filter the catalog to include only events above potential incompleteness thresholds.', 'step3': 'Determine M_min (magnitude of completeness, McLilliefors) using the Lilliefors test, a modification of the Kolmogorov-Smirnov (KS) test to check if magnitudes are exponentially distributed (null hypothesis). Perform the test as a function of candidate M_min values: for each, add random noise to binned magnitudes (ΔM=0.01) to treat them as continuous, run multiple initializations (e.g., many iterations), and compute p-values per magnitude bin. Set significance level to 0.1; McLilliefors is the lowest M_min where p-value > 0.1 for at least five consecutive bins (first exceedance).', 'step4': 'Once McLilliefors (M_min) is identified, select all events in the catalog with M ≥ M_min. Compute the sample mean magnitude ⟨M⟩ of these events, which represents the average of magnitudes larger than M_min.', 'step5': 'Apply the maximum likelihood estimation (MLE) formula for b (theorem1), accounting for discrete magnitudes: b = 1 / [ln(10) (⟨M⟩ - (M_min - ΔM/2))], where ΔM=0.01 is the fixed binning resolution, introducing the ΔM/2 offset to correct for the discrete nature (Utsu et al., 1966). This yields the b-value estimate.', 'step6': 'Validate the result by confirming the MFD follows the Gutenberg-Richter law above M_min and compute 95% confidence intervals using Shi and Bolt (1982) if needed. Repeat for subsets (e.g., lithologies or fault volumes) to ensure robustness, with N events ≥ McLilliefors ideally >1000 for stable estimates.'}	{'concept_1': 'Gutenberg-Richter law: log N(M) = a - b M, where N(M) is the number of earthquakes with magnitude greater than M, a and b are constants.', 'concept_2': 'Magnitude probability density function: f(M) = b ln(10) 10^{-b (M - M_min)}.', 'theorem_1': 'Maximum likelihood estimation method to obtain the estimate of b.'}	b = 1 / [ln(10) (⟨M⟩ - (M_min - ΔM/2))]	其他理论学科-地质力学与地球动力学-地震成因理论	100349	3	3
recuWZ9kPN0LB5	\[\begin{aligned}\max_{\Omega} n(\mathbf{x}) &\leq M_1, \quad \max_{\Omega} p(\mathbf{x}) \leq M_2, \\ \frac{\max_{\bar{\Omega}} n(\mathbf{x})}{\min_{\Omega} n(\mathbf{x})} &\leq C_1(\gamma), \quad \frac{\max_{\bar{\Omega}} p(\mathbf{x})}{\min_{\Omega} p(\mathbf{x})} \leq C_2(\gamma).\end{aligned} \tag{22}\]\n\n\nproblem :** If\n\[\begin{cases}\gamma_1 \geq \frac{C_1(\gamma)}{2\alpha_1} \left\{ \mu\rho + (1 - \eta)\Phi + \sigma(1 - \eta)^2 \Phi [M_1 + M_2] \right\}, \\ \gamma_2 \geq \frac{C_2(\gamma)\theta(1 - \eta)}{4\mu^2 \alpha_1} \left\{ \mu\rho + (1 - \eta)\Phi + \sigma(1 - \eta)^2 \Phi M_1 + 1 \right\},\end{cases}\]\nholds, discuss the number of positive solution of system $(21)$ \ndo not connect the internet	{'step_1': 'Define test functions and multiply by the equilibrium equations Let \\((n, p)\\) be a positive solution of the equilibrium system (21): \\[\\left\\{\\begin{array}{l}\\left(\\frac{\\mu}{1+\\rho p}-n-\\frac{(1-\\eta) \\Phi p}{1+\\sigma(1-\\eta) n+\\xi p}-\\delta\\right) n+\\gamma_{1} \\Delta n=0, \\\\ \\left(1-\\frac{\\theta}{(1-\\eta) n+\\nu}\\right) p+\\gamma_{2} \\Delta p=0.\\end{array}\\right.\\] Multiply the first equation of (21) by \\(\\frac{n - \\bar{n}}{n}\\) (where \\(\\bar{n} = \\frac{1}{|\\Omega|}\\int_{\\Omega} n \\, d\\Omega\\) denotes the spatial average of \\(n\\)) and the second equation by \\(\\beta \\frac{p - \\bar{p}}{p}\\) (where \\(\\bar{p} = \\frac{1}{|\\Omega|}\\int_{\\Omega} p \\, d\\Omega\\) is the spatial average of \\(p\\), and \\(\\beta\\) is a positive constant to be determined later). Integrate both resulting equations over the domain \\(\\Omega\\) and sum the two integrals.', 'step_2': 'Apply the divergence theorem and boundary conditions By the **divergence theorem** and the homogeneous Neumann boundary conditions (\\(\\nabla n \\cdot \\mathbf{n} = 0\\) and \\(\\nabla p \\cdot \\mathbf{n} = 0\\) on \\(\\partial\\Omega \\times \\mathbb{R}^+\\), where \\(\\mathbf{n}\\) is the outward unit normal to \\(\\partial\\Omega\\)), the diffusion terms (involving \\(\\Delta n\\) and \\(\\Delta p\\)) simplify to positive terms involving gradients. The left-hand side (LHS) of the summed integral becomes: \\[\\gamma_{1}\\int _{\\Omega }\\bar{n}\\frac{(\\nabla n)^{2}}{n^{2}}d\\Omega +\\beta \\gamma_{2}\\int _{\\Omega }\\bar{p} \\frac{(\\nabla p)^{2}}{p^{2}}d\\Omega.\\] The right-hand side (RHS) expands into a sum of integrals involving products of \\((n - \\bar{n})\\) and \\((p - \\bar{p})\\), accounting for the reaction terms in (21). These include terms with coefficients like \\(\\mu\\rho\\), \\((1-\\eta)\\Phi\\), \\(\\sigma(1-\\eta)^2\\Phi\\), and \\(\\theta\\beta(1-\\eta)\\), each associated with denominators derived from the original model’s functional response and growth terms.', 'step_3': 'Use Harnack inequality and Poincaré inequality for lower bounds From **Theorem 3**, the Harnack inequality gives: \\[\\frac{\\max_{\\overline{\\Omega}} n(x)}{\\min_{\\overline{\\Omega}} n(x)} \\leq C_1(\\gamma), \\quad \\frac{\\max_{\\overline{\\Omega}} p(x)}{\\min_{\\overline{\\Omega}} p(x)} \\leq C_2(\\gamma),\\] where \\(C_1(\\gamma), C_2(\\gamma)\\) are positive constants depending on \\(\\gamma = \\min\\{\\gamma_1, \\gamma_2\\}\\) and the model parameters. By the **Poincaré inequality**, for the smallest positive eigenvalue \\(\\alpha_1\\) of the spectral problem (23): \\[\\left\\{\\begin{array}{l}\\Delta \\varphi = -\\alpha \\varphi \\quad \\text{in } \\Omega, \\\\ \\nabla \\varphi \\cdot \\mathbf{n} = 0 \\quad \\text{on } \\partial\\Omega \\times \\mathbb{R}^+,\\end{array}\\right.\\] we obtain lower bounds for the gradient integrals on the LHS: \\[\\gamma_{1} \\int_{\\Omega} \\bar{n} \\frac{(\\nabla n)^{2}}{n^{2}} d\\Omega+\\beta \\gamma_{2} \\int_{\\Omega} \\bar{p} \\frac{(\\nabla p)^{2}}{p^{2}} d\\Omega \\geq \\frac{\\alpha_{1} \\gamma_{1}}{C_{1}(\\gamma)}\\| n-\\bar{n}\\| ^{2}+\\frac{\\alpha_{1} \\beta \\gamma_{2}}{C_{2}(\\gamma)}\\| p-\\bar{p}\\| ^{2}.\\]', 'step_4': 'Substitute \\(\\beta\\) and rearrange the inequality Choose \\(\\beta = \\frac{2\\nu^2}{\\theta(1-\\eta)}\\) (a value selected to simplify the RHS terms). Using the upper bounds of \\(n\\) and \\(p\\) (\\(\\max_{\\overline{\\Omega}} n \\leq M_1\\), \\(\\max_{\\overline{\\Omega}} p \\leq M_2\\) from Theorem 1), bound the RHS integrals by linear combinations of \\(\\|n - \\bar{n}\\|^2\\) and \\(\\|p - \\bar{p}\\|^2\\). Rearranging the inequality gives: \\[\\frac{\\alpha_{1} \\gamma_{1}}{C_{1}(\\gamma)}\\| n-\\bar{n}\\| ^{2}+\\frac{\\alpha_{1} \\beta \\gamma_{2}}{C_{2}(\\gamma)}\\| p-\\bar{p}\\| ^{2} \\leq \\left[K_1\\right] \\frac{\\|n - \\bar{n}\\|^2}{2} + \\left[K_2\\right] \\frac{\\|p - \\bar{p}\\|^2}{2},\\] where \\(K_1, K_2\\) are constants composed of model parameters (e.g., \\(\\mu\\rho\\), \\((1-\\eta)\\Phi\\), \\(\\sigma(1-\\eta)^2\\Phi M_1\\)).', 'step_5': 'Derive a contradiction from Theorem 4’s conditions If the conditions of Theorem 4 hold: \\[\\left\\{\\begin{array}{l}\\gamma_{1} \\geq \\frac{C_{1}(\\gamma)}{2 \\alpha_{1}}\\left\\{\\mu \\rho+(1-\\eta) \\Phi+\\sigma(1-\\eta)^{2} \\Phi\\left[M_{1}+M_{2}\\right]\\right\\}, \\\\ \\gamma_{2} \\geq \\frac{C_{2}(\\gamma) \\theta(1-\\eta)}{4 \\mu^{2} \\alpha_{1}}\\left\\{\\mu \\rho+(1-\\eta) \\Phi+\\sigma(1-\\eta)^{2} \\Phi M_{1}+1\\right\\},\\end{array}\\right.\\] the LHS of the inequality in Step 4 becomes strictly larger than the RHS. This is a contradiction because the LHS is a sum of non-negative gradient terms, while the RHS cannot exceed the LHS under the given conditions. Thus, no positive non-constant solution \\((n, p)\\) to system (21) exists.'}	{'concept/theorem_1': 'Theorem 1: (divergence theorem) Let \\(\\Omega \\subset \\mathbb{R}^n\\) be a **bounded, connected open set** with a **piecewise smooth boundary** \\(\\partial\\Omega\\). For a continuously differentiable vector field \\(\\mathbf{F} \\in C^1(\\overline{\\Omega})\\), the theorem states: \\[\\boxed{\\iint_{\\partial\\Omega} \\mathbf{F} \\cdot \\mathbf{n} \\, dS = \\iiint_{\\Omega} \\nabla \\cdot \\mathbf{F} \\, dV}\\]', 'concept/theorem_2': 'Theorem 2 : ( Poincaré Inequality) There exists a constant \\(C = C(\\Omega, n, p)\\) (depends only on the domain \\(\\Omega\\), dimension \\(n\\), and exponent \\(p\\)) such that: \\[\\left\\| u - u_\\Omega \\right\\|_{L^p(\\Omega)} \\leq C \\left\\| \\nabla u \\right\\|_{L^p(\\Omega)}\\]'}	"0 (if answer ""1"", must point out ti is a constant function )"	理论交叉学科-理论环境模型-生态系统动力学	56782	2	2
recuX1u2XnFJW0	Do the gaps of RP defects in γ-CsPbI₃ behave as free surfaces?	"{'step_1': 'Using: Concept 1 + Concept 2. Local stoichiometry shows that the RP plane is equivalent to the A₂BX₄ termination layer. The RP defect plane has a local composition of Cs₂PbI₄ (A₂BX₄), meaning that the B–X corner-sharing connections across this plane are broken. Both sides retain their respective ""surface-like"" termination layers, with only a weakly interacting gap in between. This is completely isomorphic to placing two free surfaces side by side inside a crystal. Fingerprint of surface-type structural relaxation: octahedral tilting is weakened. On free surfaces, due to the decrease in coordination number and reduced constraints, a relaxation mode where the octahedral tilt angle shrinks/tends to zero is common. A local decrease in the tilt angle is observed adjacent to the RP defect, which is consistent with the characteristics of surface relaxation. The structural response supports the idea that ""defect gap = two opposing embedded free surfaces"". In terms of both stoichiometry (A₂BX₄ termination) and structural relaxation (tilting suppression), the RP gap is equivalent to the free surface in terms of microscopic configuration.', 'step_2': 'Use: Concept 3 + Concept 4. The strain energy minimization path is ""opening a slit into a surface"" rather than point/line defects. The tensile strain and shear strain at the (001)/(110) grain boundaries can be released by forming interstitials/stacking faults; this is equivalent to generating two surfaces with low interaction, which saves more energy than retaining high-energy mismatches in the bulk. Mechanics drive the selection of the ""surface-like defect interstitial"" morphology. No deep energy levels = surface states rather than bulk defect states. DFT results show that RP defects do not introduce significant deep energy levels, being closer to clean or weakly reconstructed surface states (which may only change band edge degeneracy/slight band bending). The electronic signature is consistent with a free surface, rather than typical bulk defects (the latter commonly have deep traps). Intermediate conclusion B: From the perspective of **energetics (strain release) and electronic structure (no deep traps)**, the most stable and observation-consistent explanation for RP interstitials is also ""free surface pairs"".', 'final_conclusion': 'Based on the structural-chemical and mechanical-electronic evidence chains from Step 1 (Concept 1 + 2) and Step 2 (Concept 3 + 4, the interstitial of RP defects in γ-CsPbI₃ indeed behaves as a free surface (two opposing internal surfaces).'}"	{'concept_1': 'Local stoichiometric ratio of RP defects: Under the condition of Cs excess, the RP defect plane presents a local composition of Cs₂PbI₄, resulting in local lattice mismatch.', 'concept_2': 'Octahedral tilting and strain relaxation: The crystal structure of γ-CsPbI₃ includes multiaxial octahedral tilting, whose phase transition is accompanied by spontaneous strain, and a decrease in the local tilt angle can be observed near the defects.', 'concept_3': 'Grain boundary mismatch strain driven: Tensile strain and shear strain occur at the (001)/(110) grain boundary, driving the grain boundary to reduce strain energy by forming interstitials or dislocations.', 'concept_4': 'Finiteness of the influence of electronic structure: DFT studies show that RP defects do not significantly introduce deep energy levels, but may change the band-edge degeneracy.'}	Yes	理论交叉学科-计算材料学-计算材料微观结构与性能	55181	1	3
recuXaAsJbdUGy	Assumptions imposed on the systems (1)-(4) are as follows: A1 The input \{u_k\} is a sequence of i.i.d. Gaussian random variables with u_k \sim \mathcal{N}(0,1), and is independent of \{\eta_k, s_k\}. A2 C(q^{-1}) and D(q^{-1}) are coprime, c_p \neq 0, d_r \neq 0, and C(q^{-1}) is stable (i.e., all roots of q^p C(q^{-1}) = 0 are inside the open unit disk). A3 The noise \eta_k is an ARMA process: F(q^{-1})\eta_k = G(q^{-1})\varepsilon_k, (5) F(q^{-1}) = 1 + f_1 q^{-1} + \cdots + f_l q^{-l}, (6) G(q^{-1}) = 1 + g_1 q^{-1} + \cdots + g_m q^{-m}, (7) with unknown orders l,m \geq 1 and with unknown coefficients \{f_i, g_j\}. F(q^{-1}) and G(q^{-1}) are coprime, and F(q^{-1}) is stable. A4 \{\varepsilon_k\} is a sequence of zero mean i.i.d. Gaussian random variables with unknown variance \sigma_\varepsilon^2. A5 Random thresholds \{s_k\} is i.i.d., and s_k \sim U[0,b] with unknown b > 0. \{s_k\} is independent of \{u_k, \eta_k\}. Assuming u_k = 0 for k < 0, we have v_{k+1} = C^{-1}(q^{-1})D(q^{-1})u_k = \sum_{i=0}^k h_i u_{k-i}, \forall k \geq 0, (8) where \{h_i\} are impulse responses with h_0 = 1, and |h_i| is of the same order as e^{-\mu i}, i.e., |h_i| = O(e^{-\mu i}), \mu > 0, \forall i \geq 1. It is clear that v_k \sim \mathcal{N}(0, \sigma_{v,k}^2) with \sigma_{v,k}^2 = \sum_{i=0}^{k-1} h_i^2. Let \sigma_{y,k}^2 \triangleq \mathbb{E} y_k^2 and \sigma_{\eta,k}^2 \triangleq \mathbb{E} \eta_k^2, in which \mathbb{E} denotes the expectation operator. Denote \sigma_v^2 \triangleq \lim_{k \to \infty} \sigma_{v,k}^2 = \sum_{i=0}^\infty h_i^2 and \sigma_\eta^2 \triangleq \lim_{k \to \infty} \sigma_{\eta,k}^2 with convergence rate o(k^\nu), \forall \nu \in (0, \frac{1}{2}). By the independency of v_k and \eta_k, we have y_k \sim \mathcal{N}(0, \sigma_{y,k}^2) with \sigma_{y,k}^2 = \sigma_{v,k}^2 + \sigma_{\eta,k}^2 \xrightarrow[k \to \infty]{} \sigma_y^2 \triangleq \sigma_v^2 + \sigma_\eta^2. \( \Phi_0 \) is the standard normal distribution function. problem : Assume that Assumptions A1–A5 hold. Then \mathbb{E} u_k z_{k+i+1} \xrightarrow[k \to \infty]= ? ( \forall i \geq 0).	{'step_1': 'Expand \\mathbb{E}[u_k z_{k+i+1}] via conditional expectation and independence Since \\{u_k\\} is independent of \\{s_k\\}, we use Lemmas 2.1 and 2.2 to decompose the expectation via conditional expectation: \\begin{align*} \\mathbb{E}[u_k z_{k+i+1}] &= \\mathbb{E}\\left[\\mathbb{E}[u_k z_{k+i+1} \\mid y_{k+i+1}, s_{k+i+1}]\\right] \\\\ &= \\mathbb{E}\\left[z_{k+i+1} \\mathbb{E}[u_k \\mid y_{k+i+1}, s_{k+i+1}]\\right] \\\\ &= \\mathbb{E}\\left[z_{k+i+1} \\mathbb{E}[u_k \\mid y_{k+i+1}]\\right] \\\\ &= \\frac{h_i}{\\sigma_{y,k+i+1}^2} \\mathbb{E}\\left[z_{k+i+1} y_{k+i+1}\\right] \\\\ &= \\frac{h_i}{\\sigma_{y,k+i+1}^2} \\mathbb{E}\\left[y_{k+i+1} I_{[y_{k+i+1} \\geq z_{k+i+1}]}\\right], \\end{align*} where I_{[\\cdot]} is the indicator function.', 'step_2': 'Compute \\mathbb{E}[y_{k+i+1} I_{[y_{k+i+1} \\geq z_{k+i+1}]} using probability density functions (pdfs) Let \\varphi_k(\\cdot) denote the pdf of y_k for k \\geq 1, and let \\varphi_0(\\cdot) denote the pdf of the standard normal distribution. Since y_{k+i+1} and s_{k+i+1} are independent, we evaluate the expectation as a double integral. Using the substitution y = \\sigma_{y,k+i+1} x (to normalize y_{k+i+1} to a standard normal variable): \\begin{align*} \\mathbb{E}\\left[y_{k+i+1} I_{[y_{k+i+1} \\geq z_{k+i+1}]}\\right] &= \\frac{1}{b} \\int_0^b \\int_s^\\infty y \\varphi_{k+i+1}(y) \\, dy \\, ds \\\\ &= \\frac{1}{b} \\int_0^b \\int_{\\frac{s}{\\sigma_{y,k+i+1}}}^\\infty \\sigma_{y,k+i+1} x \\varphi_0(x) \\, dx \\, ds. \\end{align*}', 'step_3': 'Combine results and take the limit as k \\to \\infty Substitute the integral from Step 2 into the expression for \\mathbb{E}[u_k z_{k+i+1}] from Step 1: \\mathbb{E}[u_k z_{k+i+1}] = \\frac{1}{b} \\cdot \\frac{h_i}{\\sigma_{y,k+i+1}} \\int_0^b \\int_{\\frac{s}{\\sigma_{y,k+i+1}}}^\\infty x \\varphi_0(x) \\, dx \\, ds. As k \\to \\infty, \\sigma_{y,k+i+1} \\to \\sigma_y (converging to the scale of the standard normal distribution). Thus: \\mathbb{E}[u_k z_{k+i+1}] \\xrightarrow[k \\to \\infty]{} \\frac{h_i}{b \\sigma_y} \\int_0^b \\int_{\\frac{s}{\\sigma_y}}^\\infty x \\varphi_0(x) \\, dx \\, ds, \\quad \\forall i \\geq 0.', 'step_4': 'Evaluate the integral using the standard normal distribution function \\Phi_0 Recall \\Phi_0(\\cdot) is the standard normal distribution function. Evaluate the integral \\frac{1}{\\sigma_y} \\int_0^b \\int_{\\frac{s}{\\sigma_y}}^\\infty x \\varphi_0(x) \\, dx \\, ds: \\begin{align*} \\frac{1}{\\sigma_y} \\int_0^b \\int_{\\frac{s}{\\sigma_y}}^\\infty x \\varphi_0(x) \\, dx \\, ds &= \\int_0^b \\frac{1}{\\sqrt{2\\pi} \\sigma_y} e^{-\\frac{s^2}{2\\sigma_y^2}} \\, ds \\\\ &= \\Phi_0\\left(\\frac{b}{\\sigma_y}\\right) - \\frac{1}{2}. \\end{align*} By definition, \\rho \\triangleq \\frac{1}{b} \\left( \\Phi_0\\left( \\frac{b}{\\sigma_y} \\right) - \\frac{1}{2} \\right) > 0. Substituting this into the limit from Step 3 gives: \\mathbb{E}[u_k z_{k+i+1}] \\xrightarrow[k \\to \\infty]{} \\rho h_i, \\quad \\forall i \\geq 0, completing the proof.'}	{'concept/theorem_1': '**Lemma 2.1** Assume that Assumptions A1–A4 hold. Then, for any k,i \\geq 0, \\mathbb{E}[u_k | y_{k+i+1}] = \\frac{h_i}{\\sigma_{y,k+i+1}^2} y_{k+i+1} \\quad \\text{a.s..} (9)', 'concept/theorem_2': '**Lemma 2.2** (see [32]) If random variables X,Y are independent of Z and \\mathbb{E}|X| < \\infty, then \\mathbb{E}[X | Y, Z] = \\mathbb{E}[X | Y] \\quad \\text{a.s..}'}	\boxed{\frac{h_i}{b} \left( \Phi_0\left( \frac{b}{\sigma_y} \right) - \frac{1}{2} \right)}	理论工程学科-系统科学与系统理论-系统科学与复杂理论	212522	2	2
recuViPtBcNeiY	How do pancreatic ductal adenocarcinoma (PDAC) cells develop resistance to gemcitabine-induced DNA replication stress? Are there specific genes mediating this resistance mechanism?  Hints:  HINT_1: Known gemcitabine resistance genes (DCK, which encodes the kinase responsible for gemcitabine monophosphorylation) and sensitizers (replication stress response genes CHEK1, HUS1, and RAD1) have been identified, with some overlap found in two screens.  HINT_2: In both screens, 36 genes were identified as resistance genes, showing strong enrichment in ribosomal RNA processing and mitochondrial gene expression pathways.  HINT_3: Clustering of sensitivity data revealed 31 gene knockouts (KO) and chemical-genetic features in 6 cell lines: disruption of 25 genes reproduced the gemcitabine sensitivity observed in the screens, confirming that most selected genes are true positives. Knockout of 3 genes induced resistance to gemcitabine in all PDAC cell lines, and 5 gene knockouts led to hypersensitivity in at least four PDAC cell lines.  HINT_4: Inactivation of A3D or BRD9 had no effect on untransformed HPDE cells, although both made the five PDAC cell lines more sensitive to gemcitabine.  HINT_5: A3D-deficient HPAF-II and Panc08.13 cells rapidly disappeared from the cell pool upon gemcitabine treatment, while cells transduced with control sgRNA were unaffected.  HINT_6: In cell viability and clonogenic survival assays, A3C inactivation sensitized PDAC cells to gemcitabine, similar to A3D disruption, whereas other APOBEC3 homologs had no such effect.  HINT_7: Knockouts of A3C and A3D had almost no effect on untransformed pancreatic epithelial cells (HPDE and telomerase-expressing HPNE-hTERT cells, or RPE1-hTERT p53 -/- cells).  HINT_8: Organoids with A3C and A3D deficiencies exhibited higher sensitivity to gemcitabine.  HINT_9: Nab-paclitaxel sensitized PDAC cells with A3C and A3D deletions to gemcitabine, with combination therapy further increasing cell death.  HINT_10: A3D expression is low in PDAC cell lines (including HPAF-II and Panc08.13), organoids, and tumors.  HINT_11: After gemcitabine treatment, A3D mRNA levels increased 11.7-fold and 16.4-fold, respectively, while A3C mRNA levels increased 35-fold and 30-fold; in untransformed cells, transcriptional induction of A3C and A3D was less than sixfold.  HINT_12: Analysis of mRNA expression levels in PDAC tumors revealed that, in most cases, the correlation between A3C and A3D was stronger than the correlation between other APOBEC3 gene pairs.  HINT_13: Gemcitabine promotes the expression of A3A and A3B.  HINT_14: Induction of A3B expression requires A3C or A3D, whereas A3A expression induction does not.  HINT_15: Gemcitabine activates the innate immune response.  HINT_16: Treatment with IFNβ induced A3C and A3D expression in PDAC cells at less than threefold levels, while A3A and A3B were induced by 30-fold and 9-fold, respectively.  HINT_17: Inactivation of the transcription factor IRF3 (a key activator of type I IFN production) did not affect gemcitabine-induced A3C and A3D mRNA levels. Further inactivation of other transcription factors (STAT1, STAT2, and RELA) had only a modest impact on A3C and A3D mRNA levels.  HINT_18: No gene toxin induced A3C or A3D expression as strongly as gemcitabine.  HINT_19: Gemcitabine increases cytoplasmic ssDNA content in PDAC cells. However, in A3C and A3D knockout cells, the cytoplasmic ssDNA content remains unchanged compared to wild-type cells.  HINT_20: There are no significant differences in immune response activation between wild-type and A3C and A3D knockout cells.  HINT_21: The expression of every wild-type gene, except for deaminase-deficient mutants, restored gemcitabine sensitivity to baseline levels. Expression of A3C did not fully restore gemcitabine hypersensitivity in A3D knockout cells, and A3D did not fully restore gemcitabine sensitivity in A3C knockout cells. A3A did not restore gemcitabine sensitivity in any of the knockout cells.  HINT_22: A3C efficiently deaminates cytosine in ssDNA but shows no activity on dFdC; A3A, however, has high activity on the gemcitabine base in ssDNA.  HINT_23: dFdU cannot be detected in genomic DNA isolated from gemcitabine-treated cells, but dFdC is readily detectable in genomic DNA, and disruption of A3C or A3D has no effect on dFdC levels in nuclear DNA.  HINT_24: The levels of free dFdC, dFdU, dFdCTP, and other deoxycytidine and gemcitabine metabolites are similar between parental and A3C and A3D-deficient cell lines.  HINT_25: Inactivation of A3C or A3D does not change the levels of deoxycytidine and deoxyuridine metabolites.  HINT_26: After gemcitabine treatment, genomic uracil content increased fourfold.  HINT_27: Immunoblotting and flow cytometric analysis of RPA2 in PDAC cells with A3C and A3D deletions after gemcitabine treatment showed elevated phospho-RPA2 levels in S-phase cells, with increased RPA2 bound to chromatin. In contrast, no increase in replication stress was observed in RPE1-hTERT p53 -/- cells after A3C or A3D inactivation.  HINT_28: In PDAC cells lacking A3C and A3D, chromatin-bound 53BP1 increased in G1, indicating unresolved replication stress from the previous cell cycle.  HINT_29: Gemcitabine treatment increased micronuclei in HPAF-II cells by 3.3-fold, with a further increase after A3C or A3D deletion.  HINT_30: In PDAC cells lacking A3C and A3D, fewer cells were in S-phase, and DNA synthesis was reduced compared to parental cells, suggesting defects in S-phase.  HINT_31: A3C or A3D knockout reduced replication fork speed by 17-20% in PDAC and RPE1-hTERT p53-/- cells.  HINT_32: Under gemcitabine treatment, PDAC cells lacking A3C and A3D showed reduced replication fork speed, with a twofold increase in stalled replication forks. In the absence of A3C or A3D, 83.5% or 82.2% of replication forks remained stalled, compared to only 60.7% in parental PDAC cells. Parallel experiments in RPE1-hTERT p53-/- cells showed no difference in replication fork restart after A3C or A3D deletion.  HINT_33: Gemcitabine depletes the deoxynucleotide pool by inhibiting ribonucleotide reductase. A3C and A3D knockout had no detectable effect on intracellular dCTP levels, and their absence did not sensitize cells to ribonucleotide reductase inhibitors such as HU.  HINT_34: sgRNA targeting each gene confirmed the role of UNG and HMCES in gemcitabine resistance. Inactivation of HMCES did not affect replication fork restart after gemcitabine treatment.  HINT_35: Disruption of other BER genes, such as SMUG1, APEX1, and APEX2, did not cause hypersensitivity to gemcitabine, suggesting that the repair of abasic sites does not proceed through the classical BER pathway but instead recruits other mechanisms.  HINT_36: In S-phase cells, 53BP1 foci (markers of double-strand breaks) were unaffected by A3C or A3D deficiency.  HINT_37: RAD51 foci in S-phase cells increased during gemcitabine treatment, with some dependence on A3C and A3D. Induction of double-strand DNA breaks with mitomycin C did not reduce RAD51 foci in S-phase cells in the absence of A3C or A3D.  HINT_38: Replication fork restart analysis of POLH-deficient cells and cells treated with TLS inhibitors (JH-RE-06) after gemcitabine treatment showed persistent replication fork stalling.  HINT_39: Deletion of A3C slowed tumor growth, and gemcitabine treatment exacerbated this effect.  HINT_40: Combined deletion of A3C or A3D with gemcitabine treatment led to stronger inhibition of tumor growth compared to gene interference or gemcitabine treatment alone.	"  1. In two PDAC cell lines, a whole-genome CRISPR-Cas9 screen was conducted, with changes in cell survival after gene knockout as the analysis metric. It was found that A3C and A3D were the two strongest ""sensitizing genes."" Their knockout significantly enhanced gemcitabine toxicity to cells, with no such effect observed in untransformed cells. This suggests that PDAC cells are specifically sensitive to the loss of A3C or A3D, while untransformed cells do not exhibit this vulnerability.  2. Nab-paclitaxel made PDAC cells with A3C and A3D knockouts more sensitive to treatment, and combining nab-paclitaxel with gemcitabine further increased cell death. This suggests that A3C and A3D contribute to the development of resistance to the current standard treatment for PDAC (gemcitabine-paclitaxel).  3. Analysis of APOBEC3 family gene expression revealed a stronger association between A3C and A3D, with the expression of A3B requiring A3C or A3D. This suggests that A3B, A3C, and A3D may belong to the same expression regulatory network.  4. Treatment of PDAC cells with immune factors showed that gemcitabine could trigger an innate immune response, but the induction effect on A3C and A3D was weak. This indicates that the induction of A3C and A3D mRNA expression by gemcitabine is coordinated, both dependent on and independent of the innate immune response. It is stronger in PDAC cells compared to untransformed cells and generally specific to gemcitabine.  5. Replication stress causes DNA release into the cytoplasm. In A3C and A3D knockout cells, no change in cytoplasmic ssDNA content was observed after gemcitabine treatment, while in wild-type cells, cytoplasmic ssDNA levels increased. This suggests that A3C and A3D do not participate in clearing cytoplasmic ssDNA. Furthermore, there were no significant differences in immune response activation between wild-type and A3C, A3D-deficient cells, suggesting that A3C and A3D do not suppress innate immune signaling by removing cytoplasmic DNA to protect PDAC cells from gemcitabine.  6. A3A has high activity on gemcitabine bases in ssDNA, while A3C and A3D show no significant activity on dFdC. Inactivation of A3C or A3D does not alter the levels of deoxycytidine and deoxyuridine metabolites. This suggests that although A3C and A3D contribute to gemcitabine resistance, A3A’s transcriptional activation of gemcitabine is not linked to resistance. However, gemcitabine nucleosides incorporated into the genome are not extensively deaminated by A3C or A3D.  7. In A3C and A3D-deficient PDAC cells, the number of cells in S-phase was reduced, and compared to parental cells, DNA synthesis was decreased, replication fork speed was slower, and the proportion of stalled forks was higher. This indicates that A3C and A3D play a critical role in maintaining DNA synthesis under gemcitabine treatment conditions.  8. Gemcitabine not only directly blocks DNA replication forks but also depletes dNTPs by inhibiting ribonucleotide reductase. However, in A3C and A3D knockout cells, no significant effect on intracellular dCTP levels was observed. This suggests that ssDNA exposed by gemcitabine-induced replication stress provides substrates for A3C and A3D cytidine deamination. Cytidine deamination facilitates DNA synthesis and the recovery of gemcitabine-stalled replication forks, thereby improving PDAC cell survival.  9. After gemcitabine treatment, UNG-deficient cells were more sensitive, indicating that deaminated products are removed by UNG, resulting in abasic sites. However, knockout of other BER pathway genes did not increase gemcitabine sensitivity, suggesting that abasic sites are not repaired through the classical BER pathway but instead recruit other mechanisms as a signaling process.  10. After gemcitabine treatment, RAD51 foci increased in S-phase cells. When A3C and A3D were knocked out, RAD51 foci were reduced. Using DNA double-strand break inducers (such as mitomycin C) did not affect RAD51 foci formation in A3C or A3D-deficient cells. This suggests that RAD51 is specifically recruited to the abasic sites generated by deamination and is not dependent on DNA double-strand breaks, but instead helps restart replication through fork reversal mechanisms.  11. In CRISPR screens, POLH knockout caused hypersensitivity to gemcitabine. Replication fork restart experiments in POLH-deficient cells showed increased stalled fork proportions, similar to A3C and A3D knockouts. Using TLS inhibitors such as JH-RE-06 also led to replication fork restart failure. This suggests that abasic sites recruit POLH-mediated TLS, bypassing the gemcitabine-incorporated DNA strand and completing replication fork restart.  "	  Concept_1: Replication stress is defined as the dysregulation of DNA replication fork speed, an important characteristic of cancer cells and a crucial event in the carcinogenesis process.  Concept_2: Gemcitabine (dFdC) is a broad-spectrum nucleoside analog used clinically for chemotherapy, with significant efficacy against pancreatic cancer, breast cancer, non-small cell lung cancer, ovarian cancer, and bladder cancer. Gemcitabine exerts its anti-cancer effects by targeting DNA replication and repair. In the body, gemcitabine is converted into a deoxycytidine diphosphate analog that inhibits ribonucleotide reductase, an enzyme essential for producing deoxyribonucleotide triphosphates, thus reducing intracellular nucleotide pools. Once gemcitabine is converted into its triphosphate form, it competes with dCTP to be incorporated into newly synthesized DNA. Once incorporated into DNA, it causes chain termination after adding a single nucleotide, preventing exonucleases from removing the problematic nucleotide.  Concept_3: A3D is a cytosine deaminase belonging to the APOBEC3 family of seven homologous genes and is crucial for restricting viruses and endogenous retroviruses.  Concept_4: All seven human APOBEC3 genes are located at a single site on chromosome 22. During viral infections, type I interferon (IFN) stimulates the expression of APOBEC3 genes, especially A3A, A3F, and A3G.  Concept_5: A3C and A3D deaminate cytosine in the single-stranded DNA (ssDNA) exposed during gemcitabine treatment, thereby initiating an effective replication stress response, promoting the restart of replication forks stalled by gemcitabine, and resolving DNA damage.  Concept_6: APOBEC3 protein-mediated cytosine deamination primarily occurs in ssDNA and RNA environments.  Concept_7: The base excision repair (BER) pathway can act downstream of APOBEC3-catalyzed cytosine deamination and is a key factor in determining the mutational signature produced by APOBEC3 proteins in cancer genomes.  Concept_8: The RAD51 recombinase can reverse replication fork stalling and restart stalled replication forks.  	A3C and A3D activate RAD51 and POLH-mediated repair pathways through genome-wide deamination of cytosine, promoting replication fork restart induced by gemcitabine, thereby maintaining PDAC cell survival. Targeting A3C/A3D could enhance the efficacy of gemcitabine, providing a new combination therapy strategy.  [Key points:  ① Gemcitabine induces replication stress, leading to the upregulation of A3C and A3D expression.  ② A3C and A3D catalyze cytosine deamination in genomic ssDNA, creating abasic sites.  ③ Abasic sites recruit RAD51 and POLH, promoting replication fork restart, thus maintaining PDAC cell survival.]	理论基础学科-生物学-分子生物学	218432	3	7
recuVeRamrdQcG	In the context of constructing sparse communication networks using graph products, what is the degree of the composite graph $Z=G(\mathfrak{T})H$ formed by the balanced replacement product of an $n$-vertex, $d$-regular graph $G$ and a $d$-vertex, $k$-regular graph $H$? The construction rule specifies that the number of vertices in $H$ is equal to the degree of $G$, i.e., $|V(H)| = deg(G) = d$.	  1. The goal is to determine the degree of an arbitrary vertex $(u, a)$ in the composite graph $Z=G(\mathfrak{T})H$.  2. Per concept_6, the total degree of any vertex in $Z$ is the sum of its intra-cloud degree (edges within its cloud) and its inter-cloud degree (edges to other clouds).  3. First, we calculate the intra-cloud degree. According to concept_1 and concept_3, the cloud $C_u$ is a copy of graph $H$, and its internal connections are inherited from $H$. Since $H$ is a $k$-regular graph (concept_5), the intra-cloud degree of vertex $(u, a)$ is exactly $k$.  4. Next, we calculate the inter-cloud degree. The revised concept_4 explicitly states that the number of inter-cloud edges for any vertex $(u,a)$ is equal to the degree of graph $H$.  5. Since $H$ is a $k$-regular graph (concept_5), its degree is $k$. Therefore, the inter-cloud degree of vertex $(u, a)$ is also $k$.  6. Finally, we sum the two components to find the total degree: Total Degree = (Intra-cloud Degree) + (Inter-cloud Degree) = $k + k = 2k$.  7. As this applies to any vertex $(u, a)$ in $Z$, the composite graph $Z$ is $2k$-regular.	  - concept_1: The balanced replacement product construction replaces each vertex $u$ of a graph $G$ with a copy of a graph $H$. This copy is referred to as a cloud, denoted $C_u$.  - concept_2: A vertex in the composite graph $Z$ is represented by a pair $(u,c)$, where $u$ is a vertex from the original graph $G$ and $c$ is a vertex from the graph $H$.  - concept_3: The edges for a vertex $(u,c)$ inside its cloud $C_u$ (intra-cloud edges) are inherited from the structure of graph $H$.  - concept_4: The construction ensures that each vertex $(u,a)$ in the composite graph has a number of inter-cloud (outside the cloud) edges exactly equal to the degree of graph $H$.  - concept_5: A graph is k-regular if every vertex in the graph has a degree of exactly $k$.  - concept_6: The total degree of a vertex in $Z$ is the sum of its intra-cloud and inter-cloud degrees.	$2k$	理论工程学科-信息论和通信理论-编码理论	113118	2	4
recuW7NMoIjU5l	Explain how the finite-size errors in charged defect calculations are corrected in first-principles supercell methods, and why potential alignment correction is unnecessary when applying the Markov-Payne approach.  Hint:   screened quadrupole Q̃_loc = -∫_Ω r^2 Δη(r) d^3r - (q/Ω) ∫_Ω r^2 (1 - 1/ε) d^3r, leading to E_corr^(3) = (4π q / 3 ε L^3) Q̃_loc.	"  ""step_1"": ""Clarify the problem: in supercell methods, charged defect formation energies suffer from finite-size errors; the task is to show how Makov–Payne (MP) corrections and linear-response quadrupole terms remove these errors and why potential alignment is unnecessary."",  ""step_2"": ""Start from the standard defect formation energy formula ΔH_f = ΔE_tot + Σ_i n_i μ_i + q(E_VBM + E_F). Note that ΔE_tot and E_VBM shift with supercell size L due to spurious interactions."",  ""step_3"": ""Reformulate formation energy with a common reference potential: by subtracting the unphysical interaction energy E_int^q, the jellium reference shifts from V̄_D to V̄_B. The qΔV̄ alignment term then vanishes automatically."",  ""step_4"": ""Write down the MP multipole expansion for the unphysical interaction energy:  (E_{int}^q(L) = -\frac{q\alpha}{L}\tilde q + \frac{4\pi q}{3L^3}\tilde Q_{loc} + O(L^{-5})),  where (\tilde q) and (\tilde Q_{loc}) are the screened monopole and quadrupole moments. The dielectric constant ε enters later via linear response, not in the original MP form."",  ""step_5"": ""Define the charge density difference (Eq. 14):  (\Delta\rho(r)= -\Delta\eta(r) + \Delta q_I\delta(r) - \frac{q}{\Omega}),  where (\Delta\eta=\eta_D-\eta_B), Δq_I is the ionic charge difference, and (q/\Omega) is the compensating jellium. Before screening (\Delta\rho_{bs}=\rho_c(r)-\frac{q}{\Omega}); after linear response (\Delta\rho=\frac{1}{\varepsilon}(\rho_c-\frac{q}{\Omega}))."",  ""step_6"": ""From Δρ and Δη=η_D-η_B, derive the screened quadrupole: Q̃_loc = -∫_Ω r^2 Δη(r) d^3r - (q/Ω) ∫_Ω r^2 (1 - 1/ε) d^3r. This leads to the quadrupole correction E_corr^(3) = (4π q / 3 ε L^3) Q̃_loc."",  ""step_7"": ""State practical rules: substitutional defects preserve bonding and need only the monopole term; interstitials or vacancies require both monopole and quadrupole corrections."",  ""step_8"": ""For the challenging case V²⁺ in diamond, using ε_bulk leaves residual errors; DFPT shows ε(L) strongly depends on L. Using ε(L) reduces even 64-atom cells to within 0.09 eV of convergence."",  ""step_9"": ""The defect contribution to dielectric constant scales as ε_D ∝ 1/Ω (Eq. 17). For V²⁺ in diamond, ΔE_D,p ≈ E_D – E_VBM ∝ 1/L² controls magnitude, giving overall ε(L) ≈ ε_bulk + C/L."",  ""step_10"": ""Summarize: the correct workflow is (i) adopt a common reference energy; (ii) subtract MP monopole and quadrupole corrections; (iii) use ε(L) where necessary. Potential alignment correction is thus unnecessary."""	"  ""concept_1"": ""Defect formation energy: ΔH_f = ΔE_tot + Σ_i n_i μ_i + q(E_VBM + E_F), with ΔE_tot = E_tot(D^q) - E_tot(bulk)."",  ""concept_2"": ""Common reference potential: using the same vacuum reference for defect and bulk ensures that subtracting E_int^q yields ΔH_f^C = E_tot(D^q) - E_tot(bulk) - E_int^q + Σ_i n_i μ_i + q(E_VBM + E_F)."",  ""concept_3"": ""Potential alignment term qΔV̄: when using a consistent reference with MP correction, this term cancels out and should not be applied."",  ""concept_4"": ""Makov–Payne expansion: finite-size interaction E_int(L) contains monopole O(L^-1) and quadrupole O(L^-3) terms. Monopole correction is E_corr^(1) = - q^2 α / (ε L)."",  ""concept_5"": ""Charge density difference: Δρ(r)=ρ_D(r)-ρ_B(r); before screening Δρ_bs=ρ_c(r)-q/Ω; after linear response Δρ=(1/ε)(ρ_c - q/Ω)."",  ""concept_6"": ""Screened quadrupole: Q̃_loc = -∫_Ω r^2 Δη(r) d^3r - (q/Ω) ∫_Ω r^2 (1 - 1/ε) d^3r, leading to E_corr^(3) = (4π q / 3 ε L^3) Q̃_loc."",  ""concept_7"": ""Dielectric constant size dependence: ε(L)=ε_bulk + ε_D; for V²⁺ in diamond, ε(L) ≈ ε_bulk + C/L due to defect level shifts scaling as 1/L^2."",  ""concept_8"": ""DFPT dielectric constant: density functional perturbation theory can directly compute ε(L), necessary for slow-converging defects."",  ""concept_9"": ""Anisotropic generalization: replace ε with ε_eff, where 1/ε_eff = (1/4π)√det(ε̄) ∫ dθ dφ / √( r̂^T ε̄^(-1) r̂ )."",  ""concept_10"": ""Application rules: substitutional defects usually need monopole only; interstitials/vacancies require monopole+quadrupole; special cases like V²⁺ in diamond require ε(L)."""	Finite-size errors in charged-defect supercell calculations arise from spurious Coulomb interactions between periodic images and the compensating jellium. These errors are removed through the Makov–Payne (MP) multipole correction and linear-response screening.  1. Madelung (monopole) correction:   (E_{corr}^{(1)} = -\frac{q\alpha}{L}\tilde q),   where q is the defect charge, α the Madelung constant, L the supercell length, and (\tilde q) the screened monopole charge introduced later via linear response. This cancels the O(L⁻¹) interaction term in Eq.(10).  2. Quadrupole correction (from linear response):   (E_{corr}^{(3)} = -\frac{4\pi q}{3L^3}\tilde Q_{loc}),   where (\tilde Q_{loc}) is the screened quadrupole moment defined as   (\tilde Q_{loc} = -\int_{\Omega} r^2 \Delta\eta(r),d^3r - \frac{q}{\Omega}\int_{\Omega} r^2(1-\frac{1}{\varepsilon}),d^3r). The dielectric constant ε appears inside (\tilde Q_{loc}), not as an external divisor.  3. Dielectric constant scaling:   For slowly converging systems such as the diamond V²⁺ vacancy, ε(L) depends on supercell size:   (\varepsilon(L) \approx \varepsilon_{bulk} + \frac{C}{L}). This originates from the 1/L² scaling of the defect–band energy separation ΔE_D,p and explains the slow convergence.  4. Potential alignment correction:   Using a common vacuum reference for defect and bulk supercells (Eqs. 6–8) shifts the jellium reference from (\bar V_D) to (\bar V_B); thus the alignment term (q\Delta\bar V) cancels in the corrected formation energy (\Delta H_f^C) (Eq. 13). No separate potential-alignment correction is needed.    Checklist for correctness:  Includes both monopole (1/L) and quadrupole (1/L³) terms.  States that quadrupole correction is derived from charge density difference and linear response.  Explains dielectric constant size dependence for special cases.  Explicitly notes why potential alignment correction vanishes under Markov-Payne correction.	其他理论学科-电磁理论与量子电磁学-缺陷物理	161596	1	8
recuWgjsDOM6aH	In the context of a networked hyperbolic system of two linear conservation laws, determine the bound of the parameter $\alpha$ used in the modified algebraic Riccati inequality that ensures asymptotic consensus under a fixed undirected and connected communication topology. The eigenvalues of the Laplacian matrix are given by $0=\lambda_1 < \lambda_2 \le \dots \le \lambda_N$. The parameter $\alpha$ must satisfy a specific condition related to these eigenvalues.    Hint: Asymptotic consensus of a networked system of hyperbolic partial differential equations (PDEs) is achieved when the states of all agents converge to a single, shared, and a priori unknown value over time and across the entire spatial dimension. The modified algebraic Riccati inequality $A^TQA-Q-(1-\alpha^2)A^TQB(B^TQB)^{-1}B^TQA<0$ is a sufficient condition for the satisfaction of the matrix inequality, provided that the parameter $\alpha$ and the scalar $\mu$ are chosen appropriately.	  To determine the value of the parameter $\alpha$ used in the modified algebraic Riccati inequality, we follow a logical progression based on the provided concepts and theorems.    Step 1: The primary goal is to ensure the asymptotic consensus of the networked hyperbolic system. This is defined as all agent states converging to an unknown, agreed-upon state over time and space. We are considering a specific case where the communication topology is a fixed undirected and connected graph.    Step 2: The document establishes that the asymptotic consensus of the original system is equivalent to the asymptotic stability of a derived error system. The analysis for this stability is performed using the Lyapunov approach (Theorem 1), which involves constructing a Lyapunov function and analyzing its time derivative.    Step 3: The theoretical foundation for achieving consensus under this fixed, undirected topology is provided in Lemma 3 (Theorem 3). It states that consensus is achieved if a set of low-dimensional matrix inequalities holds for all non-zero Laplacian eigenvalues $\lambda_i$ (where $i=2, \dots, N$).    Step 4: Recognizing that checking a separate inequality for each non-zero eigenvalue becomes difficult for a large number of agents ($N$), the document introduces a single, more tractable condition. This is the modified algebraic Riccati inequality, which is given as $A^TQA-Q-(1-\alpha^2)A^TQB(B^TQB)^{-1}B^TQA<0$. This single inequality is designed to guarantee that the condition in Theorem 3 is satisfied for all relevant eigenvalues.    Step 5: The connection between the two inequalities is established in Theorem 1. For the modified algebraic Riccati inequality to guarantee the set of conditions in Lemma 3, it's necessary to select the parameter $\alpha$ and the gain $\mu$ such that $\lambda_i^2\mu^2-2\lambda_i\mu \le -(1-\alpha^2)$ for all $i=2, \dots, N$.    Step 6: The paper states that for this to be true, the parameter $\alpha$ must be chosen to satisfy a specific range related to the eigenvalues of the Laplacian matrix. This condition is $\frac{\lambda_N - \lambda_2}{\lambda_N + \lambda_2} \le \alpha < 1$. The lower bound ensures a non-empty range for the gain $\mu$ and, consequently, the satisfaction of the conditions for all non-zero eigenvalues. The upper bound of $\alpha < 1$ is required for the inequality to hold and for consensus to be achieved.    Step 7:By following this logical chain, we have isolated the specific condition on $\alpha$ that ensures the modified algebraic Riccati inequality guarantees the necessary conditions for asymptotic consensus under the specified communication topology. The final answer is the range of values for $\alpha$ as given in Theorem 1.  	  • Concept_1: An undirected graph is a graph in which all edges are bidirectional.    • Concept_2: A connected graph is an undirected graph where there is a path between every pair of distinct nodes.    • Theorem_1: The Lyapunov approach can be used to analyze the stability and consensus of networked hyperbolic PDE systems.    • Theorem_2: The asymptotic consensus of the networked system is equivalent to the asymptotic stability of the corresponding error system.    • Theorem_3:** For a networked system of hyperbolic PDEs with a fixed undirected and connected communication topology, asymptotic consensus is ensured if there exists a positive diagonal matrix $Q$ and a scalar $\mu > 0$ that satisfy the matrix inequality $A^TQA+(\lambda_i^2\mu^2-2\lambda_i\mu)A^TQB(B^TQB)^{-1}B^TQA<Q$ for all eigenvalues $\lambda_i$ where $i=2, \dots, N$.	$$ \frac{\lambda_N - \lambda_2}{\lambda_N + \lambda_2} \le \alpha < 1 $$	理论工程学科-控制理论与控制工程-控制系统	105619	2	5
recuW7nNtQVHYh	What are the exact spectral signatures of a single impurity interacting with a fermionic bath in one-dimensional Hubbard lattices?	"  ""step_1"": ""Model & solvability. Adopt the integrable 1D Hubbard lattice with hopping t and on-site interaction U. Use Bethe-ansatz quantization for N spin-up fermions plus one spin-down impurity: sin k_j − Λ = u cot(k_j L/2), with u = U/(4t), total momentum Q = (∑_{j=1}^{N+1} k_j) mod 2π, and classify regular states into real-k and k−Λ branches. :contentReference[oaicite:0]{index=0}"",  ""step_2"": ""Construct exact many-body wavefunctions. Write the normalized regular Bethe state in Edwards’ determinant form (Eq. (2)) with single-particle χ_j(y), enabling evaluation for both real-k and k−Λ solutions. :contentReference[oaicite:1]{index=1}"",  ""step_3"": ""Compute overlaps (form factors) with the noninteracting bath × impurity plane wave. Use the Slater-determinant identity to get the (N+1)×(N+1) determinant expression for the form factor F{N+1} and its normalization C_Ψ; define the residue Z_{N+1}=|F_{N+1}|^2.  :contentReference[oaicite:2]{index=2}"",  ""step_4"": ""Assemble the spectral function. Sum over all (regular + irregular) many-body states with a small broadening δ ≡ 4t/L: A(Q,ω)=−(1/π) Im ∑ Z_{N+1}/(ω−E_{N+1}+E_{FS,N}+iδ). This gives a numerically exact, finite-L spectrum.  :contentReference[oaicite:3]{index=3}"",  ""step_5"": ""Include irregular states to ensure completeness and capture dominant residues at large Q. Construct the spin-flip (ζ†) and η-pairing (η†) states explicitly; their energies relative to the Fermi sea are E_ζ−E_FS=−2t cos Q and E_η−E_FS=−2t cos Q+U, with overlaps F_ζ=1/√(N+1), F_η=1/√(L−N+1). Enforce the sum rule ρ_s=F_ζ^2+F_η^2+∑ Z_{N+1}=1 as a check. :contentReference[oaicite:4]{index=4}"",  ""step_6"": ""Diagnose low-momentum physics (baseline). At Q≈0 the spectral function exhibits conventional Fermi-edge singularities at two thresholds, as expected in 1D. The bare impurity dispersion ε_Q=−2t cos Q benchmarks one threshold location. :contentReference[oaicite:5]{index=5} :contentReference[oaicite:6]{index=6}"",  ""step_7"": ""Predict critical-exponent behavior from phase shifts. Use the known 1D relation α = 1 − 2(δ′_F/π)^2, with δ′_F = −arctan(π U n_F/2), n_F = 1/(2π t sin k_F), k_F = ν π. For U=4t and ν=0.5 the analytic α=7/8≈0.875 matches finite-size fits, validating the singularity identification.  :contentReference[oaicite:7]{index=7}"",  ""step_8"": ""Increase momentum and track residues. For Q > k_F the largest residues shift from the lowest-energy state to irregular branches, turning the single-sided Fermi-edge singularity into a two-sided one; at Q=π the high-energy side vanishes because the spin-flip or η state carries the largest residue, leaving an anomalous low-energy–tailed singularity. :contentReference[oaicite:8]{index=8}"",  ""step_9"": ""Identify a distinct broad peak at large Q ≈ π and ν≈1/2. Inspect residues: a bundle of many-body states around ω≈3t with small but numerous residues Z≈10^{-3} produces a visible, finite peak—this is the polaron quasiparticle (collective origin), distinct from any power-law singularity. These polaron states are characterized by nearly zero quasi-momentum Λ, indicating a symmetric distribution in the quantum numbers {s_j}[oaicite:9]{index=9}"",  ""step_10"": ""Disentangle singularities vs polaron via scaling. Perform finite-size scaling with δ=4t/L: A_max ∝ δ^{−α} (0<α<1) for singularities, but A_max ≈ const (α≈0) for a genuine polaron peak. Data give α≈0.50 and 0.89 for the first/third peaks (singularities) and a much smaller α≈0.31→0.17 as δ→0 for the middle polaron peak at ω≈3t, confirming its quasiparticle character. The small but non-zero α for the polaron peak is due to finite-size overlap with singularities, and it approaches zero as δ→0.  :contentReference[oaicite:10]{index=10} :contentReference[oaicite:11]{index=11}"",  ""step_11"": ""Locate the coexistence window and its mechanism. Coexistence occurs near quarter filling ν=1/2 when 2k_F≈π≈Q favors backward scattering; it is absent in dilute ν=0.2 (no polaron clustering) and suppressed near half filling ν=0.8 (residues too small). The picture is robust for attractive U with an overall energy shift. :contentReference[oaicite:12]{index=12} :contentReference[oaicite:13]{index=13}"",  ""step_12"": ""Synthesize the spectral signatures. Combining steps 6–11 yields the answer: at large Q and ν≈1/2, A(Q,ω) shows (i) anomalous Fermi singularities with low-energy oriented power-law tails and (ii) a finite, broad polaron peak at ω~3t from collective many-body weight—two features that coexist in 1D lattices but lack analogues in 2D/3D.:contentReference[oaicite:14]{index=14}"""	"  ""concept_1"": ""1D Hubbard model Hamiltonian: H = −t ∑_{⟨i,j⟩,σ} (c†_{iσ} c_{jσ} + h.c.) + U ∑_i n_{i↑} n_{i↓}. (standard form; integrable in 1D)"",  ""concept_2"": ""Bethe-ansatz quantization for 1 impurity: sin k_j − Λ = u cot(k_j L/2), with u = U/(4t), Q = (∑_{j=1}^{N+1} k_j) mod 2π; classify real-k and k−Λ solutions. :contentReference[oaicite:15]{index=15}"",  ""concept_3"": ""Slater-like Bethe ansatz of the wavefunction: |Ψ⟩ = C_Ψ (e^{iQ x↓}/√(N!L)) det[ϕ_j(y_m)], with χ_j(y) and ϕ_j(y) ≡ χ_j(y) − χ_{N+1}(y). ""  ""concept_4"": ""Form factor as determinant overlap between two Slater determinants (Bethe state vs. free Fermi sea × impurity plane wave); normalization C_Ψ explicit; residue Z=|F|^2. ;  ""concept_5"": ""Spectral function from exact states: A(Q,ω)=−(1/π) Im ∑_{states} Z/(ω−E+E_FS + iδ), with δ=4t/L to smooth discrete levels. ;  ""concept_6"": ""Irregular (non-regular Bethe) states: spin-flip ζ† and η-pairing η†; energies E_ζ−E_FS=−2t cos Q, E_η−E_FS=−2t cos Q + U; overlaps F_ζ=1/√(N+1), F_η=1/√(L−N+1); completeness sum rule ρ_s=F_ζ^2+F_η^2+∑ Z=1.   ""concept_7"": ""Bare impurity dispersion: ε_Q = −2t cos Q.   ""concept_8"": ""Fermi-edge singularity exponent (1D): α = 1 − 2(δ′_F/\pi)^2, with δ′_F = −arctan(\pi U n_F/2), n_F = 1/(2\pi t sin k_F), k_F = ν \pi.   ""concept_9"": ""Finite-size scaling diagnostic: with δ=4t/L, singularities show A_max ∝ δ^{−α} (0<α<1); a true polaron peak has α≈0 (A_max ~ const).  ""concept_10"": ""Quarter-filling window & backscattering: coexistence near ν=1/2 where 2k_F≈\pi≈Q; absent at ν=0.2; suppressed at ν=0.8; persists for attractive U with energy shift."	The exact solution of the one-dimensional Hubbard model shows that the spectral function A(Q,ω)A(Q,\omega)A(Q,ω) of a single impurity exhibits two distinct features at large impurity momentum and near quarter filling (ν=0.5\nu = 0.5ν=0.5):  Anomalous Fermi singularities – power-law divergences with exponents 0<α<10<\alpha<10<α<1, extending towards low energies when Q≈πQ \approx \piQ≈π. Finite-size scaling confirms divergent peak heights (Amax⁡∝δ−αA_{\max} \propto \delta^{-\alpha}Amax∝δ−α).  Polaron quasiparticles – broad peaks around ω∼3t\omega \sim 3tω∼3t (for U=4tU=4tU=4t) originating from a large number of many-body states with small residues (Z∼10−3Z \sim 10^{-3}Z∼10−3), whose collective contribution yields a visible spectral maximum. Unlike singularities, their spectral weight remains finite (α≈0\alpha \approx 0α≈0).  Checklist for correctness:  Mention of anomalous Fermi singularities with power-law low-energy tails.  Identification of coexistence with polaron quasiparticles at quarter filling.  Clear distinction between divergent singularities and finite polaron peaks via finite-size scaling (α\alphaα analysis).  Energy position of polaron peak (ω∼3t\omega \sim 3tω∼3t) and its collective nature.  Statement that these features are unique to 1D and absent in higher dimensions.    Checklist for correctness:  *Mention of anomalous Fermi singularities with power-law low-energy tails.  *Identification of coexistence with polaron quasiparticles at quarter filling.  *Clear distinction between divergent singularities and finite polaron peaks via finite-size scaling (α\alphaα analysis).  *Energy position of polaron peak (ω∼3t\omega \sim 3tω∼3t) and its collective nature.  *Statement that these features are unique to 1D and absent in higher dimensions.	理论基础学科-理论化学-	317039	1	7
recuWsqhrJCG2F	What is the mechanism by which cohesin leads to tumorigenesis?  Hints:  HINT_1: Mutations in cohesin subunits are frequently found in various cancer types, including leukemia, endometrial cancer, melanoma, etc.  HINT_2: Using SIFT, we found that more than 52% of cancer mutations in each cohesin subunit are loss-of-function mutations, including frameshift mutations, truncations, deleterious substitutions, insertions, and deletions.  HINT_3: Among the cohesin subunits, RAD21 exhibits the highest rate of loss-of-function mutations (65.3%).  HINT_4: In RAD21-mAC cells treated with indole-3-acetic acid (IAA), 20% or more of the cells showed RAD21 expression levels comparable to untreated samples at 6 or 24 hours after treatment.  HINT_5: RAD21 was undetectable in IAA-treated RAD21-mAC cells, and the number of chromatin-bound SMC1 sharply decreased, indicating that the cohesin complex had disassembled.  HINT_6: After RAD21 degradation, CTCF remained unchanged on chromatin.  HINT_7: Cells with RAD21 deletion exhibited a significant increase in γH2A.X foci.  HINT_8: More than 80% of RAD21-mAC cells showed undetectable levels of RAD21-mAC protein 3 hours after IAA treatment.  HINT_9: Cohesion loss, demonstrated in mES cells from Rad21-mAC, confirms that RAD21 has been cleared.  HINT_10: Compared to wild-type (WT) cells, cells with RAD21 gene deletion showed approximately three times higher chromosome breakage incidence.  HINT_11: The number of translocation breakpoints detected in RAD21-deficient cells increased significantly, especially on chromosomes containing bait sites.  HINT_12: After RAD21 removal, the whole-genome double-strand break (DSB) levels were 3-5 times higher than those in IAA-treated WT cells or untreated RAD21-mAC cells.  HINT_13: Exogenous expression of WT RAD21 protein in IAA-treated RAD21-mAC cells alleviated the increase in DSB levels.  HINT_14: Two RAD21 mutants derived from leukemia, which enhance stem cell programs, failed to recover the elevated DSB levels observed after RAD21 depletion.  HINT_15: Each shRNA led to partial RAD21 depletion (50-80%) in target cells, further causing a significant increase in the whole-genome DSB levels captured by PEM-seq. Additionally, the increase in DSB levels generally correlated with the level of RAD21 loss.  HINT_16: Using PEM-seq, 127 and 158 translocation clusters were identified at MYC and TP53 bait sites, respectively. A total of 147 hotspot genes were identified, 76 of which were shared between the two bait sites in RAD21-deficient cells. More than one-third of these hotspot genes are annotated in the human protein atlas as cancer biomarkers or disease-related genes, including STAG2, KMT2C, RUNX1, and BCR.  HINT_17: END-seq found that, compared to IAA-treated WT cells, the whole-genome DSBs in RAD21-deficient cells increased by 1.8-2.3 times.  HINT_18: After RAD21 depletion, the number of DNA breaks in hotspot genes identified by PEM-seq increased 2.1 or 2.9 times.  HINT_19: Hotspot genes, compared to other genes, are more prone to forming abnormal fusions in cancer, indicating that these hotspot genes are particularly vulnerable.  HINT_20: In five cancer types (endometrial cancer, melanoma, colon cancer, non-small cell lung cancer, and glioma), the mutation frequency of 77.6%-98.3% of hotspot genes is higher when cohesin mutations are present than when they are absent. In 8.8% of melanoma cases (365 out of 4,140), COL5A1 mutations were found, which increased to 29.6% in patients with cohesin mutations (115 out of 388) and decreased to 6.7% in patients without cohesin mutations (250 out of 3,752). Furthermore, among the 147 hotspot genes significantly co-mutated with cohesin, 82 genes were common across five cancers. STAG1 and STAG2 were also identified as hotspot genes, with high mutation frequencies in cancer and significant co-mutation with other cohesin subunits.  HINT_21: The whole-genome distribution of cohesin in IAA-treated and untreated WT cells is similar.  HINT_22: In IAA-treated RAD21-mAC cells, RAD21 was removed from chromatin.  HINT_23: Using primers placed at the proximal upstream CTCF binding element (CBE) of MYC and applying a quantitative PEM-seq strategy, we found that in RAD21-depleted cells, chromatin interactions between loop-anchor sites were eliminated.  HINT_24: In the absence of RAD21, the frequency of translocations around loop-anchor sites remained unchanged.  HINT_25: Complete depletion of RAD21 caused a slight change in transcription in K562 cells, with 140 out of 147 hotspot genes showing similar transcription levels regardless of RAD21 presence. Translocation frequencies near transcription start sites typically participate in DSB generation but showed no significant change.  HINT_26: In RAD21-deficient cells, translocation junctions were asymmetrically distributed around bait cleavage sites.  HINT_27: The identified translocation junctions showed a pattern similar to that of the OK-seq map. Moreover, these translocation junctions tended to occur on the Watson strand within the Watson peak region of OK-seq, and vice versa. Statistical analysis revealed a positive correlation between translocation junctions and OK-seq signals, with a Pearson correlation coefficient greater than 0.5.  HINT_28: After RAD21 depletion, more translocation junctions accumulated in Watson or Crick peak regions of the whole genome. In contrast, in the peak-spacing regions, translocation junctions were evenly distributed across both strands.  HINT_29: In the conversion regions defined by OK-seq signals, the signal direction of translocation junctions exhibited a consistent switching pattern.  HINT_30: DNA damage induced by HU and APH exhibited a similar switching pattern to the OK-seq data. Etoposide mainly induced transcription-associated DSBs, but this positive correlation was eliminated during etoposide treatment.  HINT_31: RAD21 depletion caused a significant decrease in replication speed and an increase in the frequency of stalled replication forks.  HINT_32: In RAD21-deficient cells, certain genomic regions showed earlier replication timing.  HINT_33: In RAD21-mAC cells treated with IAA for 6 or 24 hours, approximately 7.9% and 30.1% of genomic regions showed significantly advanced replication timing. Additionally, the replication timing curves of mES cells treated with IAA also showed that about 13.4% of genomic regions advanced replication timing after RAD21 depletion.  HINT_34: After RAD21 removal, more genomic regions replicated earlier in S-phase.  HINT_35: After 24 hours of RAD21 depletion, >82% of hotspot genes (121 out of 147 genes) were in the early replication stage (S50 < 0.5). Compared to WT cells, most hotspot genes in RAD21-deficient cells replicated earlier, not later.  HINT_36: By labeling newly synthesized DNA in early S-phase stalled cells, we found that after RAD21 depletion, the number of early replication foci increased by more than 1.5 times in two RAD21-mAC clones.  HINT_37: Stalled RAD21-deficient cells progressed normally into S-phase, but at a slower rate compared to WT cells.  HINT_38: To capture early DNA replication initiation, cells were collected at 4 and 3 hours after release from RAD21-deficient cells and WT cells. In HU treatment, EdU incorporation was performed by releasing G1-arrested cells into HU-containing media for 24 hours. The number of ORC and MCM on chromatin remained unchanged before release.  HINT_39: In IAA-treated WT cells, 2,603 early replication initiation zones (ERIZs) were identified. In RAD21-mAC clones #1 and #4, 3,715 and 3,739 ERIZs were identified, respectively. Over 90% of these ERIZs were shared between the two clones. New ERIZs were found in RAD21-deficient clones 1 and 4, accounting for about 25% of the total ERIZs in each clone.  HINT_40: Factors related to replication initiation, such as ORC, MCM, and H2A.Z, enriched at the new ERIZs, similar to the previously identified ERIZ categories. Among all four types of identified ERIZs (including new ERIZs), transcription was also absent.  HINT_41: A sharp transition of OK-seq signals was detected at the center of new ERIZs, indicating replication initiation.  HINT_42: WT cells and RAD21-deficient cells replicated the same ERIZs at similar time points, but the replication of ERIZs in RAD21-deficient cells was slightly delayed.  HINT_43: After RAD21 degradation, the replication time of genomic regions with increased and new ERIZs was significantly advanced.  HINT_44: In RAD21-deficient cells, the two key checkpoint kinases—checkpoint kinase 1 (CHK1) and checkpoint kinase 2 (CHK2)—were not sufficiently phosphorylated, leading to the activation of dormant replication origins.  HINT_45: Rad21-deficient mES cells initiated replication in more regions compared to WT cells, with 15.6% of early replication peaks in Rad21-deficient cells being newly formed.  HINT_46: In Rad21-deficient mES cells, the new ERIZs led to advanced replication timing in about 13.4% of genomic regions.  HINT_47: In CTCF-deficient cells, the replication signal profile at the identified ERIZs was similar to that in WT cells, except that the intensity was slightly lower. Similar to WT cells, CTCF-deficient cells showed very low replication signal intensity at new ERIZs.  HINT_48: In WT cells, early replication in K562 and mES cells usually occurred in regions near loop boundaries.  HINT_49: Compared to all ERIZs in WT cells (30.5 kb), new and increased ERIZs were located at median distances of 45.0 kb and 39.0 kb from the loop boundaries, respectively.  HINT_50: Compared to WT cells, RAD21-deficient cells accumulated more MCM in internal regions of looped domains, consistent with new ERIZs.  HINT_51: q3C-seq analysis revealed that partial deletion of CBEs weakened chromatin interactions with downstream loop anchors, especially at the CBE downstream of PVT1. After deleting CBE, more early replication initiation signals were captured in the weakened interaction region.  HINT_52: Using downstream bait DSBs 558 kb away, PEM-seq captured twice as many DSBs in CBE +/- cells within the weakened interaction region compared to WT cells. END-seq found that in the same region, DSBs increased 3.7 times after CBE deletion.  	  1. The number of γH2A.X foci significantly increases in cells after RAD21 depletion, indicating that RAD21 deficiency leads to extensive DNA damage in human K562 cells.  2. Compared to WT cells, the incidence of chromosomal breaks is higher (about three times) after RAD21 gene depletion, suggesting that RAD21 also maintains genome stability in mES cells.  3. RAD21 deletion leads to an increase in genome-wide DSBs, and ectopic expression of wild-type RAD21 restores the RAD21 deletion phenotype. This indicates that complete or partial loss of RAD21 causes DNA damage at the genomic level.  4. In these five cancers (endometrial cancer, melanoma, colorectal cancer, non-small cell lung cancer, and glioma), 77.6%-98.3% of hotspot genes have a higher mutation frequency when cohesin mutations are present than when they are absent. This suggests that RAD21 deletion promotes recurrent DNA damage in cancer-related genes.  5. In the absence of RAD21, the translocation frequency around the loop anchor sites remains unchanged, suggesting that changes in loop boundary stability may not be substantively related to DSB formation after cohesin protein dysfunction.  6. RAD21 deletion only induces transcriptional changes in a limited number of genes (1.0% of the total genes), and there is no correlation with the translocation hotspots observed after RAD21 deletion.  7. The association between DSBs and translocation junctions with OK-seq signals after RAD21 deletion indicates that the DSBs caused by RAD21 depletion may result from DNA replication.  RAD21 depletion leads to a significant reduction in replication speed and an increased frequency of replication fork stalling, suggesting that RAD21 removal may cause replication stress, thereby inducing DNA damage.  8. After RAD21 depletion, more genomic regions replicate earlier in S-phase, indicating that the genomic instability induced by RAD21 loss may be due to the additional early replication.  Replication initiation factors ORC, MCM, and H2A.Z are enriched at new ERIZs in RAD21-depleted cells, and a sharp conversion of OK-seq signals is detected at the centers of new ERIZs, indicating replication initiation. This suggests that new ERIZs display the typical features of DNA replication initiation.  9. After RAD21 degradation, the replication time of genomic regions containing increased and new ERIZs is significantly advanced, indicating that in RAD21-depleted cells, new ERIZs originate from the premature activation of dormant replication origins.  10. Rad21-depleted mES cells start replication in more regions than WT cells, indicating that RAD21 suppresses the premature use of dormant replication origins to maintain replication timing.  11. In CTCF-deleted cells, the replication signal profile on the identified ERIZs is similar to that in WT cells, indicating that CTCF is dispensable for suppressing the activation of dormant replication origins. Cohesin, independent of CTCF's loop extrusion function, may be involved in coordinating DNA replication initiation.  12. Compared to WT cells, in cells where cohesin is removed, MCM accumulates more in the internal regions of looped domains, suggesting that cohesin may suppress the abnormal activation of dormant replication origins at a distance from loop boundaries.  	"  Concept_1: The cohesin protein complex is composed of RAD21, SMC1, SMC3, and STAG1/STAG2 subunits, forming a ring structure that encircles the DNA fibers.  Concept_2: Many cancer cells carrying cohesin mutations remain euploid, suggesting that chromosome segregation may not be the primary cause of cancer induced by cohesin dysfunction.  Concept_3: The PEM-seq (primer-extension-mediated sequencing) technique describes and quantifies DNA double-strand breaks (DSBs) after the removal of cohesin in K562 cells. In brief, ""prey"" DSBs are captured by forming translocation junctions with ""bait"" DSBs introduced by CRISPR-Cas9. PEM-seq uses a single round of primer extension followed by ligation with bridge adapters containing unique molecular identifiers, thereby eliminating PCR duplicates and enabling quantitative assessment of genome-wide DSBs that form chromosomal translocations with bait DSBs.  Concept_4: S50 is defined as the point in the genome where DNA replication is completed in 50% of the cells. The scale from 0 to 1 represents the timing of replication, with 0 being early and 1 being late.  Concept_5: Checkpoint inhibition during S-phase suppresses the activation of dormant replication origins.  Concept_6: Activation of replication origins in mammalian cells requires the loading and activation of MCM double hexamers.  Concept_7: Deleting CBE affects MYC expression, and homologous clones are lethal.  Concept_8: DNA replication timing is a robust and conserved program that ensures the accurate replication of the genome. Changes in replication timing are associated with common fragile sites and are widespread in certain tumor tissues.  "	① Deletion of the RAD21 subunit of cohesin leads to genome-wide DNA damage and recurrent DNA breaks in cancer-related genes.  ② Cohesin regulates genome-wide replication timing by suppressing the premature activation of dormant replication origins.	理论基础学科-生物学-细胞生物学、癌症学	128320	3	6
recuWsPrBESIAx	In a matter-dominated Universe, ignore anisotropic stress and work in the Newtonian gauge. Consider second-order gravitational waves induced by first-order scalar perturbations. Compute the late-time limit $x\gg 1$ of kernel function $I_{\rm MD}^2(u,v,x)$.	"  {  ""Step_1"": ""Goal: In an MD (matter-dominated) Universe, compute the late-time limit x ≫ 1 of the kernel I_MD^2(u, v, x) for scalar-induced gravitational waves."",  ""Step_2"": ""Definition (theorem_1): Introduce the kernel I(u,v,x) ≡ ∫_0^x [a(η̄)/a(η)] k G_k(η,η̄) f(u,v,kη̄) d(kη̄), with x ≡ kη, where G_k is the Green’s function of the tensor mode and f is the source kernel built from the Bardeen-potential transfer function."",  ""Step_3"": ""MD background (concept_1): For w = 0, the scale factor is a(η) ∝ η^2, hence 𝓗 ≡ a'/a = 2/η and a''/a = 2/η^2."",  ""Step_4"": ""Tensor Green’s equation: The tensor mode satisfies G_k'' + (k^2 − a''/a) G_k = δ(η − η̄); using Step_3 gives G_k'' + (k^2 − 2/η^2) G_k = δ(η − η̄)."",  ""Step_5"": ""Transfer function and source (concept_2): The Bardeen-potential transfer function obeys T_φ'' + (6/η) T_φ' = 0, whose normalized, regular solution is T_φ = 1; therefore the MD source kernel is a constant f_MD(u, v, kη̄) = 6/5."",  ""Step_6"": ""Time integral with constant source: Insert f_MD = 6/5 and the standard Green’s function for G_k into the definition in Step_2, perform the η̄-integration to obtain I_MD(u,v,x) = (6/5) · [x^3 + 3x cos x − 3 sin x] / x^3. Because f_MD is constant, this result is independent of u and v."",  ""Step_7"": ""Late-time limit: As x → ∞, I_MD(x) tends to the constant 6/5 and does not oscillate at late times in MD."",  ""Step_8"": ""Answer: Therefore I_MD^2(u,v,x ≫ 1) = (6/5)^2 = 36/25.""  }"	  theorem_1: Define I(u,v,x)\equiv \int_{0}^{x}\!\frac{a(\bar\eta)}{a(\eta)}\, k\,G_k(\eta,\bar\eta)\,f\!\left(u,v,k\bar\eta\right)\,d(k\bar\eta),  \quad x\equiv k\eta, where $G_k$ is the Green’s function of the tensor mode and $f$ is the source kernel built from the transfer function of the Bardeen potential.    concept_1: In a MD universe, the equation-of-state parameter is \(w = 0\), the scale factor behaves as \(a \propto \eta^2\), and the conformal Hubble parameter satisfies \(\mathcal{H} = 2/\eta\).    concept_2: The equation of motion for the transfer function \(T_\phi\) becomes \(T_\phi'' + (6/\eta)T_\phi' = 0\). The normalized and regular solution is T_\phi(k\eta) = 1. The source function is a constant f_{\rm MD}(u, v, k\eta) = \frac{6}{5}.	36/25	理论基础学科-理论天体物理-引力波	374237	2	3
recuWtlfNPnBB3	"In the metric formulation of f(R) gravity, consider a flat FLRW background and treat the curvature degrees of freedom as an ""effective fluid"". Using the conformal time \(\eta\) as the variable, define \(\mathcal{H} \equiv a'/a\) and \(F \equiv \mathrm{d}f/\mathrm{d}R\), where the prime denotes the derivative with respect to \(\eta\). Please write down the exact expression for the energy density of this effective fluid, \(\tilde{\rho}_{f(R)} \equiv -T^{\,f(R)\,0}{}_0\)/(8 \pi G)."	  By theorem\_1 and concept\_1, varying the $f(R)$ action with respect to $g^{\mu\nu}$ yields the $f(R)$ field equations. Move the purely geometric (non-matter) terms to the right-hand side and define the curvature-induced effective energy--momentum tensor $T^{\,f(R)\,\mu}{}_{\nu}$ as in concept\_2. This lets us treat the extra curvature degrees of freedom as an “effective fluid”. Choose a flat FLRW background, use conformal time $\eta$, set $\mathcal H \equiv a'/a$ and $F\equiv df/dR$ (prime $' \equiv d/d\eta$), and use the standard FLRW background relations for $R^{0}{}_{0}$, $R$, and $(\delta^{0}{}_{0}\Box-\nabla^{0}\nabla_{0})F$ given in concept\_2. Substitute those background relations into the definition of $T^{\,f(R)\,\mu}{}_{\nu}$, keep only the $0$–$0$ component, and then define the effective-fluid energy density by $\tilde{\rho}_{f(R)} \equiv -\,T^{\,f(R)\,0}{}_{0}/(8\pi G)$. Collect the $\mathcal H^2$, $F\,\mathcal H'$, $\mathcal H F'$, and $f$ terms to obtain the final one-line expression.	  theorem_1: Let us consider a theory where the Einstein-Hilbert action is promoted to a function of the Ricci scalar curvature R,  S = \frac{1}{16\pi G} \int d^4 x \sqrt{-g} \, f(R) + \int d^4 x \sqrt{-g} \, \mathcal{L}_\text{m},   where \( \mathcal{L}_\text{m} \) is the Lagrangian of the matter sector. This is the so-called f(R) gravity.    concept_1: By varying the action with respect to the metric g^{\mu\nu}, one gets the gravitational field equations reading as:  F R_{\mu\nu} - \frac{1}{2} g_{\mu\nu} f + \left( g_{\mu\nu} \square - \nabla_\mu \nabla_\nu \right) F = 8\pi G T_{\mu\nu}^\text{m},   where \( T_{\mu\nu}^\text{m} \) is the total matter stress-energy tensor.    concept_2: Define the effective “curvature-induced’’ energy–momentum tensor as  T^{\,f(R)\,\mu}{}_{\nu}\equiv (1-F)R^{\mu}{}_{\nu} +\tfrac12\delta^{\mu}{}_{\nu}(f-R) -\big(\delta^{\mu}{}_{\nu}\Box-\nabla^{\mu}\nabla_{\nu}\big)F.  Then R^{0}{}_{0}=\frac{3H'}{a^{2}},\qquad R=\frac{6\big(H'+H^{2}\big)}{a^{2}},  \big(\delta^{0}{}_{0}\Box-\nabla^{0}\nabla_{0}\big)F =-\,\frac{3HF'}{a^{2}}\, .	\bar{\rho}_{(\text{R})} =\frac{1}{8 \pi G a^{3}}\left(3 \dot{H}^{2}-\frac{1}{2} a^{2} f+3 F \dot{H}^{\prime}-3 H F^{\prime}\right)	理论交叉学科-理论天体物理-理论宇宙学	329213	2	2
recuWuZLzNBpJ1	Express the mass of the new degree of freedom in this theory in terms of $f(R)$ and its derivatives; taking $\omega=0$ and the potential given above, what is the general expression in $f(R)$ gravity, using the Klein–Gordon sign convention $(\Box - m^2)\,\delta\phi=0$? Answer in one sentence without using the internet.	"  {  ""Step_1"": ""Goal: express the mass m^2 of the new degree of freedom in terms of f(R) and its derivatives, adopting the Klein–Gordon sign convention (\Box - m^2)\,\delta\phi=0, and give the result in one sentence."",  ""Step_2"": ""Scalar–tensor mapping (Concept_1): introduce an auxiliary field \chi, define \phi \equiv f_R(\chi) and V(\phi)=\chi(\phi)\,\phi - f(\chi(\phi)); with \omega=0 this is dynamically equivalent to a Brans–Dicke theory for the scalar \phi."",  ""Step_3"": ""Trace equation (Concept_2): vary the action and take the trace to obtain 3\,\Box f_R + f_R R - 2f = 8\pi G\,T; in vacuum or about a fixed background set T=0 and evaluate near R_0."",  ""Step_4"": ""Linearization (Concept_3): expand R=R_0+\delta R and \phi=\phi_0+\delta\phi with the identity \delta\phi = f_{RR}(R_0)\,\delta R (since \phi=f_R), then keep terms linear in perturbations in the traced field equation."",  ""Step_5"": ""Klein–Gordon identification: cast the linearized trace equation into (\Box - m^2)\,\delta\phi=0 under the stated sign convention, which fixes the mass as m^2 = (f_R - R f_{RR})/(3 f_{RR}) evaluated at R_0."",  ""Step_6"": ""One-sentence answer: \(\boxed{\,m^{2}=\dfrac{f_{R}-R f_{RR}}{3 f_{RR}}\,}\) (evaluated at the background R_0).""  }"	  concept_1: Scalar–tensor equivalence of $f(R)$ gravity — introduce an auxiliary field $\chi$, define $\phi\equiv f_{R}(\chi)$ and $V(\phi)=\chi(\phi)\,\phi-f(\chi(\phi))$, so the theory is dynamically equivalent to a Brans–Dicke model with $\omega=0$.  concept_2: Trace equation — varying the action and taking the trace yields $3\,\Box f_{R}+f_{R}R-2f=8\pi G\,T$; in vacuum (or around a fixed background) this governs the dynamics of the scalar mode.  concept_3: Linearization around a background $(R_0,\phi_0)$ — expand $R=R_0+\delta R$, $\phi=\phi_0+\delta\phi$, with the identity $\delta\phi=f_{RR}(R_0)\,\delta R$ from $\phi=f_R$.	$$ \boxed{\,m^{2}=\dfrac{f_{R}-R f_{RR}}{3 f_{RR}}\,} $$	理论基础学科-理论天体物理-广义相对论与引力理论	347164	2	1
recuWCay6yq5mw	Consider a one-dimensional traffic flow model on a ring road with the partition function: $Z=\sum_S{\prod_{i\in N}{\exp \left\{ KS_iS_{i+1}+BS_i \right\}}}$, $S_{N+1}=S_1$, where $S_i=\pm 1$ represents vehicle states, K is the interaction coefficient between adjacent vehicles, and B is the field coefficient biasing forward movement. To perform the renormalization group transformation, first relabel the spins in the fine-grained partition function as $i=1,2,3,…,\frac{N}{2}$ . Then, perform a partial sum over all even-numbered spins (effectively decimating them). This leads to the following expression for the partition function:  $ Z=\sum_{\left\{ \cdots ,S_1,S_3,S_5,\cdots \right\}}{\prod_{i=2,4,6,\cdots}{\sum_{\left\{ S_i \right\}}{\exp \left\{ KS_{i-1}S_i+KS_iS_{i+1}+B\left( \frac{S_{i-1}}{2}+S_i+\frac{S_{i+1}}{2} \right) \right\}}}} $. Since the fine-grained and coarse-grained partition functions represent the same physical system, they must be equivalent. Question: Using the equivalence of the partition functions before and after the decimation transformation, how are the renormalized interaction coefficient K′ and field coefficient B′ determined? Please present the final results without using hyperbolic cosine functions.	"{'step_1': '1: Clarify the System and Partition Function Basics\nIdentify system properties: This model is a 1D ring road traffic flow system, mathematically equivalent to the 1D Ising model (Concept 1). It contains \\(N\\) lattice sites corresponding to vehicle positions, where each site has a state \\(S_i = \\pm 1\\) — \\(S_i = +1\\) represents an occupied site (with a vehicle), and \\(S_i = -1\\) represents an empty site. The periodic boundary condition \\(S_{N+1} = S_1\\) holds (since it is a ring road).\n\nConfirm the form of the partition function: The original fine-grained partition function is given by \n\\(Z = \\sum_S \\prod_{i \\in N} \\exp\\left\\{ K S_i S_{i+1} + B S_i \\right\\}\\), \nwhere \\(K\\) is the interaction coefficient between adjacent lattice sites (describing the correlation between neighboring vehicles), and \\(B\\) is the field coefficient biasing forward movement (favoring occupied or empty states).', 'step_2': ""2: Define the Goal and Core Operation of Renormalization\nCore operation: Perform the renormalization group (RG) transformation specified — **decimate all even-numbered sites**. This means summing over the states of all even-indexed spins (\\(S_2, S_4, \\dots, S_N\\)) and only retaining the odd-indexed spins (\\(S_1, S_3, \\dots, S_{N-1}\\)), reducing the system’s degrees of freedom from \\(N\\) to \\(N/2\\) (assuming \\(N\\) is even).\n\nKey principle: The coarse-grained partition function \\(Z'\\) after decimation must be **equivalent to the original partition function** (they describe the same physical system). Thus, \\(Z'\\) must maintain the same functional form as \\(Z\\), i.e., \n\\(Z' = \\sum_{\\{S_{\\text{odd}}\\}} \\prod_{j} \\exp\\left( K' S_{2j-1} S_{2j+1} + B' S_{2j-1} \\right)\\), \nwhere \\(K'\\) is the renormalized interaction coefficient (between non-adjacent original odd sites, now adjacent in the coarse-grained system) and \\(B'\\) is the renormalized field coefficient. A spin-independent constant factor \\(f(K,B)\\) (from the partial sum over even spins) is allowed in the equivalence."", 'step_3': '3: Extract Key Matching Conditions (Based on Neighboring Coarse Spin Configurations)\nWhen decimating even sites, each even spin \\(S_i\\) (even-indexed) is surrounded by two odd spins: \\(S_{i-1}\\) and \\(S_{i+1}\\) (odd-indexed). The partial sum over \\(S_i = \\pm 1\\) for each ""odd-even-odd"" block must match the corresponding term in the coarse-grained partition function. There are 3 distinct configurations of the outer odd spins, leading to 3 core equations:\n\n1. **Case 1: Outer odd spins are both \\(+1\\) (\\(S_{i-1} = S_{i+1} = +1\\))** \n Sum over \\(S_i = \\pm 1\\): \n \\(\\exp\\left\\{ K \\cdot (+1) \\cdot S_i + K \\cdot S_i \\cdot (+1) + B\\left( \\frac{+1}{2} + S_i + \\frac{+1}{2} \\right) \\right\\}\\) (from the problem’s given partial sum exponent) \n Simplifies to summing over \\(S_i = \\pm 1\\): \\(\\exp(2K S_i + B(1 + S_i)) = \\exp(B) \\cdot \\exp((2K + B)S_i)\\). \n Evaluating for \\(S_i = +1\\) and \\(S_i = -1\\), the sum becomes: \n \\(\\exp(2K + 2B) + \\exp(-2K) = f(K,B) \\cdot \\exp(K\' + B\')\\) \n (matches the coarse-grained term for adjacent coarse spins \\(+1, +1\\)).\n\n2. **Case 2: Outer odd spins are both \\(-1\\) (\\(S_{i-1} = S_{i+1} = -1\\))** \n Similarly, sum over \\(S_i = \\pm 1\\): \n \\(\\exp\\left\\{ K \\cdot (-1) \\cdot S_i + K \\cdot S_i \\cdot (-1) + B\\left( \\frac{-1}{2} + S_i + \\frac{-1}{2} \\right) \\right\\} = \\exp(-B) \\cdot \\exp((-2K + B)S_i)\\). \n Evaluating for \\(S_i = +1\\) and \\(S_i = -1\\), the sum becomes: \n \\(\\exp(-2K) + \\exp(2K - 2B) = f(K,B) \\cdot \\exp(K\' - B\')\\) \n (matches the coarse-grained term for adjacent coarse spins \\(-1, -1\\)).\n\n3. **Case 3: Outer odd spins have opposite signs (\\(S_{i-1} = -S_{i+1} = \\pm 1\\))** \n Let \\(S_{i-1} = +1\\) and \\(S_{i+1} = -1\\) (the result is symmetric for \\(S_{i-1} = -1, S_{i+1} = +1\\)). Sum over \\(S_i = \\pm 1\\): \n \\(\\exp\\left\\{ K \\cdot (+1) \\cdot S_i + K \\cdot S_i \\cdot (-1) + B\\left( \\frac{+1}{2} + S_i + \\frac{-1}{2} \\right) \\right\\} = \\exp(B S_i)\\). \n Evaluating for \\(S_i = +1\\) and \\(S_i = -1\\), the sum becomes: \n \\(\\exp(B) + \\exp(-B) = f(K,B) \\cdot \\exp(-K\')\\) \n (matches the coarse-grained term for adjacent coarse spins with opposite signs).', 'step_4': ""4: Eliminate the Constant Factor \\(f(K,B)\\) to Establish Equations for \\(K'\\) and \\(B'\\)\nFirst, solve for \\(f(K,B)\\) from Case 3 (it only involves \\(K'\\) and \\(B\\), simplifying elimination): \nFrom Case 3: \\(f(K,B) = \\frac{\\exp(B) + \\exp(-B)}{\\exp(-K')} = (\\exp(B) + \\exp(-B)) \\cdot \\exp(K')\\) — denoted as Equation (3a).\n\nSubstitute Equation (3a) into Case 1 and Case 2 to eliminate \\(f(K,B)\\): \n- For Case 1: \n \\(\\exp(2K + 2B) + \\exp(-2K) = (\\exp(B) + \\exp(-B)) \\cdot \\exp(K') \\cdot \\exp(K' + B')\\) \n Simplifies to: \\(\\frac{\\exp(2K + 2B) + \\exp(-2K)}{\\exp(B) + \\exp(-B)} = \\exp(2K' + B')\\) — denoted as Equation (1a). \n\n- For Case 2: \n \\(\\exp(2K - 2B) + \\exp(-2K) = (\\exp(B) + \\exp(-B)) \\cdot \\exp(K') \\cdot \\exp(K' - B')\\) \n Simplifies to: \\(\\frac{\\exp(2K - 2B) + \\exp(-2K)}{\\exp(B) + \\exp(-B)} = \\exp(2K' - B')\\) — denoted as Equation (2a)."", 'step_5': ""5: Solve for the Renormalized Interaction Coefficient \\(K'\\)\nTo eliminate \\(B'\\), multiply Equation (1a) by Equation (2a): \n- Left-hand side (LHS): \n \\(\\frac{[\\exp(2K + 2B) + \\exp(-2K)] \\cdot [\\exp(2K - 2B) + \\exp(-2K)]}{(\\exp(B) + \\exp(-B))^2}\\) \n Expand the numerator using \\((A+B)(C+D) = AC + AD + BC + BD\\): \n \\(\\exp((2K+2B)+(2K-2B)) + \\exp((2K+2B)+(-2K)) + \\exp((-2K)+(2K-2B)) + \\exp((-2K)+(-2K))\\) \n Simplifies to: \\(\\exp(4K) + \\exp(2B) + \\exp(-2B) + \\exp(-4K)\\). \n\n- Right-hand side (RHS): \n \\(\\exp(2K' + B') \\cdot \\exp(2K' - B') = \\exp(4K')\\) (since exponents add, and \\(B'\\) terms cancel). \n\nEquate LHS and RHS: \n\\(\\frac{\\exp(4K) + \\exp(2B) + \\exp(-2B) + \\exp(-4K)}{(\\exp(B) + \\exp(-B))^2} = \\exp(4K')\\) \n\nTake the natural logarithm of both sides and divide by 4 to solve for \\(K'\\): \n\\(K' = \\frac{1}{4} \\ln\\left( \\frac{\\exp(4K) + \\exp(2B) + \\exp(-2B) + \\exp(-4K)}{(\\exp(B) + \\exp(-B))^2} \\right)\\)"", 'step_6': ""6: Solve for the Renormalized Field Coefficient \\(B'\\)\nTo eliminate \\(K'\\) and \\(f(K,B)\\), divide Equation (1a) by Equation (2a): \n- Left-hand side (LHS): \n \\(\\frac{\\exp(2K + 2B) + \\exp(-2K)}{\\exp(2K - 2B) + \\exp(-2K)}\\) \n\n- Right-hand side (RHS): \n \\(\\frac{\\exp(2K' + B')}{\\exp(2K' - B')} = \\exp(2B')\\) (since exponents subtract, and \\(K'\\) terms cancel). \n\nEquate LHS and RHS: \n\\(\\frac{\\exp(2K + 2B) + \\exp(-2K)}{\\exp(2K - 2B) + \\exp(-2K)} = \\exp(2B')\\) \n\nTake the natural logarithm of both sides and divide by 2 to solve for \\(B'\\): \n\\(B' = \\frac{1}{2} \\ln\\left( \\frac{\\exp(2K + 2B) + \\exp(-2K)}{\\exp(2K - 2B) + \\exp(-2K)} \\right)\\)"", 'step_7': ""7: Final Results\nThe renormalized coefficients, containing only exponential functions (no hyperbolic cosines), are:\n\n1. Renormalized interaction coefficient: \n\\(K' = \\frac{1}{4} \\ln\\left( \\frac{\\exp(4K) + \\exp(2B) + \\exp(-2B) + \\exp(-4K)}{\\exp(2B) + 2 + \\exp(-2B)} \\right)\\) \n(Note: \\((\\exp(B) + \\exp(-B))^2 = \\exp(2B) + 2 + \\exp(-2B)\\), so the denominator can be written in this simplified form.)\n\n2. Renormalized field coefficient: \n\\(B' = \\frac{1}{2} \\ln\\left( \\frac{\\exp(2K + 2B) + \\exp(-2K)}{\\exp(2K - 2B) + \\exp(-2K)} \\right)\\)""}"	"{'concept_1': 'Similar to the one-dimensional Ising model, the dynamics can be modeled with N sites, each occupying the space of a vehicle, with state Si∈{−1,+1}. Si=−1 represents an empty site, and Si=+1 represents an occupied site. The Hamiltonian H of the entire system is given by $H=-J\\sum_{\\left< i,j \\right>}{S_i}S_j-F\\sum_{i=1}^N{S_i}$, where $S_i$ and $S_j$ are the states of sites i and j, and $\\left< i,j \\right> $ denotes neighboring pairs.', 'concept_2': ""Case 1: $S_{i-1}=S_{i+1}=+1$, meaning both sites are in state +1, we can obtain:$\\exp \\left\\{ 2K+2B \\right\\} +\\exp \\left\\{ -2K \\right\\} =f\\left( K,B \\right) \\exp \\left\\{ K'+B' \\right\\} $ ; Case 2: $S_{i-1}=S_{i+1}=-1$, meaning both sites are in state -1, we can obtain:$\\exp \\left\\{ -2K \\right\\} +\\exp \\left\\{ 2K-2B \\right\\} =f\\left( K,B \\right) \\exp \\left\\{ K'-B' \\right\\} $ ; Case 3: $S_{i-1}=-S_{i+1}=\\pm1$, meaning the status of these two stations is different from that of the other stations, we can obtain:$\\exp \\left\\{ B \\right\\} +\\exp \\left\\{ -B \\right\\} =f\\left( K,B \\right) \\exp \\left\\{ -K' \\right\\} $""}"	$$B'=\frac{1}{2}\ln \left\{ \frac{e^{\left\{ 2K+2B \right\}}+e^{\left\{ -2K \right\}}}{e^{\left\{ 2K-2B \right\}}+e^{\left\{ -2K \right\}}} \right\} $$     $$K'=\frac{1}{4}\ln \left\{ \frac{\left( e^{\left\{ 2K+2B \right\}}+e^{\left\{ -2K \right\}} \right) \left( e^{\left\{ -2K \right\}}+e^{\left\{ 2K-2B \right\}} \right)}{e^{\left\{ -2B \right\}}+2+e^{\left\{ 2B \right\}}} \right\} $$	理论交叉学科-交通流理论-交通流建模	194687	3	2
recuWFIUj74GY9	In a curved spacetime, consider a high--frequency electromagnetic wave (wavelength $\lambda \ll$ the geometric variation scale) in a circularly polarized state. Starting from the action W=\frac18 \int F^2 \sqrt{-g}\, d^4x, \qquad F^2 = F_{\mu\nu}\,\bar F^{\mu\nu}, what is the field-strength invariant F^2?	"  {  ""Step_1"": ""Goal: In curved spacetime, for a high-frequency (ω ≫ 1/ℓ_geom) circularly polarized electromagnetic wave starting from W = \\tfrac{1}{8} \\int F^2 \\sqrt{-g} \\, d^4x, derive the field-strength invariant F^2 \\equiv F_{\\mu\\nu} \\bar F^{\\mu\\nu}."",  ""Step_2"": ""High-frequency ansatz (Concept_1): Take the complex potential A_\\mu = a \\, M_\\mu \\, e^{i\\omega S}, with real amplitude a and phase S, and k_\\mu \\equiv S_{,\\mu}; the complex null vector M^\\mu satisfies M_\\mu M^\\mu = \\bar M_\\mu \\bar M^\\mu = 0, M_\\mu \\bar M^\\mu = 1, and M^\\mu k_\\mu = 0 (circular polarization encoded by M)."",  ""Step_3"": ""Order expansion of the field strength (Concept_2): F_{\\mu\\nu} = A_{\\nu;\\mu} - A_{\\mu;\\nu} = e^{i\\omega S}(H_{\\mu\\nu} + G_{\\mu\\nu}), with H_{\\mu\\nu} = i\\omega a (k_{\\mu} M_{\\nu} - k_{\\nu} M_{\\mu}) = \\mathcal{O}(\\omega) and G_{\\mu\\nu} = (a M_{\\nu})_{;\\mu} - (a M_{\\mu})_{;\\nu} = \\mathcal{O}(1). Hence F^2 = H_{\\mu\\nu} \\bar H^{\\mu\\nu} + (H_{\\mu\\nu} \\bar G^{\\mu\\nu} + G_{\\mu\\nu} \\bar H^{\\mu\\nu}) + G_{\\mu\\nu} \\bar G^{\\mu\\nu}."",  ""Step_4"": ""Power counting (Concept_2,3): Keep \\mathcal{O}(\\omega^2) and \\mathcal{O}(\\omega) terms—i.e., H\\bar H (leading) and the cross term H\\bar G + G\\bar H; drop G\\bar G = \\mathcal{O}(1) as subleading in the high-frequency expansion."",  ""Step_5"": ""Compute H\\bar H via the bivector inner-product identity and constraints M·\\bar M = 1, M·k = 0: writing H_{\\mu\\nu} = i\\omega a (k \\wedge M)_{\\mu\\nu} gives H_{\\mu\\nu} \\bar H^{\\mu\\nu} = 2 \\omega^2 a^2 (k \\cdot k) = 2 \\omega^2 a^2 (\\nabla S)^2."",  ""Step_6"": ""Prepare the cross term (Concept_3): Use G_{\\mu\\nu} = a(M_{\\nu;\\mu} - M_{\\mu;\\nu}) + (a_{;\\mu} M_{\\nu} - a_{;\\nu} M_{\\mu}) and the identity \\bar M_\\nu M^\\nu = 1 \\Rightarrow (\\bar M_{\\nu;\\mu} M^\\nu + \\bar M_\\nu M^{\\nu}{}_{;\\mu}) = 0."",  ""Step_7"": ""Introduce the spin-connection one-form B_\\mu \\equiv i \\, \\bar M_\\nu M^{\\nu}{}_{;\\mu} and simplify: (H_{\\mu\\nu} \\bar G^{\\mu\\nu} + G_{\\mu\\nu} \\bar H^{\\mu\\nu}) = -4 \\omega a^2 B_\\mu k^\\mu + (\\text{total derivative}) + \\mathcal{O}(\\omega^0) (Concept_4)."",  ""Step_8"": ""Assemble terms and drop boundary/subleading pieces: F^2 \\simeq H\\bar H + (H\\bar G + G\\bar H) = 2 \\omega^2 a^2 (k \\cdot k) - 4 \\omega a^2 B_\\mu k^\\mu."",  ""Step_9"": ""Rewrite with k_\\mu = S_{,\\mu} and (\\nabla S)^2 \\equiv k_\\mu k^\\mu; factor out 4\\omega^2 a^2."",  ""Step_10"": ""Final result: F^2 = 4 \\, \\omega^2 a^2 \\!\\left[ \\tfrac{1}{2} (\\nabla S)^2 - \\frac{1}{\\omega} B^\\mu S_{,\\mu} \\right], which is the desired high-frequency, polarization-corrected field-strength invariant.""  }"	  concept_1: Consider a complex Maxwell field F_{\mu\nu} the action for which is of the formW = \frac{1}{8} \int \mathcal{F}^2 \sqrt{-g} d^4 x, \quad \mathcal{F}^2 = F_{\mu\nu} \bar{F}^{\mu\nu}. To develop the spinoptics approach we write a complex field potential A_\mu in the form A_\mu = a M_\mu e^{i\omega S}. Here amplitude \( a \) and phase \( S \) are real scalar functions. \( M^\mu \) is a complex null vector, satisfying the following conditions M_\mu M^\mu = \bar{M}_\mu \bar{M}^\mu = 0, \ M_\mu \bar{M}^\mu = 1, \ M^\mu S_{,\mu} = 0. As in the standard geometric optics a surface \( S = \text{const} \) defines a wavefront and the last condition means that the complex null vectors M and \bar{M} are tangent to these surfaces.    concept_2: Order expansion of the field strength tensor: F_{\mu\nu} = A_{\nu;\mu} - A_{\mu;\nu} = e^{i\omega S} (H_{\mu\nu} + G_{\mu\nu}), where H_{\mu\nu} = i\omega a (k_{\mu} M_{\nu} - k_{\nu} M_{\mu}) = \mathcal{O}(\omega),\quad G_{\mu\nu} = (a M_{\nu})_{;\mu} - (a M_{\mu})_{;\nu} = \mathcal{O}(1). The conjugate is \(\bar{F}^{\mu\nu} = e^{-i\omega S} (\bar{H}^{\mu\nu} + \bar{G}^{\mu\nu})\). Thus, F^2 \equiv F_{\mu\nu} \bar{F}^{\mu\nu} = H_{\mu\nu} \bar{H}^{\mu\nu} + H_{\mu\nu} \bar{G}^{\mu\nu} + G_{\mu\nu} \bar{H}^{\mu\nu} + G_{\mu\nu} \bar{G}^{\mu\nu}.    concept_3: Simple calculations give the following expression for the complex field strength F_{\mu\nu} &= A_{\nu;\mu} - A_{\mu;\nu} = \left[ i\omega a(S_{,\mu}M_{\nu} - S_{,\nu}M_{\mu}) \right. \left. + \left[(aM_{\nu})_{;\mu} - (aM_{\mu})_{;\nu}\right] \right] \exp(i\omega S). Substituting this expression in \( \mathcal{F}^2 \) and keeping terms of the second and first order in \( \omega \). Substitute (G_{\mu\nu} = a\left(M_{\nu;\mu} - M_{\mu;\nu}\right) + \left(a_{;\mu} M_\nu - a_{;\nu} M_\mu\right)) into ( H_{\mu\nu} \bar{G}^{\mu\nu} + G_{\mu\nu} \bar{H}^{\mu\nu} = 2\operatorname{Re}\left(H_{\mu\nu} \bar{G}^{\mu\nu}\right) ).     concept_4: By calculation and using (\bar{M}\nu M^\nu = 1 \implies \bar{M}{\nu;\mu} M^\nu + \bar{M}\nu M^\nu{;\mu} = 0). For simplification, the cross term can be written as (H_{\mu\nu} \bar{G}^{\mu\nu} + G_{\mu\nu} \bar{H}^{\mu\nu} = -4\omega a^2 B_\mu k^\mu + (\text{total derivative}) + \mathcal{O}(\omega^0)), where ( B_\mu ) is defined as (B_\mu \equiv i \bar{M}\nu M^{\nu}{}{;\mu}).	\mathcal{F}^2 = 2\omega^2 a^2 k_\mu k^\mu - 4\omega a^2 B_\mu k^\mu + \mathcal{O}(\omega^0)	理论交叉学科-电磁理论与量子电磁学-电磁波传播	170614	2	4
recuWOoknAO2mx	Within the framework of the 3D global geodynamic models to simulate mantle convection, provide the specific mathematical formula used to parameterize the mantle's viscosity, $\eta$. This formula must explicitly account for the material's dependence on temperature, depth, and the presence of the post-perovskite (pPv) mineral phase.	  1. The goal is to construct the comprehensive mathematical formula for mantle viscosity, $\eta$, by systematically incorporating its dependencies on depth, temperature, and mineral phase.  2. First, a baseline viscosity is established that accounts for the increase in pressure with depth. This is represented by a depth-dependent prefactor, $\eta_r$ (Concept_2). This forms the base of the viscosity calculation.  3. Next, the strong influence of temperature on mantle rheology is incorporated. This is modeled as an exponential relationship where viscosity decreases with increasing temperature, governed by the activation coefficient $E$ and the non-dimensional temperature $T$. This term, $exp[E(0.5-T)]$, is multiplied by the base viscosity (Concept_1).  4. The model must also account for the significant change in material properties due to the bridgmanite (Bdg) to post-perovskite (pPv) phase transition in the deep mantle (Concept_3).  5. To quantify this effect, a phase function, $\Gamma$, is introduced. This function smoothly varies from 0 (representing pure Bdg) to 1 (representing pure pPv), indicating the fraction of the material that has transformed into the pPv phase at a given location (Concept_4).  6. The presence of the pPv phase alters the viscosity by a specific multiplicative factor, $\eta_{ppv}$ (Concept_5). The effect of this factor is scaled by the fraction of pPv present, which is mathematically expressed as $(\eta_{ppv})^\Gamma$.  7. Finally, all these components are combined into a single equation. The depth-dependent prefactor $\eta_r$ is multiplied by the temperature-dependent term and the phase-dependent term. The two exponential effects are combined into a single exponent, using the mathematical identity that $exp(a) \cdot (\eta_{ppv})^\Gamma = exp(a) \cdot exp[\Gamma~ln(\eta_{ppv})] = exp[a + \Gamma~ln(\eta_{ppv})]$. This yields the final comprehensive formula: $\eta=\eta_{r}exp[E(0.5-T)+\Gamma~ln(\eta_{ppv})]$.	  - concept_1: The viscosity of the mantle is modeled with a strong dependence on temperature, where higher temperatures lead to lower viscosity. This is captured by an exponential term involving an activation coefficient, E, and the non-dimensional temperature, T.  - concept_2: A depth-dependent viscosity prefactor, $\eta_r$, is used to account for the general increase in viscosity from the upper mantle to the lower mantle due to pressure.  - concept_3: The model incorporates the solid-state phase transition from bridgmanite (Bdg) to post-perovskite (pPv) that occurs in the lowermost mantle, as this transition significantly affects material properties.  - concept_4: A phase function, $\Gamma$, which varies from 0 for pure Bdg to 1 for pure pPv, is used to quantify the fraction of pPv phase present at a given depth and temperature.  - concept_5: The viscosity change due to the pPv phase transition is modeled using a multiplicative factor, $\eta_{ppv}$, which modifies the local viscosity based on the fraction of pPv present.	$\eta=\eta_{r}exp[E(0.5-T)+\Gamma~ln(\eta_{ppv})]$	其他理论学科-地质力学与地球动力学-地应力模型	107823	2	5
recuWRd7d8pLsA	After the main earthquake, what was the magnitude and direction of the rotation of the dominant orientation of the active fault system (as represented by the P-axis direction of the summed focal mechanisms) within the Ridgecrest main earthquake's centroid zone compared to that before the main earthquake?	"  Step 1：To obtain the dominant orientation before and after the mainshock, we follow the spatiotemporal sampling strategy. Specifically, we extract focal mechanisms within the centroid zone (defined in Fig. 1 of the paper) and apply separate time windows for the pre- and post-mainshock periods:Spatial window: centroid zone region around the Ridgecrest Mw 7.1 mainshock centroid (same as in Atterholt et al. 2025).Temporal windows:Pre-mainshock window: seismicity prior to July 4, 2019 (up to foreshock sequence).Post-mainshock window: early post-seismic period within 0–0.1 years after mainshock (as used in Fig. 2 of the paper).Event selection criteria:Minimum number of events per window: N≥150N \ge 150N≥150 focal mechanisms (consistent with SMS temporal sampling).Minimum event quality: only focal mechanisms with strike, dip, rake uncertainty < 7.5∘7.5^\circ7.5∘ are used (as stated in Section ""Moment Tensor Catalogue Construction"").Only double-couple mechanisms retained (removes pathological solutions; consistent with authors’ filtering)Then the normalized moment tensors in each time window are stacked using SMS:Sij=∑k=1NP^ij(k),P^ij(k)=Pij(k)M0(k)S_{ij} = \sum_{k=1}^{N} \hat{P}^{(k)}_{ij}, \quad \hat{P}^{(k)}_{ij} = \frac{P_{ij}^{(k)}}{M_0^{(k)}}Sij=k=1∑NP^ij(k),P^ij(k)=M0(k)Pij(k)The P-axis azimuth of SijS_{ij}Sij defines θpre\theta_{\text{pre}}θpre and θpost\theta_{\text{post}}θpost.  Step 2：Calculate rotation magnitude and direction with axial correction  Using Concept 1, the dominant average orientations before and after the mainshock are represented by θpre\theta_{\text{pre}}θpre and θpost\theta_{\text{post}}θpost, obtained via P-axis azimuths from SMS. Because the orientation is axis-symmetric (equivalent under +180°), the rotation is computed using the corrected axis-angle expression:  Δθ=mod(θpost−θpre+90∘,180∘)−90∘\Delta\theta = \text{mod}\big(\theta_{\text{post}} - \theta_{\text{pre}} + 90^\circ, 180^\circ\big) - 90^\circΔθ=mod(θpost−θpre+90∘,180∘)−90∘The result yields Δθ<0\Delta\theta < 0Δθ<0, indicating a counterclockwise rotation, and ∣Δθ∣≈20∘|\Delta\theta|\approx 20^\circ∣Δθ∣≈20∘, consistent with moderate rotational amplitude.  Step 3: Verify whether the rotation recovers over time (using Concept 3): Physical explanation: Referring to the post-earthquake stress field adjustment mechanism (Concept 3) and combining with observational results, the rotation angle in the main slip zone of the main earthquake remains stable for many years after the earthquake, indicating that this counterclockwise rotation reflects the permanent adjustment of the stress field caused by the main earthquake.  Comprehensive conclusion: The average directions before and after the main shock were obtained through Concept 2. Using the formula of Concept 1, it was calculated that \Delta\theta < 0, indicating that the dominant fault orientation in the main slip zone of Ridgecrest rotated counterclockwise after the main shock, with an amplitude of approximately 20 degrees. Moreover, according to Concept 3, this rotation basically did not recover in the years after the earthquake.  Comprehensive conclusion: The average directions before and after the main shock were obtained through Concept 2. Using the formula of Concept 1, it was calculated that \Delta\theta < 0, indicating that the dominant fault orientation in the main slip zone of Ridgecrest rotated counterclockwise after the main shock, with an amplitude of approximately 20 degrees. Moreover, according to Concept 3, this rotation basically did not recover in the years after the earthquake."	  Concept 1: Mathematical definition of stress field rotation: \(\Delta\theta = \theta_{\text{post}} - \theta_{\text{pre}}\), where \(\theta_{\text{pre}}\) and \(\theta_{\text{post}}\) are the dominant fault strike or stress principal axis azimuths before and after the main earthquake, respectively, with 0° at north and increasing clockwise. If \(\Delta\theta > 0\), it indicates a clockwise rotation; if \(\Delta\theta < 0\), it indicates a counterclockwise rotation; \(|\Delta\theta|\) is the rotation amplitude. In the main slip zone of the Ridgecrest main earthquake, observational results show a significant left-lateral rotation in the direction after the main earthquake, which belongs to a moderate-amplitude rotation.  Concept 2: Superposition of focal mechanism tensors: S_{ij} = \sum_{k=1}^{N} \hat{P}^{(k)}_{ij}, where \hat{P}^{(k)}_{ij} is the normalized seismic moment tensor. The P-axis direction of the superimposed tensor can be used as the average stress direction of the spatiotemporal window, which is used to calculate \theta_{\text{pre}} and \theta_{\text{post}}.  Concept 3: Post-earthquake stress field adjustment mechanism: After the main earthquake, the stress field gradually reloads due to afterslip and viscoelastic relaxation. However, in the main slip zone of the main earthquake, observations show that the dominant orientation after rotation remains basically unchanged and does not significantly return to the pre-earthquake state. This means that the average stress direction in the years after the earthquake is basically stable, and the rotation angle can be directly estimated using early post-earthquake data.	Rotate approximately 20 degrees counterclockwise.	其他理论学科-地质力学与地球动力学-地震成因理论	102969	2	3
recuVPuZERWrr4	Under the short-range potential approximation and in the low-energy scattering regime, what is the asymptotic form of the scattering-state wave function in the limit $r \to \infty$?Answer in one sentence without using the internet.	"  {  ""Step_1"": ""Goal: derive, in one sentence, the large-r asymptotic form of the scattering-state wave function under a short-range potential at low energy (s-wave dominance)."",  ""Step_2"": ""Short-range potential (Concept_1): for r ≫ range(V), the interaction vanishes and ψ obeys the free Helmholtz equation (∇² + k²)ψ = 0 in the asymptotic region."",  ""Step_3"": ""Low-energy regime (Concept_2): assume kR ≪ 1 with R the potential range; retain only the s-wave (ℓ = 0) in the partial-wave series since higher ℓ are suppressed."",  ""Step_4"": ""Incoming regular solution (Concept_3): the ℓ = 0 free regular solution is the spherical Bessel j₀(kr) = sin(kr)/(kr), which we write as the incident component."",  ""Step_5"": ""Outgoing scattered solution (Concept_4 & Concept_5): the asymptotic outgoing solution is the spherical Hankel h₀^{(+)}(kr) ∼ e^{ikr}/(ikr); imposing the Sommerfeld radiation condition selects a purely outgoing term ∝ e^{ikr}/r."",  ""Step_6"": ""Scattering amplitude (Concept_6): in the s-wave limit the scattering amplitude is angle-independent; denote it by f(k), absorbing constant 1/ik factors and phase conventions into f(k)."",  ""Step_7"": ""Normalization of the incident piece (Concept_7): represent the incident s-wave as N · sin(kr)/(kr), where N is a normalization constant fixed by the chosen convention (e.g., matching to an s-wave component of a plane wave)."",  ""Step_8"": ""Linear superposition (Concept_8): by linearity in the asymptotic region, write ψ_asy as the sum of the incoming regular solution and the outgoing scattered wave."",  ""Step_9"": ""Optional parametrization (Concept_9): at low k, relate f(k) to the s-wave phase shift via the effective-range expansion, f(k) = [k cot δ₀(k) − i k]^{-1}, if one needs an explicit low-energy model for f(k)."",  ""Step_10"": ""Units/conventions (Concept_10): set ℏ = c = 1 if desired; residual constants and overall phases are absorbed into N and f(k) without changing the asymptotic structure."",  ""Step_11"": ""Domain of validity (Concept_11): result holds for r → ∞ and kR ≪ 1 with s-wave dominance; angular dependence is neglected consistently."",  ""Step_12"": ""Assemble the result (Answer): ψ_{asy}(\\mathbf r,\\mathbf k) = N\\,\\dfrac{\\sin(kr)}{kr} + f(k)\\,\\dfrac{e^{ikr}}{r}, \\quad (r \\to \\infty).""  }"	  Concept_1. Assume a **short-range potential** so that for **large distances** $r\gg \text{range}(V)$ the interaction is negligible and the wave function satisfies the **free Helmholtz equation**.    Concept_2. Work in the **low-energy regime** $kR\ll1$ (with $R$ the potential range), so that **$s$-wave (ℓ=0) dominance** applies and higher partial waves can be neglected.    Concept_3. Use the **partial-wave expansion** and keep only ℓ=0: the regular free solution is the **spherical Bessel** $j_0(kr)=\sin(kr)/(kr)$.    Concept_4. The outgoing free solution at large $r$ is the **spherical Hankel** $h_0^{(+)}(kr)\sim e^{ikr}/(ikr)$; any constant factors can be **absorbed into the scattering amplitude**.    Concept_5. Impose the **Sommerfeld radiation condition** to select the **purely outgoing** scattered component $\propto e^{ikr}/r$.    Concept_6. Define the **scattering amplitude** for $s$-wave as an **angle-independent** function $f(k)$ (isotropic scattering in the $s$-wave limit).    Concept_7. Represent the **incident component** as an **incoming spherical wave** $N\,\sin(kr)/(kr)$ (equivalent to the $s$-wave part of an incident plane wave), with $N$ a **normalization constant** fixed by the chosen convention.    Concept_8. Recognize that the **total asymptotic wave** is a **superposition** of the incoming regular solution and the outgoing scattered wave.    Concept_9. (Optional link) Relate $f(k)$ to the **phase shift** via the **effective-range expansion**: $f(k)=[k\cot\delta_0(k)-ik]^{-1}$, useful for parametrizing $f(k)$ at low $k$.    Concept_10. (Units/conventions) One may set $\hbar=c=1$; any residual constants and phase conventions are **absorbed into $N$ and $f(k)$**.    Concept_11. **Validity domain:** large-$r$ (asymptotic) region and low-$k$ $s$-wave dominance; angular dependence is neglected consistently.	\begin{equation}  \psi_{\mathrm{asy}}(\mathbf r,\mathbf k)  = N\left[\frac{\sin(kr)}{kr} + f(k)\,\frac{e^{ikr}}{r}\right],  \qquad r\to\infty .  \end{equation}	理论基础学科-理论物理-重离子碰撞	106016	2	4
recuTLFdUIZ0ty	In the language of the 3d N=4 quantum field theory, what is the monodromy for the K-theoretic vertex function for the Higgs branch	"{   ""step_1"": ""From Concept 2, the K-theoretic vertex function satisfies a q-difference equation, and thus we can use the connection matrix for the q-difference equation to describe the monodromy of the K-theoretic vertex function"",   ""step_2"": ""From Theorem 1, the monodromy of the K-theoretic vertex function is described by the elliptic stable envelopes in the equivariant elliptic cohomology."",   ""step_3"": ""From the Fact 1, we can identify the equivariant quantum K-theory of Higgs branch as the corresponding A-twisted Q_A-cohomology of a 3d N=4 quiver gauge theory, and identify the equivariant elliptic cohomology by the ordinary Q-cohomology of the 3d N=4 quiver gauge theory."",   ""step_4"": ""Using the Fact 3, we know that we can identify the elliptic stable envelopes as the Janus interface and it is expected to be true for arbitrary Higgs branch."",   ""step_5"": ""By the definition of the Janus interface in the Fact 2, we can now say that the monodromy for the K-theoretic vertex function for the Higgs branch in the context of 3d N=4 quantum field theory is the Janus interface.""  }"	"{   ""concept_1"": ""The K-theoretic vertex function for the Higgs branch is an object in the equivariant quantum K-theory. It is the pushforward of the virtual fundamental sheaf for the moduli space of genus zero nonsingular quasimap to the Higgs branch to the Higgs branch."",   ""concept_2"": ""The monodromy for the K-theoretic vertex function means the following: The K-theoretic vertex function satisfies a q-difference equation, the meaning of the monodromy here means the connection matrix for the q-difference equation."",   ""concept_3"": ""In the 3d N=4 quiver gauge theory on E_τ×[0,1] with suitable boundary conditions, and here E_τ is the elliptic curve, it is believed that the ordinary Q-cohomology gives rise to the equivariant elliptic cohomology of the Higgs branch. Also it is believed that the A-twist Q_A-cohomology gives rise to the equivariant quantum K-theory."",   ""concept_4"": ""Given two boundary conditions for an ordinary 3d N=4 quiver gauge theory on E_τ×[0,1]. If the bulk has a mass flow such that on one side of the boundary condition there is no FI mass while on the other side there are FI masses. There is a wall-crossing in the bulk, and such a wall-crossing is called the Janus interface. Warning: This is only for the ordinary 3d N=4 but not for the A-twisted 3d N=4."",   ""concept_5"": ""For the case of the type A Higgs branch, it has been computed that the Janus interface coincides with the elliptic stable envelopes for the type A Higgs branch."",   ""theorem_1"": ""In the K-theory, the monodromy for the K-theoretic vertex function for the Higgs branch can be described by an equivariant elliptic cohomology class, which is called the elliptic stable envelopes.""  }"	"Janus interface, or ""the connection between Janus interface and elliptic stable envelopes"". As long as Janus interface is mentioned and the relationship between Janus interface and elliptic stable envelopes is explained, it can be considered correct."	理论基础学科-理论物理-量子场论	497422	2	3
recuUc16ttR11G	"Gut microbiota is a large microbial community colonizing the human digestive tract, mainly composed of bacteria, viruses, fungi, and archaea, and plays an important role in nutrient metabolism, immune regulation, and maintenance of the intestinal barrier. They can not only break down dietary fiber, synthesize vitamins and short-chain fatty acids, but also affect the nervous system and emotional states through the ""gut-brain-immune axis"". In a healthy state, gut microbiota maintains homeostasis and helps defend against pathogens; while microbial dysbiosis is closely associated with various diseases such as inflammatory bowel disease, obesity, diabetes mellitus, depression, and neurodegenerative diseases, thus becoming an important focus of current biomedical research and clinical intervention. Some gut microbiota can induce neuroinflammation. If central nervous system inflammation is observed in the presence of gut microbiota, please answer which key cells induce neuroinflammation and how."	"{   ""step_1"": ""Gut microbiota proliferates after invading the intestinal tract and induces local intestinal inflammation, at which time microbiota-specific CD4 T cells and other immune cells such as neutrophils and macrophages are involved in the formation of local intestinal inflammation"",   ""step_2"": ""After activation in the intestines, T cells migrate to the circulatory system, where most T cells home to lymph nodes to guide B cell differentiation, and some T cells migrate to the blood-brain barrier"",   ""step_3"": ""Due to the selective function of the blood-brain barrier, only activated T cells enter the nervous system as immune surveillance components"",   ""step_4"": ""Due to molecular mimicry, there is similarity between gut microbiota-specific peptides and some peptides of cells in the body"",   ""step_5"": ""Similar peptides activate T cells and induce their local proliferation, thereby forming a sufficient cell population to produce cytokines such as IL-17A, IL-17F, and IL-22"",   ""step_6"": ""Inflammatory factors further inhibit the blood-brain barrier and selectively recruit more immune cells, ultimately leading to neuroinflammation""  }"	"{   ""concept_1"": ""Type 17 Inflammation Mediates Gut Microbiota-Associated Inflammation. Type 17 inflammation is a type of immune response centered on the IL-17 cytokine pathway, mainly mediated by CD4+ Th17 cells, while γδ T cells, ILC3s, NKT cells, and other cell types can also secrete IL-17 to participate in this process. The differentiation of Th17 cells depends on the transcription factor RORγt (product of the Rorc gene) and is driven by inflammatory cytokines such as IL-6, IL-1β, and TGF-β for differentiation; their maintenance and expansion require IL-23/IL-23R signaling. The effector molecules IL-17A, IL-17F, and IL-22 can promote epithelial cells to express antimicrobial peptides (e.g., Defb4, Reg3γ) and chemokines (e.g., CXCL1, CXCL8), thereby strongly recruiting and activating neutrophils and enhancing mucosal barrier function. In bacterial infections, Type 17 inflammation is crucial for defending against extracellular bacteria (e.g., Staphylococcus aureus, Escherichia coli, Streptococcus pneumoniae) and protects the host by promoting bacterial clearance and barrier repair. However, sustained overactivation of the IL-17 pathway can lead to tissue damage and chronic inflammation, contributing to the pathogenesis of inflammatory bowel disease, psoriasis, and chronic pulmonary infections. Therefore, Type 17 inflammation exhibits a double-edged sword effect in bacterial infections: it is not only a key mechanism for mucosal defense but also an important driver of chronic inflammation and immune-related diseases. https://doi.org/10.1038/s41423-024-01218-x"",   ""concept_2"": ""Immune Cell Penetration of the Blood-Brain Barrier. How immune cells cross the central nervous system (CNS) barriers to enter the brain under healthy and diseased conditions: The immune surveillance of the CNS relies on activated T cells selectively crossing the blood-brain barrier (BBB) to enter the cerebrospinal fluid (CSF) compartments under steady-state conditions. In contrast, in neuroinflammatory conditions (e.g., multiple sclerosis), more types of immune cells can breach the BBB and further cross the glia limitans to enter the CNS parenchyma, thereby driving inflammatory responses. https://doi.org/10.3389/fimmu.2022.805657"",   ""concept_3"": ""Molecular Mimicry. Molecular mimicry is one of the main mechanisms by which infectious agents or chemical factors induce autoimmunity. This process occurs when there is similarity between exogenous and self-peptides, and this similarity can lead to the activation of autoreactive T or B cells by exogenous antigens in susceptible individuals. However, molecular mimicry is unlikely to be the only potential mechanism for autoimmune responses; other factors, such as disruption of central tolerance, non-specific bystander activation, or sustained antigen stimulation, may also contribute to the development of autoimmune diseases. Host genetics, microbiota exposure, and environmental chemical exposure are important aspects for further understanding molecular mimicry.""  }"	Microbe-specific CD4 T cells and other immune cells, such as neutrophils and macrophages, are activated by microbes in the intestine and then enter the blood circulatory system. Only activated T cells, as immune surveillance components, cross the blood-brain barrier. After crossing the blood-brain barrier, activated CD4 T cells recognize microbe-like peptides in the CNS through antigen presentation, thereby undergoing in situ proliferation in the CNS and releasing cytokines such as IL-17A, IL-17F, and IL-22. These cytokines inhibit the blood-brain barrier from selectively recruiting more immune cells, ultimately leading to neuroinflammation.	理论基础学科-生物学-免疫学	207145	1	4
recuTRbG79flPN	When \( p \equiv 1 \pmod{4} \), what is the minimal degree of any polynomial for computing square roots over \(\mathbb{F}_p\)?\nHint: 1. It follows from the definitions of the vanishing polynomial for computing square roots and perfect squares that: $f(X)^2 = A(X)X^{(p-1)/2} + B(X)$, where $\deg A, \deg B = 2d - \frac{p-1}{2}$ and $A\neq 0,\,B\neq 0$.\n2. If we assume that $d < \frac{p-1}{3}$, combining with the factorization, simplifying the equation modulo $X^{(p-3)/2}$ allows us to control the degrees of each term and obtain $\frac{2f'}{f} = \frac{B'}{B}$.	"{   ""step_1"": ""According to concept_1, a polynomial f for 'computing square roots' must satisfy: for all non-zero squares a ∈ ℤ_p, f(a)^2 = a, that is, f(X)^2 - X is zero at all non-zero squares. Combined with Hint 1, the factorization f(X)^2 = A(X)X^{(p-1)/2} + B(X) can be obtained, where deg A = deg B = 2d (d = deg f) minus (p-1)/2, and A ≠ 0, B ≠ 0."",   ""step_2"": ""Hint 2 indicates that if we assume d < (p-1)/3, simplifying the factorization modulo X^{(p-3)/2} can derive 2f'/f = B'/B, and further obtain f^2 = λB (λ is a constant). Substituting into the factorization leads to A = 0, which contradicts the premise A ≠ 0. Thus, d ≮ (p-1)/3, i.e., d ≥ (p-1)/3."",   ""step_3"": ""Combined with theorem_1, for f^2 = A X^{(p-1)/2} + B, where deg f = d and deg B = 2d - (p-1)/2, the condition of 'degree below a specific threshold' is satisfied. Therefore, d + deg B ≥ (p-1)/2. Substituting deg B and simplifying gives 3d ≥ p - 1, i.e., d ≥ (p-1)/3."",   ""step_4"": ""In summary, the minimum degree of any polynomial for 'computing square roots' over ℤ_p (when p ≡ 1 mod 4) is (p-1)/3.""  }"	"{   ""concept_1"": ""Definition of 'computing square roots': For all non-zero squares a ∈ ℤ_p, f(a)^2 = a holds, which is equivalent to f(X)^2 - X vanishing on all non-zero squares."",   ""theorem_1"": ""Lemma 2.1: If f^t = A X^ℓ + B, with deg f = d, deg B = b, and the degrees of these polynomials are below a specific threshold, then it must hold that d + b ≥ ℓ.""  }"	\(\frac{p-1}{3}\)	理论基础学科-基础数学-代数	50390	1	2
