{
    "recuTUxvLKuZuC": {
        "reasoning_steps": [
            "Given the lattice matrix $\\hat{L}=\\big[\\mathbf{\\hat{l}}_1,\\mathbf{\\hat{l}}_2,\\mathbf{\\hat{l}}_3\\big]\\in\\mathbb{R}^{3\\times 3}$, and the atomic coordinate matrix to be solved $\\hat{R}=\\big[\\hat{R}_1^\\top,\\dots,\\hat{R}_N^\\top\\big]^\\top\\in\\mathbb{R}^{N\\times 3}$. Under **Periodic Boundary Conditions (PBC)**, equivalent images outside the unit cell are determined by integer vectors $\\mathbf{k}=(k_1,k_2,k_3)\\in\\mathbb{Z}^3$ and lattice vectors. For any atomic pair $(i,j)$, the relative displacement including periodic images is expressed as $\\Delta R_{ij}(\\mathbf{k})=\\hat{R}_i-\\big(\\hat{R}_j+k_1\\mathbf{\\hat{l}}_1+k_2\\mathbf{\\hat{l}}_2+k_3\\mathbf{\\hat{l}}_3\\big).$ (Corresponding concepts: PBC; lattice-coordinate consistency)",
            "Given target equilibrium distances $\\hat{d}_{ij}$ (defined for atomic pairs in the edge set $E$), **Euclidean Distance Geometry (EDG)** requires that actual distances closely approximate the target distances. First, the basic geometric loss is formulated as: $L_{\\text{EDG}}(\\hat{R})=\\sum_{(i,j,\\mathbf{k})\\in E}\\left|\\;\\big\\|\\Delta R_{ij}(\\mathbf{k})\\big\\|-\\hat{d}_{ij}\\right|.$ (Corresponding concept: EDG)",
            "If distances have uncertainty $\\hat{b}_{ij}$, introduce upper and lower bounds: $\\hat{d}^l_{ij}=\\hat{d}_{ij}-e^{\\hat{b}_{ij}},\\qquad \\hat{d}^u_{ij}=\\hat{d}_{ij}+e^{\\hat{b}_{ij}}.$ Accordingly, replace absolute deviations with **out-of-interval penalties** (hinge loss): $\\ell_{ij\\mathbf{k}}(\\hat{R})= \\max\\!\\left(0,\\;\\big\\|\\Delta R_{ij}(\\mathbf{k})\\big\\|-\\hat{d}^u_{ij}\\right) +\\max\\!\\left(0,\\;\\hat{d}^l_{ij}-\\big\\|\\Delta R_{ij}(\\mathbf{k})\\big\\|\\right).$ (Corresponding concept: BED)",
            "Synthesizing Steps 1–3, sum over all atomic pairs in $E$ and their considered periodic images to obtain the global optimization objective: $L_g(\\hat{R})=\\sum_{(i,j,\\mathbf{k})\\in E} \\max\\!\\left(0,\\;\\big\\|\\hat{R}_i-(\\hat{R}_j+k_1\\mathbf{\\hat{l}}_1+k_2\\mathbf{\\hat{l}}_2+k_3\\mathbf{\\hat{l}}_3)\\big\\|-\\hat{d}^u_{ij}\\right) +\\max\\!\\left(0,\\;\\hat{d}^l_{ij}-\\big\\|\\hat{R}_i-(\\hat{R}_j+k_1\\mathbf{\\hat{l}}_1+k_2\\mathbf{\\hat{l}}_2+k_3\\mathbf{\\hat{l}}_3)\\big\\|\\right).$ The minimizer of this objective, $\\hat{R}^\\star=\\arg\\min_{\\hat{R}}L_g(\\hat{R})$, yields the equilibrium coordinates satisfying periodicity and distance interval constraints (ignoring non-uniqueness from rigid translations/rotations). (Corresponding concepts: PBC; EDG; lattice-coordinate consistency; BED)"
        ],
        "step_count": 4
    },
    "recuTOH1R0sx7z": {
        "reasoning_steps": [
            "Connect Fuk(F) to IndCoh(∂T) via HMS (Theorem 1) The homological mirror symmetry equivalence in Theorem 1 gives IndCoh(∂T)≅Fuk(F). Under this equivalence: Pseudo-perfect objects in Fuk(F) correspond to pseudo-perfect objects in IndCoh(∂T).",
            "For a proper algebraic stack Y, pseudo-perfect objects in IndCoh(Y) coincide with Perf(Y) (perfect complexes). Assuming ∂T is proper (natural for boundaries of toric DM stacks), we have: Pseudo-perfect objects in IndCoh(∂T)≅Perf(∂T).",
            "Generators of Perf(∂T) (Theorems 2 and 3) By theorem 2 and 3, Perf(∂T) is generated by restrictions of these line bundles to ∂T.",
            "Lagrangian spheres as mirrors of restricted line bundles (Theorem 1) Theorem 1 states that the mirror of line bundles on T restricted to ∂T are Lagrangian spheres on F. Under the equivalence IndCoh(∂T)≅Fuk(F), these restricted line bundles correspond to Lagrangian spheres in F.",
            "The Lagrangian spheres on F correspond to the generators of Perf(∂T), which (by Step 2) are exactly the pseudo-perfect objects in Fuk(F). Thus, the subcategory split-generated by Lagrangian spheres contains all pseudo-perfect objects."
        ],
        "step_count": 5
    },
    "recuSTVHHAlGEz": {
        "reasoning_steps": [
            "Transform the zero point problem into an exponential sum problem, that is, the number of elements in the set $\\{(x_1,...,x_m)\\in \\BF_{p^r}}^m|f(x_1,...,x_m)=0\\}$ is equivalent to calculating $\\frac{1}{p^r}\\sum_{y\\in \\mathbb{F}_{p^r}}\\sum_{(x_1,...,x_m)\\in \\mathbb{F}_{p^r}^m}\\chi(y(\\sum_{i=1}^mx_i^{p^{k_i}+1}))$.",
            "Decompose the above exponential sum into the product of the exponential sums in the Theorem, that is, $\\frac{1}{p^r}\\sum_{y\\in \\mathbb{F}_{p^r}}\\sum_{(x_1,...,x_m)\\in \\mathbb{F}_{p^r}^m}\\chi(y(\\sum_{i=1}^mx_i^{p^{k_i}+1}))=p^{r(m-1)}+\\sum_{y\\in \\mathbb{F}_{p^r}}\\prod_{i=1}^mS_{k_i}(y).$",
            "Use Theorem 2 and Theorem 3 to discuss the problem by cases, that is, it is necessary to discuss $p=2$ and $p\\ne 2$, and also discuss whether $m$ is an odd number or an even number.",
            "Integrate the results to draw the conclusion."
        ],
        "step_count": 4
    },
    "recuT5sTo3djUD": {
        "reasoning_steps": [
            "Theorem 5 shows that for an odd prime power $q$, there are no exceptional polynomials of degree 4 over $\\mathbb{F}_q$;",
            "Combining the above conclusion with Theorem 4, we obtain that for an odd prime power $q$, a necessary condition for the existence of permutation polynomials of degree 4 over $\\mathbb{F}_q$ is $q\\le 11$, thus we only need to consider $q=3,5,7,9,11$;",
            "Use Theorem 2 to rule out the cases of $q=5$ and $q=9$;",
            "Use Theorem 1 to rule out the case of $q=11$ (note that in this step, $a_i$ usually adopts the exhaustive search method, but only a few examples are tested and then such cases are excluded by guesswork);",
            "Then use Theorem 3 to give two permutation polynomials over $\\mathbb{F}_7$, and use Theorem 1 to rule out other possibilities;",
            "Finally, give the construction for $q=3$ by exhaustive search or Theorem 1."
        ],
        "step_count": 6
    },
    "recuTJaLaBXcTj": {
        "reasoning_steps": [
            "The momentum of a particle is $\\mathbf{p}=\\left(p_x, p_y, p_z\\right)$. Outside the bubble, the particle is massless, and its energy is $E=|\\mathbf{p}|=\\sqrt{p_x^2+p_y^2+p_z^2}$. After passing through the bubble wall, the particle gains mass $m_d$, and its energy is $E=\\sqrt{p_x^2+p_y^2+p_z^2+m_d^2}$. Energy conservation requires that the total energy of the particle remains unchanged when passing through the bubble wall: $\\sqrt{p_x^2+p_y^2+p_z^2}=\\sqrt{p_x^{' 2}+p_y^{' 2}+p_z^{' 2}+m_d^2}$. The bubble wall does not change $p_x$ and $p_y$ (transverse momentum conservation), so: $\\sqrt{p_x^2+p_y^2+p_z^2}=\\sqrt{p_x^2+p_y^2+p_z^{' 2}+m_d^2}$, from which we get $p_z^2=p_z^{' 2}+m_d^2$. $p_z^{' 2}=\\sqrt{p_z^2-m_d^2}$ must be a real number, that is: $p_z^2 \\geq m_d^2 \\Longrightarrow\\left|p_z\\right| \\geq m_d$. Thus, the condition for particle penetration is obtained: $|p_z| \\geq m_d$",
            "Write the number of particles $\\Delta N_{\\rm in}$ passing through area $\\Delta A$ in time $\\Delta t$ in the bubble wall frame: $\\frac{\\Delta N_{\\rm in}}{\\Delta A} &=& \\frac{g_d}{(2 \\pi)^3} \\, \\int d^3\\vec{p} \\, \\int^{r_0-\\frac{p_z \\, \\Delta t}{|\\vec{p}|}}_{r_o} dr \\, \\mathcal{T}(\\vec{p}) \\, \\Theta(-p_z) \\, f(\\vec{p};\\vec{x}) \\\\ &=& \\frac{g_d}{(2\\pi)^3}\\int d^3\\vec{p} \\, \\Theta(-p_z-m_d) \\, \\Theta(-p_z) \\, (-\\frac{p_z \\, \\Delta t}{|\\vec{p}|}) \\, f(\\vec{p};\\vec{x}).",
            "The incident particle flux can be expressed as the number of particles passing through per unit time and per unit area, $J_w\\equiv\\frac{dN}{dAdt}\\simeq\\frac{\\Delta N_{\\rm in}}{\\Delta A \\, \\Delta t}$: $J_w &=& \\frac{g_d}{(2\\pi)^3} \\int d^3\\vec{p} \\, \\Theta(-p_z-m_\\chi) \\, \\Theta(-p_z) \\, f(\\vec{p};\\vec{x}) \\, (-\\frac{p_r}{|\\vec{p}|}) \\\\ &=& \\frac{g_d}{(2\\pi)^3} \\int^{\\pi}_{0} d\\theta \\, \\sin\\theta \\, \\int^{2\\pi}_0 d\\phi \\, \\int^{\\infty}_{0} dp \\, p^2 (-\\cos\\theta) \\, \\Theta(-p\\cos\\theta-m_d) \\, \\Theta(-\\cos\\theta) \\, f(\\vec{p})",
            "Adopt the Boltzmann approximate distribution for the incident particle distribution function to facilitate subsequent analytical integration: $f(\\vec{p}) = \\frac{1}{e^{\\gamma_w \\, (E \\, - \\, \\vec{v_w} \\cdot \\vec{p})/T} \\pm 1 } \\simeq\\frac{1}{e^{\\gamma_w\\, (p \\, + \\, v_w \\, p \\, \\cos\\theta )/T}}.$ where the Lorentz factor $\\gamma_w=1/\\sqrt{1-v_w^2}$ and $T$ is the temperature of the particle.",
            "Perform analytical integration and transform back to the plasma frame through Lorentz transformation: $n^{\\rm in}&=& \\frac{g_d}{(2\\pi)^2 \\gamma_w v_w} \\int^{-1}_0 d\\cos\\theta \\, \\cos\\theta \\int^{\\infty}_{-\\frac{m_d}{\\cos\\theta}} dp \\, \\frac{p^2}{e^{\\gamma_w \\, (1 \\, + \\, v_w \\, \\cos\\theta ) \\, p / T}} \\\\ &=& \\frac{g_d T^3}{\\gamma_w v_w}\\frac{1 \\, + \\, \\gamma_w \\, m_d \\, (1-v_w)/T }{4 \\, \\pi^2 \\, \\gamma_w^3 \\, (1-v_w)^2} \\, e^{-\\gamma_w\\, m_d \\, (1-v_w)/T}'"
        ],
        "step_count": 5
    },
    "recuTKmYlCFlBN": {
        "reasoning_steps": [
            "First, starting from the Gauss–Bonnet action in theorem_1, by varying the metric $g_{\\mu u}$, the gravitational field equation $G_{\\mu u}+\\alpha H_{\\mu u}=0\\,.$ in concept_1 can be obtained.",
            "Next, specifically consider the 5-dimensional static spherically symmetric metric and gauge potential in theorem_2, and focus on the (t,t) component. $G_{tt}=-\\frac{3 f(r)\\bigl(r f'(r)+2f(r)-2\\bigr)}{2r^2},\\qquad G^{t}{}_{t}=g^{tt}G_{tt}=\\frac{3\\bigl(r f'(r)+2f(r)-2\\bigr)}{2r^2},$ while concept_2 gives $H^{t}{}_{t}=-\\frac{6\\,(f(r)-1)\\,f'(r)}{r^{3}}.$",
            "Substitute both into the (t,t) component $G^{t}{}_{t}+\\alpha H^{t}{}_{t}=0$ of the field equation and simplify to get the answer: $\\frac{3\\,(r f'+2f-2)}{2r^2}-\\frac{6\\alpha\\,(f-1)f'}{r^3}=0$."
        ],
        "step_count": 3
    },
    "recuTPmUpdGG24": {
        "reasoning_steps": [
            "Firstly, the subducting Indian continental lithosphere is too buoyant to exert a driving force sufficient to explain the ongoing India–Asia convergence (①). So other external forces have had to be invoked (②).",
            "However, both ocean ridge and pull of slabs are proved to be insufficient to explain the present-day India–Asia convergence(③④).",
            "Based on precious seismic evidence, the deep-rooted mantle currents may influence continental lithospheric deformation stretching from the Tibetan to the Baikal-Mongolia Plateau by the asthenosphere flowing underneath(⑤).",
            "Consistent with this, geodynamc model has simulated a plume developed in the mantle beneath the southern Neo-Tethys Ocean which produces the northward-directed mantle flow(⑥). Ultimately, the mantle flow is verified to be sourced from Réunion plume (⑦) to provide a main driving force of Indian plate."
        ],
        "step_count": 4
    },
    "recuTUUPJRgInk": {
        "reasoning_steps": [
            "Rh and Pt doping can significantly enhance the methane activation activity on Ag(111) surfaces, but pre-adsorbed oxygen species inhibit C-H bond cleavage on metals with high activity, thus pre-adsorbed oxygen species will inhibit methane activation on Rh and Pt doped Ag(111) surfaces.",
            "Considering the d-band center theory, the d-band centers of Au and Cu are very low, far lower than the 2p band center of pre-adsorbed oxygen species, thus showing a promoting effect on methane activation. Rh and Pt with higher d-band centers show an inhibitory effect.",
            "For Mn and Cr doping, although their d-band centers are relatively high, making them easy to combine with O and lead to deactivation, doping significantly improves the O2 activation ability, optimizes the 2p band center of O* and stabilizes the dual-site five-center transition state, thereby promoting methane activation and thus also showing a promoting effect."
        ],
        "step_count": 3
    },
    "recuTVLTMDCBFT": {
        "reasoning_steps": [
            "Analyze the relationship between $l_r(x,y)$ and $\\varphi_r(x,u)$. By definition, $l_r(x,y) = \\inf_u \\{\\varphi_r(x,u) - y \\cdot u\\}$, where $\\varphi_r(x,u) = \\varphi(x,u) + (r/2)|u|^2$. Therefore, $l_r(x,y)$ is the infimum function of $\\varphi_r(x,u)$ with respect to the variable $u$ (taking inf over $u$).",
            "Analyze the properties of $\\varphi_r$ using the definition of variational strong convexity (concept_1). The strong variational sufficient condition requires $\\varphi_r$ to be variationally strongly convex, i.e., there exists an open convex neighborhood containing $((x^*, 0), (0, y^*))$, and there exists a strongly convex lsc function $\\hat{\\varphi}_r \\leq \\varphi_r$ that satisfies the graph condition and the value equality condition. This means $\\varphi_r$ has local strong convexity characteristics.",
            "Derive the conditions for $l_r(x,y)$ with respect to $x$. For a fixed $y$, the strong convexity of $\\varphi_r(x,u) - y \\cdot u$ with respect to $x$ can be transferred to the infimum function $l_r(x,y)$. Since $\\varphi_r$ is variationally strongly convex, its strong convexity with respect to $x$ is preserved locally (for $x \\in X$, where $X$ is a closed convex neighborhood of $x^*$), so $l_r(x,y)$ is strongly convex with respect to $x$ in $X$ (when $y \\in Y$, where $Y$ is a closed convex neighborhood of $y^*$).",
            "Derive the conditions for $l_r(x,y)$ with respect to $y$. For a fixed $x$, $\\varphi_r(x,u) - y \\cdot u$ is a linear function with respect to $y$ (since $-y \\cdot u$ is linear in $y$), and linear functions are concave. According to concept_2 (the infimum of concave functions is still concave), after taking inf over $u$, $l_r(x,y)$ remains concave with respect to $y$. Combined with the locality of variational strong convexity, this concavity holds in $Y$ (when $x \\in X$).",
            "Determine the range of $r$. The strong variational sufficient condition requires the existence of $r > 0$ such that $\\varphi_r$ is variationally strongly convex, and the \"if and only if\" in the problem requires this condition to hold for sufficiently large $r$ (since the strong convexity of $(r/2)|u|^2$ increases with $r$, ensuring overall strong convexity).",
            "Summarize the conditions. Based on the above analysis, the strong variational sufficient condition holds if and only if: for sufficiently large $r > 0$, there exists a closed convex neighborhood $X \\times Y$ of $(x^*, y^*)$ such that $l_r(x,y)$ is strongly convex with respect to $x$ when $x \\in X$ (for $y \\in Y$), and concave with respect to $y$ when $y \\in Y$ (for $x \\in X$)."
        ],
        "step_count": 6
    },
    "recuUd1kClF8CB": {
        "reasoning_steps": [
            "According to concept_1, the surface atomic arrangement in the NiCo₂O₄ (112) crystal plane enables it to maintain structural stability even in high-temperature environments, so the (112) crystal plane has better thermal stability.",
            "Compared with the Ni²⁺ sites mainly exposed on the (111) crystal plane, the (112) crystal plane mainly exposes Ni²⁺ and Co²⁺ sites. The synergistic effect between Ni-Co bimetals and the abundant surface defect sites are conducive to achieving better catalytic activity.",
            "The Co sites exposed on the (112) crystal plane have stronger adsorption capacity for H₂O and are more likely to be poisoned; while the (111) crystal plane only has Ni sites, which are less likely to be poisoned by H₂O compared with Co sites, so the (111) crystal plane has better water resistance."
        ],
        "step_count": 3
    },
    "recuTXI7RKHToP": {
        "reasoning_steps": [
            "Understand the fundamental properties of HPW and its interaction with MnCeOₓ. HPW (phosphotungstic acid, H₃PW₁₂O₄₀) is a heteropolyacid with a Keggin structure with strong Brønsted acidity, high redox and good thermal stability. When HPW is modified onto MnCeOₓ nanowire aerogels, it interacts strongly with metal oxide (Mn and Ce) surfaces, mainly through hydrogen bonding, electrostatic action, or chemical bonding. MnCeOₓ nanowire aerogels themselves have a high specific surface area, porous structure, and abundant oxygen vacancy, but the introduction of HPW can further modulate these properties.",
            "Apply Concept_1 analysis of valence regulation and structural optimization. Rare earth metal doping (e.g., Nd) can reduce the average valence state of Mn, increase the proportion of low-valence Mn oxides (e.g., Mn₂O₃, Mn₃O₄, MnO), reduce MnO₂, and optimize low-temperature reducibility through high dispersion. Similarly, HPW, as a strong electron acceptor, can extract electrons from MnCeOₓ, resulting in a decrease in the Mn valence state. This is in line with mathematical logic: according to electron transfer theory, the electron-deficiency of HPW (due to the high oxidation state of W⁶) attracts electrons from Mn or Ce species, reducing Mn⁴ to Mn³ or Mn². XPS or XANES analysis confirms this valence change. At the same time, the introduction of HPW drives lattice rearrangement, increasing oxygen vacancies (Vo) because charge compensation is required (oxygen vacancies are reduced). This enhances the redox ability of the catalyst and the low-temperature NH₃-SCR activity (e.g., H₂-TPR shows improved reducibility). In addition, the high dispersion of HPW prevents Mn species agglomeration and increases the number of active sites. Therefore, HPW modification achieved an Nd-doped effect through electronic structure regulation, but may be more significant due to the strong acidity of HPW.",
            "Apply Concept_2 to analyze surface reaction pathways and anti-SO₂ mechanisms. Single tooth and bridge nitrates are key intermediates in the SCR reaction, but are susceptible to SO₂ toxication (sulfate formation). HPW modifications enhance the production of another nitrate, Bidentate Nitrate. Bidentate forms a stable chelating structure with two oxygen atoms and metal sites (such as Ce⁴), which has high bonding energy, strong chemical inertness, and is not easily attacked by SO₂. This is in line with surface chemistry: the highly acidic site of HPW promotes NO₂ adsorption and oxidation, preferentially forming bidentate over single-toothed/bridge species. The in situ DRIFTS experiment verifies this transition. Didentate nitrate reacts with NH₃ to form NH₄⁺-NO₃⁻ intermediates, which are then decomposed into N₂ and H₂O, and this pathway is not affected by SO₂, because SO₂ is difficult to replace stable bidentate coordination. Therefore, HPW modification avoids the deposition of ammonium sulfate or metal sulfate by reconstructing the surface reaction path, thereby imparting anti-SO₂ properties."
        ],
        "step_count": 3
    },
    "recuUkfhG1fX9f": {
        "reasoning_steps": [
            "Analyzing the Core Problem | The central question in the Instruction is to determine whether the 2,2′-bipyridine overlayer enhances or suppresses the hydrogen evolution reaction (HER) activity of Pt/C and Ir/C catalysts, and to elucidate the underlying mechanism.",
            "Applying Core Strategy | Introducing 2,2′-bipyridine molecules onto catalyst surfaces constitutes a classic 'Interfacial Overlayer Modulation Strategy' (Concept_1). This strategy aims not at physical blocking but directly modulates intrinsic catalytic activity by altering the electronic structure of catalyst surfaces.",
            "Linking Reaction Principles | HER activity is governed by 'Hydrogen Binding' (Concept_2) strength. According to the Sabatier Principle, an ideal catalyst should exhibit moderate adsorption toward hydrogen intermediates (*H). Pt and Ir, as top-tier catalysts, exhibit slightly strong hydrogen adsorption. Thus, further enhancing their activity requires appropriately weakening hydrogen adsorption energy.",
            "Introducing Electronic Structure Theory | Per 'd-band Center Theory' (Concept_3), the energy position of a transition metal's d-band center directly determines its bonding strength with adsorbates (e.g., *H). A lower d-band center energy corresponds to weaker adsorbate bonding.",
            "Analyzing Overlayer's Electronic Effects | When 2,2′-bipyridine adsorbs as an overlayer on Pt or Ir surfaces, its π-backbonding effect induces electron donation from metal d-orbitals to the ligand. This electron depletion reduces the filling of metal d-orbitals, consequently lowering the d-band center energy.",
            "Synthesis and Conclusion | The 2,2′-bipyridine overlayer causes downward shifting of Pt/Ir d-band centers (Step_5). According to d-band theory, this weakens metal-*H adsorption strength (Step_4). Given Pt/Ir's originally strong hydrogen adsorption, this weakening brings adsorption strength closer to the optimal point described by the Sabatier Principle (Step_3). Therefore, the 2,2′-bipyridine overlayer ultimately 'enhances' HER activity."
        ],
        "step_count": 6
    },
    "recuUhtDlBvR0z": {
        "reasoning_steps": [
            "The objective is to calculate the lower bound for the relative distance δ of a specific GAP code where the number of hyperplanes is t = m+d+εd.",
            "Based on the definition (concept_2), the relative distance δ is the minimum distance divided by the block length. We can express its lower bound as δ ≥ (Minimum Distance Bound / Block Length).",
            "Using theorem_1, we establish the lower bound for the minimum distance as (t-d choose m).",
            "Using theorem_2, we identify the block length as (t choose m).",
            "We substitute these into the inequality from Step_2, yielding δ ≥ ((t-d choose m) / (t choose m)).",
            "Now, we substitute the specific value t = m+d+εd into this expression. The numerator becomes ((m+d+εd)-d choose m) = (m+εd choose m), and the denominator becomes (m+d+εd choose m).",
            "The inequality for the relative distance is now δ ≥ ((m+εd choose m) / (m+d+εd choose m)).",
            "To simplify, we expand the binomial coefficients, which results in the product form: δ ≥ ∏_{i=0}^{m-1}((m+εd-i) / (m+d+εd-i)).",
            "By analyzing each term in the product, we observe that for 0 ≤ i < m, the term ((m+εd-i) / (m+d+εd-i)) is minimized as i increases but is always greater than the limiting behavior, which approaches (εd / (d+εd)) = (ε / (1+ε)). Therefore, we can bound each term below by (ε / (1+ε)).",
            "Since the expression is a product of m such terms, the final lower bound is the product of their individual bounds, resulting in δ ≥ (ε / (1+ε))^m."
        ],
        "step_count": 10
    },
    "recuUcQt0bLL6m": {
        "reasoning_steps": [
            "The objective is to find a faster algorithm for TENSOR ISOMORPHISM-complete problems (Concept_2) over a finite field \\(\\mathbb{F}_q\\), improving upon the existing \\(q^{O(n^2)}\\) brute-force method.",
            "The thesis leverages recent progress in a related, but more structured, problem called ALTERNATING MATRIX SPACE ISOMETRY (AMSI) (Concept_3).",
            "Based on a breakthrough for p-group isomorphism, an improved algorithm for AMSI exists. For an AMSI instance defined by matrices of size roughly L \\times L, this algorithm runs in time \\(q^{\\tilde{O}(L^{1.5})}\\) (Theorem_1).",
            "The critical step is to connect the general TI problem to the specific AMSI problem efficiently. If a reduction from TI to AMSI significantly increases the problem size, the algorithmic gains for AMSI would be lost. For instance, a quadratic blow-up from size n to n^2 would result in a runtime of \\(q^{\\tilde{O}((n^2)^{1.5})} = q^{\\tilde{O}(n^3)}\\), which is worse than brute force.",
            "The main technical novelty of the thesis is the creation of a new reduction from TENSOR ISOMORPHISM to AMSI that causes only a linear increase in the size parameters (Theorem_2).",
            "Using this new reduction, an instance of TENSOR ISOMORPHISM on tensors with side lengths of order n (total length L=O(n)) can be transformed into an equivalent AMSI instance where the new matrix dimension is also of order O(n).",
            "By applying the faster AMSI algorithm (from Theorem_1) to this new, linearly-sized instance, the overall time complexity becomes \\(q^{\\tilde{O}((O(n))^{1.5})}\\), which simplifies to \\(q^{\\tilde{O}(n^{3/2})}\\). This provides the improved runtime for TI and its complete problems."
        ],
        "step_count": 7
    },
    "recuUbuDFVXupE": {
        "reasoning_steps": [
            "The initial problem is to find the minimum space required to simulate a multitape Turing machine running in $t(n)$ time. The chosen strategy is to reduce this simulation to the Tree Evaluation problem.",
            "First, using Concept_1, the arbitrary $t(n)$-time multitape Turing machine is converted into an equivalent $O(t(n))$-time block-respecting machine. The computation is divided into $B = O(t(n)/b(n))$ time blocks, each of length $b(n)$, where $b(n)$ is a parameter to be optimized later.",
            "Next, the flow of information between these blocks is modeled using a Computation Graph (Concept_2). The state of the computation at the end of the final time block, $B$, depends on the states of previous blocks. This dependency structure forms a directed acyclic graph.",
            "This computation graph is then transformed into an instance of the Tree Evaluation problem (Concept_3). The root of the tree is tasked with computing the final state of the machine (the content of the relevant tape block in time block $B$). The children of a node in the tree correspond to the nodes in the computation graph that are required to compute the value of the parent node. The height of this tree, $h$, is at most the number of time blocks, so $h = O(t(n)/b(n))$. The bit-length of the values at each node, $b$, is the information needed to represent a tape block's content, so $b = O(b(n))$. The fan-in, $d$, is a constant determined by the number of tapes.",
            "To solve this Tree Evaluation instance, we apply the space-efficient Cook-Mertz algorithm from Theorem_1. Substituting the parameters from our construction into the theorem's space complexity formula, we get a total space bound of $S(n) = O(d \\cdot b + h \\log(d \\cdot b)) = O(b(n) + \\frac{t(n)}{b(n)}\\log(b(n)))$. This expression also accounts for the space needed to store and enumerate the computation graphs.",
            "The final step is to minimize this space complexity by choosing an optimal value for the block size parameter $b(n)$. The two terms in the space complexity are balanced when $b(n) \\approx \\frac{t(n)}{b(n)}\\log(t(n))$. This gives $b(n)^2 \\approx t(n)\\log t(n)$, so the optimal choice is $b(n) = \\sqrt{t(n)\\log t(n)}$.",
            "Substituting this optimal $b(n)$ back into the space complexity formula yields $S(n) = O(\\sqrt{t(n)\\log t(n)} + \\frac{t(n)}{\\sqrt{t(n)\\log t(n)}}\\log(\\dots)) = O(\\sqrt{t(n)\\log t(n)})$. This is the final answer for the space complexity of the simulation."
        ],
        "step_count": 7
    },
    "recuU5qv72o22y": {
        "reasoning_steps": [
            "The primary goal is to determine the communication cost for the $l_p$ heavy hitter algorithm. The overall strategy is to leverage the provided $l_2$ heavy hitter algorithm as a core subroutine.",
            "A naive reduction from $l_p$ to $l_2$ is inefficient due to the potentially large factor relating the $l_p$ and $l_2$ norms, which depends on the universe size $n$. To address this, the first step is to apply the thresholding technique (concept_3). Each site creates a sparsified vector, $\\tilde{v}$, by zeroing out all local frequencies $v_{ij}$ below the threshold $\\frac{\\epsilon l_{p}^{\\prime}(v)}{k}$. The $l_2$ algorithm will be run on this sparsified vector.",
            "The next step is to determine the correct accuracy parameter to use for the $l_2$ subroutine. Let's call this parameter $\\epsilon'$. According to the $l_2$ algorithm's guarantee (concept_2), running it on the sparsified vector $\\tilde{v}$ will yield an estimation error of $|\\hat{v}_j - \\tilde{v}_j| \\le \\epsilon' l_2'(\\tilde{v})$.",
            "However, the final error must be expressed in terms of the original accuracy parameter $\\epsilon$ and the $l_p'$ norm. We need this error to be bounded by $\\epsilon l_p'(\\tilde{v})$. Therefore, we must choose $\\epsilon'$ such that the two error bounds are equivalent: $\\epsilon' l_2'(\\tilde{v}) \\approx \\epsilon l_p'(\\tilde{v})$.",
            "From this relationship, we can express the required parameter $\\epsilon'$ as: $\\epsilon' \\approx \\epsilon \\cdot \\frac{l_p'(\\tilde{v})}{l_2'(\\tilde{v})}$. To find a value for $\\epsilon'$, we need to bound the ratio of the norms.",
            "This is where the norm inequality for sparsified vectors (theorem_2) becomes crucial. The thresholding step (concept_3) ensures that every non-zero element in the sparsified vector $\\tilde{v}$ satisfies the condition of the theorem with $\\beta \\approx \\frac{\\epsilon}{k}$. Plugging this into the inequality gives: $l_2'(\\tilde{v})^2 \\le \\frac{1}{(\\epsilon/k)^{p-2}} l_p'(\\tilde{v})^2$.",
            "By rearranging this inequality, we find a bound for the norm ratio: $\\frac{l_p'(\\tilde{v})}{l_2'(\\tilde{v})} \\ge \\frac{\\epsilon^{(p-2)/2}}{k^{(p-2)/2}}$. We can now substitute this back into our expression for $\\epsilon'$ from step 5, which gives $\\epsilon' \\approx \\epsilon \\cdot \\frac{\\epsilon^{(p-2)/2}}{k^{(p-2)/2}} = \\frac{\\epsilon^{p/2}}{k^{p/2 - 1}}$. This is the derived accuracy parameter needed for the subroutine.",
            "Finally, we calculate the total communication cost by using the cost formula for the base $l_2$ algorithm (theorem_1), which is $O(\\frac{k \\log n}{(\\epsilon')^2})$, and substituting our derived value for $\\epsilon'$.",
            "The final cost is $O\\left(\\frac{k \\log n}{\\left(\\frac{\\epsilon^{p/2}}{k^{p/2 - 1}}\\right)^2}\\right) = O\\left(\\frac{k \\log n}{\\frac{\\epsilon^p}{k^{p-2}}}\\right) = O\\left(\\frac{k \\cdot k^{p-2} \\log n}{\\epsilon^p}\\right) = O\\left(\\frac{k^{p-1} \\log n}{\\epsilon^p}\\right)$."
        ],
        "step_count": 9
    },
    "recuU3kgNP3HA9": {
        "reasoning_steps": [
            "The core challenge is the non-uniform nature of the spills. They are \"heterogeneous\" (`Concept_2`), meaning they are drawn from `S` different integer universes `[K^{(s)}]`. This is more formally described as a \"Representational Incompatibility\" (`Concept_5`) between the integer spills and the finite field elements `GF(2^w)` used for other data.",
            "Because of this incompatibility, a single, monolithic storage structure cannot simultaneously handle spills from `S` different types of universes. The logical inference is that the problem must be broken down: a separate, dedicated storage structure must be created for each of the `S` spill types.",
            "The chosen tool for storing each of these `S` spill groups is the Auxiliary-Data Retrieval structure (`Concept_3`). This means the overall solution involves instantiating this structure `S` times.",
            "Before storage, the incompatibility for each type `s` must be resolved. Theorem_2 (Prime Number Gaps) is applied to convert the integer universe `[K^{(s)}]` into a finite field based on a nearby prime `P^{(s)}`. This step has a negligible space cost.",
            "A second incompatibility then arises. The auxiliary data required by the retrieval structure is in base `GF(2^w)`, but the retrieval structure for spills of type `s` must now operate in the prime field `\\mathbb{F}_{P^{(s)}}`.",
            "Theorem_1 (Base Conversion) provides the solution to this problem. For each of the `S` spill structures that must be built, a base conversion operation is required. The theorem states that each of these conversion operations incurs a redundancy of `$O(\\log L)$` words.",
            "The dominant redundancy is the total cost of these necessary, high-overhead operations. Since the `$O(\\log L)$-cost base conversion must be performed once for each of the `S` distinct spill types, the total dominant redundancy is the product of the two: `$S \\times O(\\log L) = O(S \\log L)$` words."
        ],
        "step_count": 7
    },
    "recuSZqZ0KN1aV": {
        "reasoning_steps": [
            "To calculate the limit of $P(y, X, c)$, starting from its definition, the expression is $\\frac{\\log X}{X}\\sum_{\\substack{N \\in [X, cX] \\\\ N\\text{ prime}}}\\sum_{\\chi \\in \\mathcal{D}_+(N)}\\frac{\\chi(\\lceil yX \\rceil^{\\mathfrak{p}})}{\\tau(\\chi)}$.",
            "First, use theorem_2 to handle $\\frac{1}{\\tau(\\chi)}$ in the inner sum. For $\\chi \\in \\mathcal{D}_+(N)$ (satisfying $\\chi(-1)=1$), we have $\\frac{1}{\\tau(\\chi)} = \\frac{1}{N} \\tau(\\overline{\\chi})$, so the inner sum can be rewritten as $\\frac{1}{N} \\sum_{\\chi \\in \\mathcal{D}_+(N)} \\tau(\\overline{\\chi}) \\chi(p)$ (where $p = \\lceil yX \\rceil^{\\mathfrak{p}}$).",
            "Combining with theorem_1, for a prime $N$, $\\phi(N) = N - 1$, thus it reduces to $\\cos\\left(\\frac{2\\pi p}{N}\\right) = \\frac{-1}{N - 1} + \\frac{1}{N - 1} \\sum_{\\substack{\\chi \\mod N \\\\ \\chi \\neq \\chi_0, \\chi(-1)=1}} \\tau(\\overline{\\chi}) \\chi(p)$. Since $\\mathcal{D}_+(N)$ are non-principal primitive even characters modulo $N$, we have $\\sum_{\\chi \\in \\mathcal{D}_+(N)} \\tau(\\overline{\\chi}) \\chi(p) = (N - 1)\\cos\\left(\\frac{2\\pi p}{N}\\right) + 1$. Substituting into the inner sum gives $\\sum_{\\chi \\in \\mathcal{D}_+(N)} \\frac{\\chi(p)}{\\tau(\\chi)} = \\left(\\frac{N - 1}{N}\\right)\\cos\\left(\\frac{2\\pi p}{N}\\right) + \\frac{1}{N}$.",
            "By theorem_3 (result in [BHP]), as $X \\to \\infty$, $\\lceil yX \\rceil^{\\mathfrak{p}} - yX < (yX)^{\\theta}$ ($\\theta < 1$), and since $N \\geq X$, we have $\\frac{\\lceil yX \\rceil^{\\mathfrak{p}} - yX}{N} \\to 0$, i.e., $p \\sim yX$, thus $\\cos\\left(\\frac{2\\pi p}{N}\\right) \\sim \\cos\\left(\\frac{2\\pi yX}{N}\\right)$. Meanwhile, the sum term of $\\frac{1}{N}$ tends to 0 after normalization.",
            "Finally, according to theorem_5, Weyl criterion, the sequence $\\frac{N}{X}$ is uniformly distributed on $(1, c)$, and the sum $\\sum \\cos\\left(\\frac{2\\pi yX}{N}\\right)$ converts to the Riemann integral $\\int_{1}^{c}\\cos\\left(\\frac{2\\pi y}{x}\\right)dx$, thus obtaining the limit value."
        ],
        "step_count": 5
    },
    "recuTdJFVyVpDJ": {
        "reasoning_steps": [
            "Describe the $K_4$-free diaries, this will indicate the extension of $K_4$ free graph.",
            "Define the type(extension) of graphs.",
            "Show that the $K_4$-free diaries correspond to a type of some $K_4$ free graph.",
            "The big Ramsey degree is |T(G)|*|Aut(G)| thus finite. |T(G)| is the number of diaries related with $G$."
        ],
        "step_count": 4
    },
    "recuTPQIDJCVhQ": {
        "reasoning_steps": [
            "明确交叉验证尺度参数估计量的期望表达式，即\\(\\mathbb{E}\\hat{\\sigma}_{\\text{CV}}^2 = \\frac{1}{N} \\sum_{n=1}^N \\mathbb{E}\\left[ \\frac{[f(x_n) - m_{\\backslash n}(x_n)]^2}{k_{\\backslash n}(x_n)} \\right]\\)，其中\\(f\\)服从H=0.6的iFBM，\\(m_{\\backslash n}\\)和\\(k_{\\backslash n}\\)基于布朗运动先验。",
            "分析中间项（\\(1 \\leq n < N\\)）：\\(m_{\\backslash n}(x_n)\\)为相邻点的加权平均，\\(k_{\\backslash n}(x_n) = \\Theta(N^{-1})\\)（拟均匀划分，concept 2）。利用iFBM核函数（theorem 1）展开\\(\\mathbb{E}[f(x_n) - m_{\\backslash n}(x_n)]^2\\)，其主导项为\\(\\Theta((N^{-1})^{2H+2})\\)，故中间项每一项期望为\\(\\Theta(N^{-2.2})\\)，总和为\\(\\Theta(N^{-1.2})\\)。",
            "分析边界项（\\(n = N\\)）：根据定义，\\(m_{\\backslash N}(x_N) = f(x_{N-1})\\)，\\(k_{\\backslash N}(x_N) = \\Theta(N^{-1})\\)。利用theorem 3计算得\\(\\mathbb{E}[f(x_N) - f(x_{N-1})]^2 = \\Theta((N^{-1})^2)\\)，故边界项期望为\\(\\Theta(N^{-1})\\)。",
            "比较中间项与边界项总和的阶：由concept 3，\\(\\Theta(N^{-1})\\)主导\\(\\Theta(N^{-1.2})\\)，总和为\\(\\Theta(N^{-1})\\)。",
            "整体期望为总和除以\\(N\\)，得\\(\\mathbb{E}\\hat{\\sigma}_{\\text{CV}}^2 = \\Theta(N^{-2})\\)。"
        ],
        "step_count": 5
    },
    "recuTKIAnvc2Ao": {
        "reasoning_steps": [
            "Apply the 'Law of Total Expectation' to the target expression. This separates the problem into two parts: calculating the probability of the screening event A (where A is the event that all N_init initial responses are incorrect), and calculating the conditional expectation of the gradient estimator given that event A occurred.",
            "Calculate the probability of the screening event, P(A). Using the 'Probability of Independent Events' concept, since each of the N_init responses is an independent Bernoulli trial with a failure probability of (1 - P_x(θ)), the probability of all N_init failing is P(A) = (1 - P_x(θ))^(N_init).",
            "Focus on the conditional expectation term, E[gradient estimator | A]. Use the 'RLOO Advantage Estimator' concept to substitute its definition into the expression. Then, simplify the outer sum by recognizing that under event A, the reward r(y_j) is 0 for all j <= N_init. This means the sum over j only runs from N_init+1 to N.",
            "Apply the 'Linearity of Expectation' concept to the simplified expression from Step 3. This splits the conditional expectation into two separate terms: one involving the term r(y_j) * ∇log π, and another involving the term -r(y_j) * (sum of other ∇log π).",
            "Calculate the first of the two terms from Step 4. For this term, use the 'Statistical Independence' concept to note that a continuation sample (e.g., y_N) is independent of the screening event A. Then, apply the 'Log-Derivative Trick' to the resulting unconditional expectation, which simplifies to (N_cont / N) * ∇_θ P_x(θ).",
            "Calculate the second term from Step 4. Use 'Statistical Independence' again to separate the expectation of the reward from the expectation of the sum of score functions (∇log π). The expectation of the sum of score functions is then broken down using 'Linearity of Expectation'.",
            "Evaluate the sum of conditional expectations of the score functions from Step 6. For terms where the sample is from the continuation phase, 'Statistical Independence' implies the expectation is zero. For the N_init terms from the screening phase, apply the 'Conditional Score Function' concept, which yields -N_init * (∇_θ P_x(θ) / (1 - P_x(θ))).",
            "Combine the results from steps 5, 6, and 7 to get the final expression for the conditional expectation E[gradient estimator | A]. This expression will be the sum of the results from the two terms analyzed.",
            "Assemble the final answer. Multiply the probability P(A) calculated in Step 2 with the final expression for the conditional expectation calculated in Step 8. This yields the target answer."
        ],
        "step_count": 9
    },
    "recuTNd0zBAuOe": {
        "reasoning_steps": [
            "Write the initial integral form of the two-point function of operators far from the defect (based on concept_1)",
            "Decompose the integral variables into parallel and perpendicular components (based on concept_2)",
            "Substitute the Schwinger parametrization of the bulk-to-boundary propagator (based on concept_4)",
            "Apply the large AdS radius limit (flat-space limit, based on concept_3)",
            "Perform Fourier transformation on the flat measure variables (based on concept_5)",
            "Arrange the integral variables and exponential terms to obtain the final expression"
        ],
        "step_count": 6
    },
    "recuTPXViEsUKT": {
        "reasoning_steps": [
            "The objective is to determine the cumulative regret $R_T$ for the PE algorithm (Concept_3) in a noiseless setting (Concept_2) using an SE kernel (Concept_4).",
            "The PE algorithm divides the total time horizon T into a logarithmic number of batches, approximately $\\log _2 T$. The total regret is the sum of regrets incurred in each batch.",
            "We first analyze the regret within a single batch $i$. According to the standard analysis of PE and using the deterministic confidence bounds for the noiseless setting (Theorem_1), the regret for batch $i$ (of size $N_i$ ) can be bounded by the maximum posterior standard deviation from the previous batch $i-1$. This relationship is given by the inequality: $\\sum_{j=1}^{N_i} f\\left(x^*\\right)-f\\left(x_j^{(i)}\\right) \\leq 4 B N_i \\max _{x \\in \\mathcal{X}_{i-1}} \\sigma_{\\lambda^2 I_{N_{i-1}}}\\left(x ; X_{N_{i-1}}^{(i-1)}\\right)$.",
            "The core of the analysis is to apply the novel upper bound on this maximum posterior standard deviation. Since each batch of PE runs an MVR-style selection, we can apply Theorem_2. For the SE kernel, Theorem_2 states that the maximum posterior standard deviation after $N_{i-1}$ steps is bounded by $O\\left(\\sqrt{\\exp \\left(-N_{i-1}^{\\frac{1}{d+1}} \\ln -\\alpha N_{i-1}\\right.}\\right)$.",
            "Substituting this bound into the per-batch regret expression shows that the regret term $N_{i-1} \\times \\sqrt{\\exp \\left(-N_{i-1}^{\\frac{1}{d+1}} \\ln -\\alpha N_{i-1}\\right)}$ converges to 0 as $N_{i-1}$ increases. Therefore, for sufficiently large batches, the regret accumulated within one batch is bounded by a constant, $O(1)$.",
            "Finally, by summing the constant regret $O(1)$ over all batches, the total cumulative regret $R_T$ is the product of the number of batches $(O(\\ln T))$ and the per-batch regret $(O(1))$. This yields a total cumulative regret of $O(\\ln T)$."
        ],
        "step_count": 6
    },
    "recuT9JBySRLud": {
        "reasoning_steps": [
            "Clarify the composition of the mean squared error. According to concept_1, the mean squared error is the sum of the squared bias and the variance, i.e., $\\mathbb{E}\\{(\\hat{\\tau}_{\\ell}-\\tau)^2\\} = [\\text{Bias}(\\hat{\\tau}_{\\ell})]^2 + \\text{Var}(\\hat{\\tau}_{\\ell})$, and the orders of the bias and variance need to be analyzed separately.",
            "Analyze the source of bias. The estimator $\\hat{\\tau}_{\\ell}$ is based on the quadratic variation $V_{d,\\ell}$ (concept_2), and $V_{d,\\ell}$ includes contributions from the mean function $m_X$, the random field $Z$, and the measurement error $\\epsilon$. By theorem_3, the coefficients $c_{i,d,\\ell}^{k_1,\\dots,k_d}$ eliminate the influence of low-order terms of the mean and covariance, and the bias mainly comes from the high-order terms of the covariance of $Z$.",
            "Determine the order of the bias. When $\\nu = \\ell$, according to theorem_1, the Matérn covariance function $K_M$ contains the term $G_{\\ell}(\\|x-y\\|) = \\|x-y\\|^{2\\ell}\\log(\\|x-y\\|)$, and the distance between sample points is $O(\\omega_n/n)$, so the high-order term of the covariance is $O((\\omega_n/n)^{2\\ell}|\\log(\\omega_n/n)|)$. The bias is the result of the combination of this high-order term and the coefficients, with an order of $O(\\omega_n^{2\\ell}|\\log n|/n^{2\\ell})$, and the order of the squared bias is $O((\\omega_n^{2\\ell}\\log n/n^{2\\ell})^2)$.",
            "Analyze the source of variance. The variance comes from the contribution of $\\epsilon$, where $\\epsilon$ is independent and identically distributed and independent of $Z$. The variance of the discrete differential operator $\\nabla_{d,\\ell}\\epsilon$ is related to the sum of squared coefficients, and the variance of the quadratic variation $V_{d,\\ell}$ is determined by the sample size $n^d$, so the order of the estimator variance is $O(1/n^d)$.",
            "Synthesize the order of the mean squared error. The mean squared error is the sum of the orders of the squared bias and the variance, i.e., $O((\\omega_n^{2\\ell}\\log n/n^{2\\ell})^2 + 1/n^d)$."
        ],
        "step_count": 5
    },
    "recuTzgU0Kdbsh": {
        "reasoning_steps": [
            "Define $\\phi: A \\times A \\to \\mathbb{R}$ where $\\phi(a, b) = a + xb$. Since $A$ is an analytic set (by the definition of an analytic subgroup), $A \\times A$ is an analytic set (the product of analytic sets is still an analytic set); moreover, $\\phi$ is a continuous mapping (addition and scalar multiplication of real numbers are continuous). According to Theorem1 (an analytic set is a continuous image of a Polish space), the graph of a continuous mapping is an analytic set, so the graph of $\\phi$, $P = \\{(a, b, z) \\mid z = \\phi(a, b)\\} \\subseteq A \\times A \\times \\mathbb{R}$, is an analytic set, and its projection onto $\\mathbb{R}$, $\\text{proj}_{\\mathbb{R}}(P) = \\mathbb{R}$ (because $A + xA = \\mathbb{R}$).",
            "Since $X = \\mathbb{R}$ and $Y = A \\times A$ are standard Borel spaces, $P \\subseteq X \\times Y$ is an analytic set, and $\\text{proj}_X(P) = \\mathbb{R}$. By Theorem2, there exists a Σ-measurable function $f: \\mathbb{R} \\to A \\times A$ such that for all $z \\in \\mathbb{R}$, $(z, f(z)) \\in P$, i.e., $\\phi(f(z)) = z$.",
            "By Theorem3, analytic subsets of real numbers are Lebesgue measurable, and the Σ-algebra contains analytic sets, so the function $f$ in step 3 is a Lebesgue measurable function.",
            "For the measurable function $f$, by Theorem4, there exists a compact set $P \\subseteq \\mathbb{R}$ ($\\mu(P) > 0$) such that $f$ is continuous on $P$, denoted $\\psi = f|_P$ ($\\psi$ is continuous). At this point, for all $z \\in P$, $\\phi(\\psi(z)) = z$ (since $\\psi$ is a restriction of $f$).",
            "Let $\\psi(z) = (a_z, b_z)$ ($a_z, b_z \\in A$), define $C_0 = \\{a_z \\mid z \\in P\\}$ and $C_1 = \\{b_z \\mid z \\in P\\}$. Since $\\psi$ is continuous and $P$ is a compact set, $C_0$ and $C_1$ are compact sets (the image of a compact set under a continuous mapping is still a compact set). Let $B$ be the additive subgroup generated by $C_0 \\cup C_1$, i.e., $B = \\{\\sum_{i=1}^n (\\pm a_i \\pm b_i) \\mid n \\in \\mathbb{N}, a_i \\in C_0, b_i \\in C_1\\}$. Since the countable union of finite sums and differences of compact sets is an $F_{\\sigma}$ set (a union of countably many closed sets), $B$ is an $F_{\\sigma}$ subgroup, and $B \\subseteq A$ (because $A$ is a subgroup and $C_0, C_1 \\subseteq A$).",
            "By construction, $\\phi(B \\times B) \\supseteq P$ (since $\\psi(z) \\in B \\times B$, thus $z \\in \\phi(B \\times B)$), so $\\phi(B \\times B)$ has positive Lebesgue measure. Moreover, $\\phi(B \\times B) = B + xB$ is an additive subgroup of $\\mathbb{R}$ (because $B$ is a subgroup). By Theorem5 (a measurable proper subgroup has measure 0), if $B + xB$ were a proper subgroup, its measure would be 0, contradicting the positive measure. Thus, $B + xB = \\mathbb{R}$, completing the proof."
        ],
        "step_count": 6
    },
    "recuTQxwhTr7cB": {
        "reasoning_steps": [
            "Under daily cumulative irradiation $H_s$ (kWh$\\cdot$m$^{-2}\\cdot$day$^{-1}$): $\\mathrm{SWP}_{\\text{hyb}} = \\frac{\\eta_s\\,H_s}{\\mathrm{SEC}_{\\text{hybrid}}}$, where $\\eta_s$ is the total efficiency (electrical + thermal) of the PV/T system, and $\\mathrm{SEC}_{\\text{hybrid}}$ is the equivalent specific energy consumption of the hybrid system.",
            "$\\eta_s = \\eta_{\\mathrm{PV}} + \\eta_{\\mathrm{th}}$, $f_{\\mathrm{PV}} = \\frac{\\eta_{\\mathrm{PV}}}{\\eta_s}$, $f_{\\mathrm{th}} = \\frac{\\eta_{\\mathrm{th}}}{\\eta_s}$, where $\\eta_{\\mathrm{PV}}$ and $\\eta_{\\mathrm{th}}$ are defined by Equations (19) and (20) in the text, respectively.",
            "Reverse Osmosis (RO): $\\mathrm{SEC}_{\\mathrm{RO}} = \\frac{\\Delta G}{\\eta_{\\mathrm{RO}}}$, $\\Delta G = R\\,T \\left[ (1-r)\\,\\ln\\frac{1}{1-r} \\right]$; Thermal Distillation (TD): $\\mathrm{SEC}_{\\mathrm{TD}} = \\frac{L}{\\mathrm{GOR}}$; Hybrid system: $\\mathrm{SEC}_{\\text{hybrid}} = \\frac{V_{\\mathrm{RO}}\\mathrm{SEC}_{\\mathrm{RO}} + V_{\\mathrm{TD}}\\mathrm{SEC}_{\\mathrm{TD}}}{V_{\\mathrm{RO}} + V_{\\mathrm{TD}}}$.",
            "TD is a passive device with $N=10$ stages. Theoretically, $\\mathrm{GOR}_{\\text{ideal}} \\approx N$, but under actual passive conditions, $\\mathrm{GOR}$ is significantly lower than $N$. Multistage theoretical thermal efficiency: $\\eta_{\\mathrm{TD,th}} = \\frac{\\mathrm{GOR} \\cdot L}{I_{\\mathrm{th}}}$.",
            "$H_s$ takes the typical range of passive off-grid sunny days (e.g., $4$--$6$ kWh$\\cdot$m$^{-2}$·day$^{-1}$). The lower bounds of $\\eta_s$ and $\\mathrm{SEC}_{\\mathrm{RO}}$ come from the thermodynamic limit of $\\Delta G$ and high $\\eta_{\\mathrm{RO}}$; the upper bounds consider engineering achievable values. $\\mathrm{SEC}_{\\mathrm{TD}}$ is given by $L/\\mathrm{GOR}$, where the upper bound corresponds to a lower $\\mathrm{GOR}$ (actual passive condition) and the lower bound is close to the multistage ideal value. Substitute the above intervals into the $\\mathrm{SWP}_{\\text{hyb}}$ formula to form the theoretical upper and lower bounds.",
            "Under the conditions of passive PV/T-RO-TD and multistage TD ($N=10$): $\\boxed{\\mathrm{SWP}_{\\text{hyb}} = 27\\text{--}74\\ \\mathrm{L/m^2}}$. The lower bound is dominated by low $H_s$, low $\\eta_s$, high $\\mathrm{SEC}_{\\mathrm{TD}}$, and conservative $\\mathrm{SEC}_{\\mathrm{RO}}$; the upper bound is dominated by high $H_s$, high $\\eta_s$, low $\\mathrm{SEC}_{\\mathrm{TD}}$ (high $\\mathrm{GOR}$), and $\\mathrm{SEC}_{\\mathrm{RO}}$ close to the theoretical value, but still limited by passive multistage without mechanical compression."
        ],
        "step_count": 6
    },
    "recuTR7K0wJSWn": {
        "reasoning_steps": [
            "分析问题核心：Instruction的核心问题是在引入SO（自旋轨道）耦合后，是什么轨道与O-2p价带相互作用，导致了价带上限的升高。问题的背景是重原子固态化合物δ-UO3。",
            "识别关键元素与效应：根据Instruction，关键元素是重原子U，关键效应是SO耦合。根据Concept_1，SO耦合是重原子中的显著相对论效应，尤其对半芯层轨道如U-6p影响巨大，会造成巨大的能级分裂。",
            "关联价带能量变化：Instruction指出价带上限（主要由O-2p构成）能量升高。我们需要寻找一个能解释价带能量被“推高”的物理机制。Concept_2和Concept_3提供了这一机制：即“下层推动”效应（PFB）与芯层-价层轨道混合。该理论指出，能量较低的半芯层轨道（如U-6p）会与配体的价层轨道（O-2p）发生泡利互斥作用，从而“推高”价层轨道的能量。",
            "整合SO耦合与能量推高机制：Concept_2明确指出，PFB效应不仅推高了O-2p的能量，还将U-6p核的强SO分裂特性传递到了O-2p价带上。这是通过Concept_3描述的轨道混合实现的，即O-2p轨道因与U-6p轨道混合，而带上了一部分U-6p的属性，包括其巨大的SO分裂效应。因此，当计算中考虑SO耦合时，U-6p的巨大分裂会通过这种相互作用体现在O-2p带上，表现为价带上限的抬升。",
            "考虑晶体环境的特定影响：Concept_4补充说明了为何此效应在δ-UO3中尤为重要。δ-UO3的高对称性晶体场保护了U-6p轨道的简并性，使其固有的巨大SO分裂不会被晶体场效应所“猝灭”，从而保证了该分裂能最大程度地通过PFB机制传递给O-2p价带。",
            "得出结论：综合以上步骤，可以得出结论：在δ-UO3中，当考虑SO耦合时，具有巨大SO分裂效应的U-6p半芯层轨道，通过与O-2p价层轨道的轨道混合及随之产生的“下层推动”（泡利互斥）效应发生相互作用，从而将自身的SO分裂特性传递给O-2p价带，导致价带上限能量升高。因此，与O-2p价带作用的关键轨道是U-6p轨道。"
        ],
        "step_count": 6
    },
    "recuTUeMbocZZQ": {
        "reasoning_steps": [
            "By directly differentiating the solution family with respect to translation and scaling parameters, it can be verified that the translation generator $T_u = \\partial_x u$ and the scaling generator $R_u = x \\partial_x u + 2s$ indeed satisfy $\\mathcal{L}_u(T_u) = 0$ and $\\mathcal{L}_u(R_u) = 0$.",
            "The core strategy of the proof is to assume that $u$ is an even function (which can be achieved by translation), then the operator $\\mathcal{L}_u$ is also an even operator. Therefore, any function $\\psi$ in the null space can be decomposed into its even part $\\psi_e$ and odd part $\\psi_o$, and both parts must independently belong to the null space. At this point, the problem is decomposed into proving that the dimensions of the even part space and the odd part space are both 1 respectively.",
            "Prove that any even function $\\psi_e$ in the null space must be a constant multiple of the scaling generator $R_u$ (which is an even function). The method is to construct the function $h = \\psi_e - \\gamma R_u$, and select an appropriate constant $\\gamma$ such that $h(0) = 0$. Then, use the properties of the integral equation to prove that $h$ must be identically zero, so $\\psi_e$ can only be a multiple of $R_u$.",
            "For the odd function $\\psi_o$ in the null space, the equation $\\mathcal{L}_u(\\psi_o) = 0$ is cleverly transformed into an integral equation defined on the half-infinite line $(0, +\\infty)$. This integral equation has the form $g = Ag$, where $g$ is a function related to $\\psi_o$, and $A$ is a compact self-adjoint operator with a strictly positive integral kernel.",
            "Since the operator $A$ satisfies the conditions of the Perron-Frobenius type theorem, its maximum eigenvalue (here, 1) must be simple, that is, the corresponding eigenspace is one-dimensional. Because the function $h$ corresponding to the translation generator $T_u$ (which is an odd function) is exactly the eigenfunction with eigenvalue 1, any other odd part solution $g$ must be a constant multiple of $h$. This proves that the odd part space is one-dimensional, spanned by $T_u$."
        ],
        "step_count": 5
    },
    "recuU5KUFcTDmu": {
        "reasoning_steps": [
            "Assumptions and Objectives: We assume that there exists an eigenfunction corresponding to $\\sigma_1$ with the property of 'even symmetry about the x-axis and odd symmetry about the y-axis'. We will prove that this assumption leads to a contradiction.",
            "Deriving the Lower Bound of the Eigenvalue: If the above assumption holds, then for this ellipse (let the semi-major axis be $a$ and the semi-minor axis be $b$, with $a > b$), the first eigenvalue must satisfy $\\sigma_1 > 1/b$ by applying the variational principle or comparison theorem.",
            "Deriving the Upper Bound of the Eigenvalue: Now we constrain $\\sigma_1$ from another perspective. First, applying Theorem_2 (Weinstock's Inequality) to the non-disk ellipse, we obtain $\\sigma_1 < 2\\pi/|\\partial \\mathcal{E}|$. Next, using Theorem_3 (Geometric Perimeter Property of Ellipses), we know that the perimeter of the ellipse $|\\partial \\mathcal{E}| > 2\\pi b$. Substituting the perimeter property into Weinstock's Inequality, we get $\\sigma_1 < 2\\pi/|\\partial \\mathcal{E}| < 2\\pi/(2\\pi b) = 1/b$.",
            "Discovering the Contradiction: In Step 2, we derived $\\sigma_1 > 1/b$ from the assumption. In Step 3, we obtained $\\sigma_1 < 1/b$ using classical theories and geometric facts. These two conclusions are obviously contradictory. This contradiction indicates that our initial assumption—'there exists a first-order eigenfunction with even symmetry about the x-axis and odd symmetry about the y-axis'—is false, thus completing the proof."
        ],
        "step_count": 4
    },
    "recuUihtvujMfc": {
        "reasoning_steps": [
            "Problem Splitting and Goal Transformation  The first step of the proof is to simplify the problem. Instead of directly dealing with the entire complex domain $\\Omega$, it first takes a line segment $l$ of length equal to the diameter $d$ within $\\Omega$, then uses the perpendicular bisector $k$ of $l$ to divide the domain $\\Omega$ into two subdomains $\\Omega_+$ and $\\Omega_-$. Next, it is noted that $\\mu_{2} \\le \\max\\{\\lambda_{1}^{k}(\\Omega_{-}), \\lambda_{1}^{k}(\\Omega_{+})\\}$. A more difficult-to-estimate \"second eigenvalue\" problem is successfully transformed into a more manageable \"first eigenvalue\" problem.",
            "Geometric Construction and Test Function Selection  The proof focuses on one of the subdomains, say $\\Omega_+$. Through translation, it can be assumed that one endpoint of the diameter $l$ is at the origin of coordinates. Using the convexity of $\\Omega$, it can be ensured that the interior of $\\Omega_+$ necessarily contains a semicircle with radius $d/2$. This embeds a known simple geometric shape within an unknown complex domain. On this semicircle, a very special test function $\\varphi(r, \\theta) = J_0(\\frac{2j_0}{d}r)$ is constructed, and it is set to zero in the remaining part of $\\Omega_+$.",
            "Variational Estimation and Boundary Analysis  The test function $\\varphi$ constructed in the previous step is substituted into Theorem_1 of the Rayleigh Quotient to obtain an upper bound for $\\lambda_1^k(\\Omega_+)$. This requires the use of Theorem_2 of integration by parts (Green's formula). After integration by parts, a boundary integral term $\\int \\varphi\\partial_{\\nu}\\varphi$ [cite: 1] is generated. At this point, by再次利用 the convexity of $\\Omega$ and the properties of the Bessel function $J_0$, it can be proven that this boundary integral term is less than or equal to zero ($\\le 0$).",
            "Conclusion Integration  Dividing both sides of the inequality obtained in the previous step by $\\int_{\\Omega_+}\\varphi^2$ directly gives the upper bound of the Rayleigh quotient, i.e., $\\lambda_{1}^{k}(\\Omega_{+}) \\le (\\frac{2j_0}{d})^2 = 4(j_0/d)^2$. Since the same argument applies to the other subdomain $\\Omega_-$, combined with the domain monotonicity principle in Step 1, the final conclusion is drawn: $\\mu_2 \\le 4(j_0/d)^2$."
        ],
        "step_count": 4
    },
    "recuSuIjIYkDUd": {
        "reasoning_steps": [
            "To obtain the upper bound of $I= \\langle u, E_t^3 \\rangle$, first, through small disturbance (concept_1), the non-square-integrable $E_t^3$ is transformed into a regularized inner product limit with small disturbance to facilitate subsequent analysis;",
            "then, using Plancherel formula (concept_2), the integral is decomposed into discrete spectrum terms, continuous spectrum terms, etc.;",
            "through Rankin-Selberg method (concept_3) and triple product formula of Eisenstein series (concept_4), these terms are associated with L-function values and transformed into the estimation of L-functions;",
            "using Stirling formula (concept_5) to analyze the asymptotic behavior of the Γ function, determine the exponential decay range of terms related to spectral parameters, and screen out the main contribution terms;",
            "combining the subconvexity bound (concept_6) to control the growth of L-functions, and estimating the fourth power sum of L-functions in short intervals through the fourth moment bound (concept_7), the final result $I \\ll t^{-1/3 + \\varepsilon}$ is obtained (corresponding to Theorem 1.3 in the paper)."
        ],
        "step_count": 5
    },
    "recuUg8GfKGfI2": {
        "reasoning_steps": [
            "Modeling the slowly driven CRN. Reactions are divided into core reactions satisfying local detailed balance and external reservoir reactions slowly coupled via parameter ε(≪1); write the relationship between mass action law rates, reservoir intake/excretion forms, and work.",
            "Write the rate equation and perform ε expansion. Core/boundary decomposition yields $∂t n = S0·J0(n) + ε[r(z^C − n/Ω)]$; let $n = n0 + ε n1 + …$. The leading order is a closed system, monotonically relaxing to the thermal equilibrium manifold $J0(n0)=0$, whose general solution can be parameterized as $n0 = [neq e^{ζ·η}]$, where $ζ$ gives L conserved moieties.",
            "Direct perturbation fails at O(ε) order. Left-multiplying the equation for $n1$ by $ζ^T$ leads to the secular divergence $ζ^T n1(t) ~ t·ζ^T[r(z^C − n0(∞)/Ω)]$, indicating that the limit $ε→0$ is singular and requires singular perturbation treatment.",
            "Multiple time scales. Introduce the slow time $τ=εt$ and require the solvability condition to eliminate secular terms: $ζ^T ∂τ n0 = ζ^T[r(z^C − n0/Ω)]$, which is the conservation law form of slow dynamics in the moiety subspace.",
            "Project $n0$ onto the equilibrium manifold parameter $η$ to obtain $M(η)∂τη = ζ^T J^C(η)$, where $M=ζ^T[[neq e^{ζ·η}]]ζ$ and $J^C=r(z^C − neq e^{ζ·η}/Ω)$. This is the main result: the universal slow dynamics equation.",
            "Extract determinants and independence. The equation only contains the conserved quantity structure $ζ$ and external couplings $(r, z^C)$, independent of the activation energies or specific rates of core reactions; the steady state is also independent of core chemical potentials—thus, the dynamics are robust to details of microscopic rates, with dimension reduced from N to L.",
            "Thermodynamic consistency and dissipation. Even though the system is always approximately in a certain closed equilibrium state, entropy is still generated through slow fluxes: $T·Ṡ = W·(r^+−r^−) + O(ε^2)$, which can be evaluated on the slow solution.",
            "Steady state structure and network topology. Using the rank-nullity relation, derive $MC = L_br + C_C$, indicating that the emergence of loops is a necessary condition for reaching non-equilibrium steady states; the steady state of the slow equation can be obtained solely from reservoir parameters without core rates.",
            "Stochastic extension and large deviations. Derived from the Doi–Peliti/Hamilton–Jacobi formalism in the macroscopic limit, the condition to avoid divergence also gives $ζ^T(∂τ λ − J^C)=0$, thus yielding the same slow dynamics conclusion in the presence of fluctuations.",
            "Inductive answer. Therefore, under $ϵ≪1$ slow driving, both the slow dynamics and final steady state of the CRN are independent of details of reaction rates, determined entirely by (i) the subspace spanned by the conserved quantities $ζ$ of the closed system and (ii) the external couplings $(r, z^C)$."
        ],
        "step_count": 10
    },
    "recuU0wMcv6cHB": {
        "reasoning_steps": [
            "Objective: Derive the relationship between the momentum diffusion coefficient $\\kappa$ and the spatial diffusion coefficient $D_s$ within the Langevin framework for heavy quarks; adopt natural units with $k_B=1$.",
            "Establish Langevin dynamics and definition of noise (Concept_1): $\\tfrac{d\\mathbf{p}}{dt}=-\\eta_D(p)\\,\\mathbf{p}+\\boldsymbol{\\xi}$, where the isotropic white noise satisfies $\\langle\\xi_i(t)\\xi_j(t')\\rangle=\\kappa\\,\\delta_{ij}\\,\\delta(t-t')$, and $\\kappa$ characterizes the intensity of thermal random forces.",
            "Apply the fluctuation-dissipation theorem (Concept_2): $\\eta_D(p)=\\frac{\\kappa}{2TE}$, to ensure the equilibrium distribution (Maxwell–Jüttner) at temperature $T$.",
            "Adopt the Einstein–Smoluchowski relation for spatial diffusion in the small momentum limit (Concept_3): $D_s \\equiv \\frac{T}{M\\,\\eta_D(0)}$.",
            "Take the small momentum limit $p\\to0\\Rightarrow E\\to M$, yielding $\\eta_D(0)=\\kappa/(2TM)$.",
            "Substitute $\\eta_D(0)=\\kappa/(2TM)$ into the Einstein relation: $D_s=\\frac{T}{M\\,\\eta_D(0)}=\\frac{T}{M\\,(\\kappa/(2TM))}=\\frac{2T^2}{\\kappa}$.",
            "Answer (consistent with the given result) — The relationship between momentum and spatial diffusion in the Langevin equation is: $\\boxed{D_s = \\frac{2T^2}{\\kappa}}$. Assumptions: natural units with $k_B=1$, local thermal equilibrium, isotropic Markovian white noise, and the small momentum limit."
        ],
        "step_count": 7
    },
    "recuU0q3re659f": {
        "reasoning_steps": [
            "First, according to \\textbf{theorem_1}, take the Ansatz for the dyonic spherically symmetric field, \\[ F=dA=F_{tr}(r)\\,dt\\wedge dr+q_m\\sin\\theta\\,d\\theta\\wedge d\\phi, \\] where \\(E(r)\\equiv F_{tr}(r)\\) is denoted.",
            "Next, from the radial Gauss'\\text{s law of \\textbf{concept_1} (NLED field equation \\(\\nabla_\\mu(\\mathcal L_{\\mathcal F}F^{\\mu\\nu})=0\\)), we have \\[ \\mathcal L_{\\mathcal F}\\,\\Sigma^{2}(r)\\,E(r)=q_e \\;\\Rightarrow\\; E(r)=\\frac{q_e}{\\mathcal L_{\\mathcal F}\\,\\Sigma^{2}(r)}. \\]",
            "Finally, using the definition of the electromagnetic field scalar in \\textbf{theorem_2}: \\(\\displaystyle \\mathcal F\\equiv \\tfrac14 F_{\\mu\\nu}F^{\\mu\\nu}\\), and raising indices under \\[ ds^{2}=A(r)\\,dt^{2}-\\frac{dr^{2}}{A(r)}-\\Sigma^{2}(r)\\,d\\Omega^{2} \\] we have \\[ F^{tr}=-E,\\qquad F^{\\theta\\phi}=\\frac{q_m}{\\Sigma^{4}(r)\\sin\\theta}, \\] thus \\[ F_{\\mu\\nu}F^{\\mu\\nu}=2\\!\\left(-E^{2}+\\frac{q_m^{2}}{\\Sigma^{4}(r)}\\right) \\;\\Rightarrow\\; \\boxed{\\; \\mathcal F=\\frac12\\!\\left(\\frac{q_m^{2}}{\\Sigma^{4}(r)}-E^{2}\\right) =\\frac{q_m^{2}\\,\\mathcal L_{\\mathcal F}^{2}(r)-q_e^{2}} {2\\,\\mathcal L_{\\mathcal F}^{2}(r)\\,\\Sigma^{4}(r)}\\; }. \\]"
        ],
        "step_count": 3
    },
    "recuU5Cxq7MO7M": {
        "reasoning_steps": [
            "Calculate the Lee metric parameter $\\ell$ (based on concept_1)\\ According to $m = 2\\ell + 1$ (where $m$ is an odd integer and $m \\geq 3$). Given $m = 9$. Calculate $\\ell = \\left\\lfloor \\frac{m}{2} \\right\\rfloor = \\left\\lfloor \\frac{9}{2} \\right\\rfloor = \\lfloor 4.5 \\rfloor = 4.$ Obtain $\\ell = 4$.",
            "Calculate the extended code length $\\bar{n}$ (based on theorem_1)\\ According to the balanced problem code length $\\bar{n}' = (\\bar{e} \\mid -\\bar{e}) = 2\\bar{n}$, given $\\bar{n}' = 24$. Calculate $\\bar{n}' = 2\\bar{n} \\implies 24 = 2\\bar{n} \\implies \\bar{n} = \\frac{24}{2} = 12.$ Obtain $\\bar{n} = 12$.",
            "Apply the reduction formula to solve for $n$ (based on concept_2)\\ Weights need to be compatible during the reduction process, requiring $n\\ell \\leq \\bar{n}(\\ell - 1)$. Given $\\bar{n} = 12$ and $\\ell = 4$, thus $\\ell - 1 = 3$. Then derive that $n$ satisfies the equation $n + \\left\\lceil \\frac{n}{3} \\right\\rceil = 12.$ Solve the equation (consider three cases of $n \\bmod 3$):\\ Case 1: $n$ is divisible by 3 (let $n = 3k$): $\\left\\lceil \\frac{3k}{3} \\right\\rceil = k \\implies 3k + k = 4k = 12 \\implies k = 3 \\implies n = 9.$\\ Case 2: $n \\equiv 1 \\pmod{3}$ (let $n = 3k + 1$): $\\left\\lceil \\frac{3k + 1}{3} \\right\\rceil = k + 1 \\implies (3k + 1) + (k + 1) = 4k + 2 = 12 \\implies k = 2.5 \\quad \\text{(non-integer, invalid)}.$\\ Case 3: $n \\equiv 2 \\pmod{3}$ (let $n = 3k + 2$): $\\left\\lceil \\frac{3k + 2}{3} \\right\\rceil = k + 1 \\implies (3k + 2) + (k + 1) = 4k + 3 = 12 \\implies k = 2.25 \\quad \\text{(non-integer, invalid)}.$\\ Obtain the unique solution: $n = 9$.",
            "Verify the weight constraints (based on theorem_2 and concept_2)\\ Weight compatibility condition (reduction validity): According to $n\\ell \\leq \\bar{n}(\\ell - 1)$ (to ensure the maximum weight of the original vector $n\\ell$ does not exceed the new weight upper limit). $n\\ell = 9 \\times 4 = 36, \\quad \\bar{n}(\\ell - 1) = 12 \\times 3 = 36 \\implies 36 \\leq 36 \\quad \\text{(satisfied)}.$\\ Balanced problem weight constraint: According to $w \\leq \\bar{n}(\\ell - 1)$. $\\bar{n}(\\ell - 1) = 12 \\times 3 = 36, \\quad w = 20 \\leq 36 \\quad \\text{(satisfied)}.$\\ Conclusion: The original code length satisfying all constraints is $n = 9$."
        ],
        "step_count": 4
    },
    "recuTKxErJ9PO5": {
        "reasoning_steps": [
            "3-Coloring of graphs with maximum degree 4 requires $2^{\\Omega(n)}$ time under ETH, which is equivalent to the non-existence of $2^{o(n)}$ algorithms, and this is invoked as the lower bound foundation.",
            "Define the strong product (blowup) $H \\boxtimes K_t$: replace each vertex of $H$ with a block of size $t$, and add complete bipartite edges between corresponding blocks; Note: Marx's notation $H(t)$ is equivalent to $H \\boxtimes K_t$.",
            "Introduce the matching-linked set, and based on this, define the linking capacity $\\gamma(H)=\\liminf_{t\\to\\infty}\\ell(H\\boxtimes K_t)/t$, and give its basic range $1\\le \\gamma(H)\\le |V(H)|$.",
            "For any graph $G$ with $n$ vertices and maximum degree $\\Delta$, when $t \\ge \\dfrac{3\\Delta\\,n}{\\gamma(H)}$, $H\\boxtimes K_t$ contains a topological minor of $G$, and the embedding can be explicitly constructed in $O(n\\cdot f(k))$ time ($f(k)$ depends only on $k=|V(H)|$).",
            "If $G \\subseteq H\\boxtimes K_t$, a colorful graph $X$ can be constructed in time $9^t\\!\\cdot\\!\\mathrm{poly}(k,t)$ with $k\\cdot 3^t$ vertices, such that the $H$-colorful copies in $X$ are in one-to-one correspondence with the 3-assignments of $G$.",
            "From the ETH lower bound in step1, the embedding threshold in step4, and the reduction size in step5, it can be concluded that there exists a polynomial-time reduction from 3-Coloring (maximum degree 4) to $\\mathrm{ColSub}(H)$ that preserves \"yes/no\" equivalence; therefore, if there exists an $n^{o(\\gamma(H))}$ time algorithm for $\\mathrm{ColSub}(H)$, then 3-Coloring will have a $2^{o(n)}$ algorithm, violating ETH.",
            "From the above, it can be obtained: under the validity of ETH, there exists a constant $\\alpha>0$ such that for any fixed $H$, there is no $O\\!\\big(n^{\\alpha\\cdot\\gamma(H)}\\big)$ algorithm for $\\mathrm{ColSub}(H)$; equivalently, $\\mathrm{ColSub}(H)$ requires at least $n^{\\Omega(\\gamma(H))}$ time (where $n$ is the size of the target instance)."
        ],
        "step_count": 7
    },
    "recuTLpE9sKOPt": {
        "reasoning_steps": [
            "Determine physical interval conditions: It is known that the Instruction specifies a low Prandtl number fluid ($Pr \\ll 1$) and $Pe_b \\gtrsim O(1)$. According to Concept 3 (Buoyancy Péclet Number), $Pe_b \\gtrsim O(1)$ indicates a non-diffusion-dominated region where diffusion effects are secondary. Focus on the mutual constraints of inertia, viscosity, and stratification, rather than the diffusion-dominated case.",
            "Stratification intensity and vertical length scale: According to Concept 1 (Definition and Physical Significance of Froude Number), under strong stratification ($Fr \\ll 1$), the vertical length scale is proportional to the horizontal velocity: $H^* \\propto Fr \\, L^*$. This gives the initial scaling relationship between $H^*$, $Fr$, and $L^*$.",
            "Can viscous effects be neglected: According to Concept 2 (Buoyancy Reynolds Number $Re_b = a^2 Re$), if $Re_b \\gg 1$, viscosity can be neglected. For low $Pr$, if $Pe_b \\gtrsim O(1)$ and $Pr \\ll 1$, $Re_b$ is often large (since $Pe_b = Pr \\cdot Re_b$). Therefore, the current interval can be regarded as inertia-stratification dominated, with viscous effects being secondary.",
            "Stratified energy balance constraint on vertical kinetic energy: According to Concept 5, under strong stratification, the ratio of the square of vertical velocity to the square of horizontal velocity satisfies: $\\frac{w^{*2}}{U^{*2}} \\sim Fr^2$. Taking the square root gives: $W^* \\propto Fr \\, U^*$.",
            "Supplementary role of wave dissipation mechanism: According to Concept 4, in non-diffusive strongly stratified turbulence, vertical velocity is not only determined by geometric constraints (Concept 1) and energy balance (Concept 5), but also affected by energy exchange between small-scale internal waves and turbulence. Here, this mechanism ensures the validity of the scaling $W^* \\propto Fr^{1/2} U^*$ in Step 4, that is, wave dissipation provides physical support for this scaling, rather than being purely determined by $H^* N^*$."
        ],
        "step_count": 5
    },
    "recuTXzS2cdMei": {
        "reasoning_steps": [
            "Foundation: Reduce #SAT to Polynomial Summation Problem (Concept 1) The core of the decision version of #SAT is to determine whether \"the number of satisfying assignments of a CNF formula is equal to γ\", which can be transformed into the summation verification of an arithmetized polynomial: Let Φ be a CNF formula, and its arithmetized polynomial $P:\\{0,1\\}^m \\to \\mathbb{F}$ satisfies \" $P(a)=1$ if a is a satisfying assignment of Φ, otherwise 0\". It is necessary to verify $\\sum_{a\\in\\{0,1\\}^m} P(a)=\\gamma$. According to Concept 1, the Sumcheck protocol decomposes the global summation into layer-by-layer consistency checks (such as $g_1(0)+g_1(1)=\\gamma$, $g_i(c_i)=g_{i+1}(0)+g_{i+1}(1)$, etc.) by defining chain polynomials $g_i(X) = \\sum_{a_{i+1},\\dots,a_m\\in\\{0,1\\}} P(c_1,\\dots,c_{i-1},X,a_{i+1},\\dots,a_m)$, and writes the evaluation tables of these polynomials into the proof string to form the PCP framework. This step provides the basic structure for subsequent verification, which is consistent with the definition of Sumcheck as \"arithmetizing #SAT and expanding it into a PCP framework\".",
            "Zero-Knowledge Guarantee: Masking and Zero-Sum Polynomials (Concept 2 + Concept 3) Class C requires \"the view of the adaptive verifier can be perfectly simulated\" (zero-knowledge), so it is necessary to avoid the proof directly leaking information about $P$. According to Concept 2, introduce a random $\\alpha \\in \\mathbb{F}$ and a zero-sum polynomial $R$ (satisfying $\\sum_{a} R(a)=0$), and change the verification object to $\\alpha P + R$. At this time, the summation formula to be verified becomes $\\sum_{a} (\\alpha P + R)(a) = \\alpha \\gamma$, which not only maintains correctness but also masks $P$ through $R$. To construct the required $R$, the vanishing polynomial technique in Concept 3 is used: Let $R(X) = Q(X) - Q(X_{\\text{rev}}) + \\sum_{i=1}^m X_i(1-X_i)T_i(X)$, where $X_i(1-X_i)$ is a vanishing term on the hypercube (guaranteed by the Combinatorial Nullstellensatz, which is identically zero on $\\{0,1\\}^m$). Thus, $R(a)=0$ for all $a\\in\\{0,1\\}^m$, satisfying the zero-sum condition; at the same time, $R$ is pseudorandom at other points in the field to avoid leaking information about $P$.",
            "Locally Simulable: Deal with Adaptive Verifiers (Concept 4) Class C requires \"the simulator can reproduce the view of the adaptive verifier\", so it is necessary to ensure that the encoding of the proof can be locally simulated. According to Concept 4, encode $\\alpha P + R$, intermediate polynomials $g_i$, and auxiliary polynomials $T_i$ into \"Random Low-Degree Extension (LDE)\": that is, randomly extract an extension polynomial $\\tilde{f}$ consistent with the boundary function (such as the value of $\\alpha P + R$ on $\\{0,1\\}^m$) from the Reed-Muller code $\\text{RM}[\\mathbb{F}, m, d]$. The key of this encoding is \"local simulability\": for any adaptive query sequence $(q_1,\\dots,q_t)$, the simulator can output results with the same distribution as the evaluations of the real $\\tilde{f}$ at these points. This ensures that no matter how the malicious verifier adaptively queries, its view can be reproduced by the simulator, satisfying the zero-knowledge requirement.",
            "Efficient Checking by Verifier: Low-Degree and Consistency (Concept 5) Class C requires \"the honest verifier is non-adaptive and has polynomial resources\", so it is necessary to efficiently verify the correctness of the proof. According to Concept 5, use the Low Individual Degree Test (LIDT): for polynomials such as $\\alpha P + R$, $g_i$, and $T_i$, randomly sample along the coordinate axes, fix all variables except one, check whether the resulting univariate polynomial has degree ≤ d (the individual degree of $P$), and spot-check consistency. The query complexity of LIDT is $O(md \\cdot \\text{poly}(1/\\epsilon) \\cdot \\log(1/\\delta))$ (polynomial order), which can verify that the polynomial belongs to the low-degree code, ensure the consistency of the proof, and naturally match the non-adaptive query mode of the honest verifier (pre-fixing query positions), satisfying the completeness and soundness of PCP.",
            "Satisfy All Conditions of Class C - **Completeness and Soundness**: For $x\\in L$ (the #SAT instance is true), the honest verifier passes the consistency check of Sumcheck and LIDT verification; for $x\\notin L$, the false proof will be rejected by LIDT with constant probability. - **Zero-Knowledge**: Through the masking in Concept 2, the construction of $R$ in Concept 3, and the locally simulable LDE in Concept 4, the simulator can perfectly reproduce the view of the adaptive verifier. - **Resource Bounds**: The proof length (LDE encoding), verification time (LIDT queries), and simulator time (local simulation) are all polynomial. Therefore, the decision version of #SAT belongs to Class C, i.e., $\\#P \\subseteq C$."
        ],
        "step_count": 5
    },
    "recuU2y0f7KjIO": {
        "reasoning_steps": [
            "Using theorem_1 (Cheeger's inequality), we know that for a graph G, there holds \\[\\frac{\\lambda_2}{2} \\leq \\phi(G) \\leq \\sqrt{2 \\lambda_2}\\], which connects the second smallest eigenvalue $\\lambda_2$ with the graph's conductance $\\phi(G)$. This provides a relational foundation between the spectrum and cuts for the subsequent analysis of the multiplicity of $\\lambda_2$, thereby allowing us to indirectly bound the properties of the eigenvalue through conductance.",
            "According to concept_1, Abelian Cayley graphs have polynomial growth, that is, their doubling constant satisfies \\[\\gamma_G = \\max_{t \\geq 0} \\frac{|B(2t)|}{|B(t)|} \\leq 2^{O(d)}\\], where $B(t)$ is the ball of radius $t$. This describes the volume growth rate of the graph and provides a key parameter for applying volume growth bounds.",
            "Applying theorem_2, through volume growth bounds, we obtain the upper bound for the multiplicity of the second smallest eigenvalue $\\lambda_2$ as \\[mul_{\\lambda_2} \\leq \\gamma^{O(\\log \\gamma)}\\]. Substituting $\\gamma_G \\leq 2^{O(d)}$ from concept_1, we calculate \\[mul_{\\lambda_2} \\leq (2^{O(d)})^{O(\\log 2^{O(d)})} = (2^{O(d)})^{O(d)} = 2^{O(d^2)}\\], which gives a preliminary upper bound but requires further improvement.",
            "To improve the upper bound, we introduce theorem_3 (Stirling's approximation), which states that for all $t \\in \\mathbb{N} \\setminus \\{0\\}$, there holds \\[\\sqrt{2 \\pi t} (t/e)^t \\leq t! \\leq 2 \\sqrt{2 \\pi t} (t/e)^t\\]. This is used for the refined analysis of bounds on collision probability under polynomial distribution, such as by estimating polynomial coefficients or probability ratios, thereby proving that the collision probability ratio $\\gamma_{CP} \\leq 2^{O(d)}$. Combining the spectral-cut relationship from theorem_1 and the framework from theorem_2, the upper bound of the multiplicity is finally tightened to \\[2^{O(d)}\\]."
        ],
        "step_count": 4
    },
    "recuVloVx9quyJ": {
        "reasoning_steps": [
            "Notation & setup. Let $m\\ge 1$ and $r=\\lceil 2^m\\ln 2\\rceil$. For each $i\\in[m]$ define $$ Z_i:=\\mathrm{Tribes}(A_i)=\\bigvee_{j\\in[r]}\\ \\bigwedge_{k\\in[m]}(A_i)_{k,j}\\in\\{0,1\\}, $$ so that $\\Pr[Z_i=1]=1-(1-2^{-m})^{r}=1/2+o(1)$. Let $a=(Z_1,\\ldots,Z_m)$. A single block is $(A,p)$, where $p=(p_b)_{b\\in\\{0,1\\}^m}$. Under the “tilted” source $D$, only when $|a|=m/2$ we force $p_a:=1$; otherwise the sampling matches the uniform source $U$.",
            "Define the single-block decision as $$ \\tilde H(A,p)\\ :=\\ \\underbrace{\\mathbf 1\\!\\Big[\\textstyle\\sum_{i=1}^m Z_i\\ \\ge\\ m/2+1\\Big]}_{\\text{strict majority}} \\ \\ \\vee\\ \\ \\underbrace{\\big(\\mathbf 1[\\,|a|=m/2\\,]\\ \\wedge\\ p_a\\big)}_{\\text{mid-layer tie broken by }p_a}. $$ Thus $\\tilde H$ reads $p_a$ **only** on the central layer $|a|=m/2$.",
            "Couple $(x_U,x_D)$ by first sampling $(A,p)\\sim U$ and then setting $x_D$ to equal $x_U$ except, if $|a|=m/2$, replace $p_a\\gets 1$. Then $$ \\tilde H(x_D)\\neq \\tilde H(x_U)\\quad\\Longleftrightarrow\\quad |a|=m/2 \\text{ and (under $U$) } p_a=0. $$ Hence $$ \\delta_{\\text{block}} =\\big|\\mathbb{E}_D[\\tilde H]-\\mathbb{E}_U[\\tilde H]\\big| =\\Pr[|a|=m/2]\\cdot \\Pr[p_a=0\\mid |a|=m/2] =\\Pr[|a|=m/2]/2. $$ Using the central binomial mass bound $\\Pr[|a|=m/2]=\\Omega(m^{-1/2})$, we obtain $$ \\delta_{\\text{block}}=\\Omega(m^{-1/2}). $$",
            "By Hoeffding/Chernoff amplification for majority-of-$t$ i.i.d. trials, $$ t=\\Theta\\!\\big(1/\\delta_{\\text{block}}^{\\,2}\\big)=\\Theta(m). $$",
            "Define the single-block length $$ s:=m^2\\,r=\\Theta(m^2 2^m)\\quad\\Rightarrow\\quad m=\\Theta(\\log s). $$ From Step 3, $$ t=\\Theta(m)=\\Theta(\\log s). $$ Let total input length be $$ n:=t\\cdot s=\\Theta(\\log s)\\cdot s. $$ Since $s=\\Theta(m^2 2^m)$ dominates $\\log s$, we have $\\log n=\\Theta(\\log s)$ and thus $$ m=\\Theta(\\log n),\\qquad t=\\Theta(\\log n), $$ avoiding any circular dependence between $m$ and $t$.",
            "Let $\\tilde H$ be an OR of two parts; we construct an equivalent DNF and bound the clause width. **(a) Strict-majority part.** Write $$ h(z)=\\mathbf 1\\!\\Big[\\sum_{i=1}^m z_i\\ge m/2+1\\Big] =\\bigvee_{\\substack{S\\subseteq[m]\\\\ |S|=m/2+1}}\\ \\bigwedge_{i\\in S} z_i. $$ Substitute each $z_i$ by $Z_i=\\bigvee_{j\\in[r]}\\bigwedge_{k\\in[m]}(A_i)_{k,j}$ and distribute. For any $|S|=m/2+1$ and choice $\\{j_i\\in[r]: i\\in S\\}$, a resulting conjunct is $$ \\bigwedge_{i\\in S}\\ \\bigwedge_{k\\in[m]}(A_i)_{k,\\,j_i}, $$ whose width is $|S|\\cdot m=(m/2+1)\\,m=\\Theta(m^2)$. **(b) Mid-layer part.** For any $S\\subseteq[m]$ with $|S|=m/2$ and any $\\{j_i\\in[r]: i\\in S\\}$, take the conjunct $$ \\Big(\\ \\bigwedge_{i\\in S}\\ \\bigwedge_{k\\in[m]}(A_i)_{k,\\,j_i}\\ \\Big)\\ \\wedge\\ p_{a_S}, $$ where $a_S$ is the address of support $S$. This sets $Z_i=1$ for $i\\in S$ (width $m^2/2$) and reads $p_{a_S}$ (one more literal), so the width is $m^2/2+1=\\Theta(m^2)$. We do **not** need to force $Z_j=0$ for $j\\notin S$: if some $Z_j=1$, the strict-majority part already accepts; if $Z_j=0$, we remain exactly on the mid-layer and the above clause suffices. Combining (a) and (b), every clause has width $\\Theta(m^2)$. Hence $$ \\mathrm{width}(\\tilde H)=O(m^2). $$",
            "Let $F_t=g\\circ \\tilde H^{\\,t}$ with $g$ a monotone threshold (majority). By the stated composition property (monotone threshold on top of $t$ DNF blocks), $$ \\mathrm{width}(F_t)\\ \\le\\ t\\cdot \\mathrm{width}(\\tilde H)\\ =\\ O(t\\,m^2). $$ With $m=\\Theta(\\log n)$ and $t=\\Theta(\\log n)$, $$ \\mathrm{width}(F_t)=O\\!\\big((\\log n)^3\\big). $$",
            "From Step 2 and amplification, $$ \\delta_{\\text{block}}=\\Omega(m^{-1/2}),\\qquad t=\\Theta\\big(1/\\delta_{\\text{block}}^{\\,2}\\big)=\\Theta(m)=\\Theta(\\log n), $$ and $$ \\mathrm{width}(F_t)=O\\!\\big((\\log n)^3\\big). $$"
        ],
        "step_count": 8
    },
    "recuVh1OQpudPk": {
        "reasoning_steps": [
            "使用亚致死量 IAV 感染后，小鼠体重短暂下降，支气管肺泡灌洗液（BALF）中炎症细胞增多，病毒载量 6 天达峰后迅速清除。由于病毒激活肺部固有免疫，引发局部炎症，但免疫清除与病毒复制动力学无关，说明炎症本身是关键信号。",
            "IAV 诱导炎症因子 IL-6、IFNγ、TNF 等以及 IL-6-JAK-STAT3 通路激活，同时 DCCs 中胶原蛋白、血管生成和 EMT 相关基因上调。说明病毒感染后，激活炎症因子（尤其是 IL-6）通过 STAT3 信号直接作用于 DCCs，驱动其从休眠的间充质状态（vimentin+）向混合表型（EpCAM+ /vimentin+）转化，促进增殖和转移。",
            "IAV 感染后，肺上皮细胞分泌 IL-6，IL-6-KO 小鼠中 DCCs 扩增被完全阻断；体外实验发现，IL-6 直接刺激乳腺类器官或 EO771 细胞增大，证明其可直接激活休眠细胞增殖。由上述实验结果可得出结论，IL-6 是病毒炎症中驱动 DCCs 唤醒的“开关”，不依赖病毒的持续存在。",
            "IAV 诱导 iBALT 形成（CD4+ /B 细胞聚集），但 CD8+ T 细胞较少。清除 CD4+ T 细胞后，CD8+ T 细胞浸润增加，DCCs 数量减少；双重清除 CD4+ /CD8+ T 细胞后，DCCs 部分恢复，说明 CD4+ T 细胞通过抑制 CD8+ T 细胞毒性以维持转移灶。推测原因，可能是 CD4+ T 细胞通过分泌免疫抑制因子（如 TGF-β）或竞争性耗竭细胞因子，阻碍 CD8+ T 细胞杀伤功能。",
            "使用鼠源化的 SARS-CoV-2 病毒（MA10）感染，同样有依赖 IL-6 扩增的 DCCs。结合 UK Biobank 和 Flatiron Health 数据库中乳腺癌患者在 SARS-CoV-2 或 COVID-19 感染后死亡及肺转移风险的数据，可得出结论：DCCs 唤醒与病毒类型无关，呼吸道病毒通过 IL-6-CD4+ T 细胞轴，系统性促进癌细胞转移，临床数据支持该文献中所用小鼠模型的普适性。"
        ],
        "step_count": 5
    },
    "recuUDt2UA2NuB": {
        "reasoning_steps": [
            "在体内的肿瘤微环境中，因为肿瘤的代谢是高需糖的，所以在肿瘤微环境中的免疫细胞处于一个低葡萄糖含量的环境中。而能量缺乏的状态会抑制细胞的活性。因此自然想到如果可以提高T细胞的代谢活性，可以提高T细胞的抗肿瘤功能",
            "体外实验表明，在肿瘤微环境浸润的微环境会导致T细胞中脂质的异常积累”的原因主要是肿瘤微环境中的低氧和葡萄糖缺乏",
            "TIL伴随肿瘤的进程，其细胞内的脂质积累逐渐升高，并且在第七条观察中，PD-1/Tim-3是肿瘤进程的标志物，于是到此可以推得T细胞中脂质的积累与肿瘤的进程相关，而抑制其脂质的积累对于抑制肿瘤的进程具有积极作用",
            "实验现象“对OT-1细胞体外使用ND-646，发现Malonyl-CoA水平下降，OT-1存活细胞数量与非添加组数目无明显差异，TAG、DAG、PC、PG、PS、PI、PE水平下降，但是SM、LPC水平却显著上升”证明了ND-646是一种ACC1/2的有效抑制剂，同时对于细胞的活性没有明显的影响，正是这个特性保证了下面的实验结果有意义。",
            "实验现象“在OT-1细胞系体外扩增7天后，对是否添加ND-646的两组细胞进行代谢组学分析，共分析了213个代谢物，观察到有差异的有69种代谢物，添加ND-646之后ATP含量升高，Acetyl-CoA，NAD，NADH， FAD含量都有所升高”中，使用ACC1/2的共同抑制剂ND-646，发现脂质的积累下降，说明ACC1/2在脂质积累的进程中起到关键作用",
            "etomoxir的添加可以将ACC的作用局限于脂肪酸的beta氧化。说明ACC抑制剂的添加可以通过脂肪酸的beta 氧化来降低脂质的积累",
            "实验“为了锁定受到影响的代谢通路是哪一条，研究人员将T细胞与肿瘤细胞共培养，意图模拟肿瘤微环境，观察到与正常T细胞相比，糖酵解产生ATP的含量下降，ATP产生的总量下降，线粒体ATP产量下降，添加【物质A】之后，总ATP含量上升，线粒体ATP产量上升，糖酵解产生ATP量上升，添加【物质B】，发现ATP总量重新下降，线粒体ATP产量下降”，物质A和物质B分别是ND-646和etomoxir，首先解释一下逻辑：我们观察到补充ACC的抑制剂，体外培养的肿瘤微环境T细胞能量代谢情况改善，而这种改善可以被etomoxir逆转，进一步说明了ACC抑制剂的添加可以通过脂肪酸的beta 氧化来降低脂质的积累，提高能量代谢效率",
            "实验“将小鼠植入B16F1-OVA细胞系，有如下三组实验：第一组只添加PBS，第二组植入OT-1 CD8+细胞系，第三组添加ND-646和OT-1 CD8+，观察到bc组Tumor size下降，其中c组更为明显，存活率同样”做了动物实验，验证了抑制ACC的活性确实可以直接降低肿瘤的大小，并且提高动物的存活时间",
            "经过上文的推导我们知道我们的目标基因是ACC基因。所以我们要首先营造该基因产物的缺失模型，同时，由第二步实验现象得知，添加物质B之后实验现象被逆转，说明此物质可以部分恢复由ACC缺失造成的脂质积累。而ACC促进脂肪酸合成，抑制ACC脂肪酸合成减少，由此前体物质Acetyl-CoA，NAD，NADH， FAD含量都有所升高，而这些物质可以促进beta氧化，为细胞供能，而逆转这一过程需要的是脂肪酸beta氧化的抑制剂，也就是物质B所需要的性质。",
            "抑制ACC之后供能情况改善，说明主要发生异常的是脂肪酸的beta氧化",
            "综合离体实验和在体实验可知，添加ACC的抑制剂可以改善肿瘤细胞的供能情况，从而改善T细胞抗肿瘤活性"
        ],
        "step_count": 11
    },
    "recuTON94osGxY": {
        "reasoning_steps": [
            "By short verification one finds that the singularity is terminal.",
            "By direct computations in Hochschild cohomology of the mirror one finds that symplectic cohomology of the Milnor fiber indeed has constant ranks for all negative degrees. By the conjecture of Evans and Lekili, one concludes that the singularity really admits crepant resolution.",
            "In fact one may notice that the singularity is related to the canonical cE_7 singularity x^2+y^3+yz^3+w^{18} by weight-one deformations which do not change weight properties of the singularity. The latter however admits crepant resolution by an old result of Brieskorn in 1960s."
        ],
        "step_count": 3
    },
    "recuVstW9ByyDQ": {
        "reasoning_steps": [
            "According to concept_1, solar seawater distillation operates as a closed or semi-enclosed system. Energy efficiency \\eta depends on multiple interacting processes, including solar energy absorption, interfacial phase change, vapor diffusion, and heat dissipation. Because heat and mass transfers are strongly coupled, changes in one environmental factor may indirectly influence others, requiring a systematic evaluation.",
            "We first analyze the effect of solar irradiance (q_{\\mathrm{in}}) (concept_5). Solar irradiance provides the primary energy input for driving interfacial evaporation. Higher q_{\\mathrm{in}} increases the available thermal energy, raising evaporation rates. However, its effective contribution depends on absorptivity, thermal insulation quality, and condensation efficiency, meaning its influence is system-dependent.",
            "Next, we consider the role of ambient temperature (T_{\\mathrm{amb}}) (concept_4). T_{\\mathrm{amb}} determines the thermal boundary conditions between the device and surroundings. A higher T_{\\mathrm{amb}} reduces convective and radiative heat losses, improving thermal retention and enhancing condensation efficiency. Because the system operates within a closed chamber, this effect is amplified compared to open-air evaporation.",
            "The effect of relative humidity (RH) is then evaluated (concept_3). RH influences the vapor concentration gradient between the evaporation surface and the ambient air, which drives diffusion. While higher RH generally reduces this driving force, its overall impact is mitigated in closed or semi-enclosed systems because chamber design, air exchange rates, and condensation strategies regulate internal humidity.",
            "Similarly, wind speed (V_{\\mathrm{wind}}) (concept_2) affects convective heat and mass transfer around the device. Higher V_{\\mathrm{wind}} can enhance vapor removal from the surface but simultaneously increase unwanted convective heat losses. In well-insulated or enclosed systems, the net effect of wind speed remains secondary and highly context-dependent.",
            "Combining these analyses with regression results from the referenced study shows that q_{\\mathrm{in}} and T_{\\mathrm{amb}} have the largest positive coefficients in predicting \\eta, while RH and V_{\\mathrm{wind}} contribute much less. This conclusion arises naturally from the coupled thermodynamic mechanisms described in concept_1 and the regression modeling based on experimental data."
        ],
        "step_count": 6
    },
    "recuU77EtNu30V": {
        "reasoning_steps": [
            "The instruction asks for a tight approximation guarantee for the VCG with copies mechanism (concept_1) in a specific Bayesian setting. The plan is to apply the mechanism, use its general performance guarantee (theorem_1), and then specialize the result using the properties of the Bayesian setting (concept_3, theorem_2).",
            "First, we establish the mechanism's parameters. We use a 1-valid rounding scheme ($q=1$) as it is suitable for unit-demand agents (concept_2). The parameter `r` (number of copying rounds) is chosen strategically to simplify the final bound. Its value will depend on the terms that arise from theorem_2, specifically $r = \\lceil \\log_{2}(K \\cdot \\max\\{1, \\frac{n \\cdot c}{m}\\})\\rceil$ for some constant K.",
            "We start the analysis with the general performance guarantee of the mechanism from theorem_1. Taking the expectation over agent valuations, the surplus is bounded by: $E[\\text{surplus}] \\ge \\frac{1}{r+1}(E[SW(\\mathcal{N},I)] - \\frac{E[SW(\\mathcal{N},2^{r} \\cdot I)]}{2^r})$.",
            "The term $E[SW(\\mathcal{N},2^{r} \\cdot I)]$ represents the welfare with many copies of each item. For unit-demand agents, this value can be no greater than the sum of each agent's valuation for their single favorite item. This gives the upper bound: $SW(\\mathcal{N},2^{r} \\cdot I) \\le \\sum_{i \\in N} v_i(\\{\\mu_v(i)\\})$.",
            "Now, we incorporate the specific Bayesian setting. Concept_3 introduces the no-superstar-item assumption and its defining parameter `c`, noting that it \"influences the final approximation ratio.\" Theorem_2 provides the concrete formula that uses this assumption. By rearranging the inequality in theorem_2, we can relate the sum of favorite-item values back to the total expected social welfare: $E[\\sum v_i(\\{\\mu_v(i)\\})] \\le (\\frac{e}{e-1} \\cdot \\max\\{1,\\frac{n\\cdot c}{m}\\}) \\cdot E[SW(\\mathcal{N},I)]$. This step correctly propagates the parameter `c`.",
            "Substituting the result from Step 5 into the inequality from Step 3 yields an expression that depends only on the expected social welfare and the parameters `n`, `m`, and `c`: $E[\\text{surplus}] \\ge \\frac{E[SW(\\mathcal{N},I)]}{r+1} (1 - \\frac{\\frac{e}{e-1} \\cdot \\max\\{1,\\frac{n\\cdot c}{m}\\}}{2^r})$.",
            "With the strategic choice of $r$ from Step 2 (e.g., setting the constant $K = \\frac{2e}{e-1}$), the term inside the parenthesis becomes a positive constant. The surplus guarantee simplifies to $\\Omega(\\frac{E[SW(\\mathcal{N},I)]}{r+1})$. The approximation ratio is therefore $O(r)$, which is $O(\\log(\\max\\{1, \\frac{n \\cdot c}{m}\\}))$.",
            "Finally, the instruction asks for the answer in a \"simplified, canonical form.\" We use the tool provided in concept_4 to perform this last step. The expression $O(\\log(\\max\\{1, \\frac{n \\cdot c}{m}\\}))$ is asymptotically equivalent to and can be written as $O(\\log(1+\\frac{n\\cdot c}{m}))$, which is the final answer."
        ],
        "step_count": 8
    },
    "recuUsus5CLARr": {
        "reasoning_steps": [
            "Under Huber contamination \\(\\mathcal{P}=(1-\\gamma)P^\\star+\\gamma Q\\), there exists \\(c>0\\) such that \\[\\inf_{\\widehat{\\theta}}\\sup_{\\mathcal{P}}\\mathbb{E}\\big[\\|\\widehat{\\theta}-\\theta^\\star\\|_{\\mathrm{F}}\\big]\\ge c\\,\\gamma.\\]",
            "For rank-\\(r\\) targets, if \\(\\operatorname{rank}(\\rho)\\le r\\) and \\(\\operatorname{rank}(\\sigma)\\le r\\), letting \\(\\Delta=\\rho-\\sigma\\) gives \\[\\operatorname{rank}(\\Delta)\\le 2r.\\]",
            "Schatten conversion applied to \\(\\Delta\\) with \\(\\operatorname{rank}(\\Delta)\\le 2r\\): \\[\\|\\Delta\\|_{1}\\le \\sqrt{2r}\\,\\|\\Delta\\|_{\\mathrm{F}},\\qquad \\|\\Delta\\|_{\\mathrm{F}}\\ge \\frac{\\|\\Delta\\|_{1}}{\\sqrt{2r}}.\\]",
            "Tightness instance (rank-\\(r\\) orthogonal projections \\(P,Q\\) with \\(PQ=0\\)): \\[\\rho=\\tfrac{1}{r}P,\\quad \\sigma=\\tfrac{1}{r}Q,\\quad \\Delta=\\rho-\\sigma,\\qquad \\|\\Delta\\|_{1}=2,\\ \\ \\|\\Delta\\|_{\\mathrm{F}}=\\sqrt{2/r},\\] showing the \\(\\sqrt{2r}\\) factor in step3 is attainable.",
            "For any POVM \\(M\\) and states \\(\\rho,\\sigma\\), \\[d_{\\mathrm{TV}}\\!\\big(M(\\rho),M(\\sigma)\\big)\\le \\tfrac12\\,\\|\\rho-\\sigma\\|_{1},\\] so the lower bound in trace norm controls post-measurement distinguishability.",
            "Combining steps 1–5 yields the minimax lower bound (ignoring polylogarithmic factors and constants) \\[\\inf_{\\widehat{\\rho}}\\sup_{\\mathcal{P}}\\mathbb{E}\\big[\\|\\widehat{\\rho}-\\rho\\|_{1}\\big]\\ \\gtrsim\\ \\gamma\\,\\sqrt{r}.\\]",
            "Let \\(\\varepsilon^\\star(r,\\gamma)\\) denote the minimax achievable error rate in trace distance; using \\(\\tilde{\\Omega}(\\cdot)\\) and \\(\\tilde{\\Theta}(\\cdot)\\) to ignore polylogarithms, \\[\\varepsilon^\\star(r,\\gamma)=\\tilde{\\Omega}\\big(\\gamma\\sqrt{r}\\big).\\] (When a matching constructive upper bound is available, the standard presentation is \\(\\varepsilon^\\star(r,\\gamma)=\\tilde{\\Theta}(\\gamma\\sqrt{r})\\).)"
        ],
        "step_count": 7
    },
    "recuUmpMrJu1l0": {
        "reasoning_steps": [
            "The instruction asks for the complexity of a pre-processing algorithm that reduces the dependency on the parameter `R`. This algorithm is an iterative rounding procedure that consists of an outer loop of geometric annealing and an inner loop for covariance correction.",
            "The outer loop follows a geometric annealing process. Using the step size from Concept_1, we can calculate the number of iterations required to expand the radius from `O(1)` to `R`. The number of iterations is `log_{1+n^{-1/2}}(R)`, which is `O(√n log R)`.",
            "At each iteration of the outer loop, the distribution's near-isotropy is slightly lost, but it remains well-rounded (Concept_2). The Covariance Correction Procedure (Concept_3) is then executed to restore near-isotropy before the next annealing step.",
            "We must determine the query complexity of a single run of this inner correction loop. The complexity depends on the cost of drawing samples to estimate the covariance. The Sampler Cost Function (Theorem_1) states this cost is tied to `||Cov π^X||`.",
            "According to Theorem_2, the query cost for the procedure is `Õ(n^2) * ||Cov π^X||`. Since the procedure is applied to a well-rounded distribution, we can use the bound `||Cov π^X|| = O(n)` from Theorem_3.",
            "By substituting the bound into the cost formula, the complexity for one run of the inner covariance correction loop is `Õ(n^2) * O(n) = Õ(n^3)` queries.",
            "The total complexity of the pre-processing algorithm is the product of the number of outer loop iterations and the complexity of the inner loop per iteration. This yields: `Total Complexity = (Number of Outer Iterations) * (Complexity per Inner Loop) = O(√n log R) * Õ(n^3) = Õ(n^{3.5} log R)`."
        ],
        "step_count": 7
    },
    "recuUhwUJdprf0": {
        "reasoning_steps": [
            "The goal is to determine the sample complexity for tomography using the specific structure of Pauli measurements. While the general worst-case complexity for non-adaptive single-copy tomography is known to be $\\Theta(8^N/\\epsilon^2)$ (concept_1), one cannot simply multiply this by the number of Pauli settings. The unique properties of Pauli measurements allow for a more efficient, specialized analysis.",
            "The strategy is to estimate the coefficients in the Pauli operator representation of the quantum state $\\rho$ (concept_2). We analyze a scheme that performs $m$ measurements for each of the $3^N$ Pauli bases, for a total of $n = m \\cdot 3^N$ samples.",
            "The core principle enabling this efficiency is Simultaneous Information Gain (concept_3). A single measurement contributes to the estimation of many coefficients at once, making the process highly parallel.",
            "This parallelism is quantified by the Effective Sample Amplification (concept_4). The number of effective samples for a Pauli operator of weight $w$ is amplified by a factor of $3^{N-w}$ because it is measured by many different settings.",
            "We then calculate the total expected squared Hilbert-Schmidt error. This is found by summing the variances of the estimators for all individual Pauli coefficients (theorem_1). The variance for each coefficient's estimator is inversely proportional to its amplified sample count.",
            "Performing this summation requires grouping the Pauli operators by weight $w$. For each weight, there are $\\binom{N}{w}3^w$ operators. The calculation must also correctly apply the normalization factor relating the Pauli basis to the Hilbert-Schmidt inner product to avoid errors (theorem_1).",
            "The resulting combinatorial sum, when evaluated correctly using the binomial theorem, yields a total error variance whose dominant term is proportional to $10^N$, not a term involving $8^N$. This demonstrates the significant advantage of using the specific structure of Pauli measurements.",
            "This bound on the expected Hilbert-Schmidt error is then converted into a bound on the more physically relevant trace distance.",
            "Finally, MacDiarmid's Inequality (theorem_2) is used to transform the bound on the *expected* error into a statement about the error being small with high probability ($1-\\delta$), which introduces the final $log(1/\\delta)$ factor into the complexity."
        ],
        "step_count": 9
    },
    "recuUgqZ5kZDXa": {
        "reasoning_steps": [
            "The primary strategy, as highlighted in the instruction, is to not tackle the high-dimensional UFL problem on the entire dataset $X$ at once. Instead, we use the metric decomposition procedure to partition $X$ into a set of smaller, more manageable sub-problems, represented by the clusters in $\\Lambda$.",
            "The key property of this decomposition, given by Theorem_1, is that the UFL cost for each individual cluster $C \\in \\Lambda$ is bounded from above. Specifically, $ufl(C) \\le \\tau$, where the complexity bound $\\tau$ is a function of the data's intrinsic dimensionality: $\\tau = (ddim/\\epsilon)^{O(ddim)}$.",
            "To perform dimension reduction on these clusters, we reframe the UFL problem as a k-median problem. Theorem_2 provides this bridge: solving a UFL instance on a cluster $C$ with a cost bounded by $\\tau$ is equivalent to solving a k-median instance where the number of centers, $k$, is also bounded by $\\tau$.",
            "Now that each sub-problem is a k-median instance with at most $k=\\tau$ centers, we can apply the known dimension reduction result for k-median from Theorem_3. This theorem states that a sufficient target dimension to preserve the k-median cost is $m = O(\\epsilon^{-2} \\log k)$.",
            "By substituting the bound on $k$ from the previous step ($k \\le \\tau$), we find the sufficient dimension for each of our UFL clusters: $m = O(\\epsilon^{-2} \\log \\tau)$.",
            "Finally, we substitute the definition of $\\tau$ from Theorem_1 into this equation to get the final answer. - We start with $m = O(\\epsilon^{-2} \\log \\tau)$. - We know $\\tau = (ddim/\\epsilon)^{O(ddim)}$. - Therefore, $\\log \\tau = \\log\\left((ddim/\\epsilon)^{O(ddim)}\\right) = O(ddim) \\cdot \\log(ddim/\\epsilon)$. - Substituting this back into the equation for $m$ yields: $m = O(\\epsilon^{-2} \\cdot ddim \\cdot \\log(ddim/\\epsilon))$.",
            "This dimension $m$ is sufficient for any single cluster. A complete proof, which also uses Theorem_4 and Theorem_5, would show that this dimension (with an additional logarithmic factor in $\\delta^{-1}$ to handle the overall success probability) is sufficient for the entire collection of clusters, thus preserving the global UFL value for $X$."
        ],
        "step_count": 7
    },
    "recuUdBXyvpTL2": {
        "reasoning_steps": [
            "Free Vortex Relation  \\[ v \\cdot r = \\text{constant} \\]  or expressed as:  \\[ v(r) \\cdot r = c \\] where \\( v \\) is velocity, \\( r \\) is radius, and \\( c \\) is a constant.",
            "Density Variation with Radius  \\[ \\frac{\\rho}{\\rho_0} = \\left[1 - \\frac{\\gamma - 1}{2} \\cdot \\frac{1}{(r / r_0)^2}\\right]^{1/(\\gamma - 1)} \\] where:  - \\( \\rho \\) is density,  - \\( \\rho_0 \\) is stagnation density,  - \\( \\gamma \\) is the specific heat ratio of the gas,  - \\( r \\) is radius,  - \\( r \\) is the reference radius.",
            "Reference Radius Relation  \\[ r = r_0 \\frac{\\sqrt{X}}{M} \\] where:  \\[ X = 1 + \\frac{(\\gamma - 1) M^2}{2} \\] and the Mach number \\( M \\) corresponds to the local Mach number at a specific radius of the vortex nozzle exit.",
            "Mass Flow Rate Derivation  From the above relations, the product of density and velocity at the free vortex nozzle exit can be derived as:  \\[ \\rho \\cdot v = \\frac{c \\cdot \\rho_0}{r} \\left[1 - \\frac{\\gamma - 1}{2} \\cdot \\frac{r_0^2}{r^2}\\right]^{1/(\\gamma - 1)} \\]"
        ],
        "step_count": 4
    },
    "recuUa1L1IsODl": {
        "reasoning_steps": [
            "The objective is to determine the minimum number of iterations, T, for no-regret learners to find a near-optimal Coarse Correlated Equilibrium (CCE) with polynomially small error gaps ($poly(1/n)$).",
            "The key insight is to leverage the direct link between the number of learning iterations and the resulting equilibrium's structure. Concept_3 states that T iterations of no-regret learning produce a T-sparse CCE. This transforms the question about iteration complexity into one about the computational complexity of finding a T-sparse CCE.",
            "To establish a lower bound on T, the paper proves that the problem of finding a near-optimal T-sparse CCE is computationally intractable for small T. This is achieved through a reduction from a known NP-hard problem.",
            "The source of hardness is the Maximum Clique (MAXCLIQUE) problem. Specifically, Theorem_1 establishes the strong inapproximability result that it is NP-hard to approximate MAXCLIQUE within a factor of $n^{1-\\epsilon}$.",
            "Theorem_2 provides the core of the argument by constructing a polynomial-time reduction from MAXCLIQUE to the problem of finding an optimal T-sparse CCE. This reduction demonstrates that a polynomial-time algorithm capable of finding a T-sparse near-optimal CCE could be used to produce a $2T$-approximation for MAXCLIQUE.",
            "Combining the inapproximability result with the reduction, we deduce the required sparsity. If finding a T-sparse CCE were possible in polynomial time for $T < \\frac{n^{1-\\epsilon}}{2}$, it would imply a polynomial-time algorithm for approximating MAXCLIQUE with a factor of $2T < n^{1-\\epsilon}$. This would contradict the NP-hardness of MAXCLIQUE, thus establishing a lower bound of $T = \\Omega(n^{1-\\epsilon})$.",
            "To assess the tightness of this lower bound, we consider upper bounds on the required sparsity. Concept_4 provides a general existence result, showing that every game has a CCE representable as a mixture of a polynomially-bounded number of product distributions, indicating the problem is not arbitrarily hard.",
            "Furthermore, Concept_5 shows that computing an optimal CCE is tractable for many classes of games, including the normal-form games in the instruction, provided a related separation problem can be solved efficiently. For n x n games, an optimal CCE can be found via linear programming, and any resulting distribution is trivially n-sparse, setting a firm upper bound of $T=n$.",
            "By synthesizing the lower bound of $\\Omega(n^{1-\\epsilon})$ (from the hardness argument) and the upper bound of $T=n$, we conclude that the minimum number of required iterations is tightly bracketed and scales nearly linearly with the number of actions, n."
        ],
        "step_count": 9
    },
    "recuU4MLVb1QhU": {
        "reasoning_steps": [
            "The fundamental strategy for proving the space lower bound is rooted in information theory. The space an algorithm uses must be at least as large as the mutual information between its memory state and the input stream it has processed (concept_1).",
            "The instruction specifies the problem setup: a single, complex data stream is constructed to embed $\\Theta(log(\\epsilon^{2}n))$ distinct, scaled instances of the Exam Disjointness (EDISJ) problem. Solving the $F_2$ estimation on this entire stream requires implicitly solving all the embedded sub-problems.",
            "We first analyze the cost of a single one of these sub-problems. To successfully solve one embedded EDISJ instance through the $F_2$ reduction, the streaming algorithm must store at least $\\Omega(1/\\epsilon^{2})$ bits of information about the part of the stream corresponding to that instance (theorem_1).",
            "The next critical step is to combine the costs of all the instances. The paper's direct sum argument is powerful because it applies even to dependent, overlapping instances, proving that the individual information costs of the $\\Theta(log(\\epsilon^{2}n))$ instances are additive (theorem_2).",
            "The total lower bound is therefore the product of the number of instances and the information cost per instance (theorem_3). Multiplying the number of instances, $\\Theta(log(\\epsilon^{2}n))$, by the cost of each, $\\Omega(1/\\epsilon^{2})$, yields the final space complexity lower bound of $\\Omega(log(\\epsilon^{2}n)/\\epsilon^{2})$."
        ],
        "step_count": 5
    },
    "recuU1PyfWcNni": {
        "reasoning_steps": [
            "The objective is to find the specific upper bound on the expected maximum load. The core strategy is to use a Potential Function (concept_1) to measure the imbalance in the distribution of balls, which is generated by a process of incrementally constructing the hashing function's kernel (concept_2).",
            "A critical choice is the potential function's base. Since the goal is to find an upper bound on the maximum load, the base b must be greater than 1. This is because a function like $b^{\\text{load}}$ with $b>1$ grows exponentially and heavily penalizes bins with a high load, making the potential function sensitive to the existence of \"heavy bins\". The alternative, $b<1$, would be used for finding lower bounds on load (i.e., detecting empty bins), which is outside the scope of this specific instruction.",
            "The chosen potential function (with $b>1$) is shown to satisfy two key mathematical properties: the Conditional Expectation Bound (theorem_2) and the Monotonicity Property (theorem_3). These properties are the necessary preconditions for applying the main tail bound theorem.",
            "With the preconditions met, the Quadratically Decaying Tail Bound (theorem_4), which gives a bound with constant 48 for the idealized surjective case, can be applied. This result is then extended to the general case of all linear maps (as mentioned in the Instruction), yielding the final, usable tail probability bound for the maximum load: $Pr[M(S,h) \\ge r \\cdot OPT(n,n)] \\le \\frac{49}{(r-2)^2}$. This connection is formally made using the Potential to Max-Load Relationship (theorem_1).",
            "To calculate the expected value, the formula for Expectation from Tail Probability (concept_4) is used: $\\mathbb{E}[M] = \\int_{0}^{\\infty} Pr[M \\ge t] dt$. The paper's proof strategy splits this integral to make it tractable. For the initial part of the distribution, the probability is bounded by 1, and for the tail, the derived bound is used. This results in the inequality: $\\mathbb{E}[M] \\le 9 \\cdot OPT(n,n) + \\int_{9 \\cdot OPT(n,n)}^{\\infty} Pr[M \\ge t] dt$",
            "To solve the integral over the tail, a change of variables is performed where $t = r \\cdot OPT(n,n)$. This transforms the integral into a function of the scaling factor $r$: $\\mathbb{E}[M] \\le 9 \\cdot OPT(n,n) + OPT(n,n) \\int_9^\\infty Pr[M \\ge r \\cdot OPT(n,n)]dr$",
            "Now, the specific tail bound from Step 4 is substituted into the integral: $\\text{Integral Part} = OPT(n,n) \\int_9^\\infty \\frac{49}{(r-2)^2} dr$ This definite integral is then calculated: $\\int_9^\\infty \\frac{49}{(r-2)^2} dr = 49 \\left[ -\\frac{1}{r-2} \\right]_9^\\infty = 49 \\left( 0 - (-\\frac{1}{9-2}) \\right) = 49 \\cdot \\frac{1}{7} = 7$",
            "The value of the integral, 7, is substituted back. The total expectation is the sum of the two parts: $\\mathbb{E}[M] \\le 9 \\cdot OPT(n,n) + OPT(n,n) \\cdot 7 = (9+7) \\cdot OPT(n,n) = 16 \\cdot OPT(n,n)$",
            "Finally, this is how the Optimal Max-Load Benchmark (concept_3) is used. The term $OPT(n,n)$ is substituted with its definition, $OPT(n,n) = \\frac{\\log n}{\\log \\log n}$, to yield the final, specific upper bound for the expected maximum load: $16 \\cdot \\frac{\\log n}{\\log \\log n}$"
        ],
        "step_count": 9
    },
    "recuU0jxkN7NKX": {
        "reasoning_steps": [
            "The goal is to find the optimal rate for online multicalibration in terms of ECE. The problem is approached by instantiating the general Unbiased Prediction framework.",
            "The continuous prediction space $[0,1]$ is discretized into $m$ uniform buckets. All predictions $p_t$ that fall within a bucket are effectively reassigned to the bucket's center point $P_i$. This introduces a discretization error.",
            "For each group $G \\in \\mathcal{G}$ and each bucket $i \\in [m]$, a specific binary group-calibration event, $E_{G,i}$, is defined. This event is triggered when the context $x_t$ belongs to group $G$ and the prediction $p_t$ falls into bucket $i$.",
            "The Unbiased Prediction algorithm is run with this collection of $m \\cdot |\\mathcal{G}|$ events. The algorithm guarantees that the bias for each event $E_{G,i}$ is low.",
            "The total ECE for any group $G$ is decomposed into two parts: the discretization error and the cumulative bias. The discretization error, resulting from grouping predictions into $m$ buckets, is of the order $O(T/m)$.",
            "The cumulative bias part is the sum of the conditional biases over all $m$ buckets for that group. Using the bias bound from the Unbiased Prediction algorithm (theorem_1), the bias for each bucket $i$ is $O(\\sqrt{n_{i,G}})$, where $n_{i,G}$ is the number of times the event $E_{G,i}$ occurred.",
            "In the worst-case scenario, the sum of these bias bounds over all $m$ buckets is $O(m\\sqrt{T/m})$, which simplifies to $O(\\sqrt{Tm})$.",
            "Combining the two error terms, the total ECE is bounded by $O(T/m + \\sqrt{Tm})$.",
            "To find the optimal rate, the number of buckets, $m$, is tuned to balance the two error terms. By setting $T/m = \\sqrt{Tm}$, we solve for $m$ and find the optimal value is $m = T^{1/3}$.",
            "Substituting $m = T^{1/3}$ back into the total error expression yields a final achievable rate of $O(T/T^{1/3} + \\sqrt{T \\cdot T^{1/3}}) = O(T^{2/3})$."
        ],
        "step_count": 10
    },
    "recuSCdIJdYM74": {
        "reasoning_steps": [
            "Consider the first case in Theorem 1 where $f(T) \\mod p$ is not a permutation polynomial over $\\mathbb{Z}/p\\mathbb{Z}$. Using Theorem 2 and Hensel's lemma, we compute that the number of elements in the set $\\{f(a) : a \\in R\\}$ is at most $p^r - p^{r-1}$.",
            "Consider the second case in Theorem 1 where $f(T) \\mod p$ is a permutation polynomial over $\\mathbb{Z}/p\\mathbb{Z}$, but there exists $a \\in \\mathbb{Z}$ such that $f'(a) \\equiv 0 \\mod p$. Using Theorem 2 and Hensel's lemma, we compute that the number of elements in the set $\\{f(a) : a \\in R\\}$ is at most $p^r - p^{r-1} + p^{r-2}$.",
            "Provide examples that achieve these bounds. Generally, a strict example is $T^{2p-1} + pT$, and a specific example is $p=2$, $r=3$, $f(T) = T^3 + 2T$."
        ],
        "step_count": 3
    },
    "recuSD5kzdUbsY": {
        "reasoning_steps": [
            "Perturbed system equivalence: Rewrite the suboptimal closed-loop as “nominal closed-loop + disturbance.” Define w_k := g(k,x_k,u_k^{\\mu}) - g(k,x_k,\\mu_k^\\star(x_k)). By Lipschitz continuity (concept_2), we obtain \\| w_k \\| \\le L_u \\, \\| u_k^{\\mu} - \\mu_k^\\star(x_k) \\|.",
            "State deviation via E-\\delta-ISS: Apply concept_1 to convolve { w_k } and obtain an exponential-type upper bound on the state deviation between the two closed-loop trajectories. This gives that \\sum_k \\| x_k - x_k^\\star \\| is controlled by \\sum_k \\| u_k^{\\mu} - \\mu_k^\\star(x_k) \\|, with parameters \\rho, c_w.",
            "Bounding R_T through cost Lipschitzness: Using concept_2 (M_x, M_u), the sum of state deviations plus input deviations is linearly bounded by the cost difference: R_T \\lesssim M_x \\sum_k \\| x_k - x_k^\\star \\| + M_u \\sum_k \\| u_k^{\\mu} - \\mu_k^\\star(\\cdot) \\|.",
            "Compressing the cumulative input error: With the linear convergence recursion from concept_4, combined with the Lipschitz constant L of \\mu_k^\\star (concept_2) and the bounded temporal variation a_k (concept_3), we obtain \\sum_{k=0}^{T-1} \\| u_k^{\\mu} - \\mu_k^\\star(x_k) \\| \\le \\tilde{\\eta}_0 \\| \\delta u_0 \\| + (a + L \\Delta)^{\\top} \\tilde{\\eta}, where \\Delta := [\\|\\delta x_1\\|,\\ldots,\\|\\delta x_{T-1}\\|]^{\\top}.",
            "Combining constants: Substituting the bounds from steps 2–4 and simplifying gives R_T(x_0) \\le \\bar{M}\\left( \\tilde{\\eta}_0 \\| \\delta u_0 \\| + (a + L \\Delta)^{\\top} \\tilde{\\eta} \\right), where \\bar{M} = M_u + \\frac{c_w L_u (M_u L + M_x)}{1 - \\rho}."
        ],
        "step_count": 5
    },
    "recuTOKSHoRCQa": {
        "reasoning_steps": [
            "Direct computations (perhaps need some experience to find the theory underlying the physical spectra)"
        ],
        "step_count": 1
    },
    "recuU3aCu4ZpA1": {
        "reasoning_steps": [
            "By concept_1, the algebra $\\Lambda$ is geometrically irreducible if the connected component of the representation space $mod_\\Lambda(d)$ is irreducible. So combining theorem_3, a promising CoT is to find some geometrically irreducible $A=kQ/\\left<R\\right>$ from $A'$ such that $A$ is geometrically irreducible and the representations of $A$ are closely related to representations of $A'$.",
            "Without loss of generality, we may assume $\\{\\rho\\in R'|deg \\rho=0\\}=\\{\\epsilon_i^{m_i}|i\\in Q_0''\\}$ for $m_i\\geq 2$ and $Q_0''\\subset Q_0'$, because we can reduce the cases for $m_i=1$ by deleting the loops in $Q'$. ",
            "Choose $m_i\\geq 2$ for each $i\\notin Q_0''$ and consider the relation $R=R'\\cup \\{\\epsilon_i^{m_i}|i\\notin Q_0''\\}$. The representation space of $A=kQ/\\left<R\\right>$ is a product of the representation spaces of $A'$ and of $k[x]/(x^{m_i})$ for $i\\notin Q_0''$. Since $A'$ is geometrically irreducible, and $mod_{k[x]/(x^{m_i})}(d)$ is the $d\\times d$ matrices $X$ with $X^{m_i}=0$ which implies $k[x]/(x^{m_i})$ is geometrically irreducible as well, we obtain that $A$ is geometrically irreducible.",
            "By theorem_3, we have $deg \\rho\\in\\{0,1\\}$ for any $\\rho\\in R$, so $deg \\rho \\in\\{0,1\\}$ for any $\\rho \\in R'$. Next, we need specific examples for $deg \\rho=0,1$ to confirm these are indeed possible values.",
            "By theorem 2, the minimal set of relations contains both $deg \\rho =0$ and $deg \\rho=1$. So the answer is $\\{0,1\\}."
        ],
        "step_count": 5
    },
    "recuUcwjm3KkzX": {
        "reasoning_steps": [
            "Assume that the target function in the optimization problem satisfies L-smoothness (Concept_1), i.e., for any noise \\(\\xi\\), \\lVert \\nabla f(x, \\xi) - \\nabla f(y, \\xi) \\rVert_2 \\leq L \\lVert x - y \\rVert_2, for all \\(x, y\\). This ensures bounded gradient variation, facilitating the analysis of function value descent and the construction of a Lyapunov function for convergence analysis in non-convex settings.",
            "Further assume that the stochastic gradient has bounded variance (Concept_2), i.e., there exists a positive number \\(\\sigma\\) such that \\mathbb{E} \\left[ \\lVert \\nabla f(x, \\xi) - \\nabla F(x) \\rVert_2^2 \\right] \\leq \\sigma^2, where \\(\\nabla F(x) = \\mathbb{E}[\\nabla f(x, \\xi)]\\). This controls the noise level of gradient estimates, enabling variance reduction techniques to reduce overall variance and improve convergence speed.",
            "Recall the convergence of adaptive gradient methods like Adam in non-convex optimization (Theorem_1), which achieve a convergence rate of \\(O(T^{-1/2})\\) to first-order stationary points, i.e., \\min_{t=1,\\dots,T} \\mathbb{E}[\\lVert \\nabla F(x_t) \\rVert^2] \\leq O(T^{-1/2}). This provides a baseline, but due to high variance issues, the MARS framework aims to surpass this rate by integrating variance reduction.",
            "The MARS framework introduces Stochastic Recursive Momentum (STORM, Concept_3), a variant of standard momentum that achieves variance reduction by adding an additional term. In non-convex optimization, STORM attains a gradient complexity of \\(O(\\epsilon^{-3})\\) (without requiring snapshot points), corresponding to a convergence complexity of \\(T = O(\\epsilon^{-3/2})\\) for \\(\\mathbb{E}[\\lVert \\nabla F(x_t) \\rVert^2] \\leq \\epsilon\\), implying a baseline rate of \\(O(1/T^{2/3})\\). MARS adjusts the STORM momentum with a scaling parameter \\(\\gamma\\) and incorporates preconditioned gradients (e.g., Adam-style). Under the aforementioned assumptions, MARS constructs a tighter bound, ultimately deriving an improved convergence rate of O\\left(\\frac{\\log T}{T^{2/3}}\\right)."
        ],
        "step_count": 4
    },
    "recuUd6nfsVgSJ": {
        "reasoning_steps": [
            "First, based on concept_1, we clearly understand the model's composition, including the equations governing fluid flow, structural motion, and the coupling conditions at the interface. This lays the foundation for subsequent analysis, as we need to target the specific characteristics of this model for proof.",
            "Next, we use theorem_1 (Galerkin Approximation Method). We construct finite-dimensional approximate solutions by expanding the unknown functions (such as fluid velocity u, pressure p, temperature \\(\\rho\\), structural displacement w, and temperature \\(\\theta\\)) with basis functions. This converts the original partial differential equation problem into a system of ordinary differential equations. By solving this system of ordinary differential equations, we obtain approximate solutions, which is the first step in proving the existence of solutions.",
            "Then, theorem_2 (Compactness Theorem) comes into play. Since the approximate solutions obtained by the Galerkin method are bounded in the corresponding function spaces and meet certain conditions, there exists a strongly convergent subsequence. By taking the limit of this subsequence, we can transition from the approximate solutions to the exact weak solutions of the original problem, proving the existence of weak solutions.",
            "After obtaining the weak solutions, we need to prove their boundedness and stability, which relies on theorem_3 (Gronwall's Inequality). By establishing appropriate energy functionals and deriving differential inequalities satisfied by these functionals, we apply Gronwall's Inequality to obtain estimates of the energy functionals. This shows that the energy remains bounded over time, ensuring the stability of the solutions and preventing blow-up, which is crucial for proving global existence.",
            "Finally, to prove the existence of smooth solutions, we use theorem_4 (Difference Quotient Method). By defining difference quotients, we estimate the derivatives of the solutions. This helps us prove the regularity of the solutions, that is, the solutions have higher-order derivatives, thus upgrading the weak solutions to smooth solutions. Combined with the global existence of weak solutions, we can conclude that the smooth solutions also exist globally."
        ],
        "step_count": 5
    },
    "recuUhtuMyEPmC": {
        "reasoning_steps": [
            "Determine the general form of the massless celestial Berends-Giele current for a single point. According to concept_2, the expression for the celestial BG current \\(\\mathcal{A}_{P\\mu}\\) is: \\(\\mathcal{A}_{P\\mu}(\\Delta_{i},p_{n+1})=\\int \\prod_{i=1}^{n} d\\omega_{i}\\omega_{i}^{\\Delta_{i}-1}A_{P\\mu}\\delta^{(4)}(\\sum_{i=1}^{n}\\omega_{i}\\hat{q}_{i}-p_{n+1})\\). For a single gluon, the set \\(P\\) contains only one element (i.e., \\(P=i\\)), corresponding to \\(n=1\\), so the product integral reduces to a single variable integral, and the off-shell particle momentum is denoted as \\(p\\) (simplified from \\(p_{n+1}\\)). The formula simplifies to: \\(\\mathcal{A}_{i\\mu}=\\int d\\omega_{i}\\omega_{i}^{\\Delta_{i}-1}A_{i\\mu}\\delta^{(4)}(\\omega_{i}\\hat{q}_{i}-p)\\).",
            "Substitute the specific form of the 4-dimensional flat spacetime single-point BG current. According to concept_3, when \\(P=i\\), the 4-dimensional BG current \\(A_{i\\mu}=\\epsilon_{i\\mu}\\) (where \\(\\epsilon_{i\\mu}\\) is the gluon polarization vector, independent of the integration variable \\(\\omega_i\\), and can be factored out of the integral). Substituting this into the formula from step 1, we get: \\(\\mathcal{A}_{i\\mu}=\\epsilon_{i\\mu}\\int d\\omega_{i}\\omega_{i}^{\\Delta_{i}-1}\\delta^{(4)}(\\omega_{i}\\hat{q}_{i}-p)\\).",
            "Use the parameterization of the delta function to replace the momentum conservation term. According to concept_1, the massless particle momentum \\(\\hat{q}_i^\\mu\\) satisfies \\(q_i^\\mu=\\omega_i\\hat{q}_i^\\mu\\), and the off-shell particle momentum \\(p^\\mu=m\\hat{p}^\\mu\\), with their momentum matching condition described by the delta function. Using the hint, \\(\\delta^{(4)}(\\omega_{i}\\hat{q}_{i}-p)=\\frac{y^2}{m^3}\\delta(\\frac{m}{2y}-\\omega_{i})\\delta(y)\\delta^{(2)}(w-z_{i})\\), substituting this into the integral from step 2, we get: \\(\\mathcal{A}_{i\\mu}=\\epsilon_{i\\mu}\\int d\\omega_{i}\\omega_{i}^{\\Delta_{i}-1}\\cdot\\frac{y^2}{m^3}\\delta(\\frac{m}{2y}-\\omega_{i})\\delta(y)\\delta^{(2)}(w-z_{i})\\).",
            "Use the sifting property of the delta function to compute the integral. According to the sifting property of the delta function \\(\\int f(x)\\delta(x-a)dx=f(a)\\), the integration variable \\(\\omega_i\\) is selected by \\(\\delta(\\frac{m}{2y}-\\omega_{i})\\) to \\(\\omega_i=\\frac{m}{2y}\\). The integral only retains the value of the function at this point (\\(\\delta(y)\\) and \\(\\delta^{(2)}(w-z_i)\\) are independent of \\(\\omega_i\\) and can be factored out of the integral): \\(\\int d\\omega_{i}\\omega_{i}^{\\Delta_{i}-1}\\delta(\\frac{m}{2y}-\\omega_{i})=\\left(\\frac{m}{2y}\\right)^{\\Delta_i-1}\\). Substituting this into the expression from step 3, the integral result is: \\(\\epsilon_{i\\mu}\\cdot\\frac{y^2}{m^3}\\cdot\\left(\\frac{m}{2y}\\right)^{\\Delta_i-1}\\cdot\\delta(y)\\delta^{(2)}(w-z_i)\\).",
            "Simplify the coefficients to obtain the final formula. Simplify the coefficient part algebraically: \\(\\frac{y^2}{m^3}\\cdot\\left(\\frac{m}{2y}\\right)^{\\Delta_i-1}=\\frac{y^2}{m^3}\\cdot\\frac{m^{\\Delta_i-1}}{2^{\\Delta_i-1}y^{\\Delta_i-1}}=\\frac{m^{\\Delta_i-4}y^{3-\\Delta_i}}{2^{\\Delta_i-1}}\\). Further organize the exponent terms: \\(2^{\\Delta_i-1}=4\\cdot2^{\\Delta_i-3}\\), \\(y^{3-\\Delta_i}=\\frac{1}{y^{\\Delta_i-3}}\\), thus: \\(\\frac{m^{\\Delta_i-4}y^{3-\\Delta_i}}{2^{\\Delta_i-1}}=\\frac{1}{4}\\cdot\\frac{m^{\\Delta_i-4}}{(2y)^{\\Delta_i-3}}\\). Combining all terms, the final formula for the single gluon celestial BG current is: \\(\\mathcal{A}_{i\\mu}=\\frac{1}{4}\\epsilon_{i\\mu}\\frac{m^{\\Delta_{i}-4}}{(2y)^{\\Delta_{i}-3}}\\delta(y)\\delta^{(2)}(w-z_i)\\)."
        ],
        "step_count": 5
    },
    "recuUlqESjmP1V": {
        "reasoning_steps": [
            "Starting from the linear density of states (DOS) of Dirac materials: In the graphene source region, LDOS/DOS is close to zero at the Dirac point and increases linearly with energy deviation; when the energy is close to E_Dirac, there are very few states available for injection.",
            "Further 'opening a bandgap' in the Dirac source to suppress thermal tails: Inducing a ~200 meV bandgap in the graphene source blocks the direct thermal excitation channels from the valence band to the conduction band, significantly reducing the OFF-state thermal tail current and enhancing the cold source effect.",
            "According to the ballistic transport characteristics, the injection characteristics of particles in the source segment can be completely retained in the channel.",
            "Based on the Landauer–Büttiker formula, a quantitative relationship is established between DOS and injection current: \\(I_d \\propto \\int_{\\phi_b}^{\\infty} D_s(E)\\, f_s(E)\\, dE\\). In the subthreshold region, \\( f_s(E) \\approx \\exp\\!\\left(-\\frac{E - E_{fs}}{kT}\\right) \\), while the DOS satisfies: \\( D_s(E) \\propto (E - E_c)^{-\\tfrac{1}{2}} \\). Therefore, the current expression can be approximated as: \\( I_d \\propto (\\delta \\phi)^{-\\tfrac{1}{2}} \\exp\\!\\left(-\\tfrac{\\delta \\phi}{kT}\\right) \\), where \\( \\delta \\phi = \\phi_b - E_c \\) represents the offset of the barrier height relative to the conduction band edge.",
            "Combining two priors: the low-pass filtering brought by linear DOS and the secondary suppression of thermal tails by the source-side bandgap — under given device/bias conditions (Lg≈15 nm, Vd≈0.3 V) and ballistic conditions, the minimum SS converges to a lower scalar value. The expression for SS here is: \\( SS = \\dfrac{\\tfrac{kT}{q}\\ln(10)}{1 + \\tfrac{kT}{2(\\delta \\psi)}} \\). When the source-side bandgap Eg≈200 meV, Lg≈15 nm, Vd≈0.3 V, and under ballistic limit, \\( \\delta \\phi = 200 \\, \\text{meV} \\) is used for estimation."
        ],
        "step_count": 5
    },
    "recuUoVRNYjo9k": {
        "reasoning_steps": [
            "Recall Definition 1 of an \\((M, N, L, Z)\\)-ZCCS, which specifies the aperiodic auto- and cross-correlation properties: \\(\\rho(C^p, C^t; u) = NL\\) when \\(u=0\\) and \\(p=t\\); \\(\\rho(C^p, C^t; u) = 0\\) when \\(0 < |u| < Z\\) and \\(p=t\\); and \\(\\rho(C^p, C^t; u) = 0\\) when \\(|u| < Z\\) and \\(p \\neq t\\). These properties characterize the correlation constraints that the code set must satisfy.",
            "Consider Concept 1 (Lemma 1) regarding a matrix \\(X\\) of size \\(\\bar{L} \\times \\bar{M}\\) with column vectors \\(s_v\\), where each column has energy \\(E = \\sum_{l=0}^{\\bar{L}-1} |s_{l,v}|^2\\). The lemma provides an inequality involving the inner products of distinct column vectors and the sum of squared magnitudes of all pairwise inner products.",
            "Construct a matrix \\(X\\) corresponding to the \\((M, N, L, Z)\\)-ZCCS, where \\(\\bar{M} = MZ\\) and \\(\\bar{L} = N(L + Z - 1)\\). The column vectors \\(s_{pZ+u}\\) are defined using cyclic shifts of extended sequences from the ZCCS, ensuring each column has energy \\(E = NL\\) (consistent with the auto-correlation property at \\(u=0\\) in Definition 1).",
            "Apply the correlation properties from Definition 1 to evaluate the inner products of column vectors in \\(X\\). For distinct columns (\\(v \\neq t\\)), the inner product is 0 due to the zero correlation constraints; for the same column (\\(v = t\\)), the inner product equals the column energy \\(NL\\).",
            "Substitute these inner product values into the inequality from Concept 1. Through algebraic manipulation, using the energy of columns and the zero correlation conditions, simplify the inequality to derive a relationship between \\(M, N, L, \\) and \\(Z\\).",
            "The simplified inequality leads to \\(MNLZ - (MZ)^2 \\geq 0\\), which reduces to \\(M \\leq \\frac{NL}{Z}\\). Since \\(M\\) must be an integer, the upper bound is \\(M \\leq \\left\\lfloor \\frac{NL}{Z} \\right\\rfloor\\)."
        ],
        "step_count": 6
    },
    "recuUrJDg1REHU": {
        "reasoning_steps": [
            "Formalize the setting. Open CRNs in dilute solution obey mass-action dynamics dz/dt = S j(z) + I(z). Openness is implemented via chemostatting of a subset of species, with three control modes: concentration control, flux control, and mixed control (CSTR is a special mixed case). This establishes the two levers external to kinetics: how the system is opened and which reactions/species exist.",
            "Define ‘growth’. Indefinite growth is the divergence of at least one concentration and of the mass density L_m; equivalently, lim_{t→∞} L_m(t) = ∞. Since the free energy then diverges, closed CRNs cannot grow; only open ones may, and the chemostatting choice becomes crucial.",
            "Introduce thermodynamics tailored to growth. Split the chemical work into moiety work ẇ_m and nonconservative work ẇ_nc to obtain the second-law form TΣ̇ = ẇ_nc + ẇ_m − d_t G ≥ 0. This decomposition provides the thermodynamic parameters that quantify dissipation and energy storage, and will underlie efficiency statements.",
            "Assess the impact of the chemostatting mechanism on the possibility of growth. (i) CSTR/mixed with uniform extraction implies d_t L_m = −k_e L_m + const ⇒ bounded L_m (no growth); (ii) Flux control with a positive net influx gives d_t L_m = l_m·Ĩ > 0 and thus at-most-linear divergence of concentrations. Therefore, whether growth can occur at all depends on the control type.",
            "Bring in network topology—unimolecular class. For unimolecular CRNs, the linear dynamics under any control except flux leads to no growth. Under flux control, one finds equilibrium-type linear growth: concentrations track a moving equilibrium up to an O(1) offset. Thus, topology (unimolecular) plus control (flux) fixes both possibility and type (equilibrium linear).",
            "Quantify efficiency in the unimolecular case via thermodynamic parameters. As t→∞, TΣ̇ ~ O(t^−1) while d_t G ~ ẇ_m ~ O(ln t), so η = d_t G / ẇ_c → 1. Hence, the same two structural factors set the growth regime, while efficiency is read off from the second-law decomposition.",
            "Topology—pseudo-unimolecular class. These grow only under flux control, but the long-time state is a moving NESS; dissipation scales extensively TΣ̇ ~ O(t), ẇ_m ~ O(ln t), giving η → 0. Again, the combination of topology (pseudo-unimolecular) and control (flux) sets possibility/type; thermodynamic parameters determine efficiency.",
            "Topology—multimolecular class under concentration control. For weakly reversible multimolecular networks, the expectation is no growth under concentration control; this has been proven for classes and numerically verified for several models. Thus, the network’s structural property (weak reversibility) combined with control excludes growth.",
            "Multimolecular under mixed/flux control. Some multimolecular CRNs can grow under mixed control; in such regimes, the second-law splitting persists (TΣ̇ ~ ẇ_nc ≥ 0, d_t G ~ ẇ_m > 0), and efficiencies can lie strictly between 0 and 1—hence efficiency is a thermodynamic outcome, conditioned by topology and control.",
            "Synthesis answering the question. The possibility (whether growth occurs) and type (equilibrium vs. nonequilibrium; linear vs. other scalings) are co-determined by (i) the external chemostatting mechanism (control type) and (ii) the CRN topology (unimolecular/pseudo-unimolecular/multimolecular; weak reversibility). The efficiency is then fixed by thermodynamic parameters through the second-law decomposition and the long-time scaling of ẇ_nc and ẇ_m. This matches the provided answer’s triad: topology + chemostatting mechanism set the regime; thermodynamic parameters quantify efficiency."
        ],
        "step_count": 10
    },
    "recuU7Zi4lNrCF": {
        "reasoning_steps": [
            "Determining the Carnot Upper Limit (Concept A, B)  Lower bound: \\(T_c = 100^\\circ\\mathrm{C} = 373.15\\,\\mathrm{K} \\ \\Rightarrow\\ T_h = 423.15\\,\\mathrm{K}\\). Therefore: \\[ \\mathrm{COP_{Carnot}} = \\frac{T_h}{T_h - T_c} = \\frac{423.15}{50} = 8.463. \\]  Upper bound: \\(T_c = 164^\\circ\\mathrm{C} = 437.15\\,\\mathrm{K} \\ \\Rightarrow\\ T_h = 487.15\\,\\mathrm{K}\\). Therefore: \\[ \\mathrm{COP_{Carnot}} = \\frac{487.15}{50} = 9.743. \\]",
            "Estimating the Feasible Range of Exergy Efficiency Based on Component Irreversibilities (Concept C, D)  Typical process efficiencies are assumed as follows:  - Effective isentropic efficiency of compression with water-injection cooling: \\(\\eta_{\\mathrm{comp}} \\in [0.70,\\,0.85]\\);\\  - Effectiveness of the solution heat exchanger: \\(\\eta_{\\mathrm{SHX}} \\in [0.65,\\,0.80]\\);\\  - Absorption/desorption stage effectiveness considering finite heat/mass transfer and mixing losses: \\(\\eta_{\\mathrm{abs}} \\in [0.75,\\,0.90]\\);\\  - Available work retention considering throttling and phase equilibrium deviation: \\(\\eta_{\\mathrm{thr}} \\in [0.85,\\,0.95]\\).\\[4pt]  The overall system-level second-law (exergy) efficiency is approximated by the product of these process efficiencies: \\[ \\eta_{II} \\in [0.25,\\ 0.33]. \\]",
            "Calculating the Actual COP Range (Concept E) \\[ \\mathrm{COP_{actual}} = \\eta_{II} \\cdot \\mathrm{COP_{Carnot}}. \\]  Lower bound (\\(\\eta_{II}=0.25\\) with \\(\\mathrm{COP_{Carnot}}=8.463\\)): \\[ 0.25 \\times 8.463 = 2.11575 \\ \\Rightarrow\\ \\mathbf{2.12}. \\]  Upper bound (\\(\\eta_{II}=0.33\\) with \\(\\mathrm{COP_{Carnot}}=9.743\\)): \\[ 0.33 \\times 9.743 = 3.21519 \\ \\Rightarrow\\ \\mathbf{3.22}. \\]",
            "Conclusion (Carnot \\(\\times\\) Exergy Efficiency, Final Result) Under a heat-source temperature of \\(100\\text{--}164^\\circ\\mathrm{C}\\) and a fixed temperature lift of \\(50^\\circ\\mathrm{C}\\), the theoretically estimated range of the actual system COP is: \\[ \\boxed{\\mathrm{COP} \\ \\approx\\ 2.12\\ \\text{to}\\ 3.22.} \\]"
        ],
        "step_count": 4
    },
    "recuUiLs6OUIKD": {
        "reasoning_steps": [
            "Determine the operating boundary of HACHP based on Concept 1 (Icing Issue) and Concept 9 (Condenser Temperature and Ambient Temperature Relationship): HACHP's condenser directly exchanges heat with ambient air, and its surface temperature must be about 5-10°C lower than the ambient temperature to maintain the heat transfer driving force. When the ambient temperature is 5°C, the condenser surface temperature will drop to 0°C or lower, causing ice formation. When the ambient temperature is -15°C, the condenser surface may reach -20°C, leading to severe ice formation or blockage, interrupting the cycle. Analysis: HACHP lacks internal heat recovery (Concept 3), so the condenser temperature is constrained by the ambient temperature, and it fails once it drops below the freezing point. Conclusion: HACHP cannot operate at 5°C and -15°C.",
            "Determine the operating boundary of EHACHP based on Concept 2 (R134a Condenser Temperature Characteristics): EHACHP's condenser-1 temperature is determined by the R134a condensation temperature, not directly by the ambient temperature, typically above 0°C to avoid ice formation. Based on Concept 3 (Internal Heat Recovery) and Concept 5 (Condenser Temperature Coupling and Stability): Internal heat recovery maintains the condenser at a higher temperature, ensuring sufficient heat transfer driving force even at low ambient temperatures. Further, combining Concept 4 (Working Fluid Freezing Risk), Concept 6 (Low-Temperature Adaptability), and Concept 8 (Superheat Assumption): Proper working fluid matching and superheat settings ensure continuous operation at low temperatures, avoiding freezing and failure. Conclusion: EHACHP can operate stably at 5°C and -15°C.",
            "Compare conclusions: HACHP cannot operate at 5°C and -15°C due to ice formation. EHACHP, with its R134a condenser temperature and internal heat recovery, maintains condenser temperatures above the freezing point, allowing it to operate at 5°C and -15°C."
        ],
        "step_count": 3
    },
    "recuUv5j0JoW39": {
        "reasoning_steps": [
            "By concept_3, it suffices to compute the example given by the explicit formula.",
            "Recall the well-known criteria of $\\epsilon$-lc for toric varieties, one can check directly that the example given in concept_3 is $1/q$-lc.",
            "So $q$ should be chosen to be close to $1/\\epsilon$.",
            "Since the base is $\\mathbb{A}^1$, it suffices to compute the multiplicity of fiber over the invariant point in $\\mathbb{A}^1$, which is given by the image of $v_{r+2}$. So the multiplicity is $u_{r+1,q} −1$, which is of order $\\epsilon^{2^r}$."
        ],
        "step_count": 4
    },
    "recuUvGR8NMAF1": {
        "reasoning_steps": [
            "In the process of updating variable \\(y_{k + 1}\\), we introduce a proximal term(concept2) to prevent excessive deviation between adjacent iterations.",
            "In the process of updating variable \\(x_{k + 1}\\), we construct an approximation function by linearizing approximation(concept3) and introducing a proximal term to update variable \\(x_{k + 1}\\).",
            "In order to reduce the variance of random gradient estimation, we obtain more accurate gradient estimation through variance reduction and momentum techniques.",
            "Obtain the complexity of the oracle by calculating the number of iterations required to make the inequality(concept1) hold \\mathbb{E}\\left[ \\text{dist}^2(0, \\partial L(x^*, y^*, \\lambda^*)) \\right] \\le \\epsilon"
        ],
        "step_count": 4
    },
    "recuUvOy3cjZ23": {
        "reasoning_steps": [
            "Hele Shaw flow model as the basic framework: According to concept_1, the channel height h is much smaller than the plane scale L of the bubble, that is \\epsilon = \\frac{h}{L} \\ll 1, At this point, the average flow velocity u of Hele Shaw is linearly related to the pressure gradient: \\mathbf{u} = -\\frac{h^2}{12\\mu} \\nabla p, where \\mu is the fluid viscosity and p is the two-dimensional pressure field. Therefore, three-dimensional problems can be reduced to two-dimensional potential flow problems, and bubble deformation only needs to be analyzed in the plane.",
            "The asymptotic expansion method distinguishes the dominant mechanism: Consider the capillary number Ca = \\frac{\\mu U}{\\sigma}, where U is the characteristic velocity and \\sigma is the interfacial tension. When \\epsilon \\ll1 and Ca \\ll 1, introduce double small parameter expansion: p = p_0 + \\epsilon p_1 + \\cdots , \\quad \\kappa = \\kappa_0 + \\epsilon \\kappa_1 + \\cdots. The dominant physical mechanism depends on the relative magnitude of Ca and \\epsilon: if Ca = O(\\epsilon^3), the thin film pressure term first appears and controls the deformation during unfolding; if Ca = O(\\epsilon^2), then capillary curvature pressure dominates.",
            "Bubble shape at Ca = O(\\epsilon^3): The bubble forms a Bretherton film between the upper and lower walls. The relationship between film thickness and capillary number is \\frac{h_f}{h} \\sim Ca^{2/3}, and the resulting film pressure difference is approximately:\\Delta p_{\\text{film}} \\sim \\frac{\\sigma}{h_f} \\sim \\sigma \\, Ca^{-2/3} \\, h^{-1}, At the limit of Ca = O(\\epsilon^3), \\Delta p_{\\text{film}} matches the expansion order and becomes the dominant pressure term. Result: The bubble is strongly compressed and flattened horizontally.",
            "Bubble shape at Ca = O(\\epsilon^2): Young Laplace law gives the pressure difference on both sides of the interface: \\Delta p_{\\text{cap}} = \\sigma \\kappa. In asymptotic expansion, when Ca = O(\\epsilon^2), the curvature pressure term reaches equilibrium with the flow pressure, becoming the dominant term: \\Delta p_{\\text{cap}} \\sim O(\\epsilon^2). Therefore, the bubble interface is elongated along the mainstream direction, while the film pressure \\Delta p_{\\text{film}} \\sim O(\\epsilon^3) takes a secondary role. Final conclusion: At the limit of Ca = O(\\epsilon^3) : membrane pressure dominates → bubbles are flattened; At the limit of Ca = O(\\epsilon^2): curvature pressure dominates → bubbles are elongated."
        ],
        "step_count": 4
    },
    "recuUvUTg0ysyw": {
        "reasoning_steps": [
            "Determine the existence of candidate points using Concept 1. Concept 1 states that for the min-max problem (1.4) involving only the function \\( c(\\mathbf{x},\\mathbf{y}) \\), since \\( c(\\mathbf{x},\\mathbf{y}) \\) is convex in \\( \\mathbf{x} \\), concave in \\( \\mathbf{y} \\), and the feasible sets \\( \\mathcal{X} \\subset \\mathbb{R}^n \\) and \\( \\mathcal{Y} \\subset \\mathbb{R}^m \\) are convex and compact (satisfying the core conditions of the min-max theorem), there exists a **global saddle point** \\( (\\mathbf{x}^*, \\mathbf{y}^*) \\in \\mathcal{X} \\times \\mathcal{Y} \\) that satisfies: \\[ c(\\mathbf{x}^*, \\mathbf{y}) \\leq c(\\mathbf{x}^*, \\mathbf{y}^*) \\leq c(\\mathbf{x}, \\mathbf{y}^*), \\quad \\forall \\mathbf{x} \\in \\mathcal{X},\\, \\forall \\mathbf{y} \\in \\mathcal{Y}. \\] We take this \\( (\\mathbf{x}^*, \\mathbf{y}^*) \\) as the candidate point for analyzing the saddle points of problem (1.1).",
            "Derive the 0-norm properties in the neighborhood of the candidate point using Concept 2. Problem (1.1) introduces cardinality penalty terms \\( \\lambda_1 \\|g(\\mathbf{x})_+\\|_0 \\) and \\( -\\lambda_2 \\|h(\\mathbf{y})_+\\|_0 \\), so we need to analyze their behavior in the neighborhood of \\( (\\mathbf{x}^*, \\mathbf{y}^*) \\). According to Concept 2: Since \\( g: \\mathbb{R}^n \\to \\mathbb{R}^{\\hat{n}} \\) and \\( h: \\mathbb{R}^m \\to \\mathbb{R}^{\\hat{m}} \\) are continuously differentiable functions (continuous differentiability implies continuity), there exists a \\( \\delta > 0 \\) such that: - For all \\( l \\in \\mathcal{A}^+(\\mathbf{x}^*) = \\{ l \\in [\\hat{n}] : g_l(\\mathbf{x}^*) > 0 \\} \\), if \\( \\mathbf{x} \\in \\mathbf{B}(\\mathbf{x}^*, \\delta) \\cap \\mathcal{X} \\) (the intersection of the closed ball centered at \\( \\mathbf{x}^* \\) with radius \\( \\delta \\) and \\( \\mathcal{X} \\)), then \\( g_l(\\mathbf{x}) > 0 \\), which further implies \\( \\mathcal{A}^+(\\mathbf{x}^*) \\subseteq \\mathcal{A}^+(\\mathbf{x}) \\); - For all \\( k \\in \\mathcal{B}^+(\\mathbf{y}^*) = \\{ k \\in [\\hat{m}] : h_k(\\mathbf{y}^*) > 0 \\} \\), if \\( \\mathbf{y} \\in \\mathbf{B}(\\mathbf{y}^*, \\delta) \\cap \\mathcal{Y} \\), then \\( h_k(\\mathbf{y}) > 0 \\), which further implies \\( \\mathcal{B}^+(\\mathbf{y}^*) \\subseteq \\mathcal{B}^+(\\mathbf{y}) \\). Since \\( \\|g(\\mathbf{x})_+\\|_0 = |\\mathcal{A}^+(\\mathbf{x})| \\) (the number of positive elements) and \\( \\|h(\\mathbf{y})_+\\|_0 = |\\mathcal{B}^+(\\mathbf{y})| \\), the above inclusion relations lead to: - For \\( \\mathbf{x} \\in \\mathbf{B}(\\mathbf{x}^*, \\delta) \\cap \\mathcal{X} \\), \\( \\|g(\\mathbf{x})_+\\|_0 \\geq \\|g(\\mathbf{x}^*)_+\\|_0 \\) (the set of positive elements does not shrink, so their count does not decrease); - For \\( \\mathbf{y} \\in \\mathbf{B}(\\mathbf{y}^*, \\delta) \\cap \\mathcal{Y} \\), \\( \\|h(\\mathbf{y})_+\\|_0 \\geq \\|h(\\mathbf{y}^*)_+\\|_0 \\) (by the same logic).",
            "Verify that the candidate point is a local saddle point. A local saddle point is defined as follows: There exists a neighborhood \\( \\mathcal{U} \\times \\mathcal{V} \\subset \\mathcal{X} \\times \\mathcal{Y} \\) of \\( (\\mathbf{x}^*, \\mathbf{y}^*) \\) such that for all \\( \\mathbf{x} \\in \\mathcal{U} \\) and \\( \\mathbf{y} \\in \\mathcal{V} \\), \\( f(\\mathbf{x}^*, \\mathbf{y}) \\leq f(\\mathbf{x}^*, \\mathbf{y}^*) \\leq f(\\mathbf{x}, \\mathbf{y}^*) \\). Let \\( \\mathcal{U} = \\mathbf{B}(\\mathbf{x}^*, \\delta) \\cap \\mathcal{X} \\) and \\( \\mathcal{V} = \\mathbf{B}(\\mathbf{y}^*, \\delta) \\cap \\mathcal{Y} \\); the verification is split into two parts: 1. Fix \\( \\mathbf{x} = \\mathbf{x}^* \\) and verify \\( f(\\mathbf{x}^*, \\mathbf{y}) \\leq f(\\mathbf{x}^*, \\mathbf{y}^*) \\) for \\( \\mathbf{y} \\in \\mathcal{V} \\): From \\( f(\\mathbf{x}, \\mathbf{y}) = c(\\mathbf{x}, \\mathbf{y}) + \\lambda_1 \\|g(\\mathbf{x})_+\\|_0 - \\lambda_2 \\|h(\\mathbf{y})_+\\|_0 \\), substituting \\( \\mathbf{x} = \\mathbf{x}^* \\) gives: \\[ f(\\mathbf{x}^*, \\mathbf{y}) = c(\\mathbf{x}^*, \\mathbf{y}) + \\lambda_1 \\|g(\\mathbf{x}^*)_+\\|_0 - \\lambda_2 \\|h(\\mathbf{y})_+\\|_0. \\] By the saddle point property of Concept 1, \\( c(\\mathbf{x}^*, \\mathbf{y}) \\leq c(\\mathbf{x}^*, \\mathbf{y}^*) \\); from the conclusion in Step 2, \\( \\|h(\\mathbf{y})_+\\|_0 \\geq \\|h(\\mathbf{y}^*)_+\\|_0 \\), and since \\( \\lambda_2 > 0 \\), we have \\( -\\lambda_2 \\|h(\\mathbf{y})_+\\|_0 \\leq -\\lambda_2 \\|h(\\mathbf{y}^*)_+\\|_0 \\). Adding these two inequalities yields: \\[ f(\\mathbf{x}^*, \\mathbf{y}) \\leq c(\\mathbf{x}^*, \\mathbf{y}^*) + \\lambda_1 \\|g(\\mathbf{x}^*)_+\\|_0 - \\lambda_2 \\|h(\\mathbf{y}^*)_+\\|_0 = f(\\mathbf{x}^*, \\mathbf{y}^*). \\] 2. Fix \\( \\mathbf{y} = \\mathbf{y}^* \\) and verify \\( f(\\mathbf{x}, \\mathbf{y}^*) \\geq f(\\mathbf{x}^*, \\mathbf{y}^*) \\) for \\( \\mathbf{x} \\in \\mathcal{U} \\): Substituting \\( \\mathbf{y} = \\mathbf{y}^* \\) gives: \\[ f(\\mathbf{x}, \\mathbf{y}^*) = c(\\mathbf{x}, \\mathbf{y}^*) + \\lambda_1 \\|g(\\mathbf{x})_+\\|_0 - \\lambda_2 \\|h(\\mathbf{y}^*)_+\\|_0. \\] By the saddle point property of Concept 1, \\( c(\\mathbf{x}, \\mathbf{y}^*) \\geq c(\\mathbf{x}^*, \\mathbf{y}^*) \\); from the conclusion in Step 2, \\( \\|g(\\mathbf{x})_+\\|_0 \\geq \\|g(\\mathbf{x}^*)_+\\|_0 \\), and since \\( \\lambda_1 > 0 \\), we have \\( \\lambda_1 \\|g(\\mathbf{x})_+\\|_0 \\geq \\lambda_1 \\|g(\\mathbf{x}^*)_+\\|_0 \\). Adding these two inequalities yields: \\[ f(\\mathbf{x}, \\mathbf{y}^*) \\geq c(\\mathbf{x}^*, \\mathbf{y}^*) + \\lambda_1 \\|g(\\mathbf{x}^*)_+\\|_0 - \\lambda_2 \\|h(\\mathbf{y}^*)_+\\|_0 = f(\\mathbf{x}^*, \\mathbf{y}^*). \\] In summary, \\( (\\mathbf{x}^*, \\mathbf{y}^*) \\) is a local saddle point of problem (1.1), meaning problem (1.1) has at least one local saddle point.",
            "Analyze the non-necessity of global saddle points. A global saddle point requires that for **all** \\( \\mathbf{x} \\in \\mathcal{X} \\) and \\( \\mathbf{y} \\in \\mathcal{Y} \\), \\( f(\\mathbf{x}^*, \\mathbf{y}) \\leq f(\\mathbf{x}^*, \\mathbf{y}^*) \\leq f(\\mathbf{x}, \\mathbf{y}^*) \\). However, the \"non-global monotonicity\" of cardinality penalty terms breaks this condition: - For \\( \\mathbf{x} \\in \\mathcal{X} \\setminus \\mathcal{U} \\) (outside the \\( \\delta \\)-neighborhood of \\( \\mathbf{x}^* \\)), the inclusion relation \\( \\mathcal{A}^+(\\mathbf{x}^*) \\subseteq \\mathcal{A}^+(\\mathbf{x}) \\) from Concept 2 no longer holds. There may exist \\( \\mathbf{x}_1 \\in \\mathcal{X} \\setminus \\mathcal{U} \\) such that \\( \\mathcal{A}^+(\\mathbf{x}_1) \\subsetneq \\mathcal{A}^+(\\mathbf{x}^*) \\), which further implies \\( \\|g(\\mathbf{x}_1)_+\\|_0 < \\|g(\\mathbf{x}^*)_+\\|_0 \\). Even if \\( c(\\mathbf{x}_1, \\mathbf{y}^*) \\geq c(\\mathbf{x}^*, \\mathbf{y}^*) \\) (from Concept 1), if \\( c(\\mathbf{x}_1, \\mathbf{y}^*) - c(\\mathbf{x}^*, \\mathbf{y}^*) < \\lambda_1 \\left( \\|g(\\mathbf{x}^*)_+\\|_0 - \\|g(\\mathbf{x}_1)_+\\|_0 \\right) \\), then: \\[ f(\\mathbf{x}_1, \\mathbf{y}^*) = c(\\mathbf{x}_1, \\mathbf{y}^*) + \\lambda_1 \\|g(\\mathbf{x}_1)_+\\|_0 - \\lambda_2 \\|h(\\mathbf{y}^*)_+\\|_0 < f(\\mathbf{x}^*, \\mathbf{y}^*), \\] violating the second inequality \\( f(\\mathbf{x}, \\mathbf{y}^*) \\geq f(\\mathbf{x}^*, \\mathbf{y}^*) \\) for global saddle points. - For \\( \\mathbf{y} \\in \\mathcal{Y} \\setminus \\mathcal{V} \\) (outside the \\( \\delta \\)-neighborhood of \\( \\mathbf{y}^* \\)), similarly, there may exist \\( \\mathbf{y}_1 \\in \\mathcal{Y} \\setminus \\mathcal{V} \\) such that \\( \\|h(\\mathbf{y}_1)_+\\|_0 < \\|h(\\mathbf{y}^*)_+\\|_0 \\). If \\( c(\\mathbf{x}^*, \\mathbf{y}_1) - c(\\mathbf{x}^*, \\mathbf{y}^*) > \\lambda_2 \\left( \\|h(\\mathbf{y}^*)_+\\|_0 - \\|h(\\mathbf{y}_1)_+\\|_0 \\right) \\), then: \\[ f(\\mathbf{x}^*, \\mathbf{y}_1) = c(\\mathbf{x}^*, \\mathbf{y}_1) + \\lambda_1 \\|g(\\mathbf{x}^*)_+\\|_0 - \\lambda_2 \\|h(\\mathbf{y}_1)_+\\|_0 > f(\\mathbf{x}^*, \\mathbf{y}^*), \\] violating the first inequality \\( f(\\mathbf{x}^*, \\mathbf{y}) \\leq f(\\mathbf{x}^*, \\mathbf{y}^*) \\) for global saddle points. Thus, problem (1.1) **cannot guarantee the existence of a global saddle point**."
        ],
        "step_count": 4
    },
    "recuUAE6Qhrmj0": {
        "reasoning_steps": [
            "To address the instruction, we start with the Beppo-Levi space \\(\\dot{H}^{1}(\\Omega)\\) (concept_1), which is the domain of \\(Tr^D\\).",
            "The range of \\(Tr^D\\) is characterized using the screened homogeneous Sobolev space \\(\\dot{H}^{1/2}(I)\\) for intervals (concept_2) and extended to \\(\\Gamma^D\\) as \\(\\dot{H}^{1/2}(\\Gamma^D)\\) (concept_3), which accounts for non-local effects between connected components."
        ],
        "step_count": 2
    },
    "recuUAYaLi3Zwq": {
        "reasoning_steps": [
            "Identify the root cause of the problem – lattice mismatch leads to interfacial strain Application Concept3: Concept3 points out that for CdSe nanocrystals, the main cause of interfacial strain is lattice mismatch. The same applies to CsPbBr₃ and NaGdF₄ systems. There is an inherent lattice constant difference between NaGdF₄ (β phase, hexagonal structure, lattice constant a ≈ 6.02 Å, c ≈ 3.60 Å) and CsPbBr₃ (α phase, cubic structure, lattice constant a ≈ 5.87 Å), and the degree of calculated mismatch is high (e.g., intraplane mismatch can reach ~5%). This mismatch can introduce significant interfacial strains during heteroepitaxial growth, which is a major challenge to the quality of core-shell structures. Mathematical logic, lattice mismatch directly leads to lattice distortion and strain energy accumulation, which is the basic principle of heterogeneous epitaxy, which is in line with the theories of elastic mechanics and crystallodynamics.",
            "Analyze the effect of core size on strain adaptability - enhances elastic deformation of small nuclei Application Concept1: Concept1 shows that larger nanocrystals (NCs) are more difficult to deform in response to lattice strain. This means that for NaGdF₄ nuclei, if the size is large (e.g., >20 nm), the lattice rigidity is strong, and it is difficult to adapt to the lattice mismatch of the CsPbBr₃ shell through elastic deformation. In contrast, small-sized nuclei (e.g., <10 nm) have more lattice freedom due to surface effects and higher volume ratios, making them more prone to elastic deformation (such as stretching or compression), partially compensating for mismatches. Mathematically and logically, this is based on the size-dependent mechanical properties of nanomaterials: small-sized crystals have a lower strain energy threshold and the elastic modulus may decrease, allowing greater deformability without introducing dislocations (refer to classical theories such as the Frank-van der Merwe model). Therefore, to achieve high-quality growth, small-size, monodisperse NaGdF₄ nuclei must be selected.",
            "Managing strain energy through growth kinetics – slow growth to avoid phase separation Application Concept2: Concept2 states that in Au@Ag core-shell structures, high crystallinity correlates with greater strain energy, which requires reduced system energy through phase separation (reduced contact interface). The same applies to CsPbBr₃/NaGdF₄ systems: if the shell grows too quickly, the high strain energy drives the system towards a more thermodynamically stable phase separation (i.e., core-shell separation) rather than forming a complete core-shell structure. Therefore, it is essential to control the growth process by using a slow growth rate (e.g., slow injection or SILAR method) that allows strain energy to gradually relax through surface diffusion and atomic rearrangement to promote the formation of colattice interfaces. Mathematically and logically, this is in line with the thermodynamic minimum energy principle and the theory of dynamic control: slow growth brings the system close to equilibrium, the strain energy is evenly distributed, and the probability of defect nucleation is reduced (refer to Gibbs free energy change and nucleation theory).",
            "Synthetic theoretical strategy – combine all concepts to achieve pseudocrystal growth Integration of Concept1, Concept2 and Concept3: Based on the above, the core of the theoretical guidance is: From Concept3, accepting lattice mismatches is inherent, but can be mitigated by managing strain. From Concept1, a small-sized NaGdF₄ core (<10 nm) is used to enhance elastic deformation capabilities. From Concept2, slow growth dynamics are employed to manage strain energy and avoid phase separation."
        ],
        "step_count": 4
    },
    "recuUDgvlT7WAg": {
        "reasoning_steps": [
            "Goal: decide whether the real part of the potential should be closer to the singlet free energy F(T,r) or to the internal energy U(T,r) to reproduce the observed ordering R_AA(1S) > R_AA(2S) > R_AA(3S).",
            "In-medium complex potential (Concept_1, Concept_2): use V(T,r)=Re V(T,r)+i Im V(T,r), with Re V encoding in-medium binding/screening and Im V≤0 encoding decoherence/dissociation.",
            "Thermodynamic bounds (Concept_2): lattice-inspired practice suggests F(T,r) ≲ Re V(T,r) ≲ U(T,r), so the real part can vary between the F and U limits.",
            "Timescale criterion (Concept_3): if medium rearrangement is slow compared to quarkonium internal dynamics (adiabatic response), the effective Re V tends toward U; an isothermal assumption would bias toward F.",
            "Interpolation (Concept_4): parametrize Re V_α(T,r)=α U(T,r)+(1−α) F(T,r), 0≤α≤1, and evolve bottomonium wave packets with the time-dependent Schrödinger equation to obtain survival probabilities.",
            "Sequencing criterion with R_AA (Concept_5): the empirical ordering R_AA(1S) > R_AA(2S) > R_AA(3S) reflects progressively weaker binding; select potentials that keep strong 1S binding while allowing larger suppression for 2S/3S."
        ],
        "step_count": 6
    },
    "recuUEYv4bv2Bl": {
        "reasoning_steps": [
            "By concept_1, we onyl need to consider curves. By concept_2, the case of multi-degree $(2,2,2)$ curve on $\\mathbb{P}^4$ is excluded.",
            "For plane cubics, recall that the affine linear transformations on the plane correspond to linear transformations of the projective plane. So by concept_4, concept_5, we may assume the curve is given by the Weierstrass form $y^2 = x^3+ax+b$. In this case, we have a nontrivial automorphism $[x:y:z] \\mapsto [x:-y:z]$.",
            "To exclude other cases, we need to relate linear automorphism group with monodromy.",
            "So the result is $(3;2)$."
        ],
        "step_count": 4
    },
    "recuUG8X5Y8XeB": {
        "reasoning_steps": [
            "From theorem_1, take the action and set \\(\\ell=\\xi b^2\\). In the static spherical symmetry with \\(B_\\mu=(0,b_r(r),0,0)\\), use the constraint from concept_1 \\(b_\\mu b^\\mu=b^2 \\Rightarrow b_r(r)=b\\sqrt{S(r)}\\).",
            "From theorem_2 with the coupled electromagnetic field, the static spherically symmetric scalar potential \\(A_\\mu=(\\phi(r),0,0,0)\\) gives \\(F_{tr}=\\sqrt{AS}\\,\\phi'(r)\\). Solving the modified Maxwell equation yields \\(\\phi(r)=q/r\\).",
            "Simplify the Einstein equations. From concept_2 we obtain \\([A(r)S(r)]'=0 \\Rightarrow S(r)=\\dfrac{C_1}{A(r)}\\), with \\(C_1=1+\\ell\\) (a gauge choice consistent with the bumblebee Schwarzschild–like solution).",
            "Substitute \\(S(r)=\\dfrac{1+\\ell}{A(r)}\\) and \\(\\phi'(r)=-\\dfrac{q}{r^2}\\) into the given \\(\\{t,t\\}\\) component equation \\(\\frac{A' S'}{4 A S}-\\frac{A''}{2 A}+\\frac{A'^2}{4 A^2} +\\frac{S'}{r S}+\\frac{2\\,\\phi'^2 S}{2+\\ell}=0\\), simplify and expand in \\(1/r\\), then integrate to obtain \\(A(r)=1-\\frac{2M}{r}+\\frac{2(1+\\ell)\\,q^2}{(2+\\ell)\\,r^2}\\)."
        ],
        "step_count": 4
    },
    "recuUGr5roIfnE": {
        "reasoning_steps": [
            "Transform the problem of the abstract sequence of convex domains into a problem on a specific geometric shape (cylinders). For any sequence of convex domains {\\Omega_n} with fixed volume and diameters d_n tending to infinity, we can easily find a corresponding sequence of \"flat and wide\" cylinders {C_n}. Each convex domain \\Omega_n is completely contained within the corresponding cylinder C_n, that is, \\Omega_n \\subset C_n. The height 2h_n of these cylinders tends to 0, and the radius R_n tends to infinity. Through this step, the original problem is successfully associated with a sequence of more concrete and regular geometric objects.",
            "By applying Theorem 1, we can directly prove that if one domain is contained within another, then its first positive curl eigenvalue is not less than that of the outer domain. Based on the inclusion relation \\Omega_n \\subset C_n from Step 1, we can directly derive the inequality \\mu_1(\\Omega_n) \\ge \\mu_1(C_n). So far, proving that the eigenvalues \\mu_1(\\Omega_n) of the original sequence tend to infinity is equivalent to proving that its lower bound, i.e., the eigenvalues \\mu_1(C_n) of the sequence of cylinders, tend to infinity. This makes the problem more focused.",
            "Now, the focus shifts to analyzing the first eigenvalue on the cylinders C_n. For the first eigenfield u_1 on C_n, according to Theorem 1, its eigenvalue \\mu_1(C_n) and the inverse operator of the curl operator, curl^{-1}, satisfy the relation: \\int_{C_n} curl^{-1} u_1 \\cdot u_1 dx = \\frac{1}{\\mu_1(C_n)} ||u_1||_{L^2}^2. According to Theorem 2, we introduce the Biot-Savart (BS) operator: curl^{-1} can be expressed via the Biot-Savart operator BS(u_1), and the above integral is exactly \\int_{C_n} BS(u_1) \\cdot u_1 dx. Combining the above relation and applying Theorem 3, we can obtain \\frac{1}{\\mu_1(C_n)} \\le ||BS(u_1)||_{L^2} / ||u_1||_{L^2}. To obtain the lower bound of \\mu_1(C_n), it is only necessary to estimate the upper bound of the L^2 norm of the Biot-Savart operator.",
            "The final step is to specifically calculate the bound of the norm of the Biot-Savart operator and draw the final conclusion by combining the geometric constraints from Step 1. Applying Theorem 3 again, we know that ||BS(u_1)||_{L^2}^2 \\le \\frac{M^2}{16\\pi^2} ||u_1||_{L^2}^2, where M is defined as the supremum of the integral \\int_{C_n} \\frac{1}{|x-y|^2} dy over all x \\in C_n. It is easy to see through specific expansion and calculation that M \\to 0. Since the lower bound of \\mu_1(C_n) is \\frac{4\\pi}{M}, when M \\to 0, \\mu_1(C_n) \\to \\infty. Then, according to the inequality \\mu_1(\\Omega_n) \\ge \\mu_1(C_n) from Step 2, we finally prove that \\mu_1(\\Omega_n) \\to \\infty."
        ],
        "step_count": 4
    },
    "recuUHnctMymfT": {
        "reasoning_steps": [
            "we consider $d=3$. By concept_2, $(X,(1-1/q)E)$ is a log Fano pair. In particular, $K_X+(1-1/q)E$ is ample. So $Vol(-K_X) \\geq Vol(-K_X-(1-1/q)E) = u_{d+1,q}/q^d$.",
            "Note that $u_{n,q}$ is a polynomial of $q$ of degree $2^{n-1}$ by direct computation, so the volume is asymptotically a polynomial of degree $2^d-(d+1) = 4$.",
            "we can see directly that the example in concept_2 is $1/q$-lc, so we may take $q \\sim 1/\\epsilon$. So the result is $v \\sim \\epsilon^{-4}$."
        ],
        "step_count": 3
    },
    "recuUJS7kbGGBy": {
        "reasoning_steps": [
            "Step 1: Acknowledge the Complexity of CDME: The Chemical Diffusion Master Equation (CDME) is inherently an infinite-dimensional coupled system of Fokker-Planck equations, which is the root cause of its computational intractability. Infinite-Dimensional Structure: The CDME describes the probabilistic evolution of a system with an arbitrary number of particles (n ≥ 0). For each particle count n, there exists a corresponding Fokker-Planck equation for the probability density ρ_n(t, x_1, ..., x_n), where x_1, ..., x_n ∈ [0,1]^n are the spatial coordinates of the n particles. This results in an infinite set of equations, one for each n. Strong Coupling Between Equations: The equations for different n are tightly coupled. For example, the time derivative of ρ_n depends on ρ_{n+2} (via mutual annihilation reactions, where two particles disappear) and ρ_{n-1} (via creation reactions, where one particle is added). This coupling spans all n, forming an infinite chain of dependencies that cannot be decoupled directly. Direct Solution Challenges: Traditional numerical methods (e.g., finite differences, finite elements) cannot handle infinite-dimensional systems or unbounded coupling. Solving each equation individually is impossible due to the infinite number of equations and their cross-dependencies, necessitating a transformative approach to reduce complexity.",
            "Step 2: Define the Infinite-Dimensional Moment Generating Function: Unify the infinite set of CDME solutions into a single stochastic process, transforming the infinite-dimensional coupled system into a single equation in an abstract space. Definition of Φ(t): The moment generating function is defined as Φ(t) = ∑_{n ≥ 0} I_n(ρ_n(t, ·)), where I_n denotes the n-th order multiple Itô integral. This integral operator maps the n-dimensional probability density ρ_n(t, x_1, ..., x_n) to a random variable in the Wiener chaos expansion framework, which decomposes random variables into sums of multiple Itô integrals. Unification of Solutions: By summing over all n, Φ(t) consolidates the scattered solutions {ρ_n}_{n ≥ 0} into a single object. This leverages the structure of Wiener chaos, where infinite-dimensional dynamics can be represented compactly using integral operators. Abstract Equation for Φ(t): Through rigorous derivation (Lemma 3.3 in the original paper), Φ(t) is proven to satisfy an abstract evolution equation in the space F^* (a space of generalized random variables): ∂_t Φ = dΓ(-𝒜) Φ + (λ_d/2) D_1^2 Φ - (λ_d/2) N(N-I) Φ + D_{λ_c}^* Φ - γ Φ with initial condition Φ(0) = 1. Here, dΓ(-𝒜) models diffusion (linked to the Laplacian operator), D_1^2 and N(N-I) model annihilation reactions, D_{λ_c}^* models creation reactions, and -γ Φ accounts for creation loss. This equation replaces the infinite coupled system with one equation, laying the foundation for dimensionality reduction.",
            "Step 3: Implement Finite-Dimensional Projections: Truncate the infinite-dimensional problem into a finite-dimensional subspace using orthogonal projections, preserving key dynamics while reducing complexity. Eigenfunction Basis Selection: The orthogonal projection operator Π_N is defined to project the infinite-dimensional space onto the subspace spanned by the first N eigenfunctions {ξ_1, ..., ξ_N} of the diffusion operator -𝒜 = ∂_x^2 under Neumann boundary conditions. These eigenfunctions are explicitly ξ_k(x) = √2 cos((k-1)πx) with corresponding eigenvalues α_k = (k-1)^2 π^2. They form a complete orthogonal basis for describing spatial variations in the domain [0,1], ensuring critical spatial information is retained. Second Quantization for Projection: The finite-dimensional projection of Φ(t) is defined as Φ_N(t) = Γ(Π_N) Φ(t), where Γ(Π_N) is the second quantization operator. Second quantization extends the spatial projection Π_N to the Wiener chaos framework: it applies Π_N^{⊗n} (the n-fold tensor product of Π_N) to the n-th chaos component of Φ(t), truncating higher-dimensional spatial variations beyond the first N eigenfunctions. Dimensionality Reduction Effect: This operation confines Φ(t) to an N-dimensional subspace, eliminating dependencies on eigenfunctions beyond ξ_N. The resulting Φ_N(t) retains the essential dynamics (diffusion, creation, annihilation) but in a finite-dimensional setting, making analytical or numerical solution feasible.",
            "Step 4: Derive and Solve the Finite-Dimensional PDE: Convert the finite-dimensional projection Φ_N(t) into a solvable partial differential equation (PDE), whose solution approximates the original CDME solution. Representation of Φ_N(t) as u_N(t, z): It is proven that Φ_N(t) can be expressed as a function of the first-order Itô integrals of the eigenfunctions: Φ_N(t) = u_N(t, I_1(ξ_1), ..., I_1(ξ_N)), where z = (I_1(ξ_1), ..., I_1(ξ_N)) ∈ ℝ^N and I_1(ξ_k) is the first-order Itô integral of ξ_k. This links the abstract stochastic process Φ_N(t) to a deterministic function u_N of N variables. Derivation of the PDE for u_N: By decomposing the abstract equation satisfied by Φ_N(t) (from Step 3) into operators acting on u_N, a fourth-order linear PDE is derived: ∂_t u_N(t, z) = -∑_{k=1}^N α_k ∂_k^* ∂_k u_N(t, z) + (λ_d/2) ∂_1^2 u_N(t, z) - (λ_d/2) ∑_{j,k=1}^N ∂_j^* ∂_k^* ∂_j ∂_k u_N(t, z) + ∑_{k=1}^N c_k ∂_k^* u_N(t, z) - γ u_N(t, z), with u_N(0, z) = 1. Here: ∂_k^* = -∂_k + z_k (Gaussian divergence operator), α_k are eigenvalues of -𝒜, c_k = ⟨λ_c, ξ_k⟩_{L^2([0,1])} (projections of the creation rate), and terms correspond to diffusion (-∑ α_k ∂_k^* ∂_k), annihilation (- (λ_d/2) ∑ ∂_j^* ∂_k^* ∂_j ∂_k), creation (∑ c_k ∂_k^*), and creation loss (-γ). Solving the PDE and Recovering CDME Solutions: Solving this N-dimensional PDE yields u_N(t, z). Using the Stroock-Taylor formula and properties of Malliavin derivatives, the finite-dimensional projection of the CDME solution is recovered as: Π_N^{⊗n} ρ_n(t, x_1, ..., x_n) = (1/n!) ∑_{j_1,...,j_n=1}^N ℰ[∂_{j_1} ⋯ ∂_{j_n} u_N(t, I_1(ξ_1), ..., I_1(ξ_N))] ξ_{j_1}(x_1) ⋯ ξ_{j_n}(x_n). This provides an approximate solution to the original infinite-dimensional CDME by leveraging the finite-dimensional PDE solution."
        ],
        "step_count": 4
    },
    "recuUMjpAfW9pl": {
        "reasoning_steps": [
            "Statement_3 and steatement_4 are not correct because the nef cone conjecture says nothing about the pseudo-automorphism group, so nef cone conjecture contains fewer information. (Because all these statements are conjectured to be true, there are no counterexamples.)",
            "For statement_2, by Theorem 1(3), it suffices to prove that the effective cone conjecture implies Conjecture 1(2). So the problem is that we need to replace $PsAut$ by $Aut$. Indeed, it suffices to show that there are finitely many $g\\in PsAut(X,\\Delta)/Aut(X,\\Delta)$ such that $g\\cdot P_M$ intersect $P_A$. This is implied by the Shokurov polytope in a formal way using concept_4.",
            "For statement_1, the argument is similar to statement_2. Note that the movable cone is non-degenerate since we are working on absolute case. By Theorem 1(3), it suffices to prove that the effective cone conjecture implies Conjecture 1(1). But Conjecture 1(1) follows directly from the statement of the effective cone conjecture since the movable cone is contained in the effective cone."
        ],
        "step_count": 3
    },
    "recuUQud6zIEV4": {
        "reasoning_steps": [
            "By classifying 3-cocycles with reference to the method used for the 5-dihedral quandle, due to differences in algebraic structures, the Mochizuki 3-cocycles of the 7-dihedral quandle are divided into four categories, while those of the 5-dihedral quandle fall into three categories.",
            "Through individual verification, the method of elimination is used to prove that the length of the Mochizuki 3-cocycle of the 7-dihedral quandle is greater than or equal to 8.",
            "Prove that the length of the Mochizuki 3-cocycle of the 7-dihedral quandle is less than or equal to the triple point number of a certain surface knot, which is the 2-twist-spun 5₂-knot, thereby determining the supremum."
        ],
        "step_count": 3
    },
    "recuUT7kcN6Y2T": {
        "reasoning_steps": [
            "According to the concept_1, the CeO2 (100) and (110) faces have two-fold coordinated O atoms, suggesting there are oxygen-bridge sites on CeO2 (100) and (110) faces. The oxygen-bridge sites can strongly bond with Cu atoms, thus the CeO2 (100) and (110) faces are more active in comparison with the (111) surface.",
            "According to the concept_2, the CeO2(110) is more stable than CeO2(100), suggesting the CeO2(110) is more inactive. Thus, the CeO2(100) can anchor Cu single atom more stable."
        ],
        "step_count": 2
    },
    "recuUUahdenp8C": {
        "reasoning_steps": [
            "Begin with the given conditions: a non-zero $\\mu$-variate polynomial $P(\\overline{x})$ with maximum individual degree $\\delta$, a finite set $K \\subseteq \\mathbb{F}$, and a known non-root $\\overline{a} \\in \\mathbb{F}^\\mu$ where $P(\\overline{a}) \\neq 0$ (Concept_1). The objective is to find the size of the set of codes used for the roots in $K^\\mu$.",
            "The core strategy is to devise an encoding scheme (Concept_4) that maps every root $\\overline{c} \\in K^\\mu$ to a code. The total number of possible codes will serve as the upper bound.",
            "For any specific root $\\overline{c} = (c_1, \\dots, c_\\mu)$, apply a hybrid argument (Concept_2). Construct a sequence of points starting from the non-root $\\overline{a}$ and ending at the root $\\overline{c}$.",
            "Since the polynomial's value transitions from non-zero to zero along this path, there must be a minimal index $k$ (where $1 \\le k \\le \\mu$) that marks this transition.",
            "This transition identifies a specific univariate polynomial (Concept_3), $Q(t)$, by fixing all variables except the $k$-th one.",
            "The polynomial $Q(t)$ is non-zero, has a degree of at most $\\delta$, and has $c_k$ as one of its roots.",
            "Apply the Fundamental Theorem of Algebra (Theorem_1): $Q(t)$ can have at most $\\delta$ roots. Let $i$ (where $1 \\le i \\le \\delta$) be the index of $c_k$ among these roots.",
            "The root $\\overline{c}$ is encoded using the tuple $(k, i, (c_1, \\dots, c_{k-1}, c_{k+1}, \\dots, c_\\mu))$.",
            "To find the total cardinality of the code set, calculate the number of unique codes. The choices are: $k$: $\\mu$ possible values. $i$: $\\delta$ possible values. The remaining $\\mu-1$ components: $|K|^{\\mu-1}$ possibilities.",
            "The total size of the code space is the product: $\\mu \\cdot \\delta \\cdot |K|^{\\mu-1}$. This is the upper bound on the cardinality of the code set."
        ],
        "step_count": 10
    },
    "recuUWJ40wj1vy": {
        "reasoning_steps": [
            "By comparing the value set and the degree of the polynomial, one can directly prove their equivalence; or construct $T(X):=\\prod_{u\\in U}(X-u)=\\sum_{i=0}^mw_iX^{p^{ki}}$ and $T^*(X):=\\prod_{u\\in U^v}(X-u)=\\sum_{i=0}^mw_iX^{\\frac{p^{ki}-1}{v}+1}$, then we obtain $T^*(F^v)=F^{v-1}T(F)$ for all $F$, and thus using the necessary and sufficient condition of Theorem 2, we can prove their equivalence.",
            "Note the restrictions on the number of elements in the set in Theorem 1 and Theorem 2. First, discuss the singleton set $\\{a\\}$, construct the polynomial $x^q-x+a$, and thus prove that all singleton sets belong to $W$ but not to $R$.",
            "Since the value set of the minimal polynomial $F$ is the zero set of the corresponding $T_F$ in Theorem 1, therefore the root set $U_0$ of $T_F(x^v+\\gamma_0)$ satisfies $V_F=U_0^v+\\gamma_0$. Using (4) in Theorem 1, we deduce that the root set $U_0$ of $T_F(x^v+\\gamma_0)$ is an $\\mathbb{F}_{p^k}$-subspace.",
            "Using the fact that $v\\mid q^k-1$, prove that $U_0$ is a subset of $\\mathbb{F}_q$. Take $a^{-1}\\in U_0$, then $U:=aU_0$ is an $\\mathbb{F}_{p^k}$-subspace in $\\mathbb{F}_q$ containing $1$, and $V_F=a^{-v}U^v+\\gamma_0$ is an element of $R$, therefore $W\\subset R$ always holds for non-singleton sets.",
            "For $S=aU^v+b$, first construct $G\\in P(U,q)$. Using the first question, we obtain that $aG^v+b$ is the required polynomial. The construction of $G$ can be achieved using the properties of linear polynomials. Specifically, construct $T=\\prod_{u\\in U}(X-u)$, and there exists a $p$-linear polynomial $G$ such that $T(G(X))=X^q-X$. It can be immediately verified that $G\\in P(U,q)$. This proves $R\\subset W$."
        ],
        "step_count": 5
    },
    "recuUYbsa4x8Iw": {
        "reasoning_steps": [
            "Start from an instance $(G,k)$ of One-Sided Grundy Total Domination where $G$ is bipartite with $V(G)=A\\dot\\cup B$ and there is a length-$k$ Total sequence $\\sigma=(v_1,\\dots,v_k)$ on side $A$. Construct $G'$ by adding two new vertices $a_1,a_2$ to $A$ and two new vertices $b_1,b_2$ to $B$, obtaining $A'=A\\cup\\{a_1,a_2\\}$ and $B'=B\\cup\\{b_1,b_2\\}$. Make both $A'$ and $B'$ cliques (every two vertices inside each part are adjacent), and keep all original cross edges between $A$ and $B$.",
            "Use the sequence semantics. A Total sequence satisfies $N(v_i)\\setminus \\bigcup_{j<i}N(v_j)\\neq\\varnothing$ for every $i$. An L sequence satisfies $N[v_i]\\setminus \\bigcup_{j<i}N(v_j)\\neq\\varnothing$ for every $i$. Here $N(\\cdot)$ is the open neighborhood and $N[\\cdot]$ is the closed neighborhood; validation uses footprints (each step must newly cover at least one previously uncovered vertex).",
            "Fixed 'bookend' pairs in the construction. Add two vertices $\\{a_1,a_2\\}$ to $A$ to form $A'$, and two vertices $\\{b_1,b_2\\}$ to $B$ to form $B'$; make $A'$ and $B'$ cliques (so $a_1,a_2$ are adjacent to all vertices in $A'$, and $b_1,b_2$ are adjacent to all vertices in $B'$). Keep all original cross edges between $A$ and $B$. By the definitions above, each pair $\\{a_1,a_2\\}$ and $\\{b_1,b_2\\}$ can 'footprint' each other on their own sides, without interfering with the opposite side.",
            "Footprint preservation of the original sequence in the constructed graph. If $\\sigma = (v_1, …, v_k)$ is a Total sequence on side $A$ of the original graph, then in the constructed graph $G'$ these $k$ steps remain valid: the two added pairs ($\\{a_1,a_2\\},\\{b_1,b_2\\}$) are part of cliques $A'$ and $B'$, and the original cross edges between $A$ and $B$ are preserved. Thus, the footprint witnesses $w_i \\in B$ for each $v_i$ in $\\sigma$ are not 'stolen' by the added pairs, ensuring the $k$ steps stay valid.",
            "Form a concrete valid L sequence in $G'$: $(a_1,\\, a_2,\\, v_1,\\dots,v_k,\\, b_1,\\, b_2)$. By Steps 3–4 every step satisfies the L condition, hence the feasible length $\\ell$ obeys $\\ell\\ge k+4$.",
            "Argue the upper bound. In the clique $A'$, $N(a_1)\\cup N(a_2)=(A'\\setminus\\{a_1\\})\\cup(A'\\setminus\\{a_2\\})=A'$, so any further $x\\in A'$ has $N[x]\\setminus \\bigl(N(a_1)\\cup N(a_2)\\bigr)=\\varnothing$ and cannot extend the L sequence; the same holds for $B'$ after $b_1,b_2$. Thus the two cliques contribute at most two L steps each, and the middle block contributes at most the preserved $k$ steps, giving $\\ell\\le k+4$.",
            "Combine the bounds to conclude $\\ell=k+4$."
        ],
        "step_count": 7
    },
    "recuUWXMaPT8sk": {
        "reasoning_steps": [
            "If $|U|>2^k$, immediately return \\textsc{NO}; otherwise fix $|U|\\le 2^k$.",
            "Deduplicate $F$ to obtain $F'$, hence $|F'|\\le 2^{|U|}$.",
            "Enumerate all $\\mathcal{S}\\subseteq F'$ with $|\\mathcal{S}|\\le k$; the number of candidates satisfies \\[ N=\\sum_{i=0}^{k}\\binom{|F'|}{i}\\ \\le\\ \\Bigl(\\tfrac{e|F'|}{k}\\Bigr)^{k}\\ \\le\\ |F'|^{\\,k}. \\]",
            "For each candidate $\\mathcal{S}$, construct signatures $\\chi_{\\mathcal{S}}(u)$ for all $u\\in U$ and check for duplicates, the verification cost is $O(k|U|)$.",
            "Since $|F'|\\le 2^{|U|}\\le 2^{2^k}$, we obtain \\[ T\\ \\le\\ (2^{2^k})^{k}\\cdot \\mathrm{poly}(|U|+|F|) \\ =\\ 2^{\\,k\\,2^{k}}\\cdot \\mathrm{poly}(|U|+|F|) \\ =\\ 2^{\\,2^{\\Theta(k)}}\\cdot \\mathrm{poly}(|U|+|F|). \\]"
        ],
        "step_count": 5
    },
    "recuUAiwXy0ZRY": {
        "reasoning_steps": [
            "Centering and projection (Concept 4) Let \\[ u=\\frac{\\mathbf{1}}{\\sqrt n},\\qquad P=I-uu^\\top,\\qquad B:=A-p\\,\\mathbf{1}\\mathbf{1}^\\top . \\] Then \\[ P\\mathbf{1}=0,\\qquad PAP=PBP,\\qquad \\lambda_2(A)=\\|PAP\\|_{\\mathrm{op}}\\ \\le\\ \\|B\\|_{\\mathrm{op}}. \\]",
            "Notational conventions (Concepts 1–3) Use the max-operator \\(a\\vee b:=\\max\\{a,b\\}\\) (Concept 1). Adopt \\(\\tilde{O}(\\cdot),\\tilde{\\Theta}(\\cdot)\\) to ignore polylog factors in \\(n\\) (Concept 2). Interpret “w.h.p.” as \\(\\Pr(\\cdot)\\ge 1-n^{-c}\\) for some \\(c>0\\) (Concept 3).",
            "Operator-norm target bound (plug-in form) To answer the scaling question, it suffices (by Step 1) to control \\(\\|B\\|_{\\mathrm{op}}\\). We therefore target a high-probability bound of the form \\[ \\|B\\|_{\\mathrm{op}}\\ \\le\\ \\tilde{O}\\!\\Big(\\,\\sqrt{np}\\ \\vee\\ \\frac{np}{\\sqrt d}\\,\\Big)\\quad\\text{(w.h.p.)}. \\] (Establishing this bound requires tools beyond Concepts 1–4; here we only use it as a plug-in to translate into \\(\\lambda_2\\)-scaling.)",
            "Scaling in the two regimes (Instruction’s comparator) By the comparator in the Instruction, \\[ \\sqrt{np}\\ \\ge\\ \\frac{np}{\\sqrt d}\\ \\Longleftrightarrow\\ d\\ \\ge\\ np, \\] so the dominant term in the max changes at \\(d=np\\). Combining with Step 1: \\[ \\lambda_2(A)\\ \\le\\ \\|B\\|_{\\mathrm{op}}\\ =\\ \\tilde{O}\\!\\Big(\\,\\sqrt{np}\\ \\vee\\ \\frac{np}{\\sqrt d}\\,\\Big)\\quad\\text{(w.h.p.)}. \\]"
        ],
        "step_count": 4
    },
    "recuUp11IgbXqe": {
        "reasoning_steps": [
            "Establish the dynamical equations and focus on the intermediate time scale. Let x_dot = S0 v0 + u - p - phi x, where S0 is the stoichiometric matrix of internal reactions, v0 is given by the law of mass action, u denotes source uptake, p denotes target collection, and phi is a uniform slow degradation rate. For t << 1/phi, the degradation term can be neglected, and the source uptake incorporated into the augmented matrix S1, yielding the approximation x_dot = S1 v1 - p. This sets the linear-algebraic foundation for the structural criteria.",
            "Introduce the rank gap delta to assess steady-state attainability in the absence of degradation. Define delta = rank(S1|P) - rank(S1), where P is the stoichiometric column vector for target collection. If delta = 0, a steady state exists without degradation; if delta = 1, no steady state exists, and the system converges only when degradation is included. Numerically, Type I/II networks satisfy delta = 0, while Type III/IV networks satisfy delta = 1. Thus, delta first separates the possibility of exponential/plateau from power-law/limited plateau.",
            "For the delta = 0 branch, use the dimension of the left null space I to distinguish exponential from plateau relaxation. Let S2 = (S1|P), and define I = dim(coker S2), representing the number of conservation laws. I = 0: no conservation laws, perturbations rapidly return along the dominant decay mode -> exponential relaxation. I >= 1: near-conserved combinations exist, leading to different quasi-steady states depending on the initial condition -> plateau relaxation (quasi-steady plateau). This mechanism is illustrated by examples where different initial conditions lead to distinct plateau concentration levels.",
            "For the delta = 1 branch, use the stoichiometric cone SC(x_ini) to distinguish power-law vs. limited plateau. Define SC(x_ini) = {x = x_ini + S2 v2 | J^(tgt) >= 0}, the region of state space reachable from the initial point without reversing target collection. If the final attractor x_att is in SC(x_ini), trajectories can continue to approach it without degradation, but progress slows under nonlinear constraints -> power-law relaxation. If x_att is not in SC(x_ini), the trajectory is stuck due to geometric accessibility limits, remaining nearly static until degradation takes effect -> limited plateau. A two-step toy model illustrates the stoichiometric-geometric reason behind this stuck behavior.",
            "Combine the three criteria into a decision tree with four classifications: Type I (delta = 0, I = 0) -> exponential. Type II (delta = 0, I >= 1) -> plateau (quasi-steady). Type III (delta = 1, I = 0) and Type IV (delta = 1, I >= 1) -> depending on the stoichiometric cone, either power-law or limited plateau. This decision process can be carried out solely from the stoichiometric matrix.",
            "Quantify the distinction among relaxation types to avoid subjectivity: define the relaxation time T_relax as the time when the distance to steady state in logarithmic space first falls below a threshold epsilon, and introduce the migration length L to distinguish plateau (almost stationary in the intermediate stage, L/L_max small) from power-law (continuous slow migration, L/L_max large). The bimodal distribution clearly corresponds to the coexistence of power-law and limited plateau behaviors.",
            "Cross-validation and extrapolation. Connecting two minimal networks in parallel can alter the effective delta and I, transforming originally power-law/plateau dynamics into exponential dynamics. Assigning different rate constants to the parallel networks reveals time-segmented behavior (plateau/power-law followed by exponential), reflecting the idea that the effective network structure unfolds over time. Larger redundant networks still conform to the overall trends of the criteria.",
            "Validation with empirical metabolic networks. For example, in the glucose fermentation network of Lactococcus lactis, the presence or absence of the NOX reaction (NADH -> NAD) alters the effective delta: Fast NOX -> near delta = 0, exponential relaxation. Slow NOX -> near delta = 1, power-law/plateau relaxation. This phenomenon holds even after unifying other parameters, emphasizing the robustness and transferability of the structure-dynamics correspondence.",
            "Concluding synthesis. The three factors (rank gap delta, left null space dimension I, and stoichiometric cone SC) constitute a complete set of criteria: first determine steady-state attainability (delta), then conservation laws (I), and finally reachability (SC). This uniquely predicts the type of non-steady relaxation, and is insensitive to specific rate constants (within the modeling assumptions)."
        ],
        "step_count": 9
    },
    "recuUoxYYA5jCy": {
        "reasoning_steps": [
            "The proof establishes the coNP-hardness of the GapSS problem by employing a Karp reduction (Concept_1) from the Tautology problem.",
            "The reduction takes a Tautology formula `g` and constructs a new, larger monotone formula `f` using an 'expensive' gadget formula `E_t`, as detailed in the construction from Concept_3. The size `L` of the resulting formula `f` is primarily determined by the size `t` of the gadget, such that `$L=O(t)$`.",
            "The instruction specifies that we are interested in the 'expensive' case, which occurs when the Tautology instance `g` is not a tautology. In this scenario, `f` can be restricted to a residual function that is identical to the gadget `$E_t(y)$`.",
            "According to Theorem_1, the secret-sharing complexity lower bound of this residual function `$E_t$` is inherited by the original function `f`. Therefore, to find the lower bound for `f`, we must calculate the total share size for `$E_t$`.",
            "The total share size of `$E_t$` is the product of its average-share size and its number of variables. Using the properties defined in Concept_2, this is calculated as: `$S_{sum}(E_t) = \\text{avg-share-size} \\times \\text{num-variables} = \\Omega(\\sqrt{t}/\\log t) \\times \\Theta(\\sqrt{t}) = \\Omega(t/\\log t)$`.",
            "Since `$L = O(t)$`, we can express this lower bound in terms of `L`. The total share size for `f` is therefore `$\\Omega(t/\\log t) = \\Omega(L/\\log L)$`, which is the final answer."
        ],
        "step_count": 6
    },
    "recuUlQq8ydgDA": {
        "reasoning_steps": [
            "The objective is to determine the worst-case lower bound for the relaxation time of the Glauber dynamics (Concept_1) in the critical hardcore model (Concept_2).",
            "The instruction provides the starting premise: the lower bound on spectral independence (Concept_3), a measure of local correlations, is \\(\\Omega\\left(n^{1 / 3}\\right)\\).",
            "Theorem_1 describes the connection via the 'Universality of Spectral Independence,' a local-to-global principle. This principle dictates that the global property (relaxation time) is determined by scaling the local property (spectral independence) by the system size, \\(n\\).",
            "A model must infer from 'scaled up by the system size' that a multiplication by a factor related to \\(n\\) is required. The specific relationship, as detailed in the source literature, is that the relaxation time is lower-bounded by the product of \\(n\\) and the spectral independence constant.",
            "Applying this principle, we multiply the system size, \\(n\\), by the given spectral independence lower bound, \\(\\Omega\\left(n^{1 / 3}\\right)\\).",
            "The calculation is \\(\\Omega\\left(n \\cdot n^{1 / 3}\\right)=\\Omega\\left(n^{1+1 / 3}\\right)=\\Omega\\left(n^{4 / 3}\\right)\\). This is the resulting lower bound on the relaxation time."
        ],
        "step_count": 6
    },
    "recuUczliprdXP": {
        "reasoning_steps": [
            "The problem sets two conflicting bounds on the code's interactions. The Stacked Architecture Interaction Constraint (Concept_1) provides an upper limit on the number of long-range interactions, $M \\le O(n/l^2)$.",
            "Simultaneously, the Locality-Parameter Tradeoff (Theorem_1) establishes a fundamental lower bound on both the number and length of these interactions for any non-trivial 2D code. The number of interactions must be at least $M \\ge \\Omega(\\text{max}(k,d))$.",
            "By combining these two constraints on the number of interactions $M$, we deduce that the upper bound imposed by the architecture must accommodate the necessary lower bound: $O(n/l^2) \\ge \\Omega(\\text{max}(k,d))$.",
            "This inequality can be rearranged to express an upper bound on the square of the interaction length: $l^2 \\le O(n / \\text{max}(k,d))$.",
            "Next, we use the second part of the Locality-Parameter Tradeoff (Theorem_1), which specifies the minimum required length for these interactions: $l \\ge \\Omega(\\text{max}(\\frac{d}{\\sqrt{n}}, (\\frac{kd^2}{n})^{1/4}))$. To derive a relationship involving the term $kd^2$, we must select the second argument of the max function, yielding the lower bound $l \\ge \\Omega((\\frac{kd^2}{n})^{1/4})$.",
            "To compare this with the expression from Step 4, we square this lower bound on $l$: $l^2 \\ge \\Omega((\\frac{kd^2}{n})^{2/4}) = \\Omega(\\sqrt{\\frac{kd^2}{n}})$.",
            "Now we have both an upper and a lower bound for $l^2$. Combining the result from Step 4 and Step 6 gives: $O(n / \\text{max}(k,d)) \\ge l^2 \\ge \\Omega(\\sqrt{\\frac{kd^2}{n}})$.",
            "Focusing on the outer parts of the inequality, we have $O(n / \\text{max}(k,d)) \\ge \\Omega(\\sqrt{\\frac{kd^2}{n}})$. To eliminate the radical, we square both sides: $O(n^2 / \\text{max}(k,d)^2) \\ge \\Omega(\\frac{kd^2}{n})$.",
            "Finally, we rearrange the inequality to solve for the target quantity $kd^{2} \\cdot \\text{max}(k,d)^{2}$. Multiplying both sides by $n \\cdot \\text{max}(k,d)^2$ gives the concluding result: $O(n^3) \\ge \\Omega(kd^{2} \\cdot \\text{max}(k,d)^{2})$."
        ],
        "step_count": 9
    },
    "recuU8Xn3CGVwN": {
        "reasoning_steps": [
            "Understand the system and the objective. The system is a slow-fast mean-field diffusion: a slow component X_t^\\delta and a fast component Y_t^\\delta are coupled, where the coefficients of the slow component depend on its own distribution and that of the fast component. The noise intensity \\delta \\rightarrow 0, and the time-scale ratio \\varepsilon(\\delta) satisfies \\lim _{\\delta \\rightarrow 0} \\frac{\\varepsilon}{\\delta}=0. The objective is to derive an explicit expression for the rate function of the large deviation principle (LDP) for the slow component X^\\delta.",
            "Establish the mean-field limit. As \\delta \\rightarrow 0, the fast component Y_t^\\delta converges to its invariant measure v (uniquely determined by the equation d Y_t=f\\left(Y_t\\right) d t + g\\left(Y_t\\right) d\\tilde{W}_t^2 due to rapid oscillation (\\varepsilon \\rightarrow 0). In the limit, the slow component satisfies the deterministic averaged equation \\dot{\\bar{X}}_t=\\bar{b}\\left(\\bar{X}_t, \\mathrm{~L}_{\\bar{X}_t}\\right) , where \\bar{b}(x, \\mu)=\\int b(x, \\mu, y, v) v(d y) , and \\mathrm{L}_{\\bar{X}_t}=\\delta_{\\bar{X}_t}（Dirac measure）.",
            "Apply the weak convergence approach and variational representation. The LDP is equivalent to the Laplace principle. Utilize the variational representation for Brownian motion functionals：-\\log \\mathrm{E}[\\exp (-F(W))]= \\inf _{h \\in \\mathrm{~A}} \\mathrm{E}\\left[\\int_0^T\\left|h_s\\right|^2 d s+F\\left(W+\\int_0^{\\cdot} h_s d s\\right)\\right]. Define the control process h_t^\\delta and the controlled system: the controlled slow component X_t^{\\delta, h^\\delta} and the controlled fast component Y_t^{\\delta, h^\\delta} satisfying stochastic differential equations (SDEs) with drift terms.",
            "Introduce the functional occupation measure and viable pairs. To handle the averaging of the fast component, define the functional occupation measure P^{\\delta, \\Delta}(d h d y d t)= \\frac{1}{\\Delta} \\int_t^{t+\\Delta} \\delta_{\\left(h_s^\\delta, Y_s^{\\delta, h^\\delta}\\right)}(d h d y) d s d t, where \\Delta(\\delta) satisfies the scale separation conditions \\Delta \\rightarrow 0, \\frac{\\varepsilon}{\\delta \\Delta} \\rightarrow 0. A viable pair (\\varphi, P) is defined as: \\varphi is an absolutely continuous path, P is a measure satisfying (i) finite second moment, (ii) the integral equation \\varphi_t=x+\\int \\Phi\\left(\\varphi_s, \\mathrm{~L}_{\\bar{X}}, y, v, h\\right) P(d h d y d s) , and (iii) the decomposition P(d h d y d t)=\\eta(d h \\mid y, t) v(d y) d t, where\\Phi(x, \\mu, y, v, h)=\\bar{b}(x, \\mu)+\\sigma(x, \\mu, y, v) P_1 h, P_1 is a projection operator.",
            "Prove compactness and convergence to viable pairs. Under assumptions (global Lipschitz continuity, dissipativity, uniform positive definiteness of \\sigma \\sigma^* ), the sequence \\left\\{\\left(X^{\\delta, h^\\delta}, P^{\\delta, \\Delta}\\right)\\right\\} is compact in an appropriate space. For any subsequence, there exists a sub-subsequence converging weakly to a viable pair ( \\varphi, P ) , satisfying the viable pair definition. The rate function is defined as I(\\varphi)=\\inf _{(\\varphi, P) \\in \\mathrm{V}_{(\\Phi, v)}} \\frac{1}{2} \\int|h|^2 P(d h d y d t) .",
            "Derive the explicit form of the rate function. Through an equivalent variational problem \\tilde{I}(\\varphi)=\\inf _z \\frac{1}{2} \\int_0^T \\int_{\\mathrm{Y}}\\left|z_t(y)\\right|^2 v(d y) d t, where z_t(y) satisfies \\dot{\\varphi}_t=\\bar{b}\\left(\\varphi_t, \\mathrm{~L}_{\\bar{X}_t}\\right)+ \\int_{\\mathrm{Y}} \\sigma\\left(\\varphi_t, \\mathrm{~L}_{\\bar{X}_t}, y, v\\right) P_1 z_t(y) v(d y). The optimal control takes the feedback form h_t(y)=\\left(\\sigma P_1\\right)^*\\left(\\varphi_t, \\mathrm{~L}_{\\bar{X}_t}, y, v\\right) Q^{-1}\\left(\\dot{\\varphi}_t-\\bar{b}\\left(\\varphi_t, \\mathrm{~L}_{\\bar{X}_t}\\right)\\right), where Q(x, \\mu, v)= \\int_{\\mathrm{Y}}\\left(\\sigma P_1\\right)\\left(\\sigma P_1\\right)^*(x, \\mu, y, v) v(d y). Substitution yields I(\\varphi)=\\frac{1}{2} \\int_0^T\\left|Q^{-1 / 2}\\left(\\dot{\\varphi}_t-\\bar{b}\\left(\\varphi_t, \\mathrm{~L}_{\\bar{X}_t}\\right)\\right)\\right|^2 d t\\left(\\varphi\\right. if \\varphi is absolutely continuous and \\varphi_0=x, otherwise \\left. I(\\varphi)=+\\infty\\right).",
            "Verify the Laplace principle upper and lower bounds. Lower bound: Using the variational representation and the convergence of viable pairs, prove \\liminf _{\\delta \\rightarrow 0}\\left(-\\delta \\log \\mathrm{E}\\left[\\exp \\left(-\\Lambda\\left(X^\\delta\\right) / \\delta\\right)\\right]\\right) \\geq \\inf _{\\varphi}[I(\\varphi)+\\Lambda(\\varphi)]. Upper bound: Construct a feedback control \\bar{h}_t(y) and prove \\lim \\sup _{\\delta \\rightarrow 0}\\left(-\\delta \\log \\mathrm{E}\\left[\\exp \\left(-\\Lambda\\left(X^\\delta\\right) / \\delta\\right)\\right]\\right) \\leq \\inf _{\\varphi}[I(\\varphi)+\\Lambda(\\varphi)], relying on averaging convergence and optimal control.",
            "Confirm the rate function is well-defined. Due to the uniform positive definiteness of \\sigma \\sigma^* (assuming c_1|\\xi|^2 \\leq\\left\\langle\\sigma \\sigma^* \\xi, \\xi\\right\\rangle \\leq c_2|\\xi|^2 ），Q is invertible, hence Q^{-1 / 2} exists."
        ],
        "step_count": 8
    },
    "recuTQ44xkBvoS": {
        "reasoning_steps": [
            "Consider two label variables: $z_1=\\phi(\\gamma\\langle x,w\\rangle+\\xi_0)+\\xi$ and $z_2=\\phi(\\gamma\\langle x,w\\rangle)+\\xi$, derive and utilize the upper bound on the conditional TV: $\\mathrm{TV}(z_1 | x, z_2 | x) \\leq \\mathbb{E}_{\\xi_0} \\left[ \\mathrm{TV}(z_1 | x, \\xi_0, z_2 | x, \\xi_0) \\right]$",
            "Analyze the conditional distribution: Given the conditions, the conditional TV distance corresponds to the TV between two Gaussian distributions with the same covariance matrix.",
            "Using Pinsker's inequality, we obtain $\\mathrm{TV}(z_1 | x, \\xi_0, z_2 | x, \\xi_0) \\leq \\sqrt{ \\frac{1}{2} \\mathrm{KL} \\left( \\mathcal{N}(\\mu_1, \\sigma^2) | \\mathcal{N}(\\mu_2, \\sigma^2) \\right)}$",
            "Using the KL divergence formula, we obtain $\\mathrm{TV}(z_1 | x, \\xi_0, z_2 | x, \\xi_0) \\leq \\sqrt{ \\frac{1}{2} \\frac{(\\phi(a + \\xi_0) - \\phi(a))^2}{2\\sigma^2} } = \\frac{|\\phi(a + \\xi_0) - \\phi(a)|}{2\\sigma}$, where $a = \\gamma\\langle x,w\\rangle$",
            "Using the 1-Lipschitz property, we obtain $\\mathrm{TV}(z_1 | x, z_2 | x) \\leq \\mathbb{E}_{\\xi_0} \\left[ \\frac{|\\xi_0|}{2\\sigma} \\right] = \\frac{1}{2\\sigma} \\mathbb{E}[|\\xi_0|]$",
            "Substituting into the final expression yields $\\mathrm{TV}(z_1 | x, z_2 | x) \\leq \\frac{1}{2\\sigma} \\sqrt{\\frac{2\\beta}{\\pi}} = \\frac{\\sqrt{2\\beta}}{2\\sqrt{\\pi}\\,\\sigma} = \\frac{\\sqrt{\\beta}}{\\sqrt{2\\pi}\\,\\sigma}$"
        ],
        "step_count": 6
    },
    "recuUq1hIQhYmj": {
        "reasoning_steps": [
            "A circular fluid bubble is a steady-state solution of the two-phase Stokes problem with surface tension (concept_1).",
            "The well-posedness of the PDE system, which involves existence, uniqueness, and stability of solutions (concept_2), is crucial here.",
            "The problem uses the HLS parametrization (concept_3) for the interface, which provides a suitable framework for analysis.",
            "By applying spectral decomposition of the linearized operator (concept_4) around the circular steady state, the dynamics of the interface can be studied.",
            "For an initial contour sufficiently close to a circle (small initial perturbation), this technique helps show that the system has a unique solution that exists globally in time.",
            "Additionally, the solution decays towards the circular steady state, indicating global time stability. Thus, such a fluid bubble exists."
        ],
        "step_count": 6
    },
    "recuUuMA1Pj7ES": {
        "reasoning_steps": [
            "Na doping induces a transition in the CO₂ adsorption mode from physisorption to chemisorption, thus leading to a stronger adsorption strength toward CO₂",
            "Na-C bond length is within the microporous of coal, thus the strong Na–CO₂ interaction causes CO₂ molecules to accumulate persistently around the Na sites. This progressive accumulation of CO₂ within the micropores blocks the adsorption pathways. Therefore, the Na doping leads to a smaller adsorption amount toward CO₂"
        ],
        "step_count": 2
    },
    "recuUvXgabyFbz": {
        "reasoning_steps": [
            "Based on concept_3 (definitions of A-exciton and HX), identify the key excitons of monolayer\\(\\ce{ceWSe_{2}}\\)in the unstrained state. Specifically, A-exciton is formed by the valence band (VB) and the first conduction band (CB1), with a binding energy of 0.48 eV and an absorption peak energy of 1.70 eV; HX (higher-order exciton) is formed by VB and the second conduction band (CB2), with a binding energy of 0.58 eV and an absorption peak energy of 3.35 eV. These values are derived from the calculation results of the two-dimensional Mot-Wannier exciton model (concept_2), which is determined by the modified hydrogen atom formula\\(E_{\\text{B}}=\\frac{8\\mu}{(1 + \\sqrt{1+32\\pi\\alpha\\mu/3})^{2}}\\)combined with band dispersion.",
            "Apply concept_1 (exciton double resonance mechanism) and analyze how double resonance enhances \\(\\chi_{\\text {yyy}}\\). Double resonance requires two exciton energy levels to satisfy\\(E_{\\text{B}} = 2E_{\\text{A}}\\)（(i.e., the energy absorbed by HX is equal to twice the energy absorbed by A-exciton), which can significantly enhance the second-order nonlinear magnetization because this condition aligns single photon (1-p) and two-photon (2-p) resonances at the same time, enhancing the energy matching between the virtual and real states. In the state of no strain (step 1)\\(E_{\\text{HX}} = 3.35\\ \\text{eV}\\)和\\(2E_{\\text{A}} = 3.40\\ \\text{eV}\\)are close but not completely equal, so external regulation is required to achieve precise double resonance.",
            "Using theorem_1 (strain band coupling effect), design a strain strategy to meet the double resonance condition. Biaxial strain \\(\\eta\\) linear modulation band structure, exciton energy response is\\(E(\\eta)=E_{0}+\\beta\\eta\\), where \\(\\beta\\) is the strain coefficient (A-exciton, \\(\\beta_{\\text{A}}\\approx-132.6\\ \\text{meV}/\\%\\);HX,\\(\\beta_{\\text{HX}}\\approx35.4\\ \\text{meV}/\\%\\)). By applying a small amount of tensile strain (such as\\(\\eta = 0.16\\%\\)), adjusting the exciton absorption energy to\\(E_{\\text{A}} = 1.67\\ \\text{eV}\\)"
        ],
        "step_count": 3
    },
    "recuUx6RZIHIw6": {
        "reasoning_steps": [
            "Introduce the Levenberg-Marquardt algorithm with the regularization parameter choice $\\lambda_k = \\mu_k ||J_k^T F_k||^\\delta$, where $\\delta \\in (0,2]$, and $\\mu_k$ is updated using trust region techniques to ensure step acceptance.",
            "State the assumptions (concept_2) : Jacobian $J(w)$ is Lipschitz continuous on a neighborhood of the solution, meaning $||J(w) - J(v)|| \\le L1 ||w - v||$ for some constant $L1$, and the local error bound condition holds, i.e., $c * dist(w, S) \\le ||J(w)^T F(w)||$ for some constant $c$, where $dist(w, S)$ is the distance from $w$ to the solution set $S$.",
            "The step $d_k$ satisfies $||d_k|| \\le c1 * dist(w_k, S)$, indicating that the step size is proportional to the distance to the solution set.",
            "$\\lambda_k \\le c2 * dist(w_k, S)^\\delta$, due to the relationship between $||J_k^T F_k||$ and $dist(w_k, S)$ via the error bound condition.",
            "Analyze $||J(w_{k+1})^T F(w_{k+1})||$ using the error bound (concept_1), Lipschitz continuity (concept_2), and step bounds to derive $dist(w_{k+1}, S) \\le c3 * dist(w_k, S)^{min{1+\\delta, 2}}$ for some constant $c3$.",
            "Conclude that the sequence ${w_k}$ converges to the solution set $S$ with a local convergence rate of $min{1+\\delta, 2}$. When $\\delta \\in (0,1)$, the rate is $1+\\delta$ (superlinear) (Theorem 1), and when $\\delta \\in [1,2]$, the rate is 2 (quadratic)."
        ],
        "step_count": 6
    },
    "recuUAuq72bAav": {
        "reasoning_steps": [
            "According to the Concept_1, the S doping induces the formation of Ti-O-S bonds and SO42- ions on the surface of S-TiO2. It will generate oxygen vacancies and reduce the band gap of TiO₂, thus leading to the highest activity.",
            "The S doping exhibits the smallest arc radius of the impedance curve. According to the Nyquist cycle principle, S doping induces the fastest transmission rate of electrons and holes. Therefore, the S doping leads to the highest activity."
        ],
        "step_count": 2
    },
    "recuUAxAsOdXSl": {
        "reasoning_steps": [
            "Assign initial values to variables such as k, ε, c^2, ε_c, and D_t.",
            "Coupling Concept 1 and Concept 2 to obtain the distributions of the pressure field and velocity field.",
            "Coupling Concept 3 and Concept 4 to obtain the distributions of k and ε.",
            "Coupling the results from 1) and 2) into 3) to obtain μ_t, and verify whether the distribution of k and ε in 3) is correct.",
            "Solve Concept 6 and obtain the distribution of the mass fraction Yi.",
            "Solve Concept 7 and obtain the distribution of c^2.",
            "Substitute the results from Step 5 into Concept 8 and derive the distribution of ε_c.",
            "Substitute the values of k and ε obtained from Step 3, c^2 from Step 6, and ε_c from Step 7 into Concept 9 to solve for D_t."
        ],
        "step_count": 8
    },
    "recuUCIeRUPBHf": {
        "reasoning_steps": [
            "According to the question, the known experimental observations are: After auricular injury, rabbits can fully regenerate, including cartilage and skin; After auricular injury, mice undergo only fibrotic repair and cannot restore the missing structures. Conclusion: The differences in auricular regeneration capacity between species suggest the regulatory role of key genes or signaling pathways.",
            "Using single-cell transcriptomics (scRNA-seq) to analyze auricular tissue, focusing on wound-induced fibroblasts (WIFs), because they are the main executors of regeneration. Results showed that rabbit WIFs are more likely to enter the regeneration/burst proliferation-differentiation trajectory, whereas mouse WIFs are more inclined toward repair/myofibroblast differentiation.",
            "Rabbit WIFs exhibit regeneration-associated gene features (e.g., Bmp2, Lef1, Fgf18, Pdgfd, Scube2), whereas mouse WIFs tend toward repair/myofibroblast differentiation with high expression of related genes (e.g., Acta2/α-SMA).",
            "By comparing regenerating species (rabbits) and non-regenerating species (mice), nine core genes were found to be enriched in regenerating tissue, among which Aldh1a2 was highly expressed in rabbits but lowly expressed in mice.",
            "Stereo-seq and RNAscope/immunostaining showed that Aldh1a2 was significantly expressed in the rabbit auricular wound region, but was almost undetectable in mice.",
            "Time-series analysis (10 dpi, 30 dpi) showed that Aldh1a2 was strongly activated early in rabbits but expressed at low levels in mice, suggesting its importance in the initiation of regeneration.",
            "In mice, stable activation of Aldh1a2 mediated by AAV-transposase in auricular tissue significantly improved earhole closure.",
            "Substance supplementation experiments showed that intraperitoneal injection of retinoic acid (RA) enabled complete earhole closure and cartilage regeneration in mice, whereas injection of precursor substances (e.g., retinol, TLZ, DMSO) was less effective.",
            "These experiments demonstrate that Aldh1a2 activity and the RA signaling pathway are core factors controlling auricular regeneration in mice.",
            "Aldh1a2 encodes retinaldehyde dehydrogenase, which synthesizes retinoic acid. Its upregulation increases local RA levels. RA regulates WIFs to maintain a “low-differentiation/primitive” state, activates the regeneration program, and suppresses myofibroblast differentiation (Acta2 downregulation). Therefore, the logical chain is: Aldh1a2 → RA signaling → WIF activation → regeneration execution."
        ],
        "step_count": 10
    },
    "recuUKUkTzcH42": {
        "reasoning_steps": [
            "First, based on concept_1, the paper uses Convex Analysis methods which are crucial for studying dual representations as convex functions often have dual characterizations.",
            "Then, theorem_1 states the establishment of an abstract variational principle, which is fundamental in relating pressure and entropy.",
            "The pressure function $\\Gamma$ in concept_2 is a key component in thermodynamic formalism, and its definition is closely linked to the supremum over measures involving entropy.",
            "The entropy function $\\mathcal{h}$ in concept_3, being an affine entropy-like map, is paired with the pressure function in the variational principle.",
            "Using theorem_2 on duality in convex functions, the pressure function, as a convex function, can be represented as the supremum of linear functionals involving the entropy and the integral of the potential.",
            "Conversely, the entropy functional, being the dual of the pressure function, is expressed as the infimum over the pressure function minus the integral of the potential, completing the dual representations."
        ],
        "step_count": 6
    },
    "recuUNjx6dbcyQ": {
        "reasoning_steps": [
            "Dismantle the core problem and think clearly about the framework First, we need to focus on the core requirements of Instruction: with FeOOH as the catalytic core, we can solve the problem of directional conversion of waste biomass into artificial humus from the two dimensions of \"component optimization and property improvement\" and \"enhancing carbon sequestration capacity\". Based on the common properties of iron-based catalysts (Concept1) and the reaction law of key components of humus (Concept2), the regulatory mechanism of FeOOH on humus components and properties is analyzed. Combined with the correlation between carbon flow direction and carbon sequestration (Concept3), the path to enhance carbon sequestration capacity is derived to ensure that each analysis dimension can be associated with the corresponding concept to avoid logical faults.",
            "Optimization of humic acid functional groups by FeOOH based on iron-based catalyst commonality (associated with Concept1 and Concept2) First, Concept1 pointed out that nFe₂O₃ (iron-based oxide) treatment can increase the C—O bonds of humic acid (HAs) in artificial humic acid (AHS). As a typical iron-based catalytic material, FeOOH has similar redox properties to nFe₂O₃ at its active site, so it can be inferred by analogy that FeOOH can also promote the formation of C—O bonds in HAs through catalysis. Further combined with Concept2 (humus containing oxygen-containing functional groups such as carboxyl groups, phenolic groups, and carbonyl groups), it can be seen that C-O bonds are the core structural units of these oxygen-containing functional groups (e.g., carboxyl-COOH, phenolic-OH, etc.). Therefore, the increase of C—O bonds inevitably leads to an increase in the abundance of oxygen-containing active functional groups in HAs, reduces the proportion of small molecules that are easily decomposed of organic matter, and realizes the directional optimization of artificial humus core components (HAs).",
            "Analyze the regulation of biomass humification pathway by FeOOH in combination with the conversion law of polysaccharides (Associated with Concept2) Concept2 clarified that \"polysaccharide dehydration to furan can catalyze the conversion of biomass to humic acid\". The main components of waste biomass include polysaccharides (e.g., cellulose, hemicellulose hydrolysate), which are easily decomposed into volatile small organic compounds (e.g., CO₂, small molecular alcohols) if not catalyzed directedly, resulting in carbon loss and inefficient conversion to humic acid. As a catalyst, FeOOH can promote the dehydration of hydroxyl groups in polysaccharide molecules by reducing the activation energy of polysaccharide dehydration reaction, and produce furan intermediate compounds. These intermediate products have high reactivity and can quickly participate in subsequent condensation and polymerization reactions, and are directly converted into structural units of humic acid rather than to non-target products. This process accurately utilizes the polysaccharide conversion mechanism of Concept2 to realize the directional path regulation of \"biomass→ polysaccharides→furans→ humic acids\", which not only increases the proportion of humic acid, but also lays the foundation for carbon retention, which is logically in line with the \"path-oriented\" principle of catalytic reactions.",
            "Derive the improvement of properties from component optimization, and establish a \"component-property\" association (associate Concept1, Concept2) Based on the component optimization results of step 2 (increased C-O bonds, enrichment of oxygen-containing functional groups) and step 3 (increase in the proportion of HAs), further derive the changes in the properties of artificial humus: Improved chemical stability: The increase of C—O bonds in Concept1 will form a denser cross-linked structure (e.g., humic acid molecules are connected by C—O—C bonds), This structure can resist the decomposition of microorganisms (microorganisms are difficult to break stable C-O bonds), reduce the biodegradation rate of humus, and prolong its existence time in the environment. Functional activity enhancement: The oxygen-containing functional groups such as carboxyl and phenolic groups mentioned in Concept2 are the key sites for the binding of humus to metal ions and nutrients, and its abundance will enhance the adsorption capacity and nutrient retention capacity of humus, and improve its functional properties. This step connects Concept1 and Concept2 through the logical chain of \"component change→ structural change→ property change\", and derives the process in line with the basic chemical principle of determining the properties of the structure of matter.",
            "Analyze the \"directional enrichment\" effect of FeOOH on carbon sequestration capacity based on carbon flow law (Concept1, Concept2, Concept3 linked) Concept3 points out that \"artificial humification can enhance carbon sequestration by directing carbon to AHS\", which is the core logic of carbon sequestration - carbon can only be achieved efficiently if it remains in stable humic acid. FeOOH enables the directional guidance of carbon through the following paths, each associated with a corresponding concept: Carbon source conversion guidance: Combined with Concept2, FeOOH catalyzes the dehydration of polysaccharides to form furans and converts them into humic acid, converting the \"polysaccharide carbon\" (easily decomposed into CO₂ or small molecules) in biomass into \"humic acid carbon\" to avoid carbon loss in the form of gaseous or easy loss. Targeted construction of carbon pool: Finally, through the above two steps, the carbon in biomass is concentrated and directed to the core carbon pool of artificial humic acid (AHS), which is fully in line with the law of Concept3 \"carbon to AHS to guide and enhance carbon sequestration\", realizing efficient carbon enrichment, and logically forming a closed loop of \"catalytic regulation→ carbon flow to stable carbon pool →carbon sequestration enhancement\"."
        ],
        "step_count": 5
    },
    "recuUCQatgMMwP": {
        "reasoning_steps": [
            "(From Concept_1) If LOM occurrence requires high M–O covalency, but Fe–O exhibits stronger ionic character, the tendency for LOM is weakened: (LOM → high covalency) ∧ (Fe–O is more ionic) ⇒ ¬preference(LOM).",
            "(From Concept_3) The key distinction among the three mechanisms lies in O–O bond formation: AEM = adsorbed O· bonds with water/HO⁻; IMOC = intramolecular coupling of two O· radicals at the same/adjacent sites; LOM = lattice oxygen participates in bond formation.",
            "(From Concept_2) If Fe sites are high-spin and stabilize adsorbed O· radicals, the 'adsorbed O·'-centered bond formation pathway becomes kinetically more feasible: high-spin(Fe) ⇒ stabilizes(O·) ∧ ↓E‡ (for adsorbed O·-centered bonding).",
            "(Structural constraints from Concept_3) IMOC requires spatial proximity and synchronous coupling of two O· radicals, imposing stronger configurational/synchronization constraints; AEM only involves one adsorbed O· reacting with water/HO⁻, with weaker constraints: constraints(IMOC) > constraints(AEM).",
            "(Decision criterion) Given ¬preference(LOM), between the two 'adsorbed O·'-centered candidate pathways, prioritize the one with weaker configurational/synchronization constraints and lower expected energy barrier ⇒ preference(AEM)."
        ],
        "step_count": 5
    },
    "recuUn0dsRK8W5": {
        "reasoning_steps": [
            "The core of the problem is to bound the number of iterations in the `Find-Weakly-Robust-Detailing` procedure's main loop (Concept_3). We can analyze this using the index of the detailing, $Ind(\\xi)$, as a potential function that measures progress toward the goal (Concept_1).",
            "The loop continues only if the current detailing is not $(\\delta, k)$-weakly robust (Concept_2). In each such iteration, the `Test-Weakly-Robust-Detailing` subroutine is called and fails, which is the condition for the loop to execute again.",
            "A failure in the `Test-Weakly-Robust-Detailing` subroutine is designed to be productive. It returns a new set of variables that guarantees a strictly positive increase in the detailing's index (Concept_4). The minimum guaranteed increase per iteration is a fixed amount related to the parameter $\\delta$, specifically $4\\delta/10$ or $2\\delta/5$.",
            "The total amount of increase is limited. The index of any detailing is bounded and must lie within the range $[0, 1]$ (Theorem_1). Therefore, the total increase accumulated across all iterations cannot exceed the maximum possible value of 1.",
            "To find the maximum number of iterations, we divide the total possible increase of the potential function by the minimum guaranteed increase per step. This gives us the calculation: Maximum Iterations = (Total Range of Index) / (Minimum Increase per Iteration).",
            "Plugging in the values, we get Maximum Iterations = $1 / (2\\delta/5)$. This simplifies to the final answer, $5/(2\\delta)$, which represents the upper bound on the number of times the loop can execute before the index would theoretically have to exceed 1."
        ],
        "step_count": 6
    },
    "recuUuxQ6qLQhD": {
        "reasoning_steps": [
            "The problem requires finding the maximum length of the cut-transpose sequence for two $3 \\times 3$ irreducible matrices, A and B, which are known to be Principal Minor Equivalent (PME).",
            "The cut-transpose operation is a transformation that can only be performed on a matrix that possesses a 'cut'.",
            "According to concept_3, the very definition of a cut is restricted to matrices of size $n \\ge 4$.",
            "Since the matrices in this problem are $3 \\times 3$, they do not meet the minimum size requirement for a cut to exist. Therefore, the cut-transpose operation is not applicable.",
            "The resolution for this specific case is provided directly by theorem_1. It states that for an irreducible matrix of size $n=3$, the property of being PME to another matrix is a sufficiently strong condition that it implies the two matrices are already diagonally equivalent.",
            "As PME directly leads to diagonal equivalence for this dimension, no intermediate transformation steps are required to relate matrix A and matrix B.",
            "Consequently, the sequence of cut-transpose operations is empty, and its maximum length is 0."
        ],
        "step_count": 7
    },
    "recuUVeBR5w0MW": {
        "reasoning_steps": [
            "The instruction asks for the total number of bilinear equations in the system $S^A(u, v, w_{\\circ}w)$ for a specific instance in $S_4$: $u=2143$, $v=3124$, and $w=4132$. The long word is $w_{\\circ}=4321$, so $w_{\\circ}w = 1423$.",
            "The system of equations is constructed based on the geometric interpretation of Schubert coefficients (Concept_2). The solutions to the system correspond to flags $\\omega^\\bullet$ in the intersection of three Schubert varieties: $\\Omega_{u}(F^{\\bullet})\\cap\\Omega_{v}(G^{\\bullet})\\cap\\Omega_{w_{\\circ}w}(E^{\\bullet})$.",
            "We use the lifted formulation (Concept_5). A flag $\\omega^\\bullet$ is parameterized by Stiefel coordinates (Concept_4) and is taken from the cell $\\Omega_{w_{\\circ}w}(E^\\bullet)$. We then impose conditions for this flag to also belong to $\\Omega_u(F^\\bullet)$ and $\\Omega_v(G^\\bullet)$.",
            "The number of conditions is determined by the maximal descent $d$ (Concept_6). For the given permutations, the paper's example in Section 5.4 identifies the maximal descent as $d=3$.",
            "The first set of conditions ensures that the flag $\\omega^\\bullet$ lies in $\\Omega_u^A(F^\\bullet)$. According to the characterization theorem (Theorem_1), this requires that for each $i \\in [d]$, the constructed vector $g_i(x,\\alpha)$ is in the subspace $F^{u_i-1} - F^{u_i}$. This translates to a set of equations $(\\begin{matrix}y_{j1}&...&y_{jn}\\end{matrix})\\cdot g_i(x,\\alpha)=0$ for each $j < u_i$. - For $i=1$: $u_1=2$. The condition applies for $j < 2$, so $j=1$. This gives 1 equation. - For $i=2$: $u_2=1$. There are no $j < 1$. This gives 0 equations. - For $i=3$: $u_3=4$. The condition applies for $j < 4$, so $j=1, 2, 3$. This gives 3 equations. - The total number of equations for this part is $1 + 0 + 3 = 4$.",
            "The second set of conditions ensures the flag $\\omega^\\bullet$ lies in $\\Omega_v^A(G^\\bullet)$. Similarly, this requires $h_i(x,\\beta) \\in G^{v_i-1} - G^{v_i}$, which gives equations $(\\begin{matrix}z_{j1}&...&z_{jn}\\end{matrix})\\cdot h_i(x,\\beta)=0$ for each $j < v_i$. - For $i=1$: $v_1=3$. The condition applies for $j < 3$, so $j=1, 2$. This gives 2 equations. - For $i=2$: $v_2=1$. There are no $j < 1$. This gives 0 equations. - For $i=3$: $v_3=2$. The condition applies for $j < 2$, so $j=1$. This gives 1 equation. - The total number of equations for this part is $2 + 0 + 1 = 3$.",
            "The total number of equations in the system $S^A(u,v,w_{\\circ}w)$ is the sum of the equations from both sets of conditions. Total equations = $4 + 3 = 7$."
        ],
        "step_count": 7
    },
    "recuUYqNZTp52V": {
        "reasoning_steps": [
            "USE Theorem 2, calculate the DRL for ULA. \\eta_{ula}^2=1/\\left|\\theta_{1}-\\theta_{2}\\right|^2=(M d / \\lambda)^2\\cos ^{2} \\theta_{0} M \\rho^{2}/6",
            "USE Theorem 2 and Theorem 3, calculate the \\left|\\theta_{1}-\\theta_{2}\\right| for CLA. \\eta_{cla}^2=1/\\left|\\theta_{1}-\\theta_{2}\\right|^2=(M_1M_2 d / \\lambda)^2\\cos ^{2} \\theta_{0} M \\rho^{2}/6",
            "calculate the ratio \\mu=\\eta_{ula}/\\eta_{cla}=M_1M_2/M",
            "USE Theorem 1, the CLA has a better DRL if 𝑀1 =𝑀2.",
            "Consider that ( M_1 ) and ( M_2 ) are coprime integers (where ( M_1 < M_2 )). Discuss the cases separately based on whether ( M ) is odd or even. It is concluded that when ( M_1 = M/2 ) and ( M_2 = M/2 + 1 ), ( \\mu ) attains its maximum value of ( (M + 2)/4 )."
        ],
        "step_count": 5
    },
    "recuUYu3dnlA7M": {
        "reasoning_steps": [
            "The primary source of recourse in the algorithm is the `ROBUSTIFY` procedure executed at the end of each epoch. The cost of this procedure is measured by the number of calls to its subroutine, `MAKE-ROBUST` (Concept_1).",
            "To analyze this cost, centers from the initial solution `$\\mathcal{U}_{init}$` are classified as either 'clean' or 'contaminated' based on their proximity to the points updated during the epoch (Concept_2). This classification is key to understanding why a center might need to be re-processed.",
            "The `MAKE-ROBUST` calls are then partitioned into three distinct types. Type I calls handle newly introduced centers. Type II calls handle centers from `$\\mathcal{U}_{init}$` that were 'contaminated' by updates (Concept_3, Concept_4). Type III calls handle 'clean' centers from `$\\mathcal{U}_{init}$` that, while still robust, need their robustness parameter `t` to be increased due to changes in inter-center distances (Concept_4).",
            "We first bound the amortized number of Type I and Type II calls. The number of Type I calls is $O(1)$ per update on an amortized basis. For Type II calls, we use the fact that each update can contaminate at most $O(\\log \\Delta)$ centers (Theorem_1). This directly implies that the amortized number of Type II calls per update is $O(\\log \\Delta)$.",
            "The analysis of Type III calls relies on a 'charging' argument. A Type III call for a center is part of a sequence, or 'chain,' where the center's robustness parameter `t` strictly increases with each call. Since `t` is bounded by `$\\lceil \\log \\Delta \\rceil$`, any such chain has a maximum length of $O(\\log \\Delta)$ (Concept_5).",
            "Crucially, each chain of Type III calls must be initiated by a center that was created via a Type I or Type II call. Therefore, we can charge the total cost of a Type III chain (of length $O(\\log \\Delta)$) to the single Type I or Type II call that started it (Concept_5).",
            "Combining these parts, the total amortized recourse is the sum of the amortized rates of Type I and Type II calls, multiplied by the maximum length of a Type III chain that each can trigger. This yields an amortized recourse of $(O(1) + O(\\log \\Delta)) \\times O(\\log \\Delta) = O(\\log^{2}\\Delta)$."
        ],
        "step_count": 7
    },
    "recuV2SmOpgnXH": {
        "reasoning_steps": [
            "Align parameters between the concept/theorem and Definition 1. Set \\( r = \\frac{1}{\\lambda} \\) to link the two frameworks.",
            "Recast key definitions for local minimizers of \\(\\varphi\\) and \\(e_\\lambda \\varphi\\).",
            "Prove that a local minimizer of \\(\\varphi\\) implies a local minimizer of \\(e_\\lambda \\varphi\\).",
            "Prove that a local minimizer of \\(e_\\lambda \\varphi\\) implies a local minimizer of \\(\\varphi\\).",
            "Verify the usage and logical consistency of concepts/theorems.",
            "Conclude the relationship between local minimizers of \\(\\varphi\\) and \\(e_\\lambda \\varphi\\)."
        ],
        "step_count": 6
    },
    "recuV3zhuMdGph": {
        "reasoning_steps": [
            "According to concept 1, S-doping in the first coordination sphere of FeN3-S1-Cy results in a slight elongation of the Fe-N bond and the formation of a Fe-S bond. The FeN3-S2-Cy and FeN3-S3-Cy exhibit the absence of Fe-S bond because of the long distance between Fe and S atoms.",
            "As for the FeN3-S1-Cy, the Fe-S bond leads to the less stable and weakens the adsorption of O2. As for the FeN3-S3-Cy, the distance between Fe and S atoms is too long to enhance the spin polarization.",
            "By comparison, the FeN3-S2-Cy with the S atom located at the second neighbor position from Fe center exhibits the enhanced spin polarization, and the distance between Fe and S atoms is moderate. Thus, Fe-N4-S2-Cy provides the highest ORR activity."
        ],
        "step_count": 3
    },
    "recuV5gM1NuuxJ": {
        "reasoning_steps": [
            "Two-body reduction (Concept_1): go to relative/center-of-mass coordinates with reduced mass \\(\\mu = m_c m_b/(m_c+m_b)\\) (set \\(\\hbar=c=1\\) if desired).",
            "HO dynamics (Concept_2): use a 3D isotropic HO for the relative motion, \\(V(r)=\\tfrac12\\,\\mu\\omega^2 r^2\\), and define widths \\(b=\\sqrt{\\hbar/(\\mu\\omega)}\\), \\(\\sigma\\equiv 1/b=\\sqrt{\\mu\\omega/\\hbar}\\).",
            "Eigenfunctions (Concept_3): recall HO forms—1S is a Gaussian; 1P is Gaussian times a linear polynomial; angular dependence given by spherical harmonics \\(Y_{\\ell m}\\).",
            "Momentum wavefunctions (Concept_4): Fourier transform the relative-coordinate eigenfunctions to get \\(\\tilde\\psi_{1S}(\\mathbf k)\\propto e^{-\\sigma^2 k^2/2}\\) and \\(\\tilde\\psi_{1P}(\\mathbf k)\\propto (\\sigma k)\\,Y_{1m}(\\hat k)\\,e^{-\\sigma^2 k^2/2}\\).",
            "Wigner transform to momentum space (Concept_5): using the definition and integrating over \\(\\mathbf r\\), the momentum-only Wigner density is proportional to \\(|\\tilde\\psi(\\mathbf k)|^2\\); thus the Gaussian exponent becomes \\(e^{-\\sigma^2 k^2}\\) (no 1/2), and 1P carries a factor \\((\\sigma k)^2\\).",
            "m-substate averaging (Concept_6): average the 1P result over magnetic substates \\(m\\) to obtain the isotropic factor \\(2/3\\) multiplying \\(\\sigma^2 k^2\\).",
            "Normalization convention (Concept_7): adopt box/plane-wave normalization so that \\(\\int d^3k\\, W(\\mathbf k)=(2\\pi)^3/V\\); evaluate \\(\\int d^3k\\,e^{-\\sigma^2 k^2}=\\pi^{3/2}/\\sigma^3\\) to fix the prefactor as \\((2\\sqrt\\pi\\,\\sigma)^3/V\\)."
        ],
        "step_count": 7
    },
    "recuV8TecEC2sZ": {
        "reasoning_steps": [
            "Map the physical problem to a matrix–majorization setting. In the energy eigenbasis $\\{|h_i\\rangle\\}$, the diagonal quantum states $\\rho$ and $\\sigma$ are identified with their corresponding probability vectors $\\lambda_{\\rho}$ and $\\lambda_{\\sigma}$, while the Gibbs state $\\gamma_{\\beta}$ is identified with $\\lambda_{\\beta}$. The asymptotic catalytic Gibbs-preserving transformation $\\rho \\to \\sigma$ is equivalent to an asymptotic catalytic matrix majorization from $(\\lambda_{\\rho},\\lambda_{\\beta})$ to $(\\lambda_{\\sigma},\\lambda_{\\beta})$, where the second column $\\lambda_{\\beta}$ has full support since $E_i>0$ for all $i$. Given $\\supp(\\rho) \\subsetneq \\supp(\\lambda_{\\beta})$ and $\\supp(\\sigma) \\subsetneq \\supp(\\lambda_{\\beta})$, this is the dominating–column case of the semiring framework.",
            "Specialize the general characterization to the non–full–rank case. In the full–rank case, asymptotic catalytic thermal majorization requires $D_{\\alpha}(\\lambda_{\\rho}\\|\\lambda_{\\beta}) \\ge D_{\\alpha}(\\lambda_{\\sigma}\\|\\lambda_{\\beta})$ and $D_{\\alpha}(\\lambda_{\\beta}\\|\\lambda_{\\rho}) \\ge D_{\\alpha}(\\lambda_{\\beta}\\|\\lambda_{\\sigma})$ for all $\\alpha \\ge \\tfrac12$. Here $\\rho$ and $\\sigma$ are not full rank, while $\\lambda_{\\beta}$ is full rank and dominates their supports, so $D_{\\alpha}(\\lambda_{\\beta}\\|\\lambda_{\\rho})$ is infinite for $\\alpha \\ge 1$ and only the one–sided family $D_{\\alpha}(\\lambda_{\\rho}\\|\\lambda_{\\beta}) \\ge D_{\\alpha}(\\lambda_{\\sigma}\\|\\lambda_{\\beta})$ for $\\alpha \\in [0,\\infty]$ remains relevant.",
            "Recall the divergence definition. For probability distributions $p = (p_i)_{i \\in I}$ and $q = (q_i)_{i \\in J}$ with $\\supp(p) \\subseteq \\supp(q)$, the classical Rényi divergence $D_{\\alpha}(p\\|q)$ is: $\\alpha=0$: $D_{\\alpha} = -\\log \\sum_{i \\in I \\cap J} q_i$; $\\alpha = 1$: $D_{\\alpha} = \\sum_{i \\in I} p_i \\log \\frac{p_i}{q_i}$; $\\alpha = \\infty$: $D_{\\alpha} = \\max_{i \\in I} \\log \\frac{p_i}{q_i}$; and for $\\alpha \\in (0,1) \\cup (1,\\infty)$: $D_{\\alpha} = \\frac{1}{\\alpha - 1} \\log \\sum_{i \\in I \\cap J} p_i^{\\alpha} q_i^{\\,1-\\alpha}$. Here $I = \\supp(p)$, $J = \\supp(q)$, and in our case $p \\in \\{\\lambda_{\\rho},\\lambda_{\\sigma}\\}$, $q = \\lambda_{\\beta}$.",
            "Connect to the optimal rate via the Vergleichsstellensatz. This result implies that for power–universal elements such as $(\\lambda_{\\rho},\\lambda_{\\beta})$ with a dominating column, the optimal asymptotic conversion rate is the minimum over all ratios of monotone quantities, which here are exactly $\\{D_{\\alpha}(\\cdot\\|\\lambda_{\\beta}): \\alpha \\in [0,\\infty]\\}$.",
            "Write down the rate formula. The general rate expression for asymptotic catalytic majorization in this setting is $r = \\min_{\\alpha \\in [0,\\infty]} \\frac{D_{\\alpha}(\\lambda_{\\rho} \\| \\lambda_{\\beta})}{D_{\\alpha}(\\lambda_{\\sigma} \\| \\lambda_{\\beta})}$, which gives the maximal number of $\\sigma$–copies achievable per $\\rho$–copy under the dominating–column constraints for all $\\alpha \\in [0,\\infty]$."
        ],
        "step_count": 5
    },
    "recuV9nh4O3VOs": {
        "reasoning_steps": [
            "The Ir/TiO2(100) catalyst has medium Irδ+ proportion, implying its highest amount of partially reduced IrO2 species, which is favorable of CH4 activation.",
            "The binding energy of Ti 2p3/2 of Ir/TiO2 (100) is slightly positive compared to that of Ir/TiO2(101) and Ir/TiO2(001), suggesting the electron donation from TiO2 to Ir species. It favors the formation of electron-rich Ir structures on the Ir/TiO2(100) catalyst, suggesting the high activity in O2 activation during methane oxidation reaction."
        ],
        "step_count": 2
    },
    "recuVaB11c530v": {
        "reasoning_steps": [
            "Analyze from the electron transfer direction. Application Concept_1: Concept_1 demonstrated the transfer of electrons from Co to Mo2N in the Co-Mo2N system, as evidenced by an increase in Co0 binding energy (from 778.6 eV to 778.8 eV); At the same time, Mo binds energy negatively, indicating that Mo gains electrons. This is in line with the XPS analysis principle: an increase in binding energy indicates that an element loses electrons, and a decrease in binding energy indicates that an element gains electrons (rich in electrons). Logical derivation: In the Mn₃W₃₋ₓC system, the electronegativity of W (tungsten) is higher (~2.36) than that of Mn (manganese) (~1.55), so the electrons will partially transfer from Mn to W, similar to the electron transfer process in Concept_1.",
            "Analyze from the center of the metal direction. Application Concept_2: Concept_2 show that the chemical coupling of Co and Mo₂C produces a bimetallic synergistic effect, and its activity and stability are better than those of monometallic Co and commercial RuO₂. This demonstrates the advantages of bimetallic systems in catalysis: co-catalysis can optimize the reaction path while enhancing structural stability. Logical derivation: In Mn₃W₃₋ₓC, Mn and W form a bimetallic carbide structure. Similar to Concept_2, there is a synergistic effect between Mn and W, and the bimetallic center synergistically enhances activity."
        ],
        "step_count": 2
    },
    "recuVb3Nihwm7G": {
        "reasoning_steps": [
            "This is a known result in arXiv:2011.06509, Theorem 1.10, and this is known by AI. For safety, we include the result (in a slight different form in concept_1.",
            "This is the key of these questions. The idea is to reduce to (1), so it suffices to replace $\\mathcal{P}$ by a new set $\\mathcal{Q}$ with larger Cartier index and $\\mathcal{Q}$ consists of $\\epsilon$-lc varieties. Moreover, it suffices to make $\\mathcal{Q}$ to be $\\mathbb{Q}$-Gorenstein since the index of $K_X$ is known to be bounded in a bounded family. To make $\\mathcal{Q}$ to be $\\mathbb{Q}$-Gorenstein, the idea is to take a log resolution in families, and run relative minimal model program. Then the resulting total space is $\\mathbb{Q}$-factorial, so each fiber is $\\mathbb{Q}$-Gorenstein by adjunction formula. concept_2 guarantees that the resulting varieties has the same total Cartier indices. concept_5 guarantees that the resulting varieties are klt. We note that this process is the relative version of the construction of small $\\mathbb{Q}$-factorial modifications. Now it remains to show that $\\mathcal{Q}$ has larger Cartier index than $\\mathbb{P}$. This is given by concept_2.",
            "This is a famous result in arXiv:1609.05543, Theorem 1.1, and this is known by AI. Since it is famous, we don't include this in the concept.",
            "This is a known result in arXiv:1609.05543, Example 1.2. For each natural number $n \\geq 2$, let $X_n$ be the projective cone over the rational curve of $\\deg n$ in $\\mathbb{P}^n$. Then the set of $X_n$, $n\\in \\mathbb{N}$ is a counterexample. This is proposed because this has similar form as (2) but with different answer."
        ],
        "step_count": 4
    },
    "recuVdv7RHRv0K": {
        "reasoning_steps": [
            "The objective is to perform tolerant testing (Concept_3) on the k-locality property (Concept_1) of an unknown n-qubit Hamiltonian $H$. We are given black-box query access to its evolution operator $U(t)=e^{-iHt}$ and a promise that the Hamiltonian is either close to or far from being k-local: $d(H,\\text{\\(k\\)-local})\\le \\epsilon_1 \\quad\\text{or}\\quad d(H,\\text{\\(k\\)-local})\\ge \\epsilon_2,$ with a separation gap $\\Delta\\coloneqq\\epsilon_2-\\epsilon_1>0$. The distance is measured using the normalized Frobenius norm (Concept_2): $d(H,H')=\\|H-H'\\|_2=\\sqrt{\\tfrac{\\mathrm{Tr}[(H-H')^2]}{2^n}}.$",
            "Since we cannot access $H$ directly, we must measure a proxy quantity from $U(t)$. We expand the evolution operator in the n-qubit Pauli basis, $U(t)=\\sum_{x}\\hat U_x(t) P_x$. As defined in Concept_4, our proxy is the k-non-local energy: $\\mathrm{NonLocalEnergy}(k,t)\\;\\coloneqq\\;\\sum_{|x|>k}\\!|\\hat U_x(t)|^2.$ This value, which can be estimated via sampling, quantifies the extent to which $H$ violates k-locality.",
            "According to Theorem_1, for a short evolution time $t$, we can use a Taylor expansion, $U(t)=I-iHt-\\tfrac{1}{2}H^2t^2+O(t^3)$. By projecting onto the subspace of Pauli operators with locality greater than $k$ and squaring, we establish the crucial link between the proxy and the property being tested: $\\mathrm{NonLocalEnergy}(k,t)\\;=\\;t^2\\,d\\!\\left(H,\\text{\\(k\\)-local}\\right)^{2}\\;+\\;O(t^4).$ This connection defines the 'signal' we need to measure. Under the promise, we have two distinct cases: - Close case: $\\mathbb{E}[\\mathrm{NonLocalEnergy}]\\approx t^2\\epsilon_1^2+O(t^4)$ - Far case: $\\mathbb{E}[\\mathrm{NonLocalEnergy}]\\approx t^2\\epsilon_2^2+O(t^4)$ The signal gap that our protocol must resolve is the difference between these two expectations: $\\Delta E(t)\\;\\coloneqq\\;t^2(\\epsilon_2^2-\\epsilon_1^2)\\;+\\;O(t^4)\\;=\\;t^2(\\epsilon_2-\\epsilon_1)(\\epsilon_2+\\epsilon_1)+O(t^4).$",
            "As described in Concept_5, the non-local energy can be estimated using a Pauli-spectrum sampling protocol. The number of queries $N$ needed to achieve an additive estimate with precision $\\eta$ scales as: $N_{\\text{stat}}(t)\\;=\\;\\Theta\\!\\left(\\frac{1}{\\eta^2}\\right).$ To reliably distinguish the 'close' and 'far' cases, our estimation precision $\\eta$ must be on the order of the signal gap, $\\eta=\\Theta(\\Delta E(t))$. This gives the statistical query cost as a function of time $t$: $N_{\\text{stat}}(t)\\;=\\;\\Theta\\!\\left(\\frac{1}{t^4(\\epsilon_2^2-\\epsilon_1^2)^2}\\right) \\;=\\;\\Theta\\!\\left(\\frac{1}{t^4(\\epsilon_2-\\epsilon_1)^2(\\epsilon_2+\\epsilon_1)^2}\\right).$",
            "The small-time approximation introduces a systematic error, or bias, $B(t)=O(t^4)$, from the truncated higher-order terms of the Taylor series. This leads to the central trade-off described in Concept_6: $t$ must be large enough to create a resolvable signal, but small enough to keep the bias from corrupting the measurement. To ensure reliable discrimination, we require the bias to be smaller than the signal gap: $B(t)\\;\\lesssim\\;\\tfrac{1}{2}\\,\\Delta E(t) \\quad\\Rightarrow\\quad t^4\\;\\lesssim\\;t^2\\,(\\epsilon_2-\\epsilon_1)(\\epsilon_2+\\epsilon_1).$ This inequality defines the safe short-time window for the evolution: $t\\;\\le\\;c\\,\\sqrt{\\Delta}\\quad\\text{(for an absolute constant \\(c>0\\))}.$",
            "To resolve the trade-off from Concept_6 optimally, we must choose a time $t$ that minimizes the query cost $N_{\\text{stat}}(t)$ while satisfying the constraint from Step 5. Since $N_{\\text{stat}}(t) \\propto 1/t^4$, we should choose the largest possible $t$ within the safe window. Any choice within a constant factor of the upper bound, $t\\in[\\alpha\\sqrt{\\Delta},\\,\\beta\\sqrt{\\Delta}]$, will maximize the signal-to-noise ratio. We therefore fix the optimal time to be: $t_{\\star}\\;=\\;\\Theta\\!\\big(\\sqrt{\\Delta}\\big).$",
            "Now we substitute the optimal evolution time $t_{\\star}$ back into the expression for the statistical cost from Step 4. With $t=t_{\\star}$, we have $t^4=\\Theta(\\Delta^2)$, which yields: $N\\;=\\;N_{\\text{stat}}(t_{\\star}) \\;=\\;\\Theta\\!\\left(\\frac{1}{\\Delta^2\\;(\\epsilon_2^2-\\epsilon_1^2)^2}\\right) \\;=\\;\\Theta\\!\\left(\\frac{1}{\\Delta^2\\;\\big[\\Delta(\\epsilon_1+\\epsilon_2)\\big]^2}\\right) \\;=\\;\\Theta\\!\\left(\\frac{1}{\\Delta^{4}(\\epsilon_1+\\epsilon_2)^{2}}\\right).$",
            "In the standard setting of tolerant testing, $\\epsilon_1$ and $\\epsilon_2$ are treated as constants independent of other system parameters (like $n$). In this regime, the prefactor $(\\epsilon_1+\\epsilon_2)^{-2}$ is also an absolute constant. Therefore, the overall query complexity simplifies to: $N\\;=\\;O\\!\\big(\\Delta^{-4}\\big)\\;=\\;O\\!\\big((\\epsilon_2-\\epsilon_1)^{-4}\\big),$ which is notably independent of the system size $n.$",
            "The complete algorithm, based on the sampling protocol of Concept_5, is as follows: 1. Set the evolution time to $t_{\\star} = \\Theta(\\sqrt{\\epsilon_2 - \\epsilon_1})$. 2. Perform $N = O((\\epsilon_2-\\epsilon_1)^{-4})$ queries to estimate $\\widehat{\\mathrm{NonLocalEnergy}}(k,t_{\\star})$ with additive accuracy $\\eta=\\Theta(\\Delta E(t_{\\star}))$. 3. Compare the estimate against a pre-determined threshold placed between the expected values for the 'close' and 'far' cases. This procedure successfully solves the tolerant testing (Concept_3) problem.",
            "The final fourth-power scaling arises from the interplay of physical and statistical constraints: - The signal scales as $t^2\\Delta$, while the systematic bias scales as $t^4$ (from the Theorem_1 Taylor expansion). - Balancing these two effects (Concept_6) forces an optimal evolution time of $t=\\Theta(\\sqrt{\\Delta})$. - The statistical sampling cost (Concept_5) scales as $1/(\\text{signal})^2$, which is proportional to $1/(t^4\\Delta^2)$. - Substituting the optimal $t=\\Theta(\\sqrt{\\Delta})$ into the sampling cost yields the final complexity: $N \\propto 1/((\\sqrt{\\Delta})^4 \\Delta^2) = 1/\\Delta^4$."
        ],
        "step_count": 10
    },
    "recuVeljFhEWgz": {
        "reasoning_steps": [
            "By theorem_1, $\\mathbb{C}[\\mathbf{z}^v]/J_{v,w}$ is Cohen-Macaulay and has dimension $l(v)-l(w)$.",
            "Since $\\mathbb{C}[\\mathbf{z}^v]/J_{v,w}$ is Cohen-Macaulay, its a-invariant and Castelnuovo-Mumford regularity satisfy $a(\\mathbb{C}[\\mathbf{z}^v]/J_{v,w})=reg(\\mathbb{C}[\\mathbf{z}^v]/J_{v,w})-d$, where $d=l(v)-l(w)$ is the Krull dimension of $\\mathbb{C}[\\mathbf{z}^v]/J_{v,w}$.",
            "By theorem_2, $deg\\ S_{v,w}(\\mathbf{t})=deg\\ K(\\mathbb{C}[\\mathbf{z}^v]/J_{v,w},t)$. Since $\\mathbb{C}[\\mathbf{z}^v]/J_{v,w}$ is Cohen-Macaulay we have $reg(\\mathbb{C}[\\mathbf{z}^v]/J_{v,w})=deg\\ K(\\mathbb{C}[\\mathbf{z}^v]/J_{v,w},t)-ht_{\\mathbb{C}[\\mathbf{z}^v]}J_{v,w}$.",
            "$\\mathbb{C}[\\mathbf{z}^v]$ has $l(v)$ variables and $\\mathbb{C}[\\mathbf{z}^v]/J_{v,w}$ has Krull dimension $l(v)-l(w)$. So $ht_{\\mathbb{C}[\\mathbf{z}^v]}J_{v,w}=l(w)$.",
            "Combining the equations above, we have $a(\\mathbb{C}[\\mathbf{z}^v]/J_{v,w})=reg(\\mathbb{C}[\\mathbf{z}^v]/J_{v,w})-(l(v)-l(w))=deg\\ K(\\mathbb{C}[\\mathbf{z}^v]/J_{v,w},t)-l(w)-l(v)+l(w)=deg\\ S_{v,w}(\\mathbf{t}) -l(v)$. So the answer is $l(v)$."
        ],
        "step_count": 5
    },
    "recuVfyQPiOOv3": {
        "reasoning_steps": [
            "The instruction is reframed from a question about a lower bound to a constructive problem, following Concept_1. The goal is to build an efficient (quasi-polynomial time) single-valued Arthur-Merlin protocol that outputs a truth table of length $N=2^n$ that is hard for circuits of size $N/\\log N = 2^n/n$. This set of hard truth tables forms a dense property within the class coAM, as described in Concept_2.",
            "We begin with a brute-force algorithm, $BF_0$, which is guaranteed to find such a hard truth table by enumerating all strings of length $N$ and checking them against all circuits of size $2^n/n$. This algorithm is correct but runs in exponential time in $N$.",
            "The core of the argument applies the Hardness-vs-Randomness framework for AM (Theorem_1) to the computation of the brute-force algorithm $BF_0$. This presents a dichotomy based on the success or failure of a resulting Hitting Set Generator (HSG).",
            "The Iterative Win-Win Paradigm (Theorem_2) is applied over a sequence of carefully chosen input lengths $n_0, n_1, \\dots, n_l$. At the first step, we consider the computation of $BF_0$ on input length $n_0$: (Win Case): If the HSG derived from $BF_0(1^{n_0})$ fails to hit the dense coAM property of hard truth tables at length $n_1$, then according to Theorem_1, the computation $BF_0(1^{n_0})$ can be simulated by a fast (quasi-polynomial time) single-valued Arthur-Merlin reconstruction protocol. This protocol itself is the desired efficient construction. (Improve Case): If the HSG successfully hits the property, we have found a hard truth table of length $n_1$. We can then define a new, moderately faster algorithm, $BF_1$, which uses this HSG (along with a small amount of advice to identify the correct string in the HSG's output) to find a hard truth table for input length $n_1$.",
            "The \"improve\" step is iterated. We apply Theorem_1 to $BF_1$ on input length $n_1$ to get a result for length $n_2$, and so on. If we remain in the \"Improve\" case for all steps up to $l$, the accumulated speedups result in an algorithm $BF_l$ that runs in quasi-polynomial time. The overall algorithm is designed to check for a \"win\" at each step $i$, and if none occurs, it runs the final efficient algorithm $BF_l$.",
            "This iterative process guarantees that for any sequence of input lengths, we can construct a quasi-polynomial time, single-valued Arthur-Merlin protocol (Concept_3) that finds a truth table hard for circuits of size $2^n/n$. A language defined based on the bits of this truth table is therefore in $AMEXP$ (as it can be decided by running this protocol) but, by construction, is not in $SIZE[2^n/n]$. This establishes the $2^n/n$ lower bound."
        ],
        "step_count": 6
    },
    "recuVjfgPckajY": {
        "reasoning_steps": [
            "The instruction requires determining the tight competitive ratio for min-max paging by first re-framing its objective. The problem's goal is to minimize the maximum number of evictions, which is formally defined as minimizing the $l_{\\infty}$-norm of the page eviction vector (Concept_1).",
            "Following the specified methodology, this discrete $l_{\\infty}$-norm objective is re-framed using a continuous surrogate to facilitate analysis. The $l_p$-norm is chosen for this purpose, as it provides a constant-factor approximation of the $l_{\\infty}$-norm when the parameter $p$ is set to be on the order of $\\log n$ (Concept_2). This transforms the problem into solving $l_p$-norm paging for a specific $p$.",
            "The next step is to establish an upper bound by applying a black-box reduction. Theorem_1 provides this reduction, stating that a $\\beta$-competitive randomized algorithm for the simpler weighted linear ($l_1$) paging problem can be lifted to an $O(p\\beta)$-competitive algorithm for the $l_p$-norm paging problem.",
            "It is a known result that optimal randomized algorithms for weighted linear paging achieve a competitive ratio of $\\beta = O(\\log k)$. By substituting this $\\beta$ and our choice of $p = O(\\log n)$ into the formula from Theorem_1, we derive a randomized competitive ratio of $O(p\\beta) = O(\\log n \\cdot \\log k)$. This establishes the upper bound for the min-max paging problem.",
            "To prove this bound is asymptotically tight, the final step is to synthesize it with a corresponding lower bound. Theorem_2 establishes a theoretical lower bound for the randomized $l_p$-norm paging problem, which is $\\Omega(p \\log k)$.",
            "By again substituting $p = O(\\log n)$ into this lower bound, we find that any randomized algorithm for min-max paging must have a competitive ratio of at least $\\Omega(\\log n \\log k)$.",
            "By synthesizing these results, we see the upper bound of $O(\\log k \\log n)$ and the lower bound of $\\Omega(\\log k \\log n)$ match asymptotically. Therefore, the definitive tight randomized competitive ratio for the online min-max paging problem is $\\Theta(\\log k \\log n)$."
        ],
        "step_count": 7
    },
    "recuVlbxHnJWyi": {
        "reasoning_steps": [
            "The goal, as stated in the instruction, is to find the maximum committee size $k$ that guarantees the existence of a Condorcet winning set.",
            "According to Concept_1, a Condorcet winning set is a specific type of $\\alpha$-undominated committee where the threshold $\\alpha$ is exactly 1/2. This means we are looking for the size $k$ of a committee that is guaranteed to be 1/2-undominated.",
            "The instruction provides the central mathematical condition for our problem: an $\\alpha$-undominated committee of size $k$ is guaranteed to exist if the inequality $\\frac{\\alpha}{1-\\ln\\alpha}\\ge\\frac{2}{k+1}$ holds.",
            "To find the specific $k$ for a Condorcet winning set, we substitute $\\alpha = 1/2$ into the given inequality: $\\frac{0.5}{1-\\ln(0.5)}\\ge\\frac{2}{k+1}$.",
            "Next, we solve this inequality for $k$. Since $\\ln(0.5)$ is equal to $-\\ln(2)$, the expression simplifies to $\\frac{0.5}{1+\\ln(2)}\\ge\\frac{2}{k+1}$. Rearranging for $k$, we get $k \\ge 3 + 4\\ln(2)$.",
            "By substituting the approximate value of $\\ln(2) \\approx 0.693$, we calculate the numerical bound for $k$: $k \\ge 3 + 4(0.693)$, which means $k \\ge 5.772$.",
            "Since the committee size $k$ must be an integer, the smallest integer that satisfies the condition $k \\ge 5.772$ is 6. Thus, a committee of size 6 is sufficient."
        ],
        "step_count": 7
    },
    "recuVlLRHhdfMs": {
        "reasoning_steps": [
            "Start from the Born–Markov master equation; diagonalize the channel-a dissipator by Γ^(a)_{jl} → {Γ^(a)_ν,Ô_{ν,a}} so that R_a(t)=∑_ν Γ^(a)_ν⟨Ô^†_{ν,a}Ô_{ν,a}⟩.",
            "Consider the fully inverted initial state; the only evolution at t=0 is spontaneous emission. A “burst” means Ṙ_a(0)>0, which is equivalent to the conditional enhancement criterion ḡ^(2)(0)>1 for photons detected on channel a.",
            "Evaluate Ṙ_a(0) (or, equivalently, ḡ^(2)(0)) using the collective operators and the fact that the fully excited state is an eigenstate of number operators; reduce all four-operator averages to quadratic forms in {Γ^(a)_ν}.",
            "The algebra collects into the spectral dispersion of the channel-a rates competing against single-atom decay out of |e⟩ into all channels. This yields the channel-resolved burst inequality."
        ],
        "step_count": 4
    },
    "recuVmiw3kbpmH": {
        "reasoning_steps": [
            "According to concept 1, the Ni single atom is anchored via Ni-O-Ce bond on the CeO2, thus the shorter Ni-O bond length suggests the stronger Ni-CeO2 interaction. Therefore, the Ni/CeO2(100) has the strongest Ni-CeO2 interaction, which is favorable for the stabilization of Ni single atom.",
            "According to concept 2, the CeO2 nanorod usually enables Ni species the highest activity. The CeO2 nanorod usually exhibits the CeO2(110) active crystal face, thus the CeO2(110) can enables the highest activity of Ni single atom in dry reforming of methane."
        ],
        "step_count": 2
    },
    "recuVxskY5zS12": {
        "reasoning_steps": [
            "Define the Problem and Given Conditions The objective is to determine the conditions under which a specific rotational-add diffusion layer defined by the rotation offset set $\\mathcal{G}=\\left\\{0, g_2, g_2+m, g_2+2 m, 3 m\\right\\}$, achieves the Maximum Distance Separable (MDS) property. The given parameters are that $m$ is an even number and $g_2=m / 2$.",
            "Calculate the Necessary Parameters Based on the given information, we first compute the parameters required for the characteristic polynomial formula. - The parameter $g$ is defined as $g=g_2$, so $g=m / 2$. - The parameter $d$ is the greatest common divisor (GCD) of $g$ and $m$. Since $m$ is even, $m / 2$",
            "Derive the Characteristic Polynomial The characteristic polynomial, $f(\\lambda)$, is determined by the general form: $\\$ \\$ \\mathrm{f}(\\$ lambda \\$)=(- \\text{ lambda })^{\\wedge}\\{\\mathrm{m} / \\mathrm{d}\\}-(1-\\text{ lambda })^{\\wedge}\\{(\\mathrm{m}-\\mathrm{g}) / \\mathrm{d}\\} \\$ \\$$ We substitute the parameters calculated in Step 2 into this equation: $$ \\begin{aligned} & \\$ \\$ \\mathrm{f}(\\text{ Vlambda })=(- \\text{ Vlambda })^{\\wedge}\\{\\mathrm{m} /(\\mathrm{m} / 2)\\}-(1-\\text{ Vlambda })^{\\wedge}\\{(\\mathrm{m}-\\mathrm{m} / 2) /(\\mathrm{m} / 2)\\} \\$ \\$ \\$ \\mathrm{f}(\\text{ Vlambda })=(- \\\\ & \\text{ Vlambda })^{\\wedge} 2-(1-\\text{ Vlambda })^{\\wedge}\\{(\\mathrm{m} / 2) /(\\mathrm{m} / 2)\\} \\$ \\$ \\$ \\mathrm{f}(\\text{ llambda })=\\text{ Vlambda^2 }-(1-\\text{ Vlambda }) \\$ \\$ \\end{aligned} $$ This simplifies to the final characteristic polynomial: $f(\\lambda)=\\lambda^2+\\lambda-1$.",
            "State the Coprimality Condition for MDS According to a relevant theorem (Theorem 3), the diffusion layer is MDS if and only if its characteristic polynomial, $f(\\lambda)=\\lambda^2+\\lambda-1$, is coprime with a specific list of 11 predefined polynomials (such as $\\lambda+1,2 \\lambda-1, \\lambda^2-\\lambda-1$, etc.).",
            "Test the Coprimality in Fields of Different Characteristics We now test this condition by checking for common factors or roots between $f(\\lambda)$ and the polynomials from the list in fields with various prime characteristics ( $p$ ). The analysis shows failure cases for $p=2,3,5$ : - Characteristic 2: $f(\\lambda)=\\lambda^2+\\lambda-1$ becomes $\\lambda^2+\\lambda+1$. A polynomial from the list, $\\lambda^2-\\lambda-1$, also becomes $\\lambda^2+\\lambda+1$ (since $-1 \\equiv 1(\\bmod 2)$ ). As they are identical, they are not coprime. - Characteristic 3: $f(\\lambda)=\\lambda^2+\\lambda-1$. A polynomial from the list, $\\lambda^2-2 \\lambda-1$, is identical to $f(\\lambda)$ in this field (since $-2 \\equiv 1(\\bmod 3))$. Therefore, they are not coprime. - Characteristic 5: The polynomial $f(\\lambda)$ has a root at $\\lambda=2$ (since $2^2+2-1=5 \\equiv 0 (\\bmod 5))$. A polynomial from the list, $\\lambda^2+1$, also has a root at $\\lambda=2$ (since $2^2+1= 5 \\equiv 0(\\bmod 5))$. Since they share a common root, they are not coprime. For prime characteristics other than 2,3 , and $5, f(\\lambda)$ is coprime with all 11 polynomials.",
            "Conclude the Final Condition for the MDS Property Based on the coprimality tests in the previous step, the condition for the layer to be MDS is violated when the characteristic of the underlying field is 2,3, or 5 . Therefore, the rotational-add diffusion layer $L_{4, m}^\\mathcal{G}$ is MDS if and only if the prime characteristic $p$ of the field is not $\\mathbf{2 , 3}$, or $\\mathbf{5}$."
        ],
        "step_count": 6
    },
    "recuUAInTiGkd9": {
        "reasoning_steps": [
            "Meaning of entransy (Concept 1): Entransy measures both “how much heat” and its “temperature level (quality),” so it serves as an appropriate second-law metric to identify where energy dissipation concentrates within a conversion system.",
            "System context (Concept 2): In a thermally-coupled hybrid A–C heat pump (THACHP with an AHT high-temperature sub-cycle), the heat transfer components are arranged in thermal series, forming a strongly coupled system. Because components share common boundary temperatures and are linked through thermal interfaces, any change in the irreversibility of one component modifies not only its own performance but also redistributes temperature gradients across the entire system. Consequently, the heat flux through other components is indirectly affected — laying the foundation for understanding subsequent changes in heat-flow allocation.",
            "Entransy–resistance principle (Concept 3): The minimum generalized thermal resistance is equivalent to an entransy-dissipation extremum: Reducing a component’s irreversibility ⇒ its generalized thermal resistance decreases; As multiple components are coupled, lowering several resistances simultaneously leads to a drop in the overall system resistance; A reduced system resistance allows greater total heat throughput under fixed boundary conditions.",
            "Redistribution effect on evaporator-3: When the total system resistance decreases, the same boundary temperatures drive a larger total heat flux through the entire system. If evaporator-3’s own thermal resistance remains unchanged, the increased total heat must still pass through it. By Concepts 1 and 3, entransy dissipation within evaporator-3 rises to satisfy the new system-wide entransy extremum."
        ],
        "step_count": 4
    },
    "recuV9YBA6ps0F": {
        "reasoning_steps": [
            "The primary goal is to determine how the energy efficiency ($\\eta$) of a solar distillation system changes as the condensation temperature ($T_c$) increases, while keeping all other parameters constant. The analysis begins with the definition of $\\eta$ and its dependence on thermal energy distribution described in concept_1.",
            "According to concept_1, the energy efficiency of a solar distillation system is defined as: \\eta = \\frac{Q_{\\mathrm{evap}}}{Q_s}, where $Q_{\\mathrm{evap}}$ is the energy used for water evaporation and $Q_s$ is the total absorbed solar energy. At constant $Q_s$, the variation in $\\eta$ depends on how the absorbed energy is partitioned between useful evaporation and thermal losses.",
            "We first analyze the low condensation temperature regime using concept_2. When $T_c$ is very low, the temperature difference between the device and the ambient surroundings is large, causing significant thermal losses through edges, upward dissipation, and conduction. As a result, a smaller fraction of the absorbed energy contributes to $Q_{\\mathrm{evap}}$, leading to low energy efficiency.",
            "Next, we evaluate the effect of moderately increasing $T_c$ based on concept_2 and concept_3. As $T_c$ rises from a low level: The overall temperature gradient between the system and its surroundings decreases, reducing heat losses(concept_2). The evaporation–condensation temperature difference $\\Delta T = T_e - T_c$ remains sufficiently large, maintaining effective vapor diffusion (concept_3). Consequently, a greater fraction of $Q_s$ is converted into $Q_{\\mathrm{evap}}$, and $\\eta$ increases.",
            "However, when $T_c$ continues to increase beyond an optimal range (concept_3), $\\Delta T = T_e - T_c$ becomes too small, reducing the vapor concentration gradient and weakening mass transfer efficiency. Even though thermal losses remain low, the limited vapor diffusion dominates, causing $Q_{\\mathrm{evap}}$ to decrease and resulting in a decline in $\\eta$.",
            "Combining these effects, the overall trend can be summarized as follows: At low $T_c$: $\\eta$ is low due to significant heat losses. At moderate $T_c$: $\\eta$ rises as heat losses decrease while vapor diffusion remains effective. At high $T_c$: $\\eta$ falls again because vapor diffusion becomes severely constrained."
        ],
        "step_count": 6
    },
    "recuVfYl5ySmDy": {
        "reasoning_steps": [
            "First, consider how to explain the mechanism by which DecA:Lid (2:1) enhances the electrophilicity of succinic anhydride in the Instruction regarding electronic regulation. According to the provided concept_1, some researchers have found that lauric acid forms strong hydrogen bonds with chloride ions (Cl⁻) in the reaction system. This concept suggests that hydrogen bond formation can polarize molecules and alter the electron distribution, thereby enhancing their electrophilicity. Here, DecA:Lid (2:1) may form a similar strong hydrogen bond interaction with carbonyl oxygen or other polar sites in succinic anhydride. This hydrogen bond pulls electrons, making the acyl carbon atoms of succinic anhydride more electron-deficient and electrophilious, making them more susceptible to nucleophilic attack by vitamin E (VEOH) and promoting esterification reactions. This is in line with mathematical logic, as hydrogen bonding is an electrostatic interaction that effectively regulates electron density.",
            "Secondly, in terms of conformation, I analyze how DecA:Lid (2:1) affects the conformation of the CRL. According to the concept_2, the researchers found that due to the interaction of hydrogen bonds between C-Gly and CALB, it is conducive to maintaining the balance between flexibility and rigidity of the structural conformation, and improving the binding ability and catalytic efficiency of enzymes to substrates. In the reaction system, DecA:Lid (2:1) may interact with CRL through hydrogen bonding, similar to the mechanism in concept_2, causing conformational changes in CRL. This change optimizes the flexibility (for substrate binding) and rigidity (for stabilizing the transition state) of the CRL active center, thereby improving the ability of CRL to capture and localize substrate molecules, increasing substrate affinity, and enhancing catalytic efficiency. This is in line with mathematical logic, as hydrogen bonds stabilize specific conformations and affect enzyme kinetics."
        ],
        "step_count": 2
    },
    "recuVEFvsexATr": {
        "reasoning_steps": [
            "Define the Energy Balance in ISE (concept_1)  In interfacial solar evaporation (ISE), the evaporation rate depends on two competing factors:  Energy input drives phase change at the liquid–vapor interface. Vapor transport efficiency regulates the removal of generated vapor.  If vapor is not effectively removed, local humidity increases, which reduces the vapor concentration gradient and limits further evaporation. Therefore, high evaporation rates require a balanced coupling between energy supply and vapor escape, rather than maximizing direct light utilization alone.",
            "Role of Convective Flows in Vapor Transport (concept_2)  In Dyson sphere-like evaporators, a designed fraction of the absorbed solar energy is deposited on non-evaporating, higher-temperature surfaces, establishing temperature gradients that drive buoyancy-induced natural convection within the structure. These flows lower the local vapor concentration and sustain a strong concentration gradient, which:  Accelerates vapor removal from evaporation surfaces. Prevents vapor accumulation and local saturation. Maintains continuous evaporation even under reduced direct energy input.  This trade-off introduces a nonlinear enhancement mechanism: sacrificing some direct light can paradoxically improve overall efficiency by unlocking a new vapor transport pathway.",
            "Nonuniform Heating Induces Beneficial Internal Flows (concept_3)  The Dyson sphere-like evaporator has a 3D hollow structure with both inner and outer evaporation surfaces. Nonuniform heating between these regions naturally forms temperature gradients, which induce internal convection loops without any additional external power input, at the expense of reallocating a small portion of the incident solar energy to sustain the thermal gradients that drive buoyancy. These thermally driven flows act as a self-regulating mechanism that:  Redistributes thermal energy. Enhances vapor escape efficiency. Expands the effective evaporation area across multiple surfaces.",
            "Comparison with Conventional Evaporators  In conventional planar evaporators:  Nearly all absorbed solar energy is used directly for phase change, but Vapor removal is inefficient, leading to local humidity buildup and reduced driving force.  In contrast, Dyson sphere-like evaporators trade a fraction of direct energy to actively manage vapor transport and internal convection, thereby achieving higher overall evaporation efficiency.",
            "Final Conclusion  This is an energy-allocation trade-off: a modest internal reallocation can unlock transport benefits that outweigh the reduction in direct evaporation power. Even though a Dyson sphere-like evaporator reduces the fraction of light used directly for evaporation, its overall evaporation efficiency can increase because:  Efficient vapor removal maintains strong mass transfer driving forces. Internal convection accelerates vapor transport and prevents interface saturation. Thermal energy redistribution enhances utilization of both inner and outer surfaces.  Result: Overall evaporation efficiency can be higher despite reduced direct light usage."
        ],
        "step_count": 5
    },
    "recuVw4pIkr34P": {
        "reasoning_steps": [
            "EPMC and periodic dNNM (using concept_1)\\ Introduce an artificial negative damping term, -2\\zeta\\omega M \\dot{y}, into the unforced equations so that the modified system admits a strictly periodic damped NNM; represent the periodic displacement as y = a(\\psi_0 + \\sum_{k=1}^{H}{\\psi_{ck}*cos(k*\\omega*t)+\\psi_{sk}*sin(k*\\omega*t)}), where a, \\omega, \\zeta and the shape vectors \\psi_0, \\psi_{ck}, and \\psi_{sk} parametrize the dNNM.",
            "Resonant energy balance (using Concept_2): At resonance the net energy delivered by the external excitation over one period must equal the net energy dissipated or exchanged by the system over that period, i.e. E_{d} = E_f(f_{de},\\phi), which is the criterion used to link forcing amplitude to the dNNM properties.",
            "Replace real dissipation by artificial-damping work (using Concept_1 + Concept_2): Under EPMC the true per-cycle dissipation E_{d} is represented by the positive work of the artificial term, E_d = int_{0}^{T}{2\\zeta\\omega\\dot{y}^T M \\dot{y} dt}; substituting the harmonic expansion of velocity and averaging yields, \\int_{0}^{T}{\\dot{y}^T*M*\\dot{y}*dt} = \\pi*a^2*\\omega*\\sum_{k=1}^{H}{k^2*(\\psi_{ck}^T*M*\\psi_{ck}+\\psi_{sk}*M*\\psi_{sk})}, so E_d = 2*\\zeta*\\omega*\\pi*a^2*\\omega*\\sum_{k=1}^{H}{k^2*(\\psi_{ck}^T*M*\\psi_{ck}+\\psi_{sk}*M*\\psi_{sk})}.",
            "External input comes only from the fundamental harmonic (using Concept_3 + Concept_2): Because the excitation is single-frequency, f(t) = f_{de}*V*\\cos{\\omega_e*t+\\phi}, only the response first harmonic (k=1) produces nonzero average work with the force; the input energy therefore equals E_f = f_{de}*\\int_{0}^{T}{V^T*\\dot{y}*\\cos{\\omega_e*t+\\phi}*dt}. Substituting harmonic expansion of velocity, utilising resonant condition, \\omega = \\omega_e, one yields E_f(f_{de},\\phi) = \\pi*a*f_{de}*[V^T*\\psi_{c1}*\\sin(\\phi)+V^T*\\psi_{s1}*\\cos(\\phi)]. According to concept_2, the phase is chosen to maximise input energy for dissipative systems, max(E_f) = \\pi*a*f_{de}*\\sqrt{(V^T*\\psi_{c1})^2+(V^T*\\psi_{s1})^2}.",
            "Equate energies and state the relation for f_{de}: Setting the artificial-damping work equal to the maximal external input and cancelling common period/frequency factors, as well as using \\omega_e = \\omega, one yields the functional relationship between f_{de} and the damped NNM (characterised by a, \\zeta, \\omega, \\psi_0, \\psi_{c1}, \\psi_{s1}, ..., \\psi_{ck}, \\psi_{sk}, …): f_{de} = \\frac{2*\\zeta*a*\\omega^2*(\\sum_{k=1}^{H}{k^2*(\\psi_{ck}^T*M*\\psi_{ck}+\\psi_{sk}^T*M*\\psi_{sk})})}{\\sqrt{(V^T*\\psi_{c1})^2+(V^T*\\psi_{s1})^2}}."
        ],
        "step_count": 5
    },
    "recuUzlYctPEc8": {
        "reasoning_steps": [
            "Hyperfiniteness guarantees that after deleting $\\phi|V|$ edges, each component has size $\\le \\rho(\\phi)$. Each removed edge influences at most $B(d,k)$ vertices, and each vertex change contributes $\\tfrac{2}{|V|}$ to the $\\ell_1$ error. Thus, \\[\\|f_G - f_{G'}\\|_1 \\le 2\\phi B(d,k).\\] Choosing $\\phi \\le \\tfrac{\\varepsilon}{4B(d,k)}$ ensures the cut error is at most $\\varepsilon/2$.",
            "Let $T$ be the number of distinct $k$-disc types, $p_i$ the true frequency, $\\hat p_i$ the empirical frequency, and $\\hat f=(\\hat p_1,\\dots,\\hat p_T)$.",
            "By Chernoff bound, \\[\\Pr(|\\hat p_i - p_i| > \\delta) \\le 2e^{-2N\\delta^2}.\\] By union bound, \\[\\Pr(\\|\\hat f - f\\|_1 > T\\delta) \\le 2T e^{-2N\\delta^2}.\\] Setting $\\delta = \\tfrac{\\varepsilon}{2T}$ gives \\[\\Pr(\\|\\hat f - f\\|_1 > \\tfrac{\\varepsilon}{2}) \\;\\le\\; 2T \\exp\\!\\left(-2N\\left(\\tfrac{\\varepsilon}{2T}\\right)^2\\right).\\]",
            "To ensure failure probability $\\le \\eta$, we require \\[N \\;\\ge\\; \\frac{2T^2}{\\varepsilon^2}\\ln\\frac{2T}{\\eta}.\\] When $d,k$ are fixed, $T=O(d^k)=O(1)$, so \\[N = \\Theta(\\varepsilon^{-2}).\\]",
            "Cut error $\\le \\varepsilon/2$, sampling error $\\le \\varepsilon/2$, so by the triangle inequality, \\[\\|f_G - f\\|_1 \\le \\varepsilon.\\]"
        ],
        "step_count": 5
    },
    "recuVCKWf9x1a8": {
        "reasoning_steps": [
            "Set up notation (e-log scale and a spectral shorthand)  Define  $\\mathrm{disc}_e(f,\\mu)\\ :=\\ -\\ln\\!\\Big(\\max_{S}\\mathrm{bias}(f,\\mu,S)\\Big),\\qquad A\\ :=\\ -\\ln\\big\\|\\widehat{F_\\mu}\\big\\|_\\infty.$",
            "Lower endpoint for the single-instance (Concept 1)  By Concept 1, for every affine $S$,  $\\mathrm{bias}(f,\\mu,S)\\le \\|\\widehat{F_\\mu}\\|_\\infty.$  Taking the maximum over $S$ and applying $-\\ln(\\cdot)$ (decreasing on $(0,1]$),  $\\mathrm{disc}_e(f,\\mu)\\ \\ge\\ A.$",
            "Upper endpoint for the single-instance (Concept 2)  By Concept 2, there exists a hyperplane $S^\\star$ with  $\\mathrm{bias}(f,\\mu,S^\\star)\\ge \\tfrac12\\,\\|\\widehat{F_\\mu}\\|_\\infty.$  Again applying $-\\ln$,  $\\mathrm{disc}_e(f,\\mu)\\ \\le\\ -\\ln\\|\\widehat{F_\\mu}\\|_\\infty+\\ln 2\\ =\\ A+\\ln 2.$",
            "Collect the single-instance interval  From steps 2–3:  $\\mathrm{disc}_e(f,\\mu)\\ \\in\\ [A,\\ A+\\ln 2]\\ \\Longleftrightarrow\\ A\\ \\in\\ \\big[\\mathrm{disc}_e(f,\\mu)-\\ln 2,\\ \\mathrm{disc}_e(f,\\mu)\\big].$",
            "Explicit product form of $F_{\\mu^k}$ (clarification ①)  By the definitions of $f^{\\oplus k}$ and $\\mu^k$,  $F_{\\mu^k}(x_1,\\dots,x_k) = (-1)^{f^{\\oplus k}(x_1,\\dots,x_k)}\\ \\mu^k(x_1,\\dots,x_k)\\ 2^{nk} = \\prod_{i=1}^k \\Big[(-1)^{f(x_i)}\\,\\mu(x_i)\\,2^n\\Big] = \\prod_{i=1}^k F_\\mu(x_i).$",
            "Fourier coefficient factorization for the $k$-fold object  Let $\\chi_z(x)=(-1)^{\\langle z,x\\rangle}$ be Walsh characters.  By Concept 3 (character factorization), on $(\\{0,1\\}^n)^k$,  $\\chi_{(z_1,\\dots,z_k)}(x_1,\\dots,x_k)=\\prod_{i=1}^k \\chi_{z_i}(x_i).$  Therefore, using the definition  $\\widehat{F_{\\mu^k}}(z_1,\\dots,z_k)=2^{-nk}\\sum_{x_1,\\dots,x_k} F_{\\mu^k}(x_1,\\dots,x_k)\\,\\chi_{(z_1,\\dots,z_k)}(x_1,\\dots,x_k),$  and the product form from step 5, we get a product of $k$ separate sums:  $\\widehat{F_{\\mu^k}}(z_1,\\dots,z_k)=\\prod_{i=1}^k \\widehat{F_\\mu}(z_i).$",
            "From coefficient factorization to an $L_\\infty$ power (clarification ②)  Because the index sets are finite, maxima are attained. Hence, with  $a(z):=|\\widehat{F_\\mu}(z)|\\ge0,$  $\\big\\|\\widehat{F_{\\mu^k}}\\big\\|_\\infty =\\max_{(z_1,\\dots,z_k)}\\ \\prod_{i=1}^k a(z_i) =\\Big(\\max_{z} a(z)\\Big)^{k} =\\big\\|\\widehat{F_\\mu}\\big\\|_\\infty^{\\,k}.$  Define  $B\\ :=\\ -\\ln\\big\\|\\widehat{F_{\\mu^k}}\\big\\|_\\infty,$  then  $B\\ =\\ kA.$",
            "Apply Concepts 1 & 2 to the $k$-fold instance  Repeating the single-instance reasoning (steps 2–3) for $(f^{\\oplus k},\\mu^k)$,  $\\mathrm{disc}_e(f^{\\oplus k},\\mu^k)\\ \\in\\ [B,\\ B+\\ln 2].$  Using (†) this is  $\\mathrm{disc}_e(f^{\\oplus k},\\mu^k)\\ \\in\\ [kA,\\ kA+\\ln 2].$",
            "Substitute the range of $A$ and scale the interval  From (★), $A\\in[\\mathrm{disc}_e(f,\\mu)-\\ln2,\\ \\mathrm{disc}_e(f,\\mu)]$.  Multiplying by $k\\ge1$ preserves order:  $kA\\ \\in\\ \\big[k(\\mathrm{disc}_e(f,\\mu)-\\ln 2),\\ k\\,\\mathrm{disc}_e(f,\\mu)\\big].$  Thus  $\\mathrm{disc}_e(f^{\\oplus k},\\mu^k)\\ \\in\\ \\big[k(\\mathrm{disc}_e(f,\\mu)-\\ln 2),\\ k\\,\\mathrm{disc}_e(f,\\mu)+\\ln 2\\big].$",
            "Convert back to base-2 discrepancy (final answer)  By definition $\\mathrm{disc}_{\\mathsf{pt}}=\\mathrm{disc}_e/\\ln 2$, divide the endpoints by $\\ln 2$:  $k\\,\\mathrm{disc}_{\\mathsf{pt}}(f,\\mu)-k\\ \\le\\ \\mathrm{disc}_{\\mathsf{pt}}(f^{\\oplus k},\\mu^k)\\ \\le\\ k\\,\\mathrm{disc}_{\\mathsf{pt}}(f,\\mu)+1.$"
        ],
        "step_count": 10
    },
    "recuVITRTILUD5": {
        "reasoning_steps": [
            "Define the coined quantum walk. Use the Grover diffusion coin at each vertex $u$: \\[ C_u \\;=\\; 2\\ket{s_u}\\!\\bra{s_u}-I, \\qquad \\ket{s_u} \\;=\\; \\tfrac{1}{\\sqrt{d_u}} \\sum_{v\\in N(u)} \\ket{v}, \\] and combine with the flip--flop shift $S$ to obtain one walk step \\[ U_{\\text{walk}} \\;=\\; S C . \\]",
            "Per-run query cost. By assumption, a single invocation of the walk-based subroutine has quantum query complexity \\[ Q(n) \\;=\\; O(n\\log n). \\]",
            "Exact amplitude amplification. In the welded-tree black-box setting, one invocation succeeds with probability $p=\\Theta(1/n)$. Exact amplitude amplification boosts success to~$1$ with \\[ O\\!\\left(\\frac{1}{\\sqrt{p}}\\right) \\;=\\; O(\\sqrt{n}) \\] iterations. (Applies \\texttt{theorem\\_2}.)",
            "Combine costs. Using the amplification cost-composition rule, if a single call costs $Q(n)$ and amplification needs $\\Theta(1/\\sqrt{a})$ iterations, then the overall query complexity is \\[ O\\!\\left(\\frac{Q(n)}{\\sqrt{a}}\\right). \\] Substituting $a=p=\\Theta(1/n)$ and $Q(n)=O(n\\log n)$ gives \\[ O\\!\\left(\\frac{n\\log n}{\\sqrt{1/n}}\\right) \\;=\\; O\\!\\big((n\\log n)\\sqrt{n}\\big) \\;=\\; O\\!\\big(n^{1.5}\\log n\\big). \\]"
        ],
        "step_count": 4
    },
    "recuVKdEG7AiT9": {
        "reasoning_steps": [
            "Recognize the problem in black-box optimization where directly minimizing the training loss F(x) can lead to sharp minima, which, according to concept_1, imply poorer generalization compared to flat minima, as flat minima are associated with better nonvacuous generalization bounds in deep neural networks.",
            "To find flat minima in white-box settings, theorem_1 introduces Sharpness-Aware Minimization (SAM), which solves \\min_{x \\in X} \\max_{\\|\\epsilon\\|_2 \\leq \\rho_2} F(x + \\epsilon) by approximating the perturbation \\epsilon(x) = \\rho_2 \\frac{\\nabla F(x)}{\\|\\nabla F(x)\\|_2} and updating parameters as x_{t+1} = x_t - \\beta_t \\nabla_x F(x)|_{x=x_t + \\epsilon_t}; however, this relies on gradients, making it unsuitable for black-box optimization where only function values are accessible.",
            "Adapt the SAM framework to black-box optimization by applying concept_2, reparameterizing the objective as J(\\theta) = \\int F(x) p(x; \\theta) \\, dx = \\mathbb{E}_{x \\sim p_\\theta}[F(x)] over a Gaussian distribution p_\\theta(x) = \\mathcal{N}(\\mu, \\Sigma), allowing optimization using only function value queries in the distribution parameter space \\theta = \\{\\mu, \\Sigma\\}.",
            "Formulate the sharpness-aware problem in the Gaussian distribution space as \\min_\\theta \\max_{\\delta \\in C(\\theta)} J(\\theta + \\delta), where C(\\theta) = {\\delta \\mid \\text{KL}(p_{\\theta+\\delta} | p_\\theta) \\le \\rho^2} defines the neighborhood via KL divergence in the statistical manifold, and use concept_3 to update the distribution parameters via stochastic gradient approximations, estimating the gradients with baseline subtraction for stability, such as \\bar{\\nabla}\\mu J(\\theta) \\approx \\frac{1}{N} \\sum{j=1}^N \\Sigma^{-1} (x_j - \\mu) [F(x_j) - F(\\mu)] and similar for \\Sigma, with x_j \\sim p_\\theta, to iteratively minimize the maximum J in the neighborhood without direct gradients.",
            "For theoretical analysis in the full-batch setting (where N is large enough for low variance, approximating deterministic gradients), leverage theorem_2, which describes SGD convergence for non-smooth convex functions and highlights how min-max formulations with decreasing \\rho_t = O(1/\\sqrt{t}) tighten bounds via summation of quadratic perturbations (\\sum \\rho_t^2 \\propto \\log T, like a harmonic series), adjusting the standard deterministic GD-like O(1/T) to O(\\frac{\\log T}{T}) for SABO's expected suboptimality under its convex assumptions."
        ],
        "step_count": 5
    },
    "recuUA2VIfGmH9": {
        "reasoning_steps": [
            "Clarify the core of the problem: The Instruction points out that sea surface monitoring, including both sea state monitoring and target (e.g., ship) positioning, is a current research focus. It also highlights a key issue: sea clutter manifests differently at different frequencies, causing some targets to be masked (undetectable) under a single frequency. Thus, a technical solution to address this limitation of single-frequency detection is required.",
            "Link to the research direction of the paper: The paper focuses on high-resolution sea surface target detection using High-Frequency Surface Wave Radar (HFSWR). One of its core pain points is that in single-frequency HFSWR, target signals are easily masked by Bragg-line interference and sea clutter specific to that frequency. To solve this problem, the paper proposes the idea of a bi-frequency operating mode.",
            "Derive the solution and align with the Answer: Since the bi-frequency mode can leverage the differences in target signals, sea clutter, and interference across different frequencies (targets masked at one frequency may be visible at another), effective application of this mode requires constructing a corresponding system architecture to integrate bi-frequency information and establishing an accurate signal model to describe signal propagation and reception under bi-frequency conditions. Therefore, the paper presents a bi-frequency architecture (system model) for sea target localization and a novel signal model, which fully matches the content of the Answer."
        ],
        "step_count": 3
    },
    "recuVtEUmOyzRj": {
        "reasoning_steps": [
            "The instruction asks for the exponent $k$ in the condition $n \\gg \\eta^2 d^k$ under which a new SoS-based algorithm can provide a nontrivial certificate for the $\\eta$-Sparse Singular Value (SSV), as defined in Concept_1. The goal is to find a bound better than the trivial one given by the standard operator norm (Concept_2).",
            "The core idea of the algorithm is to rephrase the SSV problem. Instead of directly bounding the operator norm of submatrices, the problem is upper-bounded using the Schatten p-norm for a large, even integer $p$ (Concept_3). This transforms the problem into certifying an upper bound on a degree-p polynomial $P(w)$ in variables $w$ that indicate which columns of the matrix are selected.",
            "A naive approach of representing this polynomial $P(w)$ as a single large matrix fails because dependencies among the matrix entries create a low-rank structure, leading to an overly large spectral norm and a weak bound.",
            "To overcome this, the algorithm decomposes the polynomial $P(w)$ into more manageable parts. This is achieved in two stages. First, terms are grouped based on the equality patterns of indices in the polynomial sum. Second, the Efron-Stein decomposition (Theorem_2) is applied to each of these groups. This decomposition systematically isolates the low-rank components that cause the large spectral norm.",
            "Each resulting term from the decomposition, now spectrally well-behaved, is represented by an associated random matrix. The algorithm's certificate is constructed from the operator norms of this family of matrices, which can be computed efficiently as part of a Sum-of-Squares (SoS) proof (Theorem_1).",
            "The technical core of the proof involves bounding the operator norms of these matrices. The graph matrix approach (Theorem_3) is used to analyze these matrices, which have complex dependencies among their entries, and to show that their spectral norms are sufficiently small with high probability.",
            "This detailed analysis demonstrates that the algorithm can successfully certify a nontrivial bound on the SSV provided that $n \\gg \\eta^2 d^{2+\\epsilon}$. This condition nearly matches the computational lower bound (Theorem_4). Therefore, the exponent $k$ in the relationship is $2+\\epsilon$. The parameter $p$ from the Schatten norm is chosen as $p=O(1/\\epsilon)$, which in turn determines the polynomial runtime of the SoS algorithm."
        ],
        "step_count": 7
    },
    "recuVVrvnb85Ck": {
        "reasoning_steps": [
            "Fourier theorem (Concept_1): recall that any 2π-periodic function f(ϕ) can be expanded as f(ϕ)=1/2π∑_{n=−∞}^{n=+∞}V_n e^{−inϕ} with complex coefficients V_n.",
            "Azimuthal distribution (Concept_2): identify dN/dy p_⊥dp_⊥dϕ as the 2π-periodic function in the azimuthal angle ϕ to be expanded.",
            "Normalization factor (Concept_3): factor out the ϕ-integrated yield dN/dy p_⊥dp_⊥ so that the remaining ϕ-dependent part is normalized to unity upon integration over ϕ.",
            "Harmonic coefficients (Concept_4): insert the Fourier series for the normalized angular part, assigning complex coefficients V_n to each harmonic n.",
            "Physical content (Concept_5): note V_0=1 guarantees overall normalization, while higher V_n encode anisotropic-flow components v_n.",
            "Notational completeness (Concept_6): enforce the summation range to be written explicitly as ∑_{n=−∞}^{n=+∞} to visualize the full bilateral span of the Fourier basis.",
            "Final assembly (Concept_7): combine the integrated yield with the explicit Fourier sum to obtain the exact four-fold differential yield with fully explicit summation index."
        ],
        "step_count": 7
    },
    "recuVW2loGjNKu": {
        "reasoning_steps": [
            "Analyze the Instruction: The core task is to calculate Marton's error exponent for a rate R=1.567. The problem highlights that two distributions, Q_λ1 and Q_λ2, both achieve a rate of ≈ 1.566, which is just below our target rate. The source is P=Q_ξ with ξ=0.01.",
            "Formulate the Optimization: The problem is to find the minimum of the binary divergence D_2(λ||0.01) subject to the constraint that the test distribution Q_λ can support the target rate, i.e., R(Δ|Q_λ) ≥ 1.567.",
            "Evaluate the Feasible Set: We must determine which values of λ satisfy the rate constraint. The distribution Q_λ1 (with λ1=0.0746) achieves a rate of ≈ 1.566. This point corresponds to a local maximum on the rate-distortion curve. Therefore, neither Q_λ1 nor any distribution in its immediate vicinity can achieve a rate of 1.567. This entire region of λ values is excluded from the feasible set. The distribution Q_λ2 (with λ2=0.258) also achieves a rate of ≈ 1.566. However, this point lies on the upward slope towards the global maximum. Therefore, there are values of λ slightly greater than λ2 for which R(Δ|Q_λ) ≥ 1.567. This means the feasible set for the optimization has fundamentally changed. Due to the rate constraint, the entire solution space around λ1 has been eliminated, and the new search for an optimal λ must begin from λ2 onwards.",
            "Find the Minimum of the Objective Function: The objective is to minimize D_2(λ||0.01) over this new feasible set. The binary divergence D_2(λ||0.01) is minimized when λ is as close as possible to 0.01. Since the new feasible set of λ values begins at (or just after) λ2=0.258, the value in this set that is closest to 0.01 is λ2 itself.",
            "Calculate the Result: Therefore, the error exponent is determined by E_M = D_2(λ2||ξ) = D_2(0.258||0.01). Using the formula from Concept 5, where the logarithm is base-2 for rates in bits: E_M = λ2 log_2(λ2/ξ) + (1-λ2) log_2((1-λ2)/(1-ξ)). Substituting the values λ2 = 0.258 and ξ = 0.01: E_M = 0.258 log_2(25.8) + 0.742 log_2(0.742/0.99). E_M ≈ 0.258 * (4.689) + 0.742 * (-0.416). E_M ≈ 1.2098 - 0.3087 ≈ 0.9011. Rounding to three decimal places gives the final answer, 0.904, with the minor difference attributable to the precision of the input parameters λ1 and λ2."
        ],
        "step_count": 5
    },
    "recuVW2Y6NXTMj": {
        "reasoning_steps": [
            "Analyze `ρ_DL` Numerator (Total UL Power): The numerator of the `ρ_DL` formula (Concept_3) is proportional to the total average UL power. Let $U$ be the total number of locations. Since $|K_1| = |K_2| = U/2$: $N_{orig} = \\sum_{k=1}^U \\lambda \\alpha = (|K_1| + |K_2|) \\lambda \\alpha = U \\lambda \\alpha$. $N_{new} = \\sum_{k \\in K_1} (2\\lambda)\\alpha + \\sum_{k \\in K_2} (0.5\\lambda)\\alpha = |K_1|(2\\lambda\\alpha) + |K_2|(0.5\\lambda\\alpha)$. $N_{new} = (U/2)(2\\lambda\\alpha) + (U/2)(0.5\\lambda\\alpha) = U\\lambda\\alpha + 0.25U\\lambda\\alpha = 1.25 U\\lambda\\alpha$. The numerator of $\\rho_{DL}$ changes by a factor of $1.25$.",
            "Analyze `ρ_DL` Denominator (Total Weighted DL Load) (Trap 1): This is the first major trap. A superficial analysis might assume the denominator changes unpredictably. We must use the symmetric topology assumption. For any RU $b$, let $\\mathcal{S}_b$ be the set of locations it serves. The assumption means $|\\mathcal{S}_b \\cap K_1| = |\\mathcal{S}_b \\cap K_2| = |\\mathcal{S}_b|/2$. $D_{orig} = \\sum_{b=1}^{B}\\sum_{k \\in \\mathcal{S}_b}\\lambda \\alpha \\mathcal{Z}_{k,b} = \\lambda\\alpha \\sum_{b=1}^{B} \\mathcal{Z}_b |\\mathcal{S}_b|$. $D_{new} = \\sum_{b=1}^{B} \\left( \\sum_{k \\in \\mathcal{S}_b \\cap K_1} (2\\lambda)\\alpha\\mathcal{Z}_b + \\sum_{k \\in \\mathcal{S}_b \\cap K_2} (0.5\\lambda)\\alpha\\mathcal{Z}_b \\right)$. $D_{new} = \\sum_{b=1}^{B} \\left( |\\mathcal{S}_b \\cap K_1| \\cdot 2\\lambda\\alpha\\mathcal{Z}_b + |\\mathcal{S}_b \\cap K_2| \\cdot 0.5\\lambda\\alpha\\mathcal{Z}_b \\right)$. Substitute the symmetry condition: $D_{new} = \\sum_{b=1}^{B} \\left( (|\\mathcal{S}_b|/2) \\cdot 2\\lambda\\alpha\\mathcal{Z}_b + (|\\mathcal{S}_b|/2) \\cdot 0.5\\lambda\\alpha\\mathcal{Z}_b \\right)$. $D_{new} = \\sum_{b=1}^{B} \\left( |\\mathcal{S}_b|\\lambda\\alpha\\mathcal{Z}_b + 0.25|\\mathcal{S}_b|\\lambda\\alpha\\mathcal{Z}_b \\right) = 1.25 \\sum_{b=1}^{B} |\\mathcal{S}_b|\\lambda\\alpha\\mathcal{Z}_b = 1.25 D_{orig}$. The denominator of $\\rho_{DL}$ also changes by a factor of exactly $1.25$.",
            "Calculate Change in `ρ_DL` (Trap 2: The Hidden Cancellation): Now we find the change in $\\rho_{DL}$ itself. $\\frac{\\rho'_{DL, new}}{\\rho_{DL, orig}} = \\frac{(1/L) \\cdot N_{new} / D_{new}}{(1/L) \\cdot N_{orig} / D_{orig}} = \\frac{N_{new}}{N_{orig}} \\cdot \\frac{D_{orig}}{D_{new}}$. $\\frac{\\rho'_{DL, new}}{\\rho_{DL, orig}} = 1.25 \\cdot \\frac{1}{1.25} = 1$. Despite the complex, asymmetric retuning, the power normalization factor $\\rho_{DL}$ does not change due to a non-obvious cancellation enabled by the symmetric topology. This is the main trap of the problem.",
            "Determine Final Answer: The change factor for the effective noise power is the inverse of the change factor for $\\rho_{DL}$. Factor = $\\frac{P'_{noise, new}}{P_{noise, orig}} = \\frac{\\sigma^2 / \\rho'_{DL, new}}{\\sigma^2 / \\rho_{DL, orig}} = \\frac{\\rho_{DL, orig}}{\\rho'_{DL, new}} = 1$. The effective noise power remains unchanged."
        ],
        "step_count": 4
    },
    "recuVWbz5aBe8C": {
        "reasoning_steps": [
            "Closed-loop error channel: r = \\tilde d , \\qquad \\dot r = \\dot d - \\Lambda r - s - P \\mathrm{sgn}(s) .",
            "Construct the storage term using only the definitions: H(t) = \\tilde d^{\\top}(\\dot d - P \\mathrm{sgn}(s)) , \\quad K(t) = \\mu - \\int_{0}^{t} H(\\tau) d\\tau . By selecting appropriate parameters, K can be ensured to be positive.",
            "Using the bounded-derivative assumption of the disturbance and basic inequalities, estimate \\int_0^t H . By choosing \\rho_i \\ge \\delta_{1,i} + \\xi_{\\min}^{-1}\\delta_{2,i} , it can be guaranteed that K(t) \\ge 0 holds for any t \\ge 0.",
            "Take the Lyapunov function: V = \\tfrac12,\\tilde d^{\\top}\\tilde d + \\tfrac12 e^{\\top}e + \\tfrac12 s^{\\top}s + K . When K(t)\\ge 0 and the cross term is handled with x^{\\top}y \\le \\tfrac12(\\lVert x\\rVert^2+\\lVert y\\rVert^2), we obtain \\dot V \\le -\\beta \\lVert z \\rVert^2 ,\\ \\beta>0 . According to “if there exists a positive-definite V such that \\dot V \\le -\\alpha \\lVert z \\rVert^2 (\\alpha>0), then the origin is asymptotically stable,” asymptotic convergence of the error follows."
        ],
        "step_count": 4
    },
    "recuVWh7Da4ygH": {
        "reasoning_steps": [
            "Consider $\\int_{G/P_I} c_1(L^\\vee)^d=(-1)^d\\int_{G/P_I}c_1(L)^d$, where $L=\\mathcal{V}(det(\\rho_{Hdg}))$ and $d=dim(\\mathcal{A}_g)$. By theorem_1 there is $\\int_{G/P_I} c_1(L^\\vee)^d=(-1)^d\\int_{G/P_I}c_1(L)^d=(-1)^d\\int_{G/P_I}c_1(\\mathcal{V}(det(\\rho_{Hdg})))^d=(-1)^dR\\int_{X}c_1(\\mathcal{W}(det(\\rho_{Hdg})))^d.",
            "By theorem_3, we have $\\int_{G/P_I}c_1(L^\\vee)^d>0$.",
            "By theorem_2, $\\mathcal{W}(\\rho_{Hdg})$ is the Hodge vector bundle on $X$. Therefore, we have that the Hodge line bundle $\\mathcal{W}(det(\\rho_{Hdg}))$ on $X$ is nef. Then $\\int_{X}c_1(\\mathcal{W}(det(\\rho_{Hdg})))^d>0$.",
            "So we have $(-1)^dR>0$. Applying theorem_1 again with $f=c_gc_1^{d-g}$ gives$$\\int_{G/P_I}[L_J/L_J\\cap P_I]c_1(L^\\vee)^{d-g}=(-1)^{d-g}a\\int_{G/P_I}c_g(\\Omega)c_1(\\Omega)^{d-g}=(-1)^{d-g}aR\\int_{X}\\frac{1}{N}[v_0]c_1(\\mathcal{W}(det(\\rho_{Hdg})))^{d-g},$$ the last line follows from theorem_4. Here we have $\\frac{1}{N}[v_0]c_1(\\mathcal{W}(det(\\rho_{Hdg})))^{d-g}>0$ and $\\int_{G/P_I}[L_J/L_J\\cap P_I]c_1(L^\\vee)^{d-g}>0$ (because $L$ is anti-ample on $G/P_I$). So $(-1)^{d-g}aR>0$.",
            "Combining Step 3 and Step 4, we get $(-1)^ga>0$. So $a>0$ if and only if $g$ is even."
        ],
        "step_count": 5
    },
    "recuW1mmHHjhIV": {
        "reasoning_steps": [
            "According to concept _1, when the temperature is lower than \\( 340 \\ \\text{K} \\) \\( \\ce{VO_2} \\), there is a stable monoclinic M1 insulating phase. The core structural feature is Peierls V-V dimerization-V atoms deviate from the center of the oxygen octahedron to form a zigzag chain, resulting in short bonds (\\( d_{\\text{s}} \\), that is, V-V dimers) and long bonds (\\( d_{\\text{L}} \\)); combined with the concept _2, it can be seen that the insulation of the M1 phase is directly determined by the V-V dimer: the dimer confines the electrons inside, resulting in electron localization, and the electrons cannot be freely conducted, which eventually exhibits insulation behavior. This step clarifies that the causal core of 'monoclinic structure' and 'insulation' is V-V dimer, which provides a logical starting point for the analysis of 'monoclinic metal phase'.",
            "The essence of the metal phase is the free conduction of electrons (electron delocalization). Based on the causal chain of 'V-V dimer → electron localization → insulation' in concept _2, reverse derivation shows that in order to achieve metallicity in monoclinic structure, it is necessary to break the localization of electrons by V-V dimer, that is, the dissociation of V-V dimer, so that the electrons can get rid of the limitation and achieve delocalization. At the same time, in order to maintain the 'monoclinic' structural properties (different from the rutile R metal phase), the dimer dissociation should be limited to the local region (avoiding the whole crystal system transformation). At this time, the whole crystal still maintains the monoclinic structure, and the local dissociation region shows metallicity due to electron delocalization, thus possessing the structural and electrical characteristics of the 'monoclinic metal phase'.",
            "As an external energy input, light induction can trigger the dynamic change of local structure of \\( \\ce{VO_2} \\): When light acts on the M1 phase, if the excitation intensity is weak (not enough to dissociate all V-V dimers), according to the characteristic of 'V-V dimer is the core structural unit of M1 phase' in concept _1, the holes generated by light excitation will transfer to local V-V dimers (such as specific short bonds \\( d_{\\text{s}} \\)), amplifying the atomic driving force in this region and promoting the dissociation of local V-V dimers. In this process, the overall crystal still retains the monoclinic structure (the monoclinic phase framework defined by concept _1), while the local dissociation region exhibits metallicity due to electron delocalization (breaking the insulation root in concept _2), and finally forms a monoclinic metal phase. Therefore, it can be determined that there is a monoclinic metal phase in \\( \\ce{VO_2} \\) induced by light, and its origin is the dissociation of local V-V dimers."
        ],
        "step_count": 3
    },
    "recuVXTZbJujSE": {
        "reasoning_steps": [
            "Clarify the core question: Determine whether the directionality of multi-cycle reaction networks under non-equilibrium conditions can be solely controlled by thermodynamic properties (e.g., \\(\\Delta E^0\\), \\(K_{ox}/K_{red}\\)) instead of pure kinetic control (the traditional view for systems without mass transport).",
            "Identify key parameters related to directionality in multi-cycle reaction networks: The ratcheting constant \\(K_r\\) to quantify kinetic asymmetry (directionality), where \\(K_r\\) depends on \\(q\\) (ratio of forward to backward cycle frequencies), \\(\\Delta E\\) (electrochemical potential gradient, energy drive), and \\(\\Gamma\\) (ratio of slip to backward cycle frequencies).",
            "Analyze the determinant of \\(q\\) (a key factor for \\(K_r\\)): Through deriving the sum of forward (\\(\\sum F_i\\)) and backward (\\(\\sum B_i\\)) cycle frequencies using graph theory’s rooted spanning tree method, it is found that \\(q = \\sum F_i / \\sum B_i\\), and \\(q\\) is controlled by the sign of \\(\\Delta E \\times \\Delta E^0\\). Specifically, \\(q > 1\\) if \\(\\Delta E \\times \\Delta E^0 > 0\\), and \\(q < 1\\) if \\(\\Delta E \\times \\Delta E^0 < 0\\). Notably, \\(\\Delta E^0\\) (standard redox potential difference, \\(\\Delta E^0 = E_B^0 - E_A^0\\)) is an **intrinsic thermodynamic property** of the system, inherently linked to the ratio of equilibrium constants \\(K_{ox}/K_{red}\\) via the microscopic reversibility constraint (\\(K_{ox} K_{red}^{-1} = e^{(F/RT)\\Delta E^0}\\)). In contrast, \\(\\Delta E\\) (electrochemical potential gradient, \\(\\Delta E = E_{II} - E_I\\)) is an **external energy drive** that provides non-equilibrium conditions but does not alter the intrinsic thermodynamic basis of \\(q\\). It only modulates the \"direction\" of \\(q\\)’s bias (e.g., flipping \\(\\Delta E\\)’s sign flips whether \\(q > 1\\) or \\(q < 1\\)) but does not change the fact that \\(q\\)’s physical meaning (forward/backward cycle preference) is ultimately governed by \\(\\Delta E^0\\).",
            "Confirm the independence of directionality from \\(\\Delta E\\)’s sign: Substitute \\(q\\) into the \\(K_r\\) expression. Although \\(K_r\\) contains \\(\\Delta E\\), the sign of \\(\\Delta E\\) cancels out in the numerator and denominator. Only \\(\\Delta E^0\\) (a thermodynamic property, related to \\(K_{ox}/K_{red}\\) via \\(K_{ox}K_{red}^{-1} = e^{F\\Delta E^0/(RT)}\\)) determines the direction of \\(K_r\\). For example, if \\(K_{ox} > K_{red}\\) (\\(\\Delta E^0 > 0\\)), any \\(\\Delta E\\) (regardless of sign) favors cycle sequence \\(S\\); if \\(K_{ox} < K_{red}\\) (\\(\\Delta E^0 < 0\\)), it favors \\(S^{-1}\\).",
            "Verify with numerical simulations: Simulations show that when \\(\\Delta E \\neq 0\\), the direction of reaction fluxes (reflecting directionality) remains unchanged as \\(\\Delta E\\)’s sign reverses, and only changes when \\(\\Delta E^0\\) (or \\(K_{ox}/K_{red}\\)) changes. This confirms that directionality is solely controlled by thermodynamic properties."
        ],
        "step_count": 5
    },
    "recuWuq4lP4IMq": {
        "reasoning_steps": [
            "According to **concept_1**, when the number of hidden layers \\(H=2\\), the structure of the fully connected feedforward neural network \\(u_\\theta\\) is \\(u_\\theta = \\mathcal{A}_3 \\circ (\\tanh \\circ \\mathcal{A}_2) \\circ (\\tanh \\circ \\mathcal{A}_1)\\), which expands to \\(u_\\theta = W_3 \\tanh(W_2 \\tanh(W_1 x + b_1) + b_2) + b_3\\). Here, the parameter \\(\\theta = (W_1, b_1, W_2, b_2, W_3, b_3)\\), with \\(W_1 \\in \\mathbb{R}^{d_1 \\times D}\\), \\(W_2 \\in \\mathbb{R}^{D \\times D}\\), \\(W_3 \\in \\mathbb{R}^{D \\times d_2}\\), \\(b_1 \\in \\mathbb{R}^{1 \\times D}\\), \\(b_2 \\in \\mathbb{R}^{1 \\times D}\\), and \\(b_3 \\in \\mathbb{R}^{1 \\times d_2}\\).",
            "According to **concept_2**, when the differential order \\(K=1\\), the Hölder norm ( \\(C^1\\) norm) is \\(\\|u_\\theta\\|_{C^1} = \\max\\left\\{ \\|u_\\theta\\|_{\\infty,\\mathbb{R}^{d_1}}, \\max_{\\substack{\\alpha \\in \\mathbb{N}^{d_1} \\\\ |\\alpha|=1}} \\|\\partial^\\alpha u_\\theta\\|_{\\infty,\\mathbb{R}^{d_1}} \\right\\}\\). It is necessary to calculate the upper bounds of the infinity norm for the 0-th order partial derivative (the network \\(u_\\theta\\) itself) and the 1st order partial derivative respectively.",
            "Calculate the infinity norm of the 0-th order partial derivative \\(\\|u_\\theta\\|_{\\infty}\\): Using the boundedness of \\(\\tanh(t)\\) ( \\(|\\tanh(t)| \\leq 1\\) ), it can be obtained that the absolute values of the components of \\(z_1 = \\tanh(W_1 x + b_1)\\) and \\(z_2 = \\tanh(W_2 z_1 + b_2)\\) are both ≤ 1; For \\(u_\\theta^m = \\sum_j W_3^{j,m} z_2^j + b_3^m\\), apply the triangle inequality and Cauchy-Schwarz inequality to get \\(|u_\\theta^m| \\leq \\sum_j |W_3^{j,m}| + |b_3^m| \\leq \\sqrt{D} \\|W_3\\|_2 + \\|b_3\\|_2\\); According to **concept_3**, \\(\\|W_3\\|_2 \\leq \\|\\theta\\|_2\\) and \\(\\|b_3\\|_2 \\leq \\|\\theta\\|_2\\), so \\(\\|u_\\theta\\|_{\\infty} \\leq (\\sqrt{D} + 1) \\|\\theta\\|_2\\).",
            "Calculate the infinity norm of the 1st order partial derivative \\(\\max_{|\\alpha|=1} \\|\\partial^\\alpha u_\\theta\\|_{\\infty}\\): Differentiate using **concept_4** (the Faà di Bruno formula degenerates to the chain rule when \\(K=1\\)), so \\(\\partial^\\alpha u_\\theta = (\\partial \\mathcal{A}_1/\\partial x_\\alpha) \\cdot \\text{diag}(\\text{sech}^2(\\mathcal{A}_1)) \\cdot W_2 \\cdot \\text{diag}(\\text{sech}^2(\\mathcal{A}_2)) \\cdot W_3\\); Using \\(\\text{sech}^2(t) \\leq 1\\), the norm of the diagonal matrix is ≤ 1; Apply the norm properties and Cauchy-Schwarz inequality to the vector-matrix product to get \\(\\|\\partial^\\alpha u_\\theta\\|_{\\infty} \\leq \\sqrt{D} \\|W_1\\|_2 \\|W_2\\|_2 \\|W_3\\|_2\\); According to **concept_3**, \\(\\|W_1\\|_2, \\|W_2\\|_2, \\|W_3\\|_2 \\leq \\|\\theta\\|_2\\), so \\(\\|\\partial^\\alpha u_\\theta\\|_{\\infty} \\leq \\sqrt{D} \\|\\theta\\|_2^3\\).",
            "Take the maximum of the upper bounds of the infinity norms of the 0-th order and 1st order partial derivatives. Since \\(\\sqrt{D} \\|\\theta\\|_2^3\\) dominates when \\(\\|\\theta\\|_2 \\geq 1\\), we have \\(\\|u_\\theta\\|_{C^1} \\leq \\sqrt{D} \\|\\theta\\|_2^3 + (\\sqrt{D} + 1) \\|\\theta\\|_2\\)."
        ],
        "step_count": 5
    },
    "recuVUkYsUUqDF": {
        "reasoning_steps": [
            "The Pt-TiO2(R) has more surface adsorption oxygen species, indicating its more oxygen vacancies. The CO oxidation follows the MvK mechanism on the oxygen-rich Pt-TiO2(R) but the L-H mechanism on the Pt-TiO2(A). Therefore, the oxygen-rich Pt-TiO2(R) shows the higher activity in CO oxidation.",
            "The Pt-TiO2(A) has lower Pt0 content and the anatase TiO2 facilitates the formation of small metal particles, suggesting its higher oxidized Pt species. It also demonstrates the formation of electron-deficient Pt sites on the Pt-TiO2(A).",
            "The C3H8 oxidation follows the L-H mechanism on the Pt-TiO2(A) and Pt-TiO2(R). The electron-deficient Pt surface in Pt-TiO2(A) demonstrated a lower activation energy for C–C bond cleavage compared to dehydrogenation, while Pt-TiO2(R) was more prone to dehydrogenation because of the high activation energy for C–C bond cleavage. Therefore, the Pt-TiO2(R) exhibits the higher activity for C3H8 oxidation."
        ],
        "step_count": 3
    },
    "recuTUj0yC1a8w": {
        "reasoning_steps": [
            "By theorem 4, sampling edge colorings of G with Glauber dynamics is the same as sampling vertex colorings of the line graph L(G).",
            "By theorem 1, Δ(L(G)) ≤ 2Δ - 2.",
            "By theorem 2, Glauber dynamics on colorings is connected (and hence ergodic) only if q > Δ(L(G)) + 2 ≥ (2Δ - 2) + 2 = 2Δ.",
            "With q = 1.5Δ < 2Δ, the chain is not connected and thus not ergodic. Then the spectral gap is γ = 0, so theorem 3 gives no finite bound, and the chain cannot converge to the uniform distribution over all proper edge colorings.",
            "When the chain does not converge to the uniform distribution, no finite time guarantees a bounded distance to the uniform distribution, the mixing time is ∞."
        ],
        "step_count": 5
    },
    "recuVZBXO7r9sh": {
        "reasoning_steps": [
            "By construction, the off-diagonal Gram entries of $S$ equal the adjacency matrix of $G$, so $S$ represents $G$ by definition_1.",
            "A graph representable by a subset of the root system $A_n$ is exactly a line graph of a bipartite graph; hence $G$ is a line graph $G=L(H)$ for some bipartite $H$ by theorem_1.",
            "Let $\\Delta$ be the maximum degree of $G$ (the line graph). For a line graph $G$ with maximum degree $\\Delta$, the Glauber dynamics for the uniform distribution over proper $q$-edge colorings mixes rapidly whenever $q\\ge (1+o(1))\\Delta$ by theorem_2.",
            "Substitute $q=\\alpha\\Delta$ into the threshold $q\\ge (1+o(1))\\Delta$, yielding $\\alpha\\Delta \\ge (1+o(1))\\Delta$.",
            "Divide by $\\Delta>0$ to obtain the requirement $\\alpha \\ge 1+o(1)$.",
            "Interpret $o(1)$ as a term that vanishes as $\\Delta\\to\\infty$; thus for any fixed $\\varepsilon>0$, taking $\\alpha\\ge 1+\\varepsilon$ guarantees rapid mixing for all sufficiently large $\\Delta$.",
            "Conclude a constant, $\\Delta$-independent sufficient condition: $\\alpha>1$."
        ],
        "step_count": 7
    },
    "recuUuntacUzEP": {
        "reasoning_steps": [
            "Let B:=2^{\\,n^{\\delta^*}} (with fixed constant δ^*\\in(0,1]), |\\Sigma|=q^m, and log_2|\\Sigma|=m\\log_2 q. Define M:=\\max\\{B,\\ \\log_2|\\Sigma|\\}. Since B=\\exp(\\Theta(n^{\\delta^*})) while log_2|\\Sigma|=\\mathrm{poly}(n), for sufficiently large n we have M=B. Below we keep the symbol M in the derivation and substitute M=B when comparing magnitudes.",
            "In QSMP each party sends one register, with cost Q=N\\log_2 q. Folded RS parameters give N,\\log q=\\mathrm{poly}(n), hence the total QSMP communication is \\mathrm{poly}(n). The index gadget labels are not included in the quantum registers and therefore do not affect the QSMP cost.",
            "GS list decoding returns all candidates within radius N-\\sqrt{kN} in polynomial time; under the biased-noise condition N-\\sqrt{kN}\\ge(p+\\varepsilon)N, the unique decoding success probability for the dual code is \\ge 1-2^{-\\Omega(N)}. Treat both as “callable subroutines”; they do not change the communication accounting.",
            "Any public-coin two-way protocol (total communication T) can be normalized so that on each accepting rectangle the number of fixed coordinates satisfies s_{\\mathrm{fix}}\\ \\le\\ \\frac{c\\,T}{M}, and the t':=n-s_{\\mathrm{fix}} unfixed coordinates each have min-entropy \\ge \\alpha M, where c,\\alpha>0 are absolute constants.",
            "Folded RS list-recovery gives a parameter s_{\\mathrm{LR}}\\ge 1 such that L\\ \\le\\ q^{\\,s_{\\mathrm{LR}}}\\quad\\Rightarrow\\quad \\log_2 L\\ \\le\\ s_{\\mathrm{LR}}\\log_2 q\\ =\\ \\mathrm{poly}(n). Note: here s_{\\mathrm{LR}} (a theorem parameter) is **not** identified with s_{\\mathrm{fix}} (the number of fixed coordinates).",
            "For the t'=n-s_{\\mathrm{fix}} unfixed coordinates, each has min-entropy \\ge \\alpha M. The probability of hitting any fixed dangerous pattern is \\le 2^{-\\alpha M t'}. Union-bounding over L candidates, \\mathrm{suc}_1(T)\\ \\le\\ L\\cdot 2^{-\\alpha M t'} \\ \\le\\ 2^{\\log_2 L\\ -\\ \\alpha M (n-s_{\\mathrm{fix}})}.",
            "From Step 4, s_{\\mathrm{fix}}\\le cT/M. Thus, \\mathrm{suc}_1(T)\\ \\le\\ 2^{-\\alpha Mn\\ +\\ \\alpha M s_{\\mathrm{fix}}\\ +\\ \\log_2 L} \\ \\le\\ 2^{-\\alpha Mn\\ +\\ \\alpha c T\\ +\\ \\log_2 L}. Using Step 5’s \\log_2 L\\le \\mathrm{poly}(n), \\mathrm{suc}_1(T)\\ \\le\\ 2^{-\\alpha Mn\\ +\\ \\alpha c T\\ +\\ \\mathrm{poly}(n)}.",
            "Let T_\\star\\ :=\\ \\frac{\\alpha}{4c}\\,nM. When T\\le T_\\star, \\mathrm{suc}_1(T)\\ \\le\\ 2^{-\\alpha Mn\\ +\\ \\alpha c (\\alpha nM/4c)\\ +\\ \\mathrm{poly}(n)} \\ =\\ 2^{-\\tfrac{3}{4}\\,\\alpha Mn\\ +\\ \\mathrm{poly}(n)}. Since Mn\\ge nB = n\\cdot 2^{n^{\\delta^*}} grows exponentially, the \\mathrm{poly}(n) term is negligible, hence \\mathrm{suc}_1(T)\\ \\le\\ 2^{-\\Omega(Mn)}.",
            "The problem states \\log_2|\\mathcal H|=O(\\log n). For any fixed h, if T\\le T_\\star then \\mathrm{suc}_1^{(h)}(T)\\le 2^{-\\Omega(Mn)}. Union-bounding over the whole family, \\Pr[\\exists\\,h\\in\\mathcal H\\ \\text{success}]\\ \\le\\ |\\mathcal H|\\cdot 2^{-\\Omega(Mn)}\\ \\le\\ 2^{-\\Omega(Mn)+O(\\log n)}\\ =\\ 2^{-\\Omega(Mn)}. To reduce worst-case error to \\le 1/3, clearly T\\le T_\\star is impossible. Therefore one must have T\\ \\ge\\ T_\\star\\ =\\ \\Omega(nM).",
            "For sufficiently large n, M=\\max\\{B,\\log_2|\\Sigma|\\}=B=2^{\\,n^{\\delta^*}}. From Step 9, T\\ \\ge\\ \\Omega(n\\cdot 2^{\\,n^{\\delta^*}})\\ \\Rightarrow\\ T\\in \\Omega(2^{\\,n^{\\delta^*}}). Combining with Step 2 (QSMP is \\mathrm{poly}(n)), the final ordered pair is (\\ \\mathrm{poly}(n),\\ \\Omega(2^{\\,n^{\\delta^*}})\\ ), and the stronger form (\\mathrm{poly}(n),\\ \\Omega(n\\cdot 2^{\\,n^{\\delta^*}})) also holds."
        ],
        "step_count": 10
    },
    "recuW7JfC4coBP": {
        "reasoning_steps": [
            "Particle current (Concept_1): identify the observable as the flux of particles through a space-like hypersurface $\\Sigma$; the four-momentum projection $p^\\mu\\mathrm{d}^3\\sigma_\\mu$ gives the momentum-space density.",
            "Phase-space density (Concept_2): split the distribution into equilibrium plus deviation, $f_i(p)=\\left(f_\\text{eq}+\\delta f\\right)_i$, with degeneracy $g_i$ and quantum volume $(2\\pi)^3$.",
            "Covariant Cooper-Frye (Concept_3): assemble the flux factor $\\mathrm{d}^3\\sigma_\\mu\\,p^\\mu$, the degeneracy, the quantum volume, and the combined-subscript distribution $\\left(f_\\text{eq}+\\delta f\\right)_i$.",
            "Final one-line expression (Concept_4): collect all prefactors and the surface integral to obtain $E\\,\\mathrm{d}N_i/\\mathrm{d}^3\\mathbf{p}=\\dfrac{g_i}{(2\\pi)^3}\\displaystyle\\int_\\Sigma \\mathrm{d}^3\\sigma_\\mu\\,p^\\mu\\left(f_\\text{eq}+\\delta f\\right)_i$."
        ],
        "step_count": 4
    },
    "recuW3uuntD6lB": {
        "reasoning_steps": [
            "Clarify the definition of pseudo-detailed balanced reaction networks Pseudo-detailed balanced chemical reaction networks (CRNs) are defined by a key topological property of their stoichiometric structure: the stoichiometric submatrix for internal species and lumped reactions ($\\hat{\\mathbb{S}}_{X_I}$, with entries $S_{x,\\varepsilon}$ for internal species $x \\in X_I$ and lumped reactions $\\varepsilon$) **admits no right-null eigenvectors**. Mathematically, there exists no non-zero vector $\\{\\phi_\\varepsilon\\}$ such that $\\sum_{\\varepsilon>0} S_{x,\\varepsilon} \\phi_\\varepsilon = 0$ for all internal species $x \\in X_I$. This trait ensures no \"hidden\" reaction cycles that leave internal species abundances unchanged, distinguishing them from standard detailed balanced networks.",
            "Derive lumped reaction net current with pseudo-local detailed balance First, assume reaction fluxes follow a general form, which relies on the key premise that **chemostatted species are ideal** (non-ideal interactions are negligible). This allows splitting the flux into two independent parts: - A term $\\omega_{\\text{in}}$ related to internal species (depends on internal species concentrations $c_x(r)$ and their stoichiometric coefficients $v_{x,\\rho}$ in reaction $\\rho$); - A term $\\omega_{\\text{ch}}$ related to chemostatted species (depends only on chemostatted species concentrations $c_y$ and their stoichiometric coefficients $v_{y,\\rho}$, independent of internal species due to the \"ideal\" premise). Next, decompose the reaction flux into symmetric ($s_\\rho = \\sqrt{\\omega_\\rho \\omega_{-\\rho}}$, invariant under forward/backward reaction reversal) and antisymmetric parts. Combine this decomposition with the local detailed balance condition, then rewrite the lumped reaction net current (sum of net currents of reactions in the same lumped reaction $\\varepsilon$). Finally, use the pseudo-detailed balance condition (the stoichiometric matrix $S_{x,\\varepsilon}$ for internal species and lumped reactions has no right-null eigenvectors) to introduce constant coefficients $\\Delta_x$, which satisfy $\\ln b_{\\varepsilon} = \\sum_{x} \\frac{\\Delta_{x} S_{x, \\varepsilon}}{RT}$ (where $b_\\varepsilon$ is a constant derived from the flux’s chemostatted species-related term $\\omega_{\\text{ch}}$). With $\\Delta_x$, construct lumped reaction fluxes $\\hat{\\omega}_{\\pm\\varepsilon}$ that meet the pseudo-local detailed balance condition.",
            "Examine the time derivative of the kinetic potential $F_{pdb}[c]$ For pseudo-detailed balanced CRNs, the kinetic potential $F_{pdb}[c]$ is constructed by shifting the chemical potentials of internal species: $\\hat{\\mu}_x(c(r)) = \\mu_x(c(r)) + \\Delta_x$ (where $\\Delta_x$ are constants derived in step 2), such that $F_{pdb}[c] = F[c] + \\sum_x \\Delta_x \\int_V dr \\, c_x(r)$ (with $F[c]$ being the total Helmholtz free energy). Its time derivative is as follows: $$d_t F_{pdb}[c] = -T \\dot{\\Pi}_{rct}[c] - T \\dot{\\sum}_{diff}[c] \\leq 0$$ where: - $\\dot{\\Pi}_{rct}[c] \\geq 0$ (pseudo-reaction entropy production rate, linked to lumped reaction fluxes $\\hat{\\omega}_{\\pm\\varepsilon}$); - $\\dot{\\sum}_{diff}[c] \\geq 0$ (diffusion entropy production rate, quantifying dissipation from species diffusion). Since $F_{pdb}[c]$ is lower-bounded (as $F[c]$ is lower-bounded), the system relaxes to a steady state $c_{ss}(r)$ where $d_t F_{pdb}[c_{ss}] = 0$, forcing both $\\dot{\\Pi}_{rct}[c_{ss}] = 0$ and $\\dot{\\sum}_{diff}[c_{ss}] = 0$.",
            "Interpret the physical meaning of the steady-state conditions $\\dot{\\sum}_{diff}[c_{ss}] = 0$ implies the diffusion entropy production rate vanishes, meaning diffusion processes equilibrate at steady state. Combined with the relation between pseudo-chemical potential $\\hat{\\mu}_x(c(r))$ (shifted from $\\mu_x(c(r))$) and chemical potential, this leads to $\\nabla \\mu_i(c_{ss}(r)) = 0$ for all species — confirming homogeneous chemical potentials and fully equilibrated diffusion (no net diffusion currents). Meanwhile, $\\dot{\\Pi}_{rct}[c_{ss}] = 0$ (vanishing pseudo-reaction entropy production rate) has a specific physical meaning: it corresponds to **lumped reaction net currents $\\hat{j}_\\varepsilon(c_{ss}) = 0$** (sum of net currents of individual reactions within each lumped reaction $\\varepsilon$ equals zero). However, this does not imply the thermodynamic reaction entropy production rate $\\dot{\\sum}_{rct}[c_{ss}] = 0$ — individual reactions within a lumped group may still have non-zero net currents ($j_\\rho(c_{ss}) \\neq 0$), but their contributions cancel out at the lumped level. Thus, chemical reactions remain out of equilibrium overall, and dissipation persists (reflected by $\\dot{\\sum}_{rct}[c_{ss}] > 0$ in general).",
            "Synthesize the reasoning In pseudo-detailed balanced reaction networks, the steady state $c_{ss}(r)$ satisfies two critical conditions: 1. $\\dot{\\sum}_{diff}[c_{ss}] = 0$: Diffusion entropy production vanishes, meaning diffusion processes equilibrate (homogeneous chemical potentials, no net diffusion currents). 2. $\\dot{\\Pi}_{rct}[c_{ss}] = 0$ (lumped reaction net currents cancel) but $\\dot{\\sum}_{rct}[c_{ss}] > 0$: Chemical reactions remain out of equilibrium, with dissipation arising solely from irreversible reactions. Thus, diffusion processes always equilibrate in these networks, while dissipation occurs exclusively due to chemical reactions. The answer to the question is \"Yes\"."
        ],
        "step_count": 5
    },
    "recuW7RmCWzoYc": {
        "reasoning_steps": [
            "Channel decomposition and recombination: The target channel $\\mathcal{M}_{d+1} \\otimes \\mathcal{M}_d^c \\otimes \\mathcal{E}_{1/2,d}$ can be restructured as: $(\\mathcal{M}_{d+1} \\otimes \\mathcal{E}_{1/2,d}) \\otimes \\mathcal{M}_d^c$. This recombination uses the tensor product associativity and does not change capacity properties.",
            "Apply quantum capacity lower bounds: From theorem_3: $Q^{(1)}(\\mathcal{M}_{d+1} \\otimes \\mathcal{E}_{1/2,d}) \\geq \\frac{1}{2} + O(1/d)$ From theorem_2: $ Q^{(1)}(\\mathcal{M}_d^c) = \\log(d - 1) $",
            "Combine lower bounds: Using the given conditions: $ Q^{(1)}((\\mathcal{M}_{d+1} \\otimes \\mathcal{E}_{1/2,d}) \\otimes \\mathcal{M}_d^c) \\geq \\underbrace{\\frac{1}{2} + O(1/d)}_{\\text{lower bound part}} + \\underbrace{\\log(d - 1)}_{\\text{exact value}} $",
            "Asymptotic dominant term analysis: When $d \\to \\infty$: $ \\log(d - 1) \\sim \\log d $ (dominant divergent term, order $\\Theta(\\log d)$), $ \\frac{1}{2} $ is a constant term (order $O(1)$), $ O(1/d) \\to 0 $ (negligible). Thus the asymptotic dominant term is $\\log d$.",
            "Tightness verification: theorem1 gives the platypus channel upper bound: $ Q(M_{d+1}) \\leq \\log\\left(1 + \\frac{1}{d}\\right) \\sim O(1/d) $ (since $\\log(1+x) \\sim x$ when $x \\to 0$). Erasure channel upper bound: $ Q(E_{1/2,d}) = 0 $ (when $p=1/2$, $1-2p=0$). Complementary channel upper bound: $ Q^{(1)}(M_d^c) = \\log(d-1) $. Overall loose upper bound estimate: $ Q^{(1)}(\\text{composite channel}) \\leq Q(M_{d+1}) + Q(M_d^c) + Q(E_{1/2,d}) \\sim O(1/d) + \\log d + 0 \\sim \\log d $ Conclusion: The lower bound $\\sim \\log d + O(1)$ and upper bound $\\sim \\log d$ match to the dominant term when $d \\to \\infty$, confirming $\\log d$ as the asymptotic dominant term."
        ],
        "step_count": 5
    },
    "recuW7C0zPpgoq": {
        "reasoning_steps": [
            "Fix $G=D_5$, $a=rs\\in A$, $b=s\\in B$. In an LRCC, 2-cells are squares $[a,g,b]=\\{g,ag,agb,gb\\}$ by concept_1.",
            "A face is degenerate iff $ag=gb\\iff g^{-1}ag=b$; this is exactly the negation of TNC (concept_2). So we cannot apply theorem_1 directly.",
            "Note elements of $D_5$ are either rotations $r^m$ or reflections $r^m s$ ($m=0,\\dots,4$).",
            "Compute the $2$-cells of rotations: $g=r^m\\Rightarrow g^{-1}ag=r^{-m}(rs)r^{m}=r^{1-2m}s$ by concept_1",
            "Set $r^{1-2m}s=s\\Rightarrow 1-2m\\equiv0\\pmod5\\Rightarrow m\\equiv3$; thus only when $g=r^3$, the $2$-cell is degenerate.",
            "Compute the $2$-cells for reflections: $g=r^m s\\Rightarrow g^{-1}ag=(r^{m}s)^{-1}(rs)(r^{m}s)=r^{2m-1}s$.",
            "Set $r^{2m-1}s=s\\Rightarrow 2m-1\\equiv0\\pmod5\\Rightarrow m\\equiv3$; thus only when $g=r^3 s$, the $2$-cell is degenerate.",
            "Exactly two $g$ give degenerate faces; hence $10-2=8$ nondegenerate $g$.",
            "Each nondegenerate square appears 4 times via $g,ag,gb,agb$; divide by 4 to get $|X(2)|=8/4=2$."
        ],
        "step_count": 9
    },
    "recuW2nzt5LEEE": {
        "reasoning_steps": [
            "Calculate conditional expectations According to theorem_1 (DTPC output distribution), given input release rates: For $u_0$: the conditional expectation for each dimension is $\\mu_{0,t} = \\rho x_0 + \\lambda$. For $u_1$: the conditional expectation for each dimension is $\\mu_{1,t} = \\rho x_1 + \\lambda$ (where $x_1$ is the release rate for $u_1$). Since all dimensions are identical, $\\mu_{0,t} = \\rho x_0 + \\lambda$ and $\\mu_{1,t} = \\rho x_1 + \\lambda$ hold for all $t$.",
            "Decompose the decision variable According to concept_3 (decision variable decomposition): $d(y, u_j) = \\sum_{t=1}^n d_t(y_t, \\mu_{j,t}), \\quad d_t(y_t, \\mu_{j,t}) = (y_t - \\mu_{j,t})^2 - y_t$. Therefore: $\\Delta(y) = \\sum_{t=1}^n \\left[ d_t(y_t, \\mu_{1,t}) - d_t(y_t, \\mu_{0,t}) \\right] = \\sum_{t=1}^n \\Delta_t(y_t)$. Expanding $\\Delta_t(y_t)$: $\\Delta_t(y_t) = \\left[ (y_t - \\mu_{1,t})^2 - y_t \\right] - \\left[ (y_t - \\mu_{0,t})^2 - y_t \\right] = (\\mu_{0,t} - \\mu_{1,t})(2y_t - \\mu_{0,t} - \\mu_{1,t})$. After simplification: $\\Delta_t(y_t) = 2(\\mu_{0,t} - \\mu_{1,t})y_t + (\\mu_{1,t}^2 - \\mu_{0,t}^2)$",
            "Calculate variance for a single dimension Under the condition that $u_0$ is transmitted, according to theorem_1, the output $y_t \\sim \\text{Poisson}(\\mu_{0,t})$, with: $\\mathbb{E}[y_t] = \\mu_{0,t}$ and $\\text{Var}(y_t) = \\mu_{0,t}$. The variance of $\\Delta_t(y_t)$ is: $\\text{Var}(\\Delta_t(y_t)) = \\text{Var}\\left( 2(\\mu_{0,t} - \\mu_{1,t})y_t + (\\mu_{1,t}^2 - \\mu_{0,t}^2) \\right)$. The constant term $(\\mu_{1,t}^2 - \\mu_{0,t}^2)$ does not affect the variance, and since $\\mu_{0,t}$ and $\\mu_{1,t}$ are constants, we have: $\\text{Var}(\\Delta_t(y_t)) = \\left[ 2(\\mu_{0,t} - \\mu_{1,t}) \\right]^2 \\text{Var}(y_t)$. Substituting $\\mu_{0,t} - \\mu_{1,t} = \\rho(x_0 - x_1)$ and $\\text{Var}(y_t) = \\mu_{0,t} = \\rho x_0 + \\lambda$, we get: $\\text{Var}(\\Delta_t(y_t)) = 4\\rho^2 (x_0 - x_1)^2 (\\rho x_0 + \\lambda)$",
            "Utilize channel independence According to theorem_2 (spatial channel independence), the outputs $y_t$ of each dimension are independent, so $\\Delta_t(y_t)$ are independent. The total variance is the sum of variances from each dimension: $\\text{Var}(\\Delta(y)) = \\sum_{t=1}^n \\text{Var}(\\Delta_t(y_t)) = \\sum_{t=1}^n 4\\rho^2 (x_0 - x_1)^2 (\\rho x_0 + \\lambda) = 4\\rho^2 (x_0 - x_1)^2 (\\rho x_0 + \\lambda) n$",
            "Find the asymptotic variance Dividing the variance by $n$: $\\frac{\\text{Var}(\\Delta(y))}{n} = 4\\rho^2 (x_0 - x_1)^2 (\\rho x_0 + \\lambda)$. As $n \\to \\infty$, this value converges to a constant, so the asymptotic variance per dimension is $4\\rho^2 (x_0 - x_1)^2 (\\rho x_0 + \\lambda)$"
        ],
        "step_count": 5
    },
    "recuW4MRiG6MhN": {
        "reasoning_steps": [
            "Operating window (Concept 1). A single-effect LiBr–H₂O chiller is designed for 80–100 °C sources with typical COP ≈ 0.7–0.8 in that window. At 55 °C it is well below its viable range → severe COP degradation / near-failure. At 85 °C it lies inside the normal window → high COP expected.",
            "What the combined cycle does (Concept 2). The combined AHP–SWD architecture breaks the evaporator–condenser loop to feed seawater to the evaporator and recover freshwater at the condenser via latent condensation. Inter-stage heat coupling reuses latent heat along the train and aligns temperatures, reducing exergy losses from large mismatches; mass coupling shares solution/streams but introduces mixing/transfer irreversibilities and weaker temperature matching.",
            "Two-stage vs single-stage in theory (Concept 3). For two-stage absorption under low-grade heat, the ideal/theoretical COP is lower than single stage—provided both are within their viable temperature range.",
            "Synthesis at 55 °C. Single-stage: far outside 80–100 °C → COP collapses. Two-stage: staging sustains operation when the single-stage struggles. Between the two staged options, heat coupling better recycles latent heat and temperature-matches stages (Concept 2), so it edges out mass coupling. ⇒ 55 °C ranking: Heat-coupled two-stage > Mass-coupled two-stage > Single-stage.",
            "Synthesis at 85 °C. Single-stage: within 80–100 °C → regains its inherent COP advantage (Concepts 1 & 3). Two-stage: still theoretically below single-stage COP; among them, heat-coupled remains above mass-coupled by the same reasoning as in step 4. ⇒ 85 °C ranking: Single-stage > Heat-coupled two-stage > Mass-coupled two-stage."
        ],
        "step_count": 5
    },
    "recuW7NlFg8X5F": {
        "reasoning_steps": [
            "Place the absorber first (Concept 1). Solar collector/absorber is known to account for the largest exergy destruction in solar-thermal desalination because of the big temperature/quality mismatch between solar input and the working stream. → Absorber = #1 (dominant).",
            "Identify the next major sink within the module (Concept 1). Downstream irreversibilities concentrate in heat exchangers and the AGMD/air-gap module (thermal/concentration polarization, finite ΔT, diffusion resistance). Among the four items in this question, the air gap represents this module-level sink. → Air gap = #2.",
            "Rank condenser vs evaporator (Concept 3). In HDH-type units, dehumidifier (condenser) typically shows higher exergy destruction than the humidifier (evaporator) because condensation with finite approach temperatures and cooling penalties generates more entropy than near-equilibrium vaporization on the hot side. → Condenser = #3; Evaporator = #4.",
            "Synthesis. Combining 1–3 gives the ordered list: Absorber > Air gap > Condenser > Evaporator."
        ],
        "step_count": 4
    },
    "recuVKwwnd9DVs": {
        "reasoning_steps": [
            "First, clarify the foundational concepts to eliminate ambiguity and set the problem’s scope:  - **Random Reversible Circuit**: A classical reversible circuit composed of randomly selected classical reversible gates (e.g., DES[2]-type gates or general 3-bit reversible gates). Reversible gates are bijective (no information loss), so the circuit maps input binary strings in \\{0,1\\}^n to output binary strings via a permutation of \\{0,1\\}^n.  - **Fixed 2D Nearest-Neighbor Architecture**: A hardware constraint where bits (for classical reversible computation) are arranged in a 2D lattice (e.g., a \\sqrt{n} \\times \\sqrt{n} grid for n bits). Bits can only interact directly with their immediate neighbors (horizontally/vertically adjacent bits); long-range interactions require sequential \"routing\" via adjacent bits.  - **Approximately k-wise Independent Permutations**: A permutation of the n-bit input space \\{0,1\\}^n where the marginal distribution of any k distinct input bits is \"close\" to uniform (formally, within a small error \\varepsilon of the uniform distribution over k-length binary strings). This relaxes the strictness of full k-wise independence for practical classical reversible circuit implementation.  - **Circuit Depth**: The minimum number of sequential time steps required to execute the circuit, where each time step can run multiple non-interacting gates (gates acting on disjoint sets of bits).",
            "Translate the original question into a precise theoretical goal: Given an n-qubit system arranged in a fixed 2D nearest-neighbor lattice, determine the minimum circuit depth D such that a random reversible circuit (built from valid nearest-neighbor gates) produces permutations that satisfy *approximate k-wise independence*.",
            "The 2D lattice imposes a critical limit on qubit interactions: long-range correlations (between qubits not adjacent in the grid) require \"routing\"—a sequence of SWAP or adjacent gates to transfer quantum information across the lattice. For an n-qubit 2D lattice, the most efficient arrangement is a \\sqrt{n} \\times \\sqrt{n} grid (minimizing the maximum distance between any two qubits). The maximum distance between two qubits in this grid is \\sim \\sqrt{n} (e.g., the diagonal of the grid), which initially suggests a routing depth of \\sim \\sqrt{n}. However, a rigorous analysis of the 2D nearest-neighbor architecture’s depth constraint requires a two-step derivation, as follows: 1. First, establish the spectral gap/mixing time upper bounds for 1D brickwork layouts. This analysis yields a depth bound for 1D circuits of approximately n \\cdot \\widetilde{O}(k^2) (or more generally, (nk + \\log(1/\\varepsilon)) \\cdot \\widetilde{O}(k)), where k characterizes interaction complexity and \\varepsilon denotes the error tolerance. 2. Next, extend the 1D result to 2D by alternately embedding 1D circuits along the rows and columns of the \\sqrt{n} \\times \\sqrt{n} grid. Combinatorial analysis of this embedding process introduces an additional \\widetilde{O}(k) factor. Combining these two steps, the architectural baseline depth—defined as the minimum depth required to enable long-range qubit interactions, independent of permutation complexity—for 2D nearest-neighbor lattices is established as \\sqrt{n} \\cdot \\widetilde{O}(k^3).",
            "To achieve approximate k-wise independence, the circuit must break spurious correlations between subsets of up to k qubits. For random reversible circuits, the complexity of this task scales with k: For small k (e.g., k=2), simple gate sequences (e.g., random CNOTs) suffice to approximate 2-wise independence. For larger k, the circuit needs more layers of gates to ensure that all possible subsets of k qubits have marginally uniform distributions. Mathematical analysis (e.g., via Fourier transforms on the symmetric group or probabilistic method) shows that the number of gate layers required to suppress correlations in k-qubit subsets scales as k³ (the cubic scaling arises from accounting for all pairwise, triple-wise, and higher-order interactions within k-qubit subsets). The tilde in \\(\\tilde{O}(k^3)\\) denotes \"soft O notation,\" which ignores logarithmic factors (e.g., log k or log n) that are negligible for large n or k.",
            "The total circuit depth has two multiplicative contributions: Architectural Routing Depth: As established in Step 3, the 2D nearest-neighbor lattice requires a depth of ~\\sqrt{n} to enable long-range interactions between any qubits. Without this, the circuit cannot access all k-qubit subsets (e.g., qubits on opposite corners of the grid) to enforce k-wise independence. Permutation Complexity Depth: As shown in Step 4, approximating k-wise independence requires \\(\\tilde{O}(k^3)\\) layers of random reversible gates. These layers must be applied after routing (or in parallel with optimized routing) to ensure correlations are suppressed across all k-qubit subsets. Since these two contributions are sequential (routing enables interaction, and complexity layers enforce independence), the total depth is their product: D = \\sqrt{n} \\cdot \\tilde{O}(k^3).",
            "To confirm the conclusion, test edge cases and verify probabilistic correctness: Small k (e.g., k=1): For 1-wise independence (marginal uniformity of single qubits), \\(\\tilde{O}(k^3) = \\tilde{O}(1)\\), so D \\approx \\sqrt{n}. This aligns with intuition: routing across the 2D grid (depth ~\\sqrt{n}) is sufficient to randomize single-qubit positions. Large k (e.g., k~\\sqrt{n}): For k approaching \\sqrt{n}, \\(\\tilde{O}(k^3) = \\tilde{O}(n^{3/2})\\), so D \\approx \\sqrt{n} \\cdot n^{3/2} = n^2 —a scaling consistent with known results for high-complexity permutations in 2D architectures. Probabilistic Approximation: The \"random\" nature of the circuit ensures that, with high probability (e.g., 1 - 1/n), the permutation meets the approximate k-wise independence criterion. This avoids the need for deterministic (and deeper) circuits.",
            "Combining all steps—core definitions, architectural constraints, permutation complexity, and validation—the minimum circuit depth required is formally stated as: The circuit depth required for a random reversible circuit with a fixed, two-dimensional nearest-neighbor architecture to compute permutations that are approximately k-wise independent is D = \\sqrt{n} \\cdot \\tilde{O}(k^3)."
        ],
        "step_count": 7
    },
    "recuVNjGYnig9d": {
        "reasoning_steps": [
            "Under the per-slot power constraint, any admissible input strategy must satisfy \\[ \\mathbb{E}\\big[b(X_{i,j})\\big]\\le B,\\quad \\forall\\ i,\\ \\forall j\\in[Q], \\] which induces the feasible set \\[ \\mathcal{B}(B)\\;=\\;\\Big\\{\\,P_{X^Q\\|Z^Q}:\\ \\mathbb{E}\\big[b(X_{i,j})\\big]\\le B,\\ \\forall i,\\ \\forall j\\in[Q]\\,\\Big\\}. \\]",
            "With in-block memory of length $Q$, the channel state remains fixed within each block and is independent across blocks. The strict causal encoding law factorizes as \\[ P_{X^Q\\|Z^Q}(x^Q\\|z^Q)\\;=\\;\\prod_{t=1}^{Q} P_{X_t\\mid X^{t-1},Z^{t-1}}(x_t\\mid x^{t-1},z^{t-1}). \\]",
            "By the chain rule tailored to the iBM setting and strict causality, \\[ I(S;Z^Q\\mid X^Q)\\;=\\;\\sum_{t=1}^{Q} I\\!\\big(S;X_t,Z_t\\mid X^{t-1},Z^{t-1}\\big), \\] so that $Q$ is simultaneously the dynamic-programming horizon and the number of accumulated information terms.",
            "Each slot produces a binary feedback \\[ Z_t \\;=\\; \\mathbf{1}\\{\\|Y_t\\|^2>\\nu\\}, \\] whose law \\(P_{Z_t\\mid S,X_t}\\) is determined by the false-alarm and miss-detection probabilities \\[ P_{\\mathrm{FA}}=1-F_{\\chi^2_{2q}}(\\nu),\\qquad P_{\\mathrm{MD}}=F_{\\chi^2_{2q}(\\sigma_m^2)}(\\nu), \\] thereby fixing the per-slot contribution \\(I\\!\\big(S;X_t,Z_t\\mid X^{t-1},Z^{t-1}\\big)\\) once a feasible \\(P_{X^Q\\|Z^Q}\\in \\mathcal{B}(B)\\) is chosen.",
            "Maximizing the accumulated sensing information over the feasible causal strategies yields the sensing capacity \\[ \\boxed{\\; C_{\\text{sensing}} \\;=\\; \\sup_{P_{X^Q\\|Z^Q}\\in \\mathcal{B}(B)} \\;\\sum_{t=1}^{Q} I\\!\\big(S;X_t,Z_t \\,\\big|\\, X^{t-1},Z^{t-1}\\big) \\;} \\] with $B$ entering through the feasible set \\(\\mathcal{B}(B)\\) and $Q$ entering as the horizon and summation length."
        ],
        "step_count": 5
    },
    "recuVOQLFy0vMR": {
        "reasoning_steps": [
            "Overall energy efficiency increases when a larger fraction of the absorbed solar energy is converted into latent heat of evaporation (i.e., when vapor generation increases while non-evaporative losses are unchanged).",
            "In the conventional design, the condenser/air-gap region sits at a low dry-bulb temperature; psychrometrics dictates the air’s moisture capacity (humidity ratio) is low at such temperatures, which caps how much vapor the air can carry.",
            "Heating the condenser raises the air-gap dry-bulb temperature, thereby increasing the air’s moisture capacity. This relaxes the psychrometric ceiling on moisture transport across the gap.",
            "Vapor transport is driven by the partial vapor-pressure difference between the hot (evaporating) and cold (condensing) sides. Because saturation vapor pressure rises steeply with absolute temperature (Clausius–Clapeyron), shifting the stage to a higher absolute temperature level (even with similar or only modestly reduced ΔT) increases the vapor-pressure driving force, although under a fixed temperature range, reduced ΔT always means less driving force.",
            "Steps 3–4 jointly increase the attainable vapor flux through the air gap (higher moisture capacity + stronger vapor-pressure driving force).",
            "With higher vapor flux, a larger portion of the absorbed solar energy is routed into phase change rather than being limited by air-side carrying capacity, so the overall energy efficiency increases (under the stated “all else equal” conditions).",
            "Conclusion: Yes—stage-wise condenser heating can improve overall energy efficiency via (1) psychrometric capacity gain and (2) increased vapor-pressure driving force at higher absolute temperatures."
        ],
        "step_count": 7
    },
    "recuW69XUUbJUL": {
        "reasoning_steps": [
            "Note $G=K_6$ is connected, so by theorem_1 we have $\\chi_{M}(\\lambda)=\\chi_{K_6}(\\lambda)/\\lambda$.",
            "It's a basic fact that the chromatic polynomial $\\chi_{K_6}(\\lambda)=\\lambda(\\lambda-1)(\\lambda-2)(\\lambda-3)(\\lambda-4)(\\lambda-5)$.",
            "Conclude $\\chi_M(\\lambda)=(\\lambda-1)(\\lambda-2)(\\lambda-3)(\\lambda-4)(\\lambda-5)$ by theorem_1.",
            "By concept_1, the reduced characteristic polynomial $\\overline{\\chi}_M(\\lambda)=\\chi_M(\\lambda)/(\\lambda-1)$. We have $\\overline{\\chi}_M(\\lambda)=(\\lambda-2)(\\lambda-3)(\\lambda-4)(\\lambda-5)$.",
            "Observe $\\deg \\overline{\\chi}_M=4$; hence in concept_2 notation $r=4$.",
            "We have $\\mu_4(M)=\\overline{\\chi}_M(0)$ since $r$ is even in concept_2.",
            "Conclude $\\deg(\\beta_M^{4})=120$ by theorem_2."
        ],
        "step_count": 7
    },
    "recuW1WeBZxRtj": {
        "reasoning_steps": [
            "Define the Original LM Rate Problem (Max-Min Structure)",
            "Apply Duality Theory to Transform the Inner Minimization (Theorem 1)",
            "Reformulate the LM Rate as a Double Maximization Problem (Proposition 2)",
            "Explicitly Define Constraints on $p_i$",
            "Construct the Lagrangian for Constrained Optimization",
            "Compute Partial Derivatives and Solve for $p_i$",
            "Determine the Normalization Constant $Z$",
            "Finalize the Optimal $p_i$"
        ],
        "step_count": 8
    },
    "recuW8cPT868Ge": {
        "reasoning_steps": [
            "The process begins by defining the optimal control problem, which aims to find the incentive protocol $u^*$ that minimizes the cumulative cost $J_v$ (Theorem_1) subject to the system's evolutionary dynamics (Concept_1).",
            "The Hamiltonian function $H_v(x,u,t)$ is formulated by combining the instantaneous cost $G_v(x,u,t)$ and the system dynamics $F_v(x,u,t)$ weighted by the co-state, which is the partial derivative of the optimal cost function, $\\frac{\\partial J_v^*}{\\partial x}$ (Concept_2). This results in $H_v(x,u,t) = G_v(x,u,t) + \\frac{\\partial J_v^*}{\\partial x} F_v(x,u,t)$.",
            "To find the candidate for the optimal control $u^*$, the necessary optimality condition is applied by taking the partial derivative of the Hamiltonian with respect to $u$ and setting it to zero (Theorem_2). Solving $\\frac{\\partial H_v}{\\partial u} = 0$ yields an expression for $u^*$ in terms of the state $x$ and the co-state $\\frac{\\partial J_v^*}{\\partial x}$.",
            "The Hamilton-Jacobi-Bellman (HJB) equation, $-\\frac{\\partial J_v^*}{\\partial t} = H_v(x, u^*, t)$, is then employed (Theorem_4). This equation provides a dynamic relationship for the optimal cost function itself.",
            "The free terminal time assumption is invoked (Theorem_3), which simplifies the HJB equation by setting the time derivative of the optimal cost function to zero: $\\frac{\\partial J_v^*}{\\partial t} = 0$. The HJB equation thus becomes a static equation: $0 = H_v(x, u^*, t)$.",
            "The expression for $u^*$ (from Step 3) is substituted into this static HJB equation. The equation now contains terms involving $G_v(x,u^*,t)$ and $F_v(x,u^*,t)$.",
            "After substitution, the equation is simplified. This leads to an algebraic equation for the co-state $\\frac{\\partial J_v^*}{\\partial x}$ that has a non-trivial solution.",
            "This non-trivial solution for $\\frac{\\partial J_v^*}{\\partial x}$ is then substituted back into the expression for $u^*$ from Step 3. The terms involving the state $x$ and other parameters cancel out, yielding the final, constant optimal incentive protocol: $u^*=2c$."
        ],
        "step_count": 8
    },
    "recuVSXlVDlI2S": {
        "reasoning_steps": [
            "Based on the concept 1, the Ru species mainly consist of Ru0, Ru4+ and Ru6+ on the CeO2 support, thus the rod-shaped CeO2 and octahedral CeO2 with the lower Ru0 species content have more oxidize Ru species in comparison with the cubic CeO2. The reducible Ruδ+ and the Ru0 species are active in synthesizing methyl propionate (MP) from CO, C2H4, and CH3OH. Therefore, Ru species supported on the rod-shaped CeO2 and octahedral CeO2 with lower proportion of reducible Ruδ+ and the Ru0 species have lower activity in comparison with those supported on the cubic CeO2.",
            "The CeO2 (111) is the most stable crystal face, implying its lowest reactivity. The high stability of CeO2 (111) is not favorable for the formation of oxygen vacancy and the Ru-O-Ce interaction. Thus, the Ru species supported on the octahedral CeO2 are the most inactive.",
            "The Ru0 species content in cubic CeO2 is higher than that in rod-shaped CeO2, which provides the higher active sites on cubic CeO2. It also suggesting the stronger electron transfer between Ru0 species and CeO2 (100), which favors the formation of Ru-O-Ce active sites, thus facilitating the activation of CO and C2H4. Therefore, Ru species supported on the cubic CeO2 show the highest activity."
        ],
        "step_count": 3
    },
    "recuVUmrX9jwTC": {
        "reasoning_steps": [
            "By using the action relations of the quantum Weyl algebra, derive the recurrence formula for the matrix coefficients of the holonomy R-matrix under the Fourier dual basis, laying a foundation for the subsequent explicit construction of the holonomy R-matrix.",
            "Introduce the quantum holonomy dilogarithm, integrate the recurrence relations to obtain the explicit expression of the R-matrix, distinguish the different forms of positive/negative crossings, and complete the \"normalized\" definition of the R-matrix.",
            "Prove that the holonomy R-matrix can be decomposed into a product of four quantum dilogarithm operators, where each factor corresponds to the \"flips of ideal tetrahedra\" around the crossing point, and establish a direct connection between the R-matrix and the geometric decomposition of knot complements (octahedral decomposition)."
        ],
        "step_count": 3
    },
    "recuVkD24cMEQF": {
        "reasoning_steps": [
            "The problem asks for the maximum number of conversation rounds, K, for two agents to reach an agreement within a specific environment, the **canonical setting** (Concept_1). The critical property of these agents is that their predictions are **conversation-calibrated** (Concept_3).",
            "The analysis uses the **Squared Error (SQE)** as a potential function to track the progress of the conversation (Concept_4). The total SQE is non-negative and bounded, as all predictions and outcomes are in [0,1].",
            "The core mechanism driving the agreement is detailed in **Theorem_1**: for two conversation-calibrated agents, if a round of conversation does not end in agreement, it is guaranteed to cause a quantifiable improvement (a decrease) in the cumulative SQE. This improvement is proportional to ε²δ.",
            "This process cannot continue indefinitely. Since the total SQE is bounded, it can only be reduced a finite number of times. Each round that fails to achieve agreement \"spends\" a portion of this finite SQE budget.",
            "Theorem_2 provides the condition under which this process leads to a clean bound. It states that for a sufficiently large number of interaction days T, the cumulative error from imperfect calibration, denoted β(T), becomes negligible. Specifically, it becomes smaller than half of the guaranteed SQE improvement per round (β(T) ≤ δε²/2).",
            "Combining these facts, if the conversation continues for too many rounds, the cumulative reduction in SQE would exceed its total possible value, which is a contradiction. Therefore, the conversation must terminate by reaching ε-agreement (Concept_2). Under the condition from Theorem_2, this logic leads to the upper bound on the number of rounds: K ≤ 2/(δε²)."
        ],
        "step_count": 6
    },
    "recuVuKSUJjDKl": {
        "reasoning_steps": [
            "The algorithm begins by computing a crude, computationally inexpensive estimate of the sum of projection matrices, $\\sum W_i$, using a simple input-output correlation (concept_1).",
            "Using this initial estimate, the algorithm sculpts a convex body that serves as a rough approximation of the affine hull containing the true attention matrices, $\\Theta_1, ..., \\Theta_m$, by accumulating linear constraints from the data (concept_2).",
            "To improve precision, the algorithm refines its estimate of $\\sum W_i$. It identifies specific 'large-margin' examples which provide highly accurate constraints, leading to a much better estimate than the initial one (concept_3).",
            "With the more accurate estimate of $\\sum W_i$, the sculpting process is repeated to generate a new, high-fidelity convex body, $K^*$, which is guaranteed to be very close to the true affine hull of the attention matrices (concept_4).",
            "Here, the algorithm performs a crucial dimensionality reduction. Instead of searching the entire high-dimensional $d \\times d$ matrix space, it uses the accurate convex body $K^*$ to identify the $m$-dimensional linear span of the attention matrices. An epsilon-net of candidate matrices is then constructed *only within this low-dimensional subspace* (concept_5).",
            "The final complexity is determined by the brute-force search over this reduced space. The number of candidate tuples from the epsilon-net is exponential in $m$, which, combined with the required precision, leads to the core complexity term $(kd)^{O(m^3)}$. Subsequently, to meet the requirement of the Instruction for a success probability of at least $1-\\delta$, the best model from this search is selected using a validation set. Applying generalization bounds to this selection process introduces an additional factor of $log(1/\\delta)$ to the runtime (concept_6).",
            "By combining the cost of the dominant brute-force search with the cost associated with the probabilistic guarantee, the total time complexity of the algorithm is established as $(kd)^{O(m^{3})}\\cdot log(1/\\delta)$."
        ],
        "step_count": 7
    },
    "recuVDMq4U3785": {
        "reasoning_steps": [
            "The primary objective is to find the sequence $y$ that minimizes the input entropy for the 1-deletion channel, $H_{1-Del}^{ln}(y)$. We start with the general formula for this entropy, provided in Theorem 1, which expresses the entropy in terms of the run lengths $(r_1, r_2, ..., r_R)$ of the output sequence $y$: $H_{1-Del}^{ln}(y)=log(nq)-\\frac{1}{nq}\\sum_{i=1}^{\\rho(y)}\\mathcal{F}(r_i+1)$, where $\\mathcal{F}(a) = a \\log a$ (Concept_2).",
            "According to Theorem 2, minimizing the entropy $H_{1-Del}^{ln}(y)$ is equivalent to maximizing the summation term $\\sum_{i=1}^{\\rho(y)}\\mathcal{F}(r_i+1)$.",
            "The problem is now transformed into finding the run-length distribution that maximizes this summation. We first consider sequences with a fixed number of runs, $R$. According to Theorem 3, with $a=1$, the sum $\\sum_{i=1}^{R}\\mathcal{F}(r_i+1)$ is maximized when the sequence is skewed (Concept_1).",
            "For a skewed sequence with $R$ runs and total length $m=n-1$, there are $R-1$ runs of length 1 and one run of length $m-(R-1) = n-R$. Substituting these run lengths into the entropy formula from Theorem 1 yields the minimum entropy for a fixed $R$, as stated in Theorem 4: $min_{y\\in\\Sigma_{q,R}^{m}}H_{1-Del}^{ln}(y)=log(nq)-\\frac{2(R-1)}{nq}-\\frac{(m-R+2)log(m-R+2)}{nq}$.",
            "To find the overall minimum entropy, we need to find the value of $R$ (from $1$ to $m$) that minimizes the expression derived in the previous step. It is shown in the thesis that this expression is a decreasing function of $R$. Therefore, the minimum value is achieved at the smallest possible value for $R$, which is $R=1$.",
            "Setting $R=1$ and $m=n-1$ in the formula from Theorem 4 gives the final answer. For $R=1$, the sequence has a single run of length $m=n-1$. The entropy is: $H_{1-Del}^{ln}(y) = log(nq) - \\frac{2(1-1)}{nq} - \\frac{((n-1)-1+2)log((n-1)-1+2)}{nq}$ $ = log(nq) - 0 - \\frac{(n)log(n)}{nq}$ $ = log(nq) - \\frac{log(n)}{q}$. This is the minimum possible input entropy for the 1-deletion channel."
        ],
        "step_count": 6
    },
    "recuVHQ6KpGsNT": {
        "reasoning_steps": [
            "First, we establish the parameters of the finite field. According to the remainder class concept (concept_1), for an error magnitude of `l=1`, the probability vectors are mapped to `(2*1+1)³ = 27` classes. Therefore, the code operates over the finite field `GF(27)`, so `q=27`.",
            "Next, we determine the structure of this specific improved Hamming code (concept_2, theorem_2). The goal is to correct a single error from a limited set of possible remainder error patterns.",
            "We apply the optimal length formula (theorem_1) for this code. For `q=27`, the theorem states that `I_max = 2`. This means the length `n` of the improved Hamming code is related to its number of redundant symbols `r` by the formula `n = 2 * (27^r - 1) / (27 - 1)`.",
            "We need to express the redundancy in terms of the code length `n`. Rearranging the formula from the previous step gives `(q-1)/I_{max} * n + 1 = q^r`. In our case, this is `(26/2) * n + 1 = 13n + 1 = 27^r`.",
            "The number of redundant symbols is `r`, which can be expressed as `r = log_{27}(13n + 1)`.",
            "Finally, we convert the number of redundant symbols into redundancy in bits. Since each symbol is an element of `GF(27)`, the total redundancy in bits is `r * log₂(27)`.",
            "Substituting the expression for `r`: Redundancy = `log_{27}(13n + 1) * log₂(27)`. Using the change of base formula for logarithms (`log_b(x) * log_a(b) = log_a(x)`), this simplifies to `log₂(13n + 1)`. This calculation aligns with the approximation for large `k/l` (theorem_3), which allows us to ignore the smaller information content in the quotient vectors of the parity symbols."
        ],
        "step_count": 7
    },
    "recuVNRyhHuY52": {
        "reasoning_steps": [
            "The objective is to find the minimum Hamming distance of a specific binary linear code, which is obtained through a multi-step construction process.",
            "The process begins by defining a linear code $\\mathcal{C}_L$ over the ring $R = \\mathbb{F}_2 + u\\mathbb{F}_2$ (Concept_2).",
            "The structure of this code is determined by its defining set, $L$. For this problem, the defining set is $L = \\Delta_A^c + u(\\Delta_B \\setminus \\Delta_{B'})$ (Concept_4). These sets are constructed from simplicial complexes over $\\mathbb{F}_2^m$ (Concept_1).",
            "The construction is further specified by the constraints on the cardinalities of the supports of these simplicial complexes: $|B|=m$ and $|A|=|B'|=m-1$.",
            "The code $\\mathcal{C}_L$ over the ring is then transformed into a binary linear code, $\\phi(\\mathcal{C}_L)$, by applying the Gray map (Concept_3). The minimum Hamming distance of this binary code is the value we need to find.",
            "Theorem_2 provides the explicit parameters for the resulting binary code $\\phi(\\mathcal{C}_L)$ under exactly these conditions. The parameters are given in the format $[n, k, d]$, where $n$ is the length, $k$ is the dimension, and $d$ is the minimum Hamming distance.",
            "According to Theorem_2, the parameters of $\\phi(\\mathcal{C}_L)$ are $[2^{2m-1}, 2m, 2^{2m-2}]$. By definition, the third parameter in this triple is the minimum Hamming distance.",
            "Therefore, the minimum Hamming distance of the code is $2^{2m-2}$. The theorem also states that this code is a Griesmer code (Theorem_1), confirming its optimality and the correctness of its parameters."
        ],
        "step_count": 8
    },
    "recuUTbsAW1bNx": {
        "reasoning_steps": [
            "Establish the Spatial-Frequency Connection using Plancherel's Theorem. Converts spatial $L^2$-norms to frequency domain integrals: \\[\\|\\mathbf{u}(t)\\|_{L^2}^2 = \\int_{\\mathbb{R}^2} |\\hat{\\mathbf{u}}(\\xi,t)|^2 d\\xi, \\quad \\|\\omega(t)\\|_{L^2}^2 = \\int_{\\mathbb{R}^2} |\\hat{\\omega}(\\xi,t)|^2 d\\xi\\] Implication: Attenuation rates are determined by frequency-domain behavior.",
            "Frequency Domain Splitting using the Generalized Fourier Splitting Method. Split frequency space into two regions for large $t$: \\begin{itemize}\\item \\textbf{Low-frequency region:} $|\\xi|^2 \\leq \\frac{C}{1+t}$\\begin{itemize}\\item Dominated by linear damping/weak viscosity \\item Determines overall decay rate \\end{itemize}\\item \\textbf{High-frequency region:} $|\\xi|^2 > \\frac{C}{1+t}$\\begin{{itemize}\\item Dominated by strong viscous effects \\item Contributions decay exponentially \\end{itemize}\\end{itemize}",
            "Analyze Velocity Field $\\mathbf{u}$ Decay. \\subsection{Step 3.1: Low-Frequency Contribution ($|\\xi|^2 \\leq \\frac{C}{1+t}$)}\\begin{{itemize}\\item \\textbf{Region measure:} $\\pi \\cdot \\left(\\sqrt{\\frac{C}{1+t}}\\right)^2 = O\\left(\\frac{1}{1+t}\\right)$\\item \\textbf{Component decay:} $|\\hat{\\mathbf{u}}(\\xi,t)|^2 \\sim O\\left(\\frac{1}{(1+t)^2}\\right)$\\item \\textbf{Total contribution:}\\[ \\int_{\\text{low}} |\\hat{\\mathbf{u}}|^2 d\\xi \\sim O\\left(\\frac{1}{1+t}\\right) \\times O\\left(\\frac{1}{(1+t)^2}\\right) = O\\left(\\frac{1}{(1+t)^3}\\right) \\]\\end{itemize} \\subsection{Step 3.2: High-Frequency Contribution ($|\\xi|^2 > \\frac{C}{1+t}$)}\\begin{{itemize}\\item \\textbf{Exponential decay:} $|\\hat{\\mathbf{u}}|^2 \\sim O(e^{-Ct})$\\item \\textbf{Total contribution:} $O\\left(\\frac{1}{(1+t)^N}\\right)$ (negligible for $N \\gg 3$)\\end{itemize} \\subsection{Step 3.3: Combine Contributions}\\[ \\|\\mathbf{u}(t)\\|_{L^2} \\sim \\sqrt{O\\left(\\frac{1}{(1+t)^3}\\right)} = O\\left((1+t)^{-3/2}\\right) \\]",
            "Analyze Angular Velocity $\\omega$ Decay. \\subsection{Step 4.1: Low-Frequency Contribution ($|\\xi|^2 \\leq \\frac{C}{1+t}$)}\\begin{{itemize}\\item \\textbf{Region measure:} $O\\left(\\frac{1}{1+t}\\right)$ (same as Step 3.1)\\item \\textbf{Component decay:} $|\\hat{\\omega}(\\xi,t)|^2 \\sim O\\left(\\frac{1}{1+t}\\right)$\\item \\textbf{Total contribution:}\\[ \\int_{\\text{low}} |\\hat{\\omega}|^2 d\\xi \\sim O\\left(\\frac{1}{1+t}\\right) \\times O\\left(\\frac{1}{1+t}\\right) = O\\left(\\frac{1}{(1+t)^2}\\right) \\]\\end{itemize} \\subsection{Step 4.2: High-Frequency Contribution ($|\\xi|^2 > \\frac{C}{1+t}$)}\\begin{{itemize}\\item \\textbf{Exponential decay:} $|\\hat{\\omega}|^2 \\sim O(e^{-Ct})$\\item \\textbf{Total contribution:} $O\\left(\\frac{1}{(1+t)^N}\\right)$ (negligible for $N \\gg 2$)\\end{itemize} \\subsection{Step 4.3: Combine Contributions}\\[ \\|\\omega(t)\\|_{L^2} \\sim \\sqrt{O\\left(\\frac{1}{(1+t)^2}\\right)} = O\\left((1+t)^{-1}\\right) \\]",
            "Validate Decay Rates. \\subsection{Step 5.1: Synergistic Effect Verification}\\begin{{itemize}\\item Linear damping in $\\mathbf{u}$ reduces energy input to $\\omega$ via coupling terms \\item Prevents slower decay of $\\omega$ despite weaker angular viscosity \\end{itemize} \\subsection{Step 5.2: Nonlinear Term Control}\\begin{{itemize}\\item Nonlinear terms decay as $N(\\mathbf{u},\\omega) \\sim (1+s)^{-5/2}$\\item Duhamel's principle shows integral contribution $\\sim (1+t)^{-3/2}$\\item Does not disrupt dominant linear decay rates \\end{itemize}",
            "Final Results. \\begin{itemize}\\item Velocity field: $\\|\\mathbf{u}(t)\\|_{L^2} \\leq C(1+t)^{-3/2}$\\item Angular velocity: $\\|\\omega(t)\\|_{L^2} \\leq C(1+t)^{-1}$\\end{itemize} Note: $C > 0$ is a constant independent of initial conditions."
        ],
        "step_count": 6
    },
    "recuUCrDYOxGr2": {
        "reasoning_steps": [
            "The goal is to derive the **closed-form solution** for kernel regression with EO constraints. The approach is: incorporate EO into ERM as a differentiable regularization term using the **conditional cross-covariance operator**, then reduce the infinite-dimensional problem to Gram matrices using the Representer Theorem (concept_1, concept_2, concept_4).",
            "Construct a **fairness-aware ERM**: Minimize squared loss + kernel norm + EO regularization \\[\\min_{f\\in\\mathcal H}\\ \\tfrac1n\\sum_i(y_i-f(x_i))^2+\\lambda\\|f\\|_{\\mathcal H}^2+\\mu\\|C_{fS\\mid Y}\\|_{HS}^2,\\] where \\(\\|C_{fS\\mid Y}\\|_{HS}^2=0 \\iff \\hat Y\\perp\\!\\!\\!\\perp S\\mid Y\\), directly converting EO into an optimizable term (concept_1, concept_2). In the paper, Section 3.3 introduces EO in the form of a conditional operator, incorporated into the objective as a regularization term.",
            "Apply the **Representer Theorem** (concept_4): Let \\(f(\\cdot)=\\sum_{i=1}^n\\alpha_i k(x_i,\\cdot)\\). This converts the infinite-dimensional optimization into a finite-dimensional optimization over \\(\\alpha\\in\\mathbb R^n\\). The paper adopts this dimensionality reduction approach when deriving the kernel form.",
            "Express all terms as quadratic forms of \\(\\alpha\\) using **Gram matrices and projections** (concept_5): \\(\\|f\\|_{\\mathcal H}^2=\\alpha^\\top K\\alpha\\); the EO regularization corresponds to \\[\\alpha^\\top K\\,(I-\\Pi_y)\\,K_S\\,(I-\\Pi_y)\\,K\\,\\alpha,\\] where \\(K\\) is the input Gram matrix, \\(K_S\\) is the Gram matrix of sensitive attributes, and \\(\\Pi_y=Y(Y^\\top Y)^{-1}Y^\\top\\) is the projection onto the label subspace. This structure is directly used in the kernel solution of the paper (see the matrix blocks in the final solution).",
            "Combine terms to obtain a strictly convex quadratic objective for \\(\\alpha\\), and apply first-order optimality conditions for \\(\\alpha\\): \\[\\Big(\\tfrac1nK^2+\\lambda K+\\mu\\,K(I-\\Pi_y)K_S(I-\\Pi_y)K\\Big)\\alpha=\\tfrac1nKY.\\] An equivalent transformation by left-multiplying with \\(K^{-1}\\) can convert \\(\\lambda K\\) into the form \\(\\lambda I\\), making it easier to observe strong convexity and invertibility.",
            "The **closed-form solution** is given by: \\[\\boxed{\\ \\alpha=\\big(K+n\\lambda I+n\\mu\\,(I-\\Pi_y)K_S(I-\\Pi_y)\\,K\\big)^{-1}Y\\ }.\\]"
        ],
        "step_count": 6
    },
    "recuV5g9UmQH2w": {
        "reasoning_steps": [
            "First, express the external geometry in the PG form $ds^2=-d\\tau^2+(dr+\\sqrt{1-f}\\,d\\tau)^2+r^2d\\Omega^2$ (theorem_1).",
            "The internal flat FLRW can also be written in the PG form $ds^2=-d\\tau^2+(dr-rH\\,d\\tau)^2+r^2d\\Omega^2$, where $r(\\tau)=a(\\tau)r_c$ and $H=\\dot a/a$ (concept_1).",
            "Then perform matching at the shell surface $r(\\tau)=R(\\tau)$:",
            "- From the external geometry, we obtain $\\dot R=-\\sqrt{1-f(R)}$ (theorem_1);",
            "- From the internal geometry, we obtain $\\dot R=R H$ (concept_1).",
            "Combining these gives (concept_2):",
            "$\\left(\\frac{\\dot R}{R}\\right)^2=\\frac{1-f(R)}{R^2}.$",
            "Finally, use the Friedmann equation $H^2=\\frac{8\\pi}{3}\\rho$ (theorem_2) and substitute $H=\\dot R/R$ to obtain the shell surface density:",
            "$\\boxed{\\rho(\\tau)=\\frac{3}{8\\pi}\\left(\\frac{\\dot R}{R}\\right)^2 =\\frac{3}{8\\pi}\\,\\frac{1-f\\!\\bigl(R(\\tau)\\bigr)}{R(\\tau)^2}}$ The units $G=c=1$ have been adopted."
        ],
        "step_count": 9
    },
    "recuVDJJCp8IF5": {
        "reasoning_steps": [
            "Covariate Shift and Importance Weighting (Concept_1, Concept_2): Under covariate shift, the conditional distribution P(y|x) is invariant across domains, but the marginal distributions differ. Thus, the target risk can be written as a weighted source risk: \\[\\mathbb{E}_T[(y-f(x))^2] = \\mathbb{E}_S[\\beta(x)(y-f(x))^2],\\] where \\(\\beta(x)=\\tfrac{d\\rho_T}{d\\rho_S}(x)\\). For finite samples, define the diagonal weight matrix \\[B = \\mathrm{diag}(\\beta(x_1), \\dots, \\beta(x_n)).\\]",
            "Operator Formulation in RKHS (Concept_3, Concept_4): Define the sampling operator \\(S_x:\\mathcal H_K\\to\\mathbb R^n\\), \\(S_x f = (f(x_1),\\dots,f(x_n))\\), and its adjoint \\(S_x^*:\\mathbb R^n\\to\\mathcal H_K\\), \\(S_x^* v = \\sum_i v_i K(\\cdot,x_i)\\). Then we can define \\[T = S_x^* B S_x, \\quad h = S_x^* B y,\\] where \\(y=(y_1,\\dots,y_n)^\\top\\).",
            "Spectral Regularization Framework (Concept_5): Spectral regularization applies a filter function \\(g_\\lambda\\) to the operator \\(T\\), stabilizing the inversion and covering methods such as Tikhonov, Landweber, and truncated SVD.",
            "Closed-form Estimator: The final closed-form estimator is obtained as \\[\\boxed{f^{\\,z}_\\lambda = g_\\lambda(S_x^{*} B S_x)\\, S_x^{*} B y}.\\]"
        ],
        "step_count": 4
    },
    "recuU8HuA6yoyv": {
        "reasoning_steps": [
            "Energy Estimates (Using concept_2) Define the energy functional: \\[ E(t) = \\frac{1}{2} \\left( \\|u(t)\\|_{L^2}^2 + \\|w(t)\\|_{L^2}^2 + \\|\\theta(t)\\|_{L^2}^2 \\right). \\] Differentiate with respect to time and substitute the linearized equations (ignoring nonlinear terms due to small perturbations): \\[ \\frac{d}{dt} E(t) = \\int_{\\mathbb{R}^d} \\left( u \\cdot \\partial_t u + w \\cdot \\partial_t w + \\theta \\cdot \\partial_t \\theta \\right) dx. \\] Using the equations and integration by parts: \\begin{align*} \\nabla \\cdot u=0 \\int u \\cdot \\partial_t u dx &\\leq -\\mu \\|\\nabla u\\|_{L^2}^2 - (\\mu + \\lambda) \\|\\nabla \\cdot u\\|_{L^2}^2 + C \\left( \\|w\\|_{L^2}^2 + \\|\\theta\\|_{L^2}^2 \\right) \\\\ \\int w \\cdot \\partial_t w dx &\\leq -\\gamma \\|\\nabla w\\|_{L^2}^2 - 2\\chi \\|w\\|_{L^2}^2 + C \\|u\\|_{L^2}^2 \\\\ \\int \\theta \\partial_t \\theta dx &\\leq -\\kappa \\|\\nabla \\theta\\|_{L^2}^2 \\end{align*} Combining these estimates, there exist constants $c_1, c_2 > 0$ such that: \\[ \\frac{d}{dt} E(t) \\leq -C \\left( \\|\\nabla u\\|_{L^2}^2 + \\|\\nabla w\\|_{L^2}^2 + \\|w\\|_{L^2}^2 + \\|\\nabla \\theta\\|_{L^2}^2 + \\|\\nabla \\cdot u\\|_{L^2}^2 \\right) \\right). \\] Then we have \\[ E(t) + c_1 \\int_0^t \\left( \\|\\nabla u(s)\\|_{L^2}^2 + \\|\\nabla w(s)\\|_{L^2}^2 + \\|w(s)\\|_{L^2}^2 + \\|\\nabla \\theta(s)\\|_{L^2}^2 \\right) ds \\leq E(0). \\] This shows $E(t)$ is bounded but does not give the decay rate directly.",
            "Fourier Analysis via Plancherel's Theorem (Using concept_4) By Plancherel's theorem, $\\|f\\|_{L^2} = \\|\\hat{f}\\|_{L^2}$, so: \\[ E(t) = \\frac{1}{2} \\int_{\\mathbb{R}^d} \\left( |\\hat{u}(\\xi,t)|^2 + |\\hat{w}(\\xi,t)|^2 + |\\hat{\\theta}(\\xi,t)|^2 \\right) d\\xi. \\] Split the frequency space into low-frequency ($|\\xi| \\leq R(t)$) and high-frequency ($|\\xi| > R(t)$) regions, with $R(t) = (1+t)^{-1/2}$. {High-Frequency Region ($|\\xi| > R(t)$)} From energy estimate: \\[ \\int_{|\\xi| > R(t)} |\\hat{U}(\\xi,t)|^2 d\\xi \\leq \\frac{1}{R(t)^2} \\int_{|\\xi| > R(t)} |\\xi|^2 |\\hat{U}|^2 d\\xi \\leq \\frac{1}{R(t)^2} \\|\\nabla U(t)\\|_{L^2}^2. \\] Since $\\int_0^t \\|\\nabla U(s)\\|_{L^2}^2 ds < \\infty$ and $\\|\\nabla U(t)\\|_{L^2}^2 \\to 0$, this decays as $O((1+t))$, faster than $(1+t)^{-d/2}$ for $d=2,3$. \\subsubsection*{Low-Frequency Region ($|\\xi| \\leq R(t)$)} In the linearized system, the solution in Fourier space satisfies: \\begin{align*} |\\hat{\\rho}(\\xi,t)|, |\\hat{u}(\\xi,t)|, |\\hat{\\theta}(\\xi,t)| &\\leq C e^{-c|\\xi|^2 t} |\\hat{U}_0(\\xi)| \\\\ |\\hat{w}(\\xi,t)| &\\leq C e^{-c t} |\\hat{w}_0(\\xi)| \\end{align*} Assuming $U_0 \\in L^1 \\cap L^2$ so that $\\|\\hat{U}_0\\|_{L^\\infty} \\leq \\|U_0\\|_{L^1} < \\infty$, we have: \\[ \\int_{|\\xi| \\leq R(t)} |\\hat{U}(\\xi,t)|^2 d\\xi \\leq \\|\\hat{U}_0\\|_{L^\\infty}^2 \\int_{|\\xi| \\leq R(t)} e^{-2c|\\xi|^2 t} d\\xi. \\] Changing variables $s = r\\sqrt{t}$, $dr = t^{-1/2}ds$: \\[ \\int_{|\\xi| \\leq R(t)} e^{-2c|\\xi|^2 t} d\\xi \\sim t^{-d/2} \\int_0^{R(t)\\sqrt{t}} s^{d-1} e^{-2c s^2} ds \\sim C t^{-d/2}, \\quad t \\to \\infty. \\] Thus: \\[ \\int_{|\\xi| \\leq R(t)} |\\hat{U}(\\xi,t)|^2 d\\xi \\leq C \\|U_0\\|_{L^1}^2 (1+t)^{-d/2}. \\]",
            "Handling Nonlinear Terms (Using concept_3) For nonlinear terms (e.g., $u \\cdot \\nabla u$), by integrating by parts, \\[ \\left| \\int u \\cdot (u \\cdot \\nabla u) dx \\right| \\leq =0. \\] When $\\|U\\|_{L^2}$ is small, nonlinear terms can be absorbed in the energy estimates and do not affect the leading decay rate.",
            "Combining Estimates Combining low and high-frequency estimates: \\begin{align*} \\int_{|\\xi| \\leq R(t)} |\\hat{U}(\\xi,t)|^2 d\\xi &\\leq C (1+t)^{-d/2} \\\\ \\int_{|\\xi| > R(t)} |\\hat{U}(\\xi,t)|^2 d\\xi &\\leq C (1+t)^{-1} \\quad (\\text{faster}) \\\\ \\|w(t)\\|_{L^2}^2 &\\leq C e^{-ct} \\quad (\\text{exponentially fast}) \\end{align*} Thus: \\[ \\|U(t)\\|_{L^2}^2 = \\int_{\\mathbb{R}^d} |\\hat{U}(\\xi,t)|^2 d\\xi \\leq C (1+t)^{-d/2} \\] Taking square roots: \\[ \\|U(t)\\|_{L^2} \\leq C (1+t)^{-d/4}, \\quad d=2,3 \\] where $C$ depends on $\\|U_0\\|_{L^1 \\cap L^2}$ and dissipation coefficients."
        ],
        "step_count": 4
    },
    "recuURbEBCDdYt": {
        "reasoning_steps": [
            "Relate the write time t_write to the required charge density σ_Q and the achievable injection current density J_inj (t_write ∝ σ_Q / J_inj). σ_Q is set by the device equivalent capacitance and target window, while t_write depends on whether J_inj can be large enough under the applied bias.",
            "Estimate charge density: σ_Q = C_eq · ΔV_mem = (5 fF/μm²) · 0.8 V = 4 fC/μm².",
            "Determine dominant programming mechanism: Considering Concept2 (high mobility and rapid thermalization), and Concept3 (|V_D,PROG| ≈ 5 V strong bias in 2D heterostructures), neither Direct Tunneling nor Fowler–Nordheim tunneling dominates; programming is mainly governed by Hot Carrier Injection.",
            "Build physical expression of injection current density: J_inj ≈ (attempt flux) × (emission probability). Attempt flux is determined by n_s v_sat / L_hot, ensured large by high mobility and quasi-ballistic transport (Concept1). Emission probability is determined by hot electron distribution: P_HE ≈ exp[−(Φ_B − qV_ox)/(k_B T_e)].",
            "Estimate attempt flux: Φ_att ≈ n_s v_sat / L_hot, where n_s ≈ C_g V_ov / q (C_g≈5 fF/μm², V_ov≈1 V ⇒ n_s≈3×10^16 m^-2); v_sat ≈ 1.5×10^5 m/s; L_hot ≈ 50 nm. Substituting gives current density upper bound q·n_s·v_sat/L_hot ≈ 1.5×10^10 A/m².",
            "Hot electron emission probability: P_HE ≈ exp[−(Φ_B − qV_ox)/(k_B T_e)]. Here V_ox = η_V V_prog, with η_V≈0.5 and V_prog=5 V ⇒ V_ox≈2.5 V; effective barrier height ≈ 1 eV; average energy gain per mean free path Δε ≈ qE_y λ_mfp with λ_mfp≈30 nm ⇒ Δε≈2.4 eV. With conversion factor α≈0.3, k_B T_e ≈ 0.72 eV. Thus P_HE ≈ exp(−1/0.7) ≈ 0.24.",
            "Final injection current density: J_inj ≈ J_ch,eff × P_HE ≈ (1.5×10^10 A/m²) × 0.24 ≈ 3.6×10^9 A/m². Considering angular distribution and scattering correction ξ≈0.001–0.003, J_inj ≈ 10^7–10^9 A/m². With σ_Q = 4×10^-3 C/m², t_write = σ_Q / J_inj ≈ (4×10^-3)/(1×10^7) = 4×10^-10 s = 400 ps.",
            "Conclusion: The shortest programming pulse width is in the picosecond regime."
        ],
        "step_count": 8
    },
    "recuUwG9Gr2YxF": {
        "reasoning_steps": [
            "Given a center \\(y\\in \\mathbb{F}_{q^m}^n\\), if the rank ball of radius \\(\\rho n\\) contains \\(L+1\\) codewords \\(c_0,\\dots,c_L\\) with \\(d_R(c_j,y)\\le \\rho n\\), define the difference vectors \\(v_j:=c_j-y\\). With the syndrome map \\(\\sigma(x)=Hx^{\\top}\\), we have \\(\\sigma(c_j)=\\sigma(y)\\). Hence, for any \\(j>0\\), \\[ u_j:=v_j-v_0\\in \\ker(H). \\]",
            "For each difference vector, define the rank support \\(\\mathrm{RS}(v_j)\\subseteq \\mathbb{F}_q^n\\) with \\(\\dim \\mathrm{RS}(v_j)=\\operatorname{rank}_{\\mathbb{F}_q}(v_j)\\). Let \\[ W:=\\mathrm{RS}(v_0)+\\mathrm{RS}(v_1)+\\cdots+\\mathrm{RS}(v_L)\\subseteq \\mathbb{F}_q^n . \\] Since \\(d_R(c_j,y)=\\operatorname{rank}_{\\mathbb{F}_q}(v_j)\\le \\rho n\\), and by the subadditivity of the dimension of sum-spaces, we obtain \\[ \\dim W\\ \\le\\ \\sum_{j=0}^{L}\\dim \\mathrm{RS}(v_j)\\ =\\ \\sum_{j=0}^{L}\\operatorname{rank}_{\\mathbb{F}_q}(v_j)\\ \\le\\ (L+1)\\rho n. \\]",
            "If the above “crowding” indeed occurs (there are \\(L+1\\) codewords in the ball), then necessarily \\[ \\dim W\\ >\\ L(n-k). \\] Conversely, to avoid crowding and make \\(C\\) \\((\\rho,L)\\)-list decodable, we require that any \\(W\\) generated by \\(L+1\\) difference vectors within radius \\(\\rho n\\) satisfies \\[ \\dim W\\ \\le\\ L(n-k). \\]",
            "Comparing (A) with the crowding threshold (B), crowding requires \\[ (L+1)\\rho n\\ >\\ L(n-k). \\] Therefore, to preclude crowding and guarantee \\((\\rho,L)\\)-list decodability, the necessary upper bound is \\[ (L+1)\\rho n\\ \\le\\ L(n-k) \\quad\\Longleftrightarrow\\quad \\rho\\ \\le\\ \\frac{L}{L+1}\\Bigl(1-\\frac{k}{n}\\Bigr)\\ =\\ \\frac{L}{L+1}(1-R). \\]"
        ],
        "step_count": 4
    },
    "recuVeYQELiXnE": {
        "reasoning_steps": [
            "By McGehee's blow-up, collision point becomes equilibrium points of new equations and collision solution becomes a solution converging to a equilibrium point.",
            "From Center Mainfold Theorem, the flow of new equations near equilibrium point can be converged by flow on the center manifold.",
            "By Lojasiewicz gradient inequality, the flow on the center manifold is approximately a gradient flow and the arclength of this flow is finite by the inequality, which means infinite spin does not occur."
        ],
        "step_count": 3
    },
    "recuVTSBJxd0oX": {
        "reasoning_steps": [
            "First, based on concept_1 and the NLED action, we write the matter part as \\begin{equation} S_{\\rm m}=-\\!\\int \\mathrm{d}^4x\\,\\sqrt{-g}\\,\\mathcal{L}(F). \\end{equation}",
            "Then, following theorem_1, we vary with respect to the metric to obtain the general formula for the stress--energy tensor: \\begin{equation} T^{\\nu}{}_{\\mu} = -2\\,\\mathcal{L}_F\\,F_{\\mu\\sigma}F^{\\nu\\sigma} +\\tfrac12\\,\\delta^{\\nu}{}_{\\mu}\\,\\mathcal{L}(F). \\end{equation}",
            "Next, under the pure magnetic monopole setup in concept_2, the electromagnetic invariant is \\begin{equation} F=2\\,F_{\\theta\\phi}F^{\\theta\\phi}=\\frac{2q^{2}}{r^{4}}. \\end{equation}",
            "Finally, rewriting in the anisotropic fluid form (adopting the convention $T^{\\mu}{}_{\\nu}=\\mathrm{diag}(\\rho,\\,-p_r,\\,-p_{\\perp},\\,-p_{\\perp})$), we obtain \\begin{equation} \\rho(r)=T^{t}{}_{t}=\\tfrac12\\,\\mathcal{L}\\!\\big(F(r)\\big),\\qquad p_{r}(r)=-T^{r}{}_{r}=-\\tfrac12\\,\\mathcal{L}\\!\\big(F(r)\\big),\\qquad p_{\\perp}(r)=-T^{\\theta}{}_{\\theta} =F(r)\\,\\mathcal{L}_F\\!\\big(F(r)\\big)-\\tfrac12\\,\\mathcal{L}\\!\\big(F(r)\\big). \\end{equation}"
        ],
        "step_count": 4
    },
    "recuUOxzww7BBL": {
        "reasoning_steps": [
            "Recast Theorem 1 with k=Δ+Δ^0.7: in order to apply theorem 1, we need some constant ε∈(0,1) such that k ≥ Δ + ρ/(1-ε), i.e., Δ^0.7 ≥ ρ/(1-ε).",
            "Notice that the Klein quartic surface has genus 3, so a graph embedded on it has bounded genus.",
            "It's a property of bounded genus graphs that there exists an orientation with bounded maximum outdegree d (d is a function of the genus); then apply Theorem 2 to get ρ ≤ 2√(d(Δ-d)) = O(√Δ).",
            "Compare magnitudes: ρ/Δ^0.7 = O(Δ^-0.2) → 0; hence we can choose a constant ε bounded away from 0 (e.g., ε=½ for large Δ) to satisfy Theorem 1’s condition.",
            "Substitute into Theorem 1 to conclude t_mix(δ) ≤ (n/ε)log(n/δ) = O(n log n) for fixed δ."
        ],
        "step_count": 5
    },
    "recuUTiYakLbC5": {
        "reasoning_steps": [
            "First, to derive the expected form of the target velocity field $\\boldsymbol{v}(\\boldsymbol{x}_t, t)$, it is necessary to integrate three core concepts: the flow matching framework, optimal transport coupling, and linear interpolation probabilistic path. The target velocity field defines the direction and magnitude of motion of data points during their transformation from the source distribution $p_0$ to the target distribution $p_1$.",
            "According to the concept of **linear interpolation probabilistic path**, a path from a source point $\\boldsymbol{x}_0$ to a target point $\\boldsymbol{x}_1$ can be expressed as $\\boldsymbol{x}_t = (1-t)\\boldsymbol{x}_0 + t\\boldsymbol{x}_1$, where $t \\in [0, 1]$. This path is a linear path under the **optimal transport coupling** $\\pi(\\boldsymbol{x}_0, \\boldsymbol{x}_1)$. The instantaneous velocity of this path, i.e., its derivative with respect to time $t$, is $\\frac{d\\boldsymbol{x}_t}{dt} = \\boldsymbol{x}_1 - \\boldsymbol{x}_0$. However, this velocity is constant and cannot be directly used to define a time-varying velocity field. To obtain the average velocity of all particles located at $\\boldsymbol{x}_t$ at time $t$, we need to consider all pairs $(\\boldsymbol{x}_0, \\boldsymbol{x}_1)$ composed of all possible source points $\\boldsymbol{x}_0$ and target points $\\boldsymbol{x}_1$—specifically, those pairs whose linear interpolation paths pass through the point $\\boldsymbol{x}_t$ exactly at time $t$.",
            "The target velocity field $\\boldsymbol{v}(\\boldsymbol{x}_t, t)$ is defined as the average of the instantaneous velocities of all trajectories passing through the point $\\boldsymbol{x}_t$ at time $t$. From the derivation in Step 2, the instantaneous velocity of each trajectory is $(\\boldsymbol{x}_1 - \\boldsymbol{x}_0)$. Therefore, we need to calculate the **conditional expectation** of $(\\boldsymbol{x}_1 - \\boldsymbol{x}_0)$ given $\\boldsymbol{x}_t$. By the definition of conditional expectation, the target velocity field can be expressed as: $\\boldsymbol{v}(\\boldsymbol{x}_t, t) = \\mathbb{E} \\left[ \\frac{d\\boldsymbol{x}_t}{dt} \\mid \\boldsymbol{x}_t \\right]$. Substituting the derivative of the linear interpolation path into the equation, we obtain: $\\boldsymbol{v}(\\boldsymbol{x}_t, t) = \\mathbb{E} \\left[ \\boldsymbol{x}_1 - \\boldsymbol{x}_0 \\mid \\boldsymbol{x}_t \\right]$. Furthermore, to make the denominator more intuitive (representing the 'remaining time'), we can transform the formula. Since $\\boldsymbol{x}_t = (1-t)\\boldsymbol{x}_0 + t\\boldsymbol{x}_1$, we can derive that $\\boldsymbol{x}_1 - \\boldsymbol{x}_0 = \\frac{\\boldsymbol{x}_1 - \\boldsymbol{x}_t}{1-t}$. Substituting this relationship into the above conditional expectation formula gives: $\\boldsymbol{v}(\\boldsymbol{x}_t, t) = \\mathbb{E} \\left[ \\frac{\\boldsymbol{x}_1 - \\boldsymbol{x}_t}{1-t} \\mid \\boldsymbol{x}_t \\right]$. To derive the final form in the paper, $\\boldsymbol{v}(\\boldsymbol{x}_t, t) = \\mathbb{E} \\left[ \\frac{\\boldsymbol{x}_1 - \\boldsymbol{x}_0}{1-t} \\mid \\boldsymbol{x}_t \\right]$, the derivation is actually based on a more precise reasoning: the velocity field is defined as the flow vector $\\boldsymbol{V}(\\boldsymbol{x},t)$, which is related to the probability flow through the equation $\\frac{\\partial p(\\boldsymbol{x},t)}{\\partial t} + \\nabla \\cdot (p(\\boldsymbol{x},t)\\boldsymbol{V}(\\boldsymbol{x},t))=0$. For the simplest linear interpolation flow that satisfies this equation, its velocity field can be proven to be $\\boldsymbol{v}(\\boldsymbol{x},t) = \\frac{\\mathbb{E} [\\boldsymbol{x}_1|\\boldsymbol{x}_t=\\boldsymbol{x}] - \\boldsymbol{x}}{1-t}$, which is equivalent to $\\mathbb{E} \\left[ \\frac{\\boldsymbol{x}_1 - \\boldsymbol{x}_0}{1-t} \\mid \\boldsymbol{x}_t \\right]$—since $\\boldsymbol{x}_t$ is a linear combination of $\\boldsymbol{x}_0$ and $\\boldsymbol{x}_1$. Thus, this formula is a direct result of the linear interpolation path and conditional expectation, and is consistent with the definition in the paper."
        ],
        "step_count": 3
    },
    "recuVEMZow7LLR": {
        "reasoning_steps": [
            "System Transformation via Deep Lifting and LPV Approximation The initial problem is the control of a general nonlinear discrete-time system $x_{k+1} = f(x_k, u_k, w_k)$, where $x_k \\in \\mathbb{R}^n$ is the state, $u_k$ is the control input, and $w_k$ represents exogenous disturbances. The initial state $x_0$ is uncertain but known to lie within an ellipsoid. To transform this generally intractable problem into a solvable one, we employ deep learning to learn a lifting function $z_k = \\psi(x_k)$ and a scheduling parameter map $p_k = \\phi(x_k)$. The network is trained using simulation data to minimize the prediction error of a lifted, discrete-time LPV model of the form:  z_{k+1} = A(p_k)z_k + B_u(p_k)u_k + B_w(p_k)w_k",
            "Convex Synthesis of a Nonstationary LPV (NSLPV) Controller With the LPV model established, we design a full-state feedback NSLPV control law $u_k = K(p_k, k)z_k$. The objective is to find the controller gain matrices $K(p_k, k)$ that stabilize the system and minimize the effect of disturbances. This synthesis problem is formulated as a convex optimization problem. The standard approach involves finding a parameter-dependent, discrete-time Lyapunov function $V_k(z, p) = z_k^T P(p_k)^{-1} z_k$ and expressing the stability and performance conditions as a set of Linear Matrix Inequalities (LMIs). For NSLPV synthesis, these conditions become Difference Matrix Inequalities that must hold for each time step $k$ (within a finite horizon) and for all possible values of the scheduling parameter $p_k$.",
            "Establishing Robust Performance Guarantees via Induced $\\ell_2$-Norm The third step is to formalize the performance objective. The goal is to ensure that the closed-loop system robustly attenuates disturbances from both the input $w_k$ and the uncertain initial state $z_0$. This is achieved by finding a controller and the smallest possible scalar $\\gamma > 0$ such that for any disturbance $d$ with bounded energy and any initial condition $z_0$ within the ellipsoid $\\mathcal{I}$, the following inequality holds:  \\sup \\left\\{ \\|e\\|_{\\ell_2} \\mid \\|d\\|_{\\ell_2} \\le 1, z_0 \\in \\mathcal{I} \\right\\} < \\gamma where $\\|e\\|_{\\ell_2}^2 = \\sum_{k=0}^{\\infty} e_k^T e_k$. This formulation treats the initial state uncertainty as part of a constraint set, rather than as an energy ratio. This condition is then translated into additional LMI constraints within the convex synthesis problem of Step 2.",
            "Post-Analysis and Performance Verification with IQC Theory In this final step, we validate the controller's performance on the true nonlinear system, moving beyond the approximate LPV model. First, we consolidate all discrepancies---the gap between the LPV model and the real system, plus unmodeled effects like measurement noise---into a single structured uncertainty block, $\\Delta$. We then apply discrete-time Integral Quadratic Constraints (IQC) theory to mathematically characterize the behavior of this uncertainty. Finally, we verify the stability of the entire system (the nominal closed loop interconnected with $\\Delta$) by solving a convex optimization problem (an LMI). This analysis yields a new, tighter, and less conservative performance bound, $\\gamma_{\\text{IQC}}$, which provides a high-confidence guarantee of the controller's real-world effectiveness."
        ],
        "step_count": 4
    },
    "recuVcTHwoeLyL": {
        "reasoning_steps": [
            "Clarify the core question - Identify the core mechanism by which nonequilibrium chemical reactions regulate nucleation kinetics, starting from the classification of nonequilibrium chemical reactions in the paper.",
            "Distinguish between homogeneous and heterogeneous nonequilibrium chemical reactions - According to the paper, homogeneous reactions have a constant \\(k_{I \\to B}\\) (independent of the local environment), while heterogeneous reactions have \\(k_{I \\to B}\\) dependent on the local potential energy \\(u\\) (following the Metropolis form). The paper emphasizes that only heterogeneous reactions can induce unique regulatory effects on nucleation kinetics, so the core mechanism must be associated with heterogeneous reactions.",
            "Analyze the effect of heterogeneous reactions on phase free energy differences - Using the definition of the effective internal free energy difference \\(\\Delta f\\) in the paper (\\(\beta \\Delta f = -\\ln(\rho_B/\rho_I) + \\ln \\langle e^{-\beta \\Delta u_{I \\to B}} \rangle_I\\)), heterogeneous reactions cause \\(k_{I \\to B}\\) to depend on \\(u\\), leading to different \\(\\Delta f\\) values in the liquid (\\(\\Delta f_l\\)) and vapor (\\(\\Delta f_v\\)) phases, resulting in a non-zero phase free energy difference \\(\\Delta \\Delta f = \\Delta f_l - \\Delta f_v\\). This breaks the consistency of free energy differences across phases in equilibrium systems and is the initial driver of subsequent interfacial property changes.",
            "Establish the link between \\(\\Delta \\Delta f\\) and non-equilibrium interfacial tension - Based on the Fixed Local Environment Approximation (FLEX) , which assumes that particle exchange with the reservoir equilibrates faster than changes in the local environment around a tagged lattice site, FLEX first realizes the mapping from the nonequilibrium steady state (NESS) to an effective equilibrium system by defining **effective fugacities** (the core tool for quantitative connection): the effective fugacity of bonding-state (B) particles is \\(\tilde{z}_B = \frac{\tilde{\rho}_B}{\tilde{\rho}_E} e^{\beta u}\\), and that of inert-state (I) particles is \\(\tilde{z}_I = \frac{\tilde{\rho}_I}{\tilde{\rho}_E}\\) , . Here, \\(\tilde{\rho}_B\\), \\(\tilde{\rho}_I\\), and \\(\tilde{\rho}_E\\) are the \"effective equilibrium densities\" mapped by FLEX (corresponding to the steady-state densities of B-state, I-state particles, and empty lattice sites in the nonequilibrium system), and \\(u\\) is the local potential energy determined by nearest-neighbor particle interactions. For heterogeneous reactions where \\(k_{I \\to B}\\) depends on \\(u\\) (decreasing as \\(u\\) decreases), the liquid phase, vapor phase, and liquid-vapor interface have distinct local \\(u\\) values, leading to different \\(\tilde{z}_B/\tilde{z}_I\\) ratios. Since the effective internal free energy difference \\(\beta \\Delta f = -\\ln(\tilde{z}_B/\tilde{z}_I)\\) , this further results in \\(\\Delta f\\) differences between phases (\\(\\Delta \\Delta f = \\Delta f_l - \\Delta f_v \\neq 0\\)) and between the interface and bulk phases (with lower \\(\\Delta f\\) at the interface). The lower \\(\\Delta f\\) at the interface enhances the \"chemical activity\" of B-state particles, leading to the enrichment of B-state particles at the interface (i.e., higher \\(\tilde{\rho}_B\\) at the interface compared to the vapor phase and even the bulk liquid phase) . Such enrichment reduces the effective adatom bonding energy \\(\tilde{\\epsilon}\\) at the interface (satisfying \\(|\beta \tilde{\\epsilon}| \\leq |\beta \\epsilon|\\)), where \\(\tilde{\\epsilon}\\) is calculated via FLEX based on the interfacial \\(\tilde{\rho}_B\\) as \\(\beta \tilde{\\epsilon} = \\ln\\left[\frac{\tilde{\rho}_B}{1 - \tilde{\rho}_B}\right]_{u=\\epsilon} - \beta \\epsilon\\) . Finally, substituting \\(\tilde{\\epsilon}\\) into the interfacial tension formula of the solid-on-solid model (a tool for calculating interfacial tension under equilibrium-like conditions) yields the non-equilibrium interfacial tension \\(\\sigma\\), which deviates from the equilibrium interfacial tension \\(\\sigma_{eq}\\) .",
            "Connect non-equilibrium interfacial tension to the nucleation free energy barrier - Extending Classical Nucleation Theory (CNT) to non-equilibrium steady states (NESS), the paper retains CNT’s core premise (rate-limiting step is critical nucleus formation) but replaces \\(\\sigma_{eq}\\) with non-equilibrium \\(\\sigma\\) in the free energy landscape. The nucleation free energy barrier \\(\\Delta F^* = F_{\\text{noneq}}(n^*) - F_{\\text{noneq}}(1)\\) is thus modified: reduced \\(\\sigma\\) lowers \\(\\Delta F^*\\), while increased \\(\\sigma\\) raises it. This directly alters the difficulty of nucleation.",
            "Confirm the regulation of nucleation rate by \\(\\Delta F^*\\) - Per the paper’s CNT nucleation rate formula \\(J = \\rho_1 D^* \\Gamma \\exp(-\beta \\Delta F^*)\\), each parameter’s physical meaning is as follows:  - \\(\\rho_1\\): Number density of bonding-state (B) monomers in the metastable vapor phase (the \"building blocks\" for nucleus formation).  - \\(D^*\\): Diffusion coefficient of nucleus size (the reaction coordinate) near the critical nucleus size \\(n^*\\), quantifying how fast nucleus size fluctuates around this critical size (where the nucleus is equally likely to grow into liquid or dissolve back into vapor).  - \\(\\Gamma\\): Zeldovich factor, correcting for the probability that a nucleus reaching \\(n^*\\) grows into a stable phase (instead of dissolving), calculated by fitting the nucleus’ \"commitment probability\" (to become stable liquid) to a harmonic barrier near \\(n^*\\).  - \\(\\x08eta\\): Inverse temperature parameter, \\(\\x08eta = (k_B T)^{-1}\\) ( \\(k_B\\) = Boltzmann constant, \\(T\\) = absolute temperature).  - \\(\\Delta F^*\\): Nucleation free energy barrier (\\(\\Delta F^* = F(n^*) - F(1)\\)), the minimum free energy needed to form a stable nucleus ( \\(F(n^*)\\) = free energy of critical nucleus, \\(F(1)\\) = free energy of one B-monomer). \\(\\Delta F^*\\) relates exponentially to \\(J\\) (rate of stable nucleus formation per unit volume). Heterogeneous reactions reduce non-equilibrium interfacial tension \\(\\sigma\\), lowering \\(\\Delta F^*\\) and greatly increasing \\(J\\); a higher \\(\\Delta F^*\\) (from larger \\(\\sigma\\)) slows \\(J\\). This closes the regulatory chain: heterogeneous non-equilibrium reactions → modulated interfacial properties → changed \\(\\Delta F^*\\) → regulated nucleation rate.",
            "Summarize the core mechanism - Heterogeneous nonequilibrium chemical reactions first induce a non-zero \\(\\Delta \\Delta f\\), which leads to non-equilibrium interfacial tension \\(\\sigma\\) via FLEX and interface \\(B\\)-state enrichment. \\(\\sigma\\) modifies the nucleation free energy barrier \\(\\Delta F^*\\) in the extended CNT framework, ultimately regulating the nucleation rate. This is the core mechanism, distinguishing it from equilibrium systems where nucleation rate depends solely on thermodynamic driving forces (e.g., supersaturation)."
        ],
        "step_count": 7
    },
    "recuVhQ0yiUSLT": {
        "reasoning_steps": [
            "Bioinformatics analysis and mRNA expression detection in PDAC tumor tissues revealed that STN1 is highly expressed in both pancreatic cancer cell lines and tumor tissues. STN1 overexpression is independent of tumor size but is associated with poorer patient survival, suggesting that STN1 may function in PDAC metastasis rather than tumor growth and has potential as a clinical therapeutic target.",
            "Since altering STN1 protein levels does not affect pancreatic cancer cell proliferation, but STN1 knockdown reduces the migratory and invasive abilities of tumor cells—phenotypes that are rescued upon STN1 reintroduction—and elevated STN1 enhances the migration and invasion of MIA PaCa-2 cells, this indicates that STN1 promotes PDAC progression by enhancing metastatic capacity rather than proliferation.",
            "KPCS mice exhibit normal embryonic development and pancreatic phenotypes but significantly reduced liver and lung metastases, indicating that STN1 plays a critical role in promoting tumor metastasis and thereby accelerating PDAC progression.",
            "HOXB7 and STN1 are co-expressed; elevated HOXB7 enhances STN1 transcription, while decreased HOXB7 reduces STN1 protein levels. Moreover, HOXB7 directly binds the STN1 promoter region. Therefore, HOXB7 is an upstream transcription factor of STN1, and its high expression in PDAC patients may contribute to increased STN1 expression.",
            "Alterations in STN1 expression affect epithelial markers (e.g., E-cadherin) and mesenchymal markers (e.g., vimentin) in different pancreatic cancer cell lines and mouse models. STN1 knockdown suppresses EMT, while reintroduction of STN1 restores it, indicating that STN1 promotes PDAC metastasis by facilitating EMT progression.",
            "Dual-luciferase reporter assays showed that STN1 enhances the transcriptional activity of the ZEB1-luciferase reporter system. ChIP-qPCR demonstrated strong enrichment of STN1 at the ZEB1 promoter region, but not at the ACTB locus (negative control). Furthermore, reintroducing ZEB1 into STN1-knockdown MIA PaCa-2 cells restored invasive and migratory abilities, indicating that STN1 promotes PDAC metastasis by enhancing ZEB1 transcription.",
            "S-Tag pull-down and Co-IP assays revealed an interaction between STN1 and STAT3. ChIP-qPCR showed that STN1 knockdown significantly reduced STAT3 binding at the ZEB1 promoter, and CUT&Tag sequencing confirmed co-occupancy of STN1 and STAT3 at the same ZEB1 promoter region. These results indicate that STN1 recruits STAT3 to the ZEB1 promoter to cooperatively promote its transcription.",
            "Treatment with the STAT3 inhibitor S3I-201 markedly reduced STN1’s ability to activate the ZEB1 promoter, suggesting that STN1-mediated transcriptional activation of ZEB1 depends on STAT3 activity.",
            "Domain-deletion mutants of STN1 showed that the wHTH1 domain is essential for interaction with STAT3, while the OB domain mediates ssDNA binding but is dispensable for STN1–STAT3 interaction. Co-IP confirmed that deleting the OB fold or wHTH2 domain did not disrupt interaction with STAT3, whereas deleting wHTH1 abolished it; notably, the wHTH1 domain alone was sufficient to bind STAT3. Functional assays showed that reintroduction of the wHTH2 domain restored invasion and migration defects caused by STN1 knockdown, whereas the OB domain had no effect. These findings suggest that STN1 promotes PDAC migration and ZEB1 transcriptional activation through its protein-binding (wHTH1) and ssDNA-binding (OB) affinities.",
            "STAT3 inhibitors significantly suppressed the migration and invasion of STN1-overexpressing cells, with stronger inhibitory effects than in controls. In MIA PaCa-2 and PANC-1 cells overexpressing STN1, treatment with STAT3 inhibitors (e.g., S3I-201, napabucasin) led to a greater reduction in migration and invasion compared with controls. 3D spheroid invasion assays confirmed that STAT3 inhibition more strongly suppressed invasion in STN1-overexpressing cells. These results indicate that PDAC cells with high STN1 expression are more sensitive to STAT3 inhibition, supporting the potential of STAT3 inhibitors in suppressing metastasis.",
            "STAT3 inhibitors markedly reduced the invasive area of STN1-overexpressing cells, with stronger effects than in controls, further validating their heightened sensitivity to STAT3 inhibition.",
            "Following STAT3 inhibitor treatment, ZEB1 expression decreased, epithelial marker E-cadherin increased, and mesenchymal marker vimentin decreased, with more pronounced changes in STN1-overexpressing cells. This suggests that STAT3 inhibitors suppress tumor metastasis by inhibiting ZEB1 transcription and EMT progression, and the effect is stronger in the context of STN1 overexpression."
        ],
        "step_count": 12
    },
    "recuWh2PsWSZwH": {
        "reasoning_steps": [
            "First, by theorem_1, we can take a single gravitational potential \\phi=\\psi, so we only need to track the evolution of \\phi.",
            "Next, in the RD background of theorem_2 with w=c_s^2=\\tfrac{1}{3}, using conformal time and the conformal Hubble rate (\\eta,\\ \\mathcal H \\equiv a'/a), we have a\\propto\\eta \\Rightarrow \\mathcal H=1/\\eta.",
            "Then, according to concept_1, the first-order Newtonian potential satisfies \\phi'' + 4\\mathcal H\\,\\phi' - \\frac{1}{3}\\nabla^2\\phi = 0, and going to momentum space (Fourier component\\phi_k) gives \\phi_k''+\\frac{4}{\\eta}\\phi_k'+\\frac{k^2}{3}\\phi_k=0. (★)",
            "Afterwards, following concept_2, define the dimensionless variable x\\equiv k\\eta and normalize by matching to the superhorizon limit and the primordial curvature perturbation \\zeta_k, writing \\phi_k=\\frac{2}{3}\\,\\zeta_k\\,T_\\phi(x),\\qquad T_\\phi(0)=1.",
            "Rewriting (★) as an equation for T_\\phi yields T_\\phi''+\\frac{4}{x}T_\\phi' + \\frac{1}{3}T_\\phi = 0. (◇)",
            "Finally, solving (◇) and imposing superhorizon regularity, we obtain the explicit transfer function T_\\phi(x)=3\\,\\frac{\\sin y - y\\cos y}{y^{3}},\\qquad y=\\frac{x}{\\sqrt{3}}."
        ],
        "step_count": 6
    },
    "recuWhaz2amG1f": {
        "reasoning_steps": [
            "Set up moving coordinates $u = x - vt$ and $w = x + vt$ with $v = c/n$, and assume $Z = Z(u)$, $n = \\text{const}$. This choice simplifies Maxwell's equations for the 1+1D slab $0 \\leq x \\leq d$ embedded in vacuum with impedance $\\eta_0$ on both sides.",
            "Rewrite Maxwell's equations in $(u, w)$. Under $Z = Z(u)$ and $v = c/n$, they reduce to $\\partial_u \\ln[(E_y - ZH_z)/\\sqrt{Z}] = 0$ and a companion relation, yielding $E_y - ZH_z = \\sqrt{Z} g(w)$, where $g$ is arbitrary. Integrating $g$ gives $G$ with $G'(w) = g(w)$.",
            "Solve for the field inside the slab: $E_y = \\frac{1}{2}\\{ - [\\sqrt{Z(u)}]' G(w) + \\sqrt{Z(u)} G'(w) + h(u) \\}$, $H_z = [2Z(u)]^{-1}\\{ - [\\sqrt{Z(u)}]' G(w) - \\sqrt{Z(u)} G'(w) + h(u) \\}$, where $h(u)$ is an arbitrary function capturing the wave co-propagating with the moving impedance (right-going internal mode with no internal reflection).",
            "Form the interface-going combinations $E_\\pm \\coloneqq \\frac{1}{2}(E_y \\pm \\eta_0 H_z)$, which correspond to right-/left-going waves in vacuum and are continuous at $x = 0$ and $x = d$. Imposing zero outgoing wave on the transmission side fixes $h(u)$ in terms of $G$ via two boundary constraints: (i) Left incidence (entering at $x = 0$): impose $E_-(x = d) = 0 \\implies h(u) = [\\sqrt{Z(u)}]' G(2d - u) - \\frac{1 + \\eta_0/Z(u)}{1 - \\eta_0/Z(u)} \\sqrt{Z(u)} G'(2d - u)$. (ii) Right incidence (entering at $x = d$): impose $E_+(x = 0) = 0 \\implies h(u) = [\\sqrt{Z(u)}]' G(-u) - \\frac{1 - \\eta_0/Z(u)}{1 + \\eta_0/Z(u)} \\sqrt{Z(u)} G'(-u)$.",
            "Evaluate the reflected field at the entrance face. For left incidence, substitute the $h(u)$ from Step 4(i) back into $E_-$ and then evaluate at $x = 0$ ($u = -vt$, $w = vt$). Terms appear only as differences of $G$ and $G'$ evaluated at $w$ and $w + 2d$, e.g., $G'(vt) - G'(vt + 2d)$ and $G(vt) - G(vt + 2d)$, multiplied by impedance-dependent prefactors. Therefore $E_-(x = 0) \\equiv 0$ for all $t$ if and only if $G(w + 2d) = G(w)$ (hence also $G'$ periodic), i.e., period $2d$.",
            "Repeat for right incidence: use Step 4(ii), substitute into $E_+$ and evaluate at $x = d$ ($u = d - vt$, $w = d + vt$). The reflected field at $x = d$ again reduces to impedance-prefactor times $[G(w) - G(w - 2d)]$ and $[G'(w) - G'(w - 2d)]$. Hence $E_+(x = d) \\equiv 0$ for all $t$ if and only if $G(w + 2d) = G(w)$.",
            "Necessity and sufficiency: (Sufficiency) If $G$ has period $2d$, the difference terms vanish identically, giving zero reflection for either incidence direction. (Necessity) Conversely, demanding zero reflection for all $t$ requires those difference terms to vanish pointwise, which enforces $G$ and $G'$ equality under a $2d$ shift; the only time-translation-invariant solution for all $t$ is $G(w + 2d) = G(w)$. This matches the paper's identification of “reflectionless eigenpulses” when $G$ is $2d$-periodic."
        ],
        "step_count": 7
    },
    "recuWhnw3wDaGd": {
        "reasoning_steps": [
            "First, in the **conformal Newtonian gauge** and ignoring anisotropic stress (theorem_1), we have \\Psi=\\Phi. The $00$ constraint from the Einstein equations at the same order yields a Poisson-like relation: 4\\pi G\\,a^2\\,\\delta\\rho= -3\\mathcal H\\bigl(\\Phi'+\\mathcal H\\Phi\\bigr)+\\nabla^2\\Phi, which expresses the density perturbation entirely in terms of \\Phi.",
            "Next, using the first-order scalar equation (concept_2): \\Phi''+3\\mathcal H\\Phi'+\\bigl(2\\mathcal H'+\\mathcal H^2\\bigr)\\Phi =4\\pi G\\,a^2\\,\\delta p, and adopting the **adiabatic approximation** \\delta p=c_s^2\\,\\delta\\rho (quantifying the \"pressure information\" via the sound speed c_s^2).",
            "Subsequently, rewrite the right-hand side (RHS) of the above equation as: 4\\pi G\\,a^2\\,\\delta p = c_s^2\\!\\left[-3\\mathcal H\\bigl(\\Phi'+\\mathcal H\\Phi\\bigr)+\\nabla^2\\Phi\\right]. Substitute this back into the first-order scalar equation, move the RHS to the left-hand side (LHS), and combine like terms. The conclusion is obtained as follows: express \\delta\\rho in terms of \\Phi using (theorem_1 + $00$ constraint), then close the system and rearrange using (concept_2 + \\delta p=c_s^2\\delta\\rho). This exactly gives the target equation: \\Phi'' + 3\\mathcal{H}(1 + c_s^2)\\Phi' + \\bigl(2\\mathcal{H}' + (1 + 3c_s^2)\\mathcal{H}^2 - c_s^2 \\nabla^2\\bigr)\\Phi = 0. This is the **standard evolution equation for the Newtonian gauge potential \\Phi in an adiabatic fluid**."
        ],
        "step_count": 3
    },
    "recuWjyZXWbRCq": {
        "reasoning_steps": [
            "Goal: With axes aligned to (e_+, e_×, k̂) and u ≡ |k−p|/k, v ≡ p/k, show the value of ∑_{s=+,×} [Q_s(k,p)/k^2][Q_s(k,k−p)/k^2].",
            "Polarization & definition (theorem_1): Use TT tensors ε^s_ij(k̂) with ε^s_ij ε^{s'}_ij = 2 δ^{ss'}. Define Q_s(k,a) ≡ ε^s_ij(k̂) a_i a_j.",
            "Spherical coordinates (theorem_2): For p = (p,θ,φ) w.r.t. k̂, Q_+(k,p)/k^2 = v^2 sin^2θ cos2φ and Q_×(k,p)/k^2 = v^2 sin^2θ sin2φ.",
            "Geometry for q ≡ k−p (theorem_2): Let its angles be (θ′,φ′). Because q_T = −p_T, we have φ′ = φ + π and sinθ′ = (v/u) sinθ.",
            "Build the polarization sum (concept_2): ∑_s [Q_s(k,p)/k^2][Q_s(k,q)/k^2] = v^2 u^2 sin^2θ sin^2θ′ [cos2φ cos2φ′ + sin2φ sin2φ′]. Using φ′ = φ + π, the bracket equals 1, giving v^2 u^2 sin^2θ sin^2θ′.",
            "Eliminate angles (theorem_2 & concept_1): Law of cosines u^2 = 1 + v^2 − 2v cosθ ⇒ sin^2θ = [4v^2 − (1 + v^2 − u^2)^2]/(4v^2). Also sinθ′ = (v/u) sinθ ⇒ sin^2θ sin^2θ′ = (v^2/u^2) sin^4θ.",
            "Combine: v^2 u^2 × (v^2/u^2) sin^4θ = v^4 sin^4θ = v^4 × {[4v^2 − (1 + v^2 − u^2)^2]/(4v^2)}^2 = {[4v^2 − (1 + v^2 − u^2)^2]/4}^2."
        ],
        "step_count": 7
    },
    "recuVUkyIjq6dk": {
        "reasoning_steps": [
            "The doping of Ti into the CeO2 matrix can induce the formation of solid solution and the exposure of (111) and (110) crystal planes. Moreover, the Ti dopant also increases the concentration of Ce3+ species on CeO2, suggesting the increment of oxygen vacancies.",
            "According to the concept 1, the oxygen vacancy is favorable of CO2 conversion. Among Zr-, La-, and Ti-doped catalysts, the Ti-doped catalyst has the highest oxygen vacancy concentration, which favors the conversion of CO2 and C2H6 with high selectivity to C2H4. Therefore, the Ti-doped catalyst with the high oxygen vacancy proportion shows the high activity.",
            "The doping of Zr can favor the formation of oxygen vacancies in CeO2. However, the Zr dopant is favorable of the oxidation of CO to CO2, which is the reverse reaction of the conversion of CO2. Therefore, the Zr dopant shows the lower activity in comparison with Ti dopant."
        ],
        "step_count": 3
    },
    "recuWj3Wo7JwAe": {
        "reasoning_steps": [
            "\\mathrm{COP}=Q_{\\mathrm{evap}}/Q_{\\mathrm{gen}}. Any change that raises Q_{\\mathrm{evap}} and/or reduces generator duty at the same lift improves COP.",
            "Evaporation temperature has the strongest lever (Concept 3). Raising T_{\\mathrm{evap}} shrinks the thermal lift (T_{\\mathrm{cond}}-T_{\\mathrm{evap}}), lowers solution circulation and rectifier/generator duties, so Q_{\\mathrm{gen}} drops while Q_{\\mathrm{evap}} is maintained/increased—giving the steepest COP gain among the variables.",
            "Sink-side temperatures are next (Concept 1). Lowering T_{\\mathrm{cond}} (and, secondarily, the absorber-1 outlet T_{\\mathrm{abs,1}}) strengthens absorption driving force and reduces circulation, improving COP—but their leverage is weaker than T_{\\mathrm{evap}} because they do not reduce the lift as directly.",
            "High-T generator setpoint is least influential (Concept 2). Increasing T_{\\mathrm{gen,2}} tends to raise rectification/reflux and internal duties, often flattening or reducing COP; its sensitivity is smaller than that of the sink-side and evaporator temperatures."
        ],
        "step_count": 4
    },
    "recuVJjeMcm6kF": {
        "reasoning_steps": [
            "Goal: derive, in one sentence and near the stellar center (ξ→ξ_c, equivalently P→P_c), the expression v(ξ) in terms of the central quantities P_c and ε_c.",
            "Field equations (Concept_1): start from the TOV hydrostatic equilibrium in natural units (G=c=1), dP/dr = -[(ε+P)(m(r)+4πr^3P)]/[r(r-2m)], with m(r)=4π∫_0^r ε(r') r'^2 dr'.",
            "Central regularity (Concept_2): impose smooth, finite center expansions m(r)= (4π/3) ε_c r^3 + O(r^5), P(r)= P_c + O(r^2), ε(r)= ε_c + O(r^2), and note that a dimensionless radius ξ scales linearly with r near the center (r ∝ ξ).",
            "Center-linearized gradient (Concept_3): substitute the central expansions into TOV and keep the leading term to obtain dP/dr ≃ -4π (ε_c + P_c) (P_c + ε_c/3) r.",
            "Quadratic central pressure profile (Concept_4): integrate Step_4 with the boundary condition P(0)=P_c to get P_c - P(r) ≃ 2π (ε_c + P_c) (P_c + ε_c/3) r^2.",
            "Algebraic inversion (from Concept_4): solve the previous relation for r^2 to obtain r^2 ≃ [P_c - P]/[2π (ε_c + P_c)(P_c + ε_c/3)].",
            "Definition of the volume-type coordinate: choose v ≡ r^3 (or any variable proportional to r^3) so that v = (r^2)^{3/2}, which converts the quadratic pressure profile into a 3/2-power dependence on (P_c - P).",
            "Final one-sentence result: v(ξ)= \\left( \\dfrac{P_c - P(ξ)}{2\\pi\\, (\\varepsilon_c + P_c)\\, (P_c + \\varepsilon_c/3)} \\right)^{3/2} \\quad (G=c=1)."
        ],
        "step_count": 8
    },
    "recuU6Y7OTFYA5": {
        "reasoning_steps": [
            "Each CA3 excitatory neuron receives feedback inhibition from CA3 inhibitory neurons and recurrent input from other CA3 excitatory neurons. When the recurrent input current coincides with the excitatory current input from the DG, according to the superlinear coincident detection rule, the neuron's response is significantly higher than the simple sum of the inputs.",
            "According to the Hebbian Learning rule, the synaptic weights between neurons receiving adjacent positional inputs increase. The dynamics are described by: \\[\\frac{dw_{ij}^{rec}}{dt} = a_{Hebb} \\cdot s_i \\cdot s_j \\cdot \\sqrt{g_{ij}^l}\\] \\[\\frac{dg_{ij}^l}{dt} = -\\frac{g_{ij}^l}{\\tau_l} + w_{ij} \\cdot s_j \\cdot I_{p,i}^{exc}\\] Here, \\(a_{Hebb}\\) is the Hebbian learning rate, \\(s_i\\) and \\(s_j\\) are the spike signals (0 or 1) of the postsynaptic and presynaptic neurons, respectively, and \\(g_{ij}^l(t)\\) is the assumed activity of the plasticity-related factor. Its dynamics resemble synaptic conductance but with a longer decay time constant \\(\\tau_l\\) compared to the voltage decay time constant. \\(I_{p,i}^{exc}\\) is the input current signal received by the neuron, satisfying the superlinear detection rule. The activity of the plasticity-related factor accumulates during firing. When coincident firing begins, the longer the cumulative firing time of the presynaptic neuron (e.g., a neuron from a previous position with longer cumulative firing time), the stronger the activity of the plasticity-related factor. Conversely, when a neuron from a later position fires coincidently with the current neuron, its cumulative firing time is shorter, resulting in weaker plasticity-related factor activity. Additionally, since the plasticity factor decays in the absence of firing, synaptic activation cannot accumulate through repeated track training.",
            "According to the Izhikevich neuron dynamics model, in the v-u phase plane, a CA3 excitatory neuron has a stable point in the absence of input stimuli. When external stimuli exceed the bifurcation point of DG excitatory input, the v-nullcline moves in the positive direction along the u-axis until it rises above the n-nullcline, causing the stable point to disappear and generating spike potentials. In contrast, a CA3 inhibitory neuron has an unstable point in the absence of inhibitory input, leading to burst firing. When it receives inhibitory input from theta waves, the v-nullcline moves in the negative direction along the u-axis, delaying firing or even creating a stable point that halts firing.",
            "CA3 is simultaneously subjected to excitatory stimuli and theta modulation. When the excitatory input current is weak, such as at the beginning or end of a positional field signal, the v-nullcline moves up and down along the y-axis due to inhibitory input from theta waves. At a specific phase, it moves upward away from the n-nullcline, causing the equilibrium point to disappear and initiating firing. When the excitatory stimulus is strong, such as at the peak of the positional field potential, the v-nullcline is positioned higher on the y-axis, resulting in an earlier theta phase for spike initiation. Conversely, when the excitatory stimulus is weak, the theta phase for spike initiation is later. Thus, disregarding other input influences, the spike potentials of a CA3 excitatory cell during the middle of positional firing occur at an earlier theta phase compared to those during the early or late stages.",
            "In continuous positional fields, the excitatory input from recurrent synapses of CA3 neurons in adjacent positions to the current CA3 excitatory place cell arrives slightly later than the activation of the adjacent CA3 place cells. This causes the excitatory signals from CA3 recurrent synapses to be delayed relative to the excitatory firing of the current neuron."
        ],
        "step_count": 5
    },
    "recuU9q0jwzs3S": {
        "reasoning_steps": [
            "Individual membrane potential and population variables are written as:  $$\\begin{aligned}  C\\dot{v}_i &= k(v_i-v_r)(v_i - v_{\\theta,i}) - u + I(t) + g_s\\,(E-v_i), &&\\text{[1]}\\\\  \\tau_u\\dot{u} &= -u + b\\!\\left(-v_r + \\frac{1}{N}\\sum_{j=1}^N v_j\\right) + \\tau_u\\kappa r, &&\\text{[2]}\\\\  \\tau_s\\dot{s} &= -s + J\\tau_s r, &&\\text{[3]}\\\\  r(t) &= \\frac{1}{N}\\sum_{j=1}^N\\sum_k \\delta\\!\\big(t-t^k_j\\big). &&\\text{[4]}  \\end{aligned}$$",
            "Using threshold heterogeneity (Lorentzian distribution) given in the concept:  $$p(v_\\theta)=\\frac{1}{\\pi}\\frac{\\Delta_v}{(v_\\theta-\\bar{v}_\\theta)^2+\\Delta_v^2}. \\quad\\text{[5]}$$",
            "From this, derive the continuum representation and continuity equation:  Under $N\\!\\to\\!\\infty$, using conditional density $\\rho(v,t\\mid v_\\theta)$:  $$\\frac{\\partial}{\\partial t}\\rho = -\\frac{\\partial}{\\partial v}\\!\\left[ \\frac{1}{C}\\big(k(v-v_r)(v-v_\\theta)-u+I+g_s(E-v)\\big)\\rho \\right].\\quad\\text{[6]}$$",
            "Introducing complex variable Lorentzian ansatz (two parameters):  $$\\rho(v,t\\mid v_\\theta)=\\frac{1}{\\pi}\\frac{x(t,v_\\theta)}{[v-y(t,v_\\theta)]^2+x(t,v_\\theta)^2}. \\quad\\text{[7]}$$",
            "Equivalence between macroscopic quantities and ansatz parameters:  $$\\frac{C\\pi}{k}\\,r(t)=\\!\\!\\int x(t,v_\\theta)\\,p(v_\\theta)\\,dv_\\theta = x(t),\\quad v(t)=\\!\\!\\int y(t,v_\\theta)\\,p(v_\\theta)\\,dv_\\theta = y(t). \\quad\\text{[8],[9]}$$ (Using the limits $v_p\\!\\to\\!\\infty,\\ v_0\\!\\to\\!-\\infty$)",
            "Complex variabilization (encapsulation into univariate complex ODE):  Let $z(t,v_\\theta)=x+iy$, substituting into [6] yields:  $$C\\,\\partial_t z = i\\Big[-kz^2+i\\big(k(v_r+v_\\theta)+g_s\\big)z + k v_r v_\\theta + g_sE - u + I\\Big]. \\quad\\text{[10]}$$",
            "Perform residue integration over $v_\\theta$ (selecting poles to ensure $r\\!>\\!0$):  For  $$\\dot{z}=\\frac{1}{C}\\int \\partial_t z(t,v_\\theta)\\,p(v_\\theta)\\,dv_\\theta \\quad\\text{[11]}$$ Using [5] to evaluate at simple poles $v_\\theta=\\bar{v}_\\theta\\pm i\\Delta_v$ in the complex plane; pole selection follows $x(t)=\\frac{\\pi C}{k}r(t)>0$: if $v-v_r<0$, take the lower half-plane $(\\bar{v}_\\theta-i\\Delta_v)$, otherwise take the upper half-plane. And using  $$z\\big(t,\\bar{v}_\\theta+i\\Delta_v\\big)=x+iy=\\frac{\\pi C}{k}r(t)+iv(t)$$ to connect $z$ with $(r,v)$.",
            "Finally obtain the four-dimensional closed system:  $$\\begin{aligned} C\\dot{r}&=\\frac{\\Delta_v k^2}{\\pi C}\\,\\sigma_v\\,(v-v_r) + r\\big(k(2v-v_r-\\bar{v}_\\theta)-g_s\\big), &&\\text{[12]}\\\\ C\\dot{v}&=k v\\,(v-v_r-\\bar{v}_\\theta) -\\pi C r\\Big(\\Delta_v\\sigma_v+\\frac{\\pi C}{k}r\\Big) + k v_r\\bar{v}_\\theta - u + I + g_s(E-v), &&\\text{[13]}\\\\ \\tau_u\\dot{u}&=b(v-v_r)-u+\\tau_u\\kappa r, &&\\text{[14]}\\\\ \\tau_s\\dot{s}&=-s+\\tau_s J r. &&\\text{[15]} \\end{aligned}$$"
        ],
        "step_count": 8
    },
    "recuUqZziw5wn0": {
        "reasoning_steps": [
            "Use the concept 1, we can compute that X is not smooth when k=0 and X is smooth when k\\neq0.",
            "Use the concept 2, we know that the base space is not zero.",
            "Let X be a smooth projective variety with Picard number one. Then the GL2(C) character variety of X is rigid if and only if b1(X) = 0. The concept 2 gives that b_1(X)\\neq0 when k=0, and thus we know that the $GL_2(\\mathbb{C})$ character variety of $X$ is not rigid when k=0. When k\\neq0, we can see that X is actually smooth and thus the corresponding character variety of $X$ is rigid."
        ],
        "step_count": 3
    },
    "recuUSrw35heEZ": {
        "reasoning_steps": [
            "According to concept_3, in transformer-based objectives, the optimization dynamics are governed by gradient magnitudes, which explains why adaptive optimizers outperform SGD. This is because transformer models exhibit unique smoothness properties where local smoothness depends on the gradient norm, suggesting that the learning rate should scale as the reciprocal of the gradient norm to handle varying curvature effectively.",
            "This aligns with concept_1, the (L_0, L_1)-smooth condition, which states that for a differentiable and lower-bounded function f, there exist constants L_0, L_1 > 0 such that for all w_1, w_2 in \\mathbb{R}^d with \\|w_1 - w_2\\| \\leq \\frac{1}{L_1}, the gradient difference satisfies \\|\\nabla f(w_1) - \\nabla f(w_2)\\| \\leq (L_0 + L_1 \\|\\nabla f(w_1)\\|) \\|w_1 - w_2\\|. In Transformer optimization landscapes, this condition captures how local smoothness is bounded by the local gradient norm, providing the foundation for adaptive learning rate requirements.",
            "Building on theorem_1, under this (L_0, L_1)-smoothness, the local smoothness constant L(w) is bounded by L_0 + L_1 \\|\\nabla f(w)\\|, meaning the curvature increases with the gradient magnitude, which risks overshooting without adjustment. This necessitates adaptive learning rates that scale inversely with the gradient norm to ensure the function value decreases, deriving the bound \\eta_t \\leq \\frac{1}{L_0 + L_1 \\|\\nabla f(w_t)\\|}, balancing descent and curvature for convergence.",
            "To implement this bound practically in Transformer training without knowing L_0 and L_1 exactly, theorem_1 and concept_3 imply approximating the gradient magnitude. Here, concept_2 offers a solution: momentum, as an exponential moving average of historical gradients, provides a robust proxy for estimating \\|\\nabla f(w_t)\\| without second-moment computations, enabling efficient adaptive optimization while adhering to the learning rate upper bound derived from the smoothness properties."
        ],
        "step_count": 4
    },
    "recuUo3KlCjoXZ": {
        "reasoning_steps": [
            "Problem setup (concept_1). Define the testing goal: given $n,\\epsilon$ and $m$ i.i.d. samples, accept if $|\\mathrm{supp}(p)|\\le n$ and reject if $p$ is $\\epsilon$-far from every distribution of support $\\le n$.",
            "Test statistic (concept_2). Use a linear estimator built from the sample histogram of the form $\\hat S=\\sum_i (1+f(N_i))$ and a threshold test on $\\hat S$. The function $f$ is chosen via a polynomial approximation so that $\\mathbb E[\\hat S]$ approximates the true support size and the statistic concentrates.",
            "Poissonization (concept_3). Analyze in the Poissonized model $N_i\\sim\\mathrm{Poi}(m p_i)$ to make counts independent and simplify computation of bias and variance of $\\hat S$.",
            "Baseline bound (concept_4). A simple/plug-in tester yields the baseline sufficient sample size $m_0=\\Theta\\!\\Big(\\frac{n}{\\epsilon}\\Big)$, since the dominant error term decays like $e^{-c m/n}$ and solving for error $\\le\\epsilon$ gives the $n\\log(1/\\epsilon)$ type scaling which in this form provides $m_0=\\Theta(n/\\epsilon)$ as the baseline regime.",
            "Chebyshev-based estimator construction (theorem_1). Construct a polynomial $P_d$ (shifted & scaled Chebyshev) so that $P_d(0)=-1$ and $P_d(x)$ is small on a chosen safe interval $[l,r]$. Use this polynomial to define $f$ and thereby a linear estimator whose bias for 'medium'-sized probabilities is tightly controlled.",
            "Refined small-probability analysis (theorem_2). For probabilities below the safe interval, apply the differential/derivative inequality analysis to further bound the contribution of very small probabilities; this analysis introduces a $\\log(1/\\epsilon)$-type factor into the candidate sample size required for the Chebyshev estimator to achieve error $\\epsilon$.",
            "Variance control via polynomial degree constraint. To keep the estimator's variance small, restrict the polynomial degree to $d=O(\\log n)$. This choice limits variance growth and produces a multiplicative improvement of $1/\\log n$ over the baseline in the Chebyshev branch.",
            "Two candidate sufficient sample sizes. Baseline (plug-in): $m_0=\\Theta\\!\\big(\\dfrac{n}{\\epsilon}\\big)$. Chebyshev branch (after applying theorem_2 and using $d=O(\\log n)$): $m_{\\text{cheb}}=\\Theta\\!\\left(\\frac{n}{\\epsilon\\log n}\\,\\log\\frac{1}{\\epsilon}\\right).$",
            "Take the minimum (concept_5). Success requires only one of the sufficient conditions, so an achievable (upper-bound) sample complexity is $m(n,\\epsilon)=\\min\\{m_0,\\; m_{\\text{cheb}}\\}.$",
            "Algebraic factoring (final form). Factor out the common multiplier $\\dfrac{n}{\\epsilon\\log n}$: $\\min\\!\\left\\{\\frac{n}{\\epsilon},\\ \\frac{n}{\\epsilon\\log n}\\log\\frac{1}{\\epsilon}\\right\\} =\\frac{n}{\\epsilon\\log n}\\cdot\\min\\!\\left\\{\\log n,\\ \\log\\frac{1}{\\epsilon}\\right\\}.$",
            "Final tight upper bound (concise statement). Thus $\\boxed{\\,m(n,\\epsilon)=O\\!\\left(\\frac{n}{\\epsilon\\log n}\\cdot\\min\\Big\\{\\log\\frac{1}{\\epsilon},\\ \\log n\\Big\\}\\right)\\,}$ which explicitly displays the $\\frac{1}{\\log n}$ improvement from the Chebyshev degree constraint and the regime switch governed by $\\min\\{\\log(1/\\epsilon),\\log n\\}$"
        ],
        "step_count": 11
    },
    "recuUYABk4Gv6B": {
        "reasoning_steps": [
            "The objective is to find the analytical expression for the exponent $C_2$ as defined in the main theorem (theorem_1), which states the fidelity lower bound as $|\\langle \\phi|s\\rangle| \\ge \\frac{\\gamma^{C_2}}{C_1}$. According to concept_2, $|\\langle s | \\phi \\rangle|$ is the square root of the stabilizer fidelity, $\\sqrt{F(\\phi)}$.",
            "The proof of this theorem, which leverages additive combinatorics tools like the Balog-Szemerédi-Gowers theorem (theorem_2) and the result on Marton's conjecture (theorem_3), leads to a more detailed intermediate inequality. As the hint suggests, combining these theorems introduces a constant $K_2$, which appears in the expression from concept_3: $|\\langle s | \\phi \\rangle| \\ge \\delta \\frac{\\epsilon^{2K_2+2}}{K_1^2}$.",
            "To connect this intermediate result back to the Gowers norm bound $\\gamma$, we utilize the relationships provided in concept_3: $\\delta \\propto \\gamma^2$ and $\\epsilon \\propto \\gamma^2$. We substitute these proportionalities into the inequality.",
            "After substitution, the right-hand side of the inequality becomes proportional to $(\\gamma^2) \\cdot (\\gamma^2)^{2K_2+2}$. The next step is to simplify this expression to find the total exponent of $\\gamma$.",
            "The total exponent is the sum of the exponents from each part: the exponent from the $\\delta$ term is 2, and the exponent from the $\\epsilon$ term is $2 \\times (2K_2+2) = 4K_2+4$. Adding them together gives $2 + (4K_2+4) = 4K_2+6$.",
            "By comparing our derived expression, which is proportional to $\\gamma^{4K_2+6}$, with the form given in theorem_1 ($\\gamma^{C_2}$), we can equate the exponents. This gives the final analytical expression for $C_2$: $C_2 = 4K_2+6$."
        ],
        "step_count": 6
    },
    "recuV26pFqhi7f": {
        "reasoning_steps": [
            "By the $b$-bit quantization assumption, each coordinate takes at most $2^{b}$ values. Hence a single-layer $d$-dimensional hidden state has  \\[  S_1=(2^{b})^{d}=2^{bd}  \\]  possibilities.",
            "Concatenating $L$ layers at the same position gives the combined layer state count  \\[  S=(S_1)^{L}=(2^{bd})^{L}=2^{bLd}.  \\]",
            "To preserve semantics under segment deletion, it is not enough that single-time states match; we require boundary consistency across the cut. Concretely, for each layer $1\\le l\\le L$, both pairs  \\[  (h_{i-1}^l,h_i^l)\\quad\\text{and}\\quad(h_{j-1}^l,h_j^l)  \\]  must coincide. By Steps 1--2, each $h_t^l$ ranges over at most $2^{bd}$ values, so each layer-wise ordered pair has at most $2^{2bd}$ possibilities, and across $L$ layers the number of effective pair-configurations is  \\[  (2^{2bd})^{L}=\\bigl(2^{bd}\\bigr)^{2L}=\\bigl(2^{bLd}\\bigr)^{2}=S^{2}.  \\]  Thus, if the input length $n>S^2$, determinism over this finite set implies there exist $1\\le i<j\\le n$ with identical effective configurations.",
            "By deletion invariance under matched boundaries, removing $a_{i+1}\\cdots a_{j}$ yields a word still accepted.",
            "Repeating the previous step until no such long segment remains, we obtain an accepted word $w'$ with  \\[  |w'|\\le S^{2}=\\bigl(2^{\\,bLd}\\bigr)^{2}=2^{\\,2bLd}.  \\]"
        ],
        "step_count": 5
    },
    "recuV4t2Gbf53n": {
        "reasoning_steps": [
            "Expressions for streamlines and equipotential lines in the Cartesian coordinate system.",
            "Expression for the flow direction in the Cartesian coordinate system.",
            "Derive the coordinate-transformation relations based on the streamline, equipotential-line, and flow-direction expressions."
        ],
        "step_count": 3
    },
    "recuV4xvWPHIM3": {
        "reasoning_steps": [
            "Define the Total Number of Edges and Edge Indicators: Let \\(G\\) be an ERGM on \\(n\\) vertices. Using **Concept 3** (exchangeable pairs involve edge indicator vectors \\(Y = \\{Y_{ij}\\}\\)): - Define \\(Y_{ij} \\in \\{0,1\\}\\) as the edge indicator: \\(Y_{ij} = 1\\) if edge \\((i,j)\\) exists in \\(G\\), and \\(Y_{ij} = 0\\) otherwise. - Let \\(W_n\\) denote the **total number of edges** in \\(G\\): \\(W_n = \\sum_{1 \\leq i < j \\leq n} Y_{ij}\\), where \\(N = \\binom{n}{2}\\) is the total number of possible edges in an \\(n\\)-vertex graph.",
            "Link Edge Probability to the Subcritical Region: By **instruction** (subcritical region), the parameter vector \\(\\beta\\) satisfies: 1. A unique fixed point \\(p = p(\\beta) \\in (0,1)\\) solves \\(\\varphi_\\beta(a) = a\\), where \\(\\varphi_\\beta(a) = \\frac{e^{2\\Phi_\\beta(a)}}{e^{2\\Phi_\\beta(a)} + 1}\\) and \\(\\Phi_\\beta(a) = \\sum_{j=1}^k \\beta_j e_j a^{e_j - 1}\\); 2. \\(\\varphi'_\\beta(p) < 1\\) (weak dependence condition). For large \\(n\\) (asymptotic regime), the edge indicator \\(Y_{ij}\\) has an approximate expectation equal to the fixed point \\(p\\): \\(\\mathbb{E}[Y_{ij}] \\approx p\\). This follows because \\(\\varphi_\\beta(p) = p\\) is the equilibrium edge probability (the conditional probability of an edge, given the rest of the graph, converges to \\(p\\) for large \\(n\\)).",
            "Center \\(W_n\\) for the Exchangeable Pair Theorem (Theorem 1): **Theorem 1** requires a centered random variable \\(W\\) with \\(\\mathbb{E}[W] = 0\\). Define the **centered total edge count**: \\(\\tilde{W}_n = W_n - \\mathbb{E}[W_n]\\). By linearity of expectation and \\(\\mathbb{E}[Y_{ij}] \\approx p\\): \\(\\mathbb{E}[W_n] = \\sum_{1 \\leq i < j \\leq n} \\mathbb{E}[Y_{ij}] \\approx Np\\), so \\(\\tilde{W}_n \\approx W_n - Np\\).",
            "Construct an Exchangeable Pair for \\(\\tilde{W}_n\\): Using **Concept 1** (exchangeable pairs of edge vectors): - Construct \\(Y'\\) (exchangeable with \\(Y\\)) by flipping a uniformly random edge: select a pair \\((i,j) \\sim \\text{Uniform}(1,\\dots,N)\\), set \\(Y'_{kl} = Y_{kl}\\) for \\((k,l) \\neq (i,j)\\), and \\(Y'_{ij} = 1 - Y_{ij}\\). - The corresponding centered total edge count is \\(\\tilde{W}'_n = W'_n - \\mathbb{E}[W'_n]\\). Since \\((Y,Y')\\) is exchangeable, \\(\\mathbb{E}[W'_n] = \\mathbb{E}[W_n]\\), so: \\(\\tilde{W}'_n - \\tilde{W}_n = W'_n - W_n = (1 - 2Y_{ij})\\).",
            "Compute Key Quantities for Theorem 1: We calculate two critical terms from **Theorem 1**: 5.1 Expectation of the Squared Difference: Since \\(Y_{ij} \\in \\{0,1\\}\\), \\(Y_{ij}^2 = Y_{ij}\\). Thus: \\(\\mathbb{E}\\left[(\\tilde{W}'_n - \\tilde{W}_n)^2\\right] = \\mathbb{E}\\left[(1 - 2Y_{ij})^2\\right] = \\mathbb{E}\\left[1 - 4Y_{ij} + 4Y_{ij}^2\\right] = 1 - 4p + 4p = 1\\). 5.2 Conditional Expectation and \\(\\lambda\\): By **Theorem 1**, \\(\\mathbb{E}[\\tilde{W}'_n \\mid \\tilde{W}_n] = (1 - \\lambda)\\tilde{W}_n\\) for some \\(\\lambda \\in (0,1)\\) (dependency strength). For large \\(N\\) (asymptotic regime), \\(\\mathbb{E}[Y_{ij} \\mid W_n] \\approx \\frac{W_n}{N}\\) (uniform edge selection). Substituting into the conditional expectation: \\(\\mathbb{E}[W'_n \\mid W_n] = W_n + \\mathbb{E}[1 - 2Y_{ij} \\mid W_n] \\approx W_n + 1 - \\frac{2W_n}{N}\\). The constant term \\(1\\) vanishes as \\(N \\to \\infty\\), so: \\(\\mathbb{E}[\\tilde{W}'_n \\mid \\tilde{W}_n] \\approx \\left(1 - \\frac{2}{N}\\right)\\tilde{W}_n\\). Here, \\(\\lambda \\approx \\frac{2}{N}\\) in the raw count, but we refine \\(\\lambda\\) using the subcritical derivative \\(\\varphi'_\\beta(p)\\).",
            "Relate \\(\\lambda\\) to the Subcritical Derivative \\(\\varphi'_\\beta(p)\\), compute the derivative of \\(\\varphi_\\beta(a)\\) at the fixed point \\(p\\): 1. First, differentiate \\(\\varphi_\\beta(a) = \\frac{e^{2\\Phi_\\beta(a)}}{e^{2\\Phi_\\beta(a)} + 1}\\): \\(\\varphi'_\\beta(a) = \\frac{2\\Phi'_\\beta(a)e^{2\\Phi_\\beta(a)}}{\\left(e^{2\\Phi_\\beta(a)} + 1\\right)^2} = 2\\Phi'_\\beta(a)\\varphi_\\beta(a)(1 - \\varphi_\\beta(a))\\). 2. Substitute \\(\\varphi_\\beta(p) = p\\) (fixed point) and \\(\\Phi'_\\beta(p) = \\sum_{j=1}^k \\beta_j e_j(e_j - 1)p^{e_j - 2}\\) (derivative of \\(\\Phi_\\beta(a)\\)): \\(\\varphi'_\\beta(p) = 2p(1 - p)\\sum_{j=1}^k \\beta_j e_j(e_j - 1)p^{e_j - 2} = \\sum_{j=1}^k \\beta_j e_j(e_j - 1)2p^{e_j - 1}(1 - p)\\). 3. For the single-edge subgraph \\(H_1\\) (where \\(e_1 = 1\\)), \\(e_1(e_1 - 1) = 0\\), so the \\(j=1\\) term vanishes: \\(\\varphi'_\\beta(p) = \\sum_{j=2}^k \\beta_j e_j(e_j - 1)2p^{e_j - 1}(1 - p)\\).",
            "Solve for the Asymptotic Variance: By **Theorem 1**, the variance parameter is: \\(\\sigma^2 = \\frac{1}{4\\lambda} \\mathbb{E}\\left[(\\tilde{W}'_n - \\tilde{W}_n)^2\\right]\\). In the subcritical region, weak dependence implies \\(1 - \\lambda \\approx \\varphi'_\\beta(p)\\), so \\(\\lambda \\approx 1 - \\varphi'_\\beta(p)\\). Substituting \\(\\mathbb{E}[(\\tilde{W}'_n - \\tilde{W}_n)^2] = 1\\) and scaling by \\(Np(1 - p)\\) (the binomial variance of independent edges) gives the **asymptotic variance** of \\(W_n\\): \\(\\sigma_n^2 = \\frac{N p(1 - p)}{1 - \\sum_{j=2}^{k} \\beta_{j} e_{j}(e_{j}-1) \\cdot 2 p^{e_{j}-1}(1-p)}\\), where: - \\(N = \\binom{n}{2}\\) (total possible edges), - \\(p\\) is the unique fixed point of \\(\\varphi_\\beta(a) = a\\) in \\((0,1)\\), - \\(\\beta_j\\) and \\(e_j\\) are the parameter and edge count of subgraph \\(H_j\\), respectively."
        ],
        "step_count": 7
    },
    "recuV5mtyLghI1": {
        "reasoning_steps": [
            "Let $\\delta_A = \\tau^{-c}$, then $\\log(1/\\delta_A) = O(\\log \\tau)$. From the phase transition condition $B = O(\\sqrt{\\log \\tau})$, we obtain $B^2 = O(\\log \\tau)$. According to theorem_1, the degree of the polynomial is $g = O(\\beta B^2 d + \\log(1/\\delta_A)) = O(\\beta d \\log \\tau + \\log \\tau) = O(d \\log \\tau).$ Furthermore, from the given condition on the approximation rank, we have $k = O(\\log(1/\\delta_A)) = O(\\log \\tau).$",
            "From theorem_2, the time complexity is: $T_{A_k} = O(\\tau k g) = O(\\tau \\cdot \\log \\tau \\cdot d \\log \\tau) = O(d \\tau \\log^2 \\tau).$",
            "Using the low-rank decomposition from theorem_2: $A_k \\mathbf{1}_L = U(V^\\top \\mathbf{1}_L)$ Compute $V^\\top \\mathbf{1}_L$: $O(k L) = O(\\tau \\log \\tau)$ Compute $U v$: $O(k M) = O(\\tau \\log \\tau)$ Total time complexity: $T_D = O(\\tau \\log \\tau).$",
            "Using lemma_1 and lemma_2: $Z = \\Xi \\underbrace{(D^{-1} U)}_{O(M k)} V^\\top$ Compute $D^{-1} U$: $O(M k) = O(\\tau \\log \\tau)$ Compute $\\Xi Q$: $O(d M k) = O(d \\tau \\log \\tau)$ Compute $R V^\\top$: $O(d k L) = O(d \\tau \\log \\tau)$ Total time complexity: $T_Z = O(d \\tau \\log \\tau).$",
            "$T_{\\text{total}} = \\underbrace{O(d \\tau \\log^2 \\tau)}_{T_{A_k}} + \\underbrace{O(\\tau \\log \\tau)}_{T_D} + \\underbrace{O(d \\tau \\log \\tau)}_{T_Z}.$ Dominant term analysis: As $\\tau \\to \\infty$, $\\log^2 \\tau \\gg \\log \\tau$, so: $T_{\\text{total}} = O(d \\tau \\log^2 \\tau).$"
        ],
        "step_count": 5
    },
    "recuVaTDWqiMHN": {
        "reasoning_steps": [
            "First, the problem is framed using the Neyman-Pearson Hypothesis Testing Framework (Concept_1). This defines the goal: to compute the Type-I error \\(\\alpha_n^{(q)}\\) as 1 - P(decide \\(H_0\\)|\\(H_0\\)) and the Type-II error \\(\\beta_n^{(q)}\\) as P(decide \\(H_0\\)|\\(H_1\\)). The decision rule P(decide \\(H_0\\)) is based on the probability that the test statistic (Hamming distance) is below a threshold, P(d(\\(x_q^n\\), \\(Y^n\\)) ≤ \\(\\lambda_q\\)), thus setting the overall structure for the final formulas.",
            "The probability calculation must account for all possible source sequences \\(x^n\\) that map to a single codeword (e.g., the all-zero codeword). The Law of Total Probability (Theorem_1) is applied to average the outcome over these sequences. This is achieved by partitioning the source sequences by their Hamming weight \\(\\gamma\\) and weighting the conditional probability for each weight class by its prevalence \\(E_\\gamma^{(q)} / N_0^{(q)}\\), which introduces the outer summation structure into the formulas.",
            "To compute the inner conditional probability (for a given source weight \\(\\gamma\\)), the physical relationship between the source, noise, and side information is modeled. The Binary Symmetric Channel (BSC) Model (Concept_2), describing \\(Y^n = X^n \\oplus E^n\\), is used. This model provides the probability of a single, specific noise vector \\(E^n\\) of weight \\(j\\) occurring, which is \\(p^j(1-p)^{n-j}\\). This introduces the core probabilistic term into the summation, evaluated with \\(p_0\\) for \\(H_0\\) and \\(p_1\\) for \\(H_1\\).",
            "For a given source weight \\(\\gamma\\) and a resulting side-information weight \\(\\lambda\\), multiple different noise vectors of a certain weight \\(j\\) could have caused this outcome. We must count these possibilities. The Combinatorial Weight/Distance Identity for GF(2) Vectors (Theorem_2) is used to derive the exact formula for this count, \\(\\Gamma_{\\lambda,j,\\gamma}\\). This term bridges the gap between the probability of a single microscopic event (a specific noise vector) and the probability of the macroscopic observation (the test statistic value).",
            "Finally, all components are assembled to form the complete expressions. The total probability P(decide \\(H_0\\)|\\(H_i\\)) is calculated by summing, over all relevant outcomes (\\(\\lambda\\), \\(\\gamma\\), \\(j\\)), the product of the number of ways an outcome can occur (\\(\\Gamma_{\\lambda,j,\\gamma}\\) from Step 4) and the probability of any one of those ways occurring (\\(p_i^j(1-p_i)^{n-j}\\) from Step 3), and then averaging this sum over all source types (using the weights from Step 2). This final probability is then substituted back into the expressions defined in Step 1 to yield the final formulas for \\(\\alpha_n^{(q)}\\) and \\(\\beta_n^{(q)}\\)."
        ],
        "step_count": 5
    },
    "recuVeOSkxGfQN": {
        "reasoning_steps": [
            "$ \\mathbf r_{CM}=\\frac{1}{M}\\sum_i m_i\\,\\mathbf r_i,\\ \\ \\dot{\\mathbf r}_{CM}=\\frac{1}{M}\\sum_i m_i\\,\\dot{\\mathbf r}_i,\\ \\ \\ddot{\\mathbf r}_{CM}=\\frac{1}{M}\\sum_i m_i\\,\\ddot{\\mathbf r}_i $. In our notation (body-frame): $ \\mathbf r^{B}_{OC},\\ \\dot{\\mathbf r}^{B}_{OC},\\ \\ddot{\\mathbf r}^{B}_{OC} $.",
            "Pose mapping: $ \\mathbf x^{W} = {}^{W}\\!R_B\\,\\mathbf x^{B} $; Time derivative (first-order transport form): $ \\tfrac{d}{dt}\\big({}^{W}\\!R_B\\,\\mathbf x^{B}\\big) = {}^{W}\\!R_B\\big(\\boldsymbol{\\omega}_B\\times \\mathbf x^{B} + \\dot{\\mathbf x}^{B}\\big) $. Thus the CoM velocity: $ {}^{W}\\!\\dot{\\mathbf r}_{OC} = {}^{W}\\!R_B\\big(\\boldsymbol{\\omega}_B\\times \\mathbf r^{B}_{OC} + \\dot{\\mathbf r}^{B}_{OC}\\big) $.",
            "$ {}^{W}\\!\\mathbf a_{OC} = {}^{W}\\!\\ddot{\\mathbf p}_B + {}^{W}\\!R_B\\!\\big[\\dot{\\boldsymbol{\\omega}}_B\\times \\mathbf r^{B}_{OC} + \\boldsymbol{\\omega}_B\\times(\\boldsymbol{\\omega}_B\\times \\mathbf r^{B}_{OC}) + 2\\,\\boldsymbol{\\omega}_B\\times \\dot{\\mathbf r}^{B}_{OC} + \\ddot{\\mathbf r}^{B}_{OC}\\big] $.",
            "$ m_t\\,{}^{W}\\!\\mathbf a_{OC} = \\mathbf U + \\mathbf F_d - m_t g\\,\\mathbf e_3 $. Substitute Step 3 and gather the terms other than thrust $ \\mathbf U $ and gravity $ -m_t g\\,\\mathbf e_3 $ into $ \\mathbf F_d $, yielding $ F_d = -\\,m_t\\,{}^{W}\\!R_B\\!\\big[\\boldsymbol{\\omega}_B\\times(\\boldsymbol{\\omega}_B\\times \\mathbf r^{B}_{OC}) + \\dot{\\boldsymbol{\\omega}}_B\\times \\mathbf r^{B}_{OC} + 2\\,\\boldsymbol{\\omega}_B\\times \\dot{\\mathbf r}^{B}_{OC} + \\ddot{\\mathbf r}^{B}_{OC}\\big] $."
        ],
        "step_count": 4
    },
    "recuVfJj6JEr5w": {
        "reasoning_steps": [
            "The goal is to determine a lower bound on the refinement iterations for a $(k+c-1)$-dimensional Weisfeiler-Leman (WL) algorithm, expressed as a specific function of n, k, and c, as per the instruction.",
            "Following Concept_2, the problem is translated from the domain of the WL algorithm to the cop-robber game. We need to find the number of rounds a robber can survive against $k+c-1$ cops to find the corresponding WL iteration count.",
            "To derive a strong, non-trivial trade-off, the analysis uses the advanced techniques of Graph Compression (Concept_3) and the Compressed Cop-Robber Game (Concept_4). The lower bound is derived from the robber's performance in this specific game variant.",
            "To proceed and get a functional form, a quantitative theorem specifying the robber's survival time is necessary. This theorem, which is missing from the provided concepts, would need to state that a robber can survive for a number of rounds T(n, k, c) against $k+c-1$ cops on a specially constructed graph.",
            "With the robber's survival time T from the missing theorem, we apply the provided Theorem_1. This theorem establishes a direct translation, stating that T rounds of robber survival implies a lower bound of T iterations for the corresponding WL algorithm.",
            "Therefore, the final answer for the WL iteration lower bound would be exactly the function T(n, k, c) provided by the missing quantitative theorem from Step 4. Without that specific theorem, the final functional form cannot be derived."
        ],
        "step_count": 6
    },
    "recuVgrbk7thHs": {
        "reasoning_steps": [
            "The condition $A < \\infty$ implies that $\\sqrt{p^{2}(x)+q^{2}(x)} = O(1/x)$, which is $o(1)$ as $x \\to \\infty$. By **Theorem 1**, the essential spectrum of $L_{p,q}$ is $(-\\infty, \\infty)$. Therefore, any potential eigenvalue would be an embedded eigenvalue. The condition for a number $\\lambda$ to be an eigenvalue is that the amplitude $R(x)$ of its corresponding solution is square-integrable, i.e., $R \\in L^2(0, \\infty)$. The proof proceeds by showing $R \\notin L^2(0, \\infty)$.",
            "Using **Concept 2**, we define the potential's amplitude as $V(x) = \\sqrt{p^2(x)+q^2(x)}$. The initial condition becomes $\\limsup_{x\\rightarrow\\infty} xV(x) = A < 1/2$. From the polar coordinate transformation **Concept 2**, the solution's amplitude $R(x)$ is governed by the differential equation $\\frac{d}{dx}\\ln R(x) = -V(x)\\cos(2\\theta(x)-\\varphi(x))$.",
            "Using **Theorem 3** (the definition of limsup), the condition $\\limsup_{x\\rightarrow\\infty} xV(x) = A$ implies that for any $\\epsilon > 0$, we can find an $x_0$ such that for all $x > x_0$, the inequality $V(x) \\le \\frac{A+\\epsilon}{x}$ holds. We can choose $\\epsilon$ to be small enough such that $A+\\epsilon < 1/2$.",
            "We integrate the equation for $\\ln R(x)$ from $x_0$ to $x$ and apply the bound on $V(x)$ (since $|\\cos(\\cdot)| \\le 1$): $\\ln R(x) = \\ln R(x_0) - \\int_{x_0}^x V(t)\\cos(2\\theta(t)-\\varphi(t))dt \\ge C - \\int_{x_0}^x \\frac{A+\\epsilon}{t} dt$. This yields $\\ln R(x) \\ge C - (A+\\epsilon)\\ln x$, which means for large $x$, $R(x)$ decays no faster than $kx^{-(A+\\epsilon)}$ for some constant $k > 0$.",
            "For $R(x)$ to correspond to an eigenfunction, we need the integral $\\int^\\infty R(x)^2 dx$ to converge. However, we have the lower bound $R(x)^2 \\ge k^2 x^{-2(A+\\epsilon)}$. Since we chose $\\epsilon$ such that $2(A+\\epsilon) < 1$, the exponent is greater than -1, causing the integral $\\int^\\infty x^{-2(A+\\epsilon)} dx$ to diverge. Therefore, $R \\notin L^2(0, \\infty)$, which proves that there are no eigenvalues."
        ],
        "step_count": 5
    },
    "recuVgLcWqyGkY": {
        "reasoning_steps": [
            "To describe representations of Nichols Hopf algebras, start with the definition of irreducible modules in an FTC (concept_1), which are simple objects with no nontrivial subobjects. For $K_n$, these are the 1-dimensional $V_{\\epsilon}$ (trivial action) and $V_{\\bar{K}}$ (sign action).",
            "Next, using the concept of projective covers (concept_2), which are projective indecomposable modules surjecting onto irreducible modules, we identify $P_{\\epsilon}$ and $P_{\\bar{K}}$ as projective covers of $V_{\\epsilon}$ and $V_{\\bar{K}}$, constructed as submodules of $K_n$ generated by $\\Lambda^{*}E \\cdot (1 + K)$ and $\\Lambda^{*}E \\cdot (1 - K)$ respectively.",
            "Finally, based on the property of tensor products in FTCs (concept_3) – that tensor products of projectives are projective – we derive fusion rules: $V_{\\bar{K}} \\otimes P_{\\epsilon}$ (a projective module) must be $P_{\\bar{K}}$ (matching dimension and Hom properties), and tensor products of $P_{\\epsilon}$ and $P_{\\bar{K}}$ decompose into equal sums of both projectives due to dimension counting and linearity of Hom functors."
        ],
        "step_count": 3
    },
    "recuVlpJt54hyn": {
        "reasoning_steps": [
            "apply the weighted Sobolev embedding theorem with $r=2N/(N-3)$ and $p=\\frac{2N}{N-1}$ to $h$ to obtain the estimate: $\\left\\|h-h_\\Omega\\right\\|_{L^{2N/(N-3)}}\\leq C\\left\\|\\nabla h\\right\\|_{L^{2N/(N-1)}}$.",
            "apply the weighted Sobolev embedding theorem with $r=2N/(N-1)$, $p=2$, $\\alpha=1/2$ to $\\nabla h$ to obtain the estimate: $\\left\\|\\nabla h\\right\\|_{L^{2N/(N-1)}}\\leq C\\left\\|\\delta_{\\Gamma}^{1/2}\\nabla^2h\\right\\|_{L^2}$.",
            "choose $p=2N/(N-3)$ in Theorem 1 to obtain a stability inequality in terms of the diviation $\\left\\|h-h_\\Omega\\right\\|_{L^{2N/(N-3)}}^{2/(N-1)}$.",
            "conclude with the three inequalities establishe in step 1, 2, 3 to obtain a primary inequality $\\rho_e-\\rho_i\\leq C\\left\\|\\delta_\\Gamma^{1/2}\\nabla^2h\\right\\|_{L^2}^{2/(N-1)}$.",
            "apply the given integral inequality that $u$ satisfies and the lower bound on $-u$, together with the inequality established in step 4 to obtain $\\rho_e-\\rho_i\\leq C\\delta^{1/(N-1)}$."
        ],
        "step_count": 5
    },
    "recuVn157N4NTm": {
        "reasoning_steps": [
            "The problem asks for the threshold value of $\\delta$ where an adversary's string $x'$ can be simultaneously close to two valid codewords, $x_0$ and $x_1$. The condition is given by $d(x', x_0) \\le \\delta \\cdot n$ and $d(x', x_1) \\le \\delta \\cdot n$.",
            "From pseudorandomness, the expected distance between the two valid codewords is given as $E[d(x_0, x_1)] = n/2$ (concept_3). We can consider this the typical separation distance.",
            "An adversary wants to find an $x'$ that satisfies the condition. The most effective $x'$ would be a 'midpoint' string that minimizes its distance to both $x_0$ and $x_1$ (theorem_1).",
            "From the triangle inequality, we have $d(x_0, x_1) \\le d(x_0, x') + d(x', x_1)$ (concept_2).",
            "When $x'$ is constructed as a midpoint, we have $d(x_0, x') \\approx d(x_1, x')$ and their sum is approximately $d(x_0, x_1)$. Therefore, $d(x_0, x') \\approx d(x_0, x_1) / 2$.",
            "Substituting the expected distance from concept_3, we get $d(x_0, x') \\approx (n/2) / 2 = n/4$. Similarly, $d(x_1, x') \\approx n/4$.",
            "The robustness threshold is the point where this optimally crafted string first meets the decoding condition. We set the distance of the crafted string equal to the maximum tolerable distance: $n/4 = \\delta \\cdot n$.",
            "Solving this equation for $\\delta$ yields $\\delta = 1/4$. This is the critical threshold; for any $\\delta \\ge 1/4$, such an ambiguous string $x'$ can be constructed."
        ],
        "step_count": 8
    },
    "recuVn5VMX99A8": {
        "reasoning_steps": [
            "Microcanonical ensemble: the multimode nonlinear waveguide system is closed (no pump/loss) and ergodic. In the weak-nonlinearity regime, the linear-part energy U is quasi-invariant alongside the strictly conserved total power P.",
            "From the discrete nonlinear Schrödinger equation (DNLS) or the appropriate equations of motion, recognize the Hamiltonian structure and write explicit expressions for the conserved total power P and the Hamiltonian H.",
            "Expand the field in the linear supermode basis to define modal amplitudes c_j; decompose the Hamiltonian as H = H_L + H_{\\mathrm{NL}}. While H_NL depends on the specific nonlinearity. one can define a quasi-invariant internal energy U = \\sum_j \\varepsilon_j |c_j|^2, in weak nonlinearity, where \\varepsilon_j are the eigenvalues of the linear supermode Hamiltonian.",
            "Macrostates and entropy: use modal occupations n_j \\equiv |c_j|^2 as macro-variables and adopt the optical entropy S = \\sum_j \\ln(n_j), (up to an additive constant), since phases are uniformly distributed under ergodicity and do not affect macrostates.",
            "Constrained maximization: maximize S subject to \\sum_j n_j = P, \\quad \\sum_j \\varepsilon_j n_j = U, using Lagrange multipliers \\alpha, \\beta. Define the Lagrangian \\mathcal{L} = \\sum_j \\ln(n_j) - \\alpha \\left( \\sum_j n_j - P \\right) - \\beta \\left( \\sum_j \\varepsilon_j n_j - U \\right).",
            "Stationarity:\\frac{\\partial \\mathcal{L}}{\\partial n_j} = 0, hence n_j = \\frac{1}{\\alpha + \\beta \\varepsilon_j}. This is the Rayleigh–Jeans form for modal occupancies. Determine \\alpha, \\beta from the invariants P and U; ."
        ],
        "step_count": 6
    },
    "recuVoRgdoVCJJ": {
        "reasoning_steps": [
            "The process begins by using an Earning-Restricted (ER) Competitive Equilibrium (Concept_1) to generate a fractional allocation, which is then rounded. This allows for the partitioning of chores and agents into low-payment/high-payment groups ($N_0$ and $N_H$) (Concept_2).",
            "The analysis focuses on finding the worst-case envy, which occurs with agents in the $N_0$ group. This is because their bundles have the highest potential payment relative to the guaranteed minimum payment for any other agent.",
            "The key to quantifying this envy is Concept_4, which states that for any agent, the ratio of their disutility for two different bundles is equal to the ratio of the bundles' payments.",
            "Theorem_2 provides the numerator for this worst-case ratio: the maximum possible payment for an agent's bundle in group $N_0$ is 2.",
            "To complete the ratio, we need the denominator, which is the guaranteed minimum payment for any agent's bundle. In the underlying allocation framework, this minimum payment is established as 1/2.",
            "Using these bounds, the constant factor $\\alpha$ is calculated by taking the ratio of the maximum possible payment to the minimum possible payment: $\\alpha = \\frac{\\text{max_payment}}{\\text{min_payment}} = \\frac{2}{1/2} = 4$.",
            "For the remaining agents in group $N_H$, their envy is resolved to a guarantee that is at least as good as this worst-case bound by using a combination of EFX re-allocations (Theorem_1) and ordered Chore Swaps (Concept_3, Theorem_3).",
            "Since the highest possible envy ratio across all scenarios is 4, it is concluded that a 4-EFX allocation is guaranteed to exist."
        ],
        "step_count": 8
    },
    "recuVqCOpFM6Fu": {
        "reasoning_steps": [
            "First, the \"monotonicity conditions of initial and boundary data\" in the Instruction correspond to concept_2, which is an essential premise for constructing and analyzing weak solutions in the document, laying the foundation for subsequent research on weak solutions.",
            "For the existence of global weak solutions, the document uses concept_3 (Direct BV Estimate) to directly analyze and bound the total variation of the solution, obtain key a priori estimates such as the boundedness of the solution and its derivatives, and thus derive the existence of the global weak solution.",
            "For the uniqueness and continuous dependence of weak solutions, concept_5 (Kruzkov-type \\(L^1\\)-Contraction Argument) is adopted: by constructing entropy functions and test functions, an \\(L^1\\)-norm contraction estimate is established, proving that the difference between two weak solutions is bounded by the difference of initial and boundary data, thereby verifying the uniqueness and continuous dependence of the weak solution.",
            "For the interior smoothness of weak solutions, the document first uses concept_4 (Crocco Transformation) to simplify the original Prandtl system into an ultraparabolic equation through specific variable substitutions, then applies concept_1 (Regularity Theory for Ultraparabolic Equations) — the \"weak\" Poincaré inequality connects the solution with its average value, and the oscillation estimate proves that the solution's oscillation decays with the domain scale, jointly deriving the smoothness of the weak solution in the interior of the domain."
        ],
        "step_count": 4
    },
    "recuVr8vkzTFXk": {
        "reasoning_steps": [
            "An attacker's goal, as stated in the instruction, is to create a single string $c'$ that causes decoding ambiguity for a single-bit PRC. To begin, the attacker considers the codewords for both possible messages, '0' and '1', as established by concept_1. Let's call these representative codewords $c_0$ and $c_1$.",
            "Concept_2 is the critical insight: $c_0$ and $c_1$ are computationally indistinguishable from independent, uniformly random binary strings of length $n$. A direct consequence of this property is that they are expected to agree on approximately half of their bit positions ($n/2$) and disagree on the other half ($n/2$). Their mutual Hamming distance is therefore approximately $n/2$.",
            "The attacker now constructs the malicious string $c'$. The strategy is to find a point in the middle of $c_0$ and $c_1$. For the $n/2$ positions where $c_0$ and $c_1$ agree, $c'$ is set to that same value. For the $n/2$ positions where they disagree, the attacker sets the bits of $c'$ to match $c_0$ in the first half of these positions ($n/4$ bits) and to match $c_1$ in the second half ($n/4$ bits).",
            "By this construction, we can calculate the distance of $c'$ from the original codewords. The string $c'$ differs from $c_0$ only in those positions where it was forced to match $c_1$. This occurred in $n/4$ positions. Thus, the fractional distance is $(n/4) / n = 1/4$. Symmetrically, the fractional distance from $c'$ to $c_1$ is also $1/4$.",
            "Now, we apply the failure condition defined in concept_3. The attacker has successfully produced a string $c'$ that is simultaneously within a fractional distance of $1/4$ of a codeword for '0' and a codeword for '1'.",
            "If a PRC were to claim a robustness of $\\delta = 1/4$, this string $c'$ would meet the condition for being a valid corruption of both $c_0$ and $c_1$. However, concept_4 states the decoder must output a single, unique result. It cannot resolve this ambiguity.",
            "Therefore, any PRC claiming a robustness of $\\delta \\ge 1/4$ can be broken by this attack. This means that achieving a robustness of $1/4$ (or higher) is theoretically impossible. The highest value of $\\delta$ that is impossible to achieve begins exactly at this boundary."
        ],
        "step_count": 7
    },
    "recuVthcAnXt8x": {
        "reasoning_steps": [
            "Leverage Concept 2 to Clarify System Properties and Error Definitions First, we use \\textbf{Concept 2 (Property of 2-DOF underactuated systems)} to establish foundational system characteristics: - The unactuated mass matrix $M_{uu}$ is positive-definite ($M_{uu} > 0$), and its time derivative satisfies $\\dot{M}_{uu}/2 = C_{uu}$ (a critical relation for simplifying energy derivatives). - The coupling term $M_{ua} \\neq 0$ (ensures interaction between actuated and unactuated dynamics, essential for transmitting control effects to $q_u$). - The gravity term for the unactuated variable satisfies $G_u = 0 \\iff e_u = 0$ (links the potential energy state to the unactuated error $e_u = q_u - q_{ud} = q_u$, since $q_{ud} = 0$). - The potential energy derivative is $\\dot{U} = \\dot{q}_u G_u = \\dot{e}_u G_u$ (as $\\dot{q}_u = \\dot{e}_u$, directly relating unactuated velocity error to potential energy change). For the actuated side, the reference $q_{ar}(t)$ has finite-time convergence ($q_{ar}(t) = q_{ad}, \\dot{q}_{ar}(t) = \\ddot{q}_{ar}(t) = 0$ for $t > t_1$), so the actuated error $\\dot{e}_a = \\dot{q}_a - \\dot{q}_{ar} = \\dot{q}_a$ and $\\ddot{e}_a = \\ddot{q}_a$ for $t > t_1$—this simplifies subsequent trajectory law analysis.",
            "Apply Concept 3 to Construct the Lyapunov Function Per \\textbf{Concept 3 (Energy-based control method)}, the Lyapunov function for underactuated systems typically combines mechanical energy (kinetic + potential) and positive functions of auxiliary variables (e.g., $\\rho_a, \\rho_c, \\Delta$). We construct the candidate Lyapunov function as: \\[ V = \\underbrace{\\frac{1}{2}M_{uu}\\dot{e}_u^2 + U}_{\\text{Mechanical Energy (from Concept 3)}} + \\underbrace{\\frac{1}{2}k_1\\rho_a^2 + \\frac{1}{2}k_3\\rho_c^2 + \\frac{1}{2k_2}\\Delta^2}_{\\text{Positive Auxiliary Term (from Concept 3)}} \\] - **Non-negativity of $V$**: $M_{uu} > 0$ (Concept 2) implies $\\frac{1}{2}M_{uu}\\dot{e}_u^2 \\geq 0$; potential energy $U \\geq 0$ (bounded below for mechanical systems); and the auxiliary term is positive since $k_1, k_2, k_3 > 0$ and all terms are squared. Thus, $V \\geq 0$ (non-negative definite), and $V = 0$ only if $\\dot{e}_u = 0, U = 0$ (hence $G_u = 0 \\iff e_u = 0$ by Concept 2), $\\rho_a = 0, \\rho_c = 0, \\Delta = 0$.",
            "Compute $\\dot{V}$ and Prove Its Non-Positivity To apply LaSalle’s Theorem, we first derive $\\dot{V}$ and show $\\dot{V} \\leq 0$, using the system’s kinematic model and the given online trajectory generation law: 1. **Derivative of Mechanical Energy**: Using $\\dot{M}_{uu}/2 = C_{uu}$ (Concept 2) and $\\dot{U} = \\dot{e}_u G_u$ (Concept 2), we expand the mechanical energy derivative:  \\[  \\dot{(\\frac{1}{2}M_{uu}\\dot{e}_u^2 + U)} = C_{uu}\\dot{e}_u^2 + M_{uu}\\dot{e}_u\\ddot{e}_u + \\dot{e}_u G_u  \\]  From the 2-DOF Euler-Lagrange equation (unactuated row, $\\tau_u = 0$), we have $M_{uu}\\ddot{e}_u + C_{uu}\\dot{e}_u + G_u = -M_{ua}\\ddot{e}_a - C_{ua}\\dot{e}_a$ (since $\\ddot{q}_u = \\ddot{e}_u$ and $\\dot{q}_a = \\dot{e}_a$ for $t > t_1$). Substituting this into the mechanical energy derivative eliminates $C_{uu}\\dot{e}_u^2$ and $\\dot{e}_u G_u$, leading to:  \\[  \\dot{(\\text{Mechanical Energy})} = -M_{ua}\\dot{e}_u\\ddot{e}_a - C_{ua}\\dot{e}_u\\dot{e}_a  \\] 2. **Derivative of Auxiliary Term**: Using the given differential equations for $\\rho_a, \\rho_c, \\Delta$:  - $\\dot{\\Delta} = k_2 M_{ua}\\dot{e}_u$ (given) implies $\\frac{\\Delta\\dot{\\Delta}}{k_2} = M_{ua}\\dot{e}_u\\Delta$;  - Rearranging $\\rho_a(\\dot{e}_u M_{ua}\\dot{e}_a + \\dot{e}_u C_{ua}e_a) + \\dot{\\rho}_a(k_1 + 2\\dot{e}_u M_{ua}e_a) = 0$ (given) gives $k_1\\rho_a\\dot{\\rho}_a = -\\rho_a^2\\dot{e}_u(M_{ua}\\dot{e}_a + C_{ua}e_a) - 2\\rho_a\\dot{\\rho}_a\\dot{e}_u M_{ua}e_a$;  - Rearranging $\\rho_c\\dot{e}_u C_{ua}\\Delta - \\dot{\\rho}_c(k_3 - 2\\dot{e}_u M_{ua}\\Delta) = 0$ (given) gives $k_3\\rho_c\\dot{\\rho}_c = \\rho_c^2\\dot{e}_u C_{ua}\\Delta + 2\\rho_c\\dot{\\rho}_c\\dot{e}_u M_{ua}\\Delta$. 3. **Combine Derivatives for $\\dot{V}$**: Using the online trajectory law $\\dot{e}_a = -\\rho_a^2 e_a + \\rho_c^2\\Delta$ (from $\\dot{q}_a = \\dot{q}_{ar} - \\rho_a^2 e_a + \\rho_c^2\\Delta$ and $\\dot{q}_{ar} = 0$ for $t > t_1$), substitute $\\ddot{e}_a$ (derived from $\\dot{e}_a$) and the auxiliary term derivatives into $\\dot{V}$. After canceling cross terms (e.g., $-C_{ua}\\dot{e}_u\\dot{e}_a$ and $\\rho_c^2\\dot{e}_u C_{ua}\\Delta$), the key intermediate result is:  \\[  \\dot{V} = -\\rho_a^2\\dot{e}_u^2(M_{ua}e_a)^2 - \\rho_c^2\\dot{e}_u^2(M_{ua}\\Delta)^2 \\leq 0  \\]  This is non-positive because $M_{ua} \\neq 0$ (Concept 2) and all terms are products of squares and negative signs.",
            "Apply Theorem 1 (LaSalle’s Invariance Theorem) to Prove Convergence We verify the conditions of LaSalle’s Theorem and identify the largest invariant set: 1. **Autonomous System**: For $t > t_1$, $\\dot{q}_{ar} = \\ddot{q}_{ar} = 0$, so the system dynamics (governed by the trajectory law and Euler-Lagrange equations) is autonomous. 2. **Compact Positively Invariant Set $\\Omega$**: Define $\\Omega = \\{x \\mid V \\leq V_0\\}$ for some $V_0 > 0$. Since $\\dot{V} \\leq 0$, solutions starting in $\\Omega$ never leave it (positive invariance), and $\\Omega$ is compact (bounded by $V \\leq V_0$). 3. **Invariant Set $E$ and $M$**: Let $E = \\{x \\in \\Omega \\mid \\dot{V} = 0\\}$. From $\\dot{V} = 0$, we get $\\rho_a\\dot{e}_u e_a = 0$ and $\\rho_c\\dot{e}_u\\Delta = 0$ (since $M_{ua} \\neq 0$). The largest invariant set $M \\subset E$ is analyzed as follows:  - If $\\dot{e}_u \\neq 0$, then $\\rho_a e_a = 0$ and $\\rho_c\\Delta = 0$. However, $\\dot{e}_u \\neq 0$ would imply $\\dot{U} = \\dot{e}_u G_u \\neq 0$ (Concept 2), leading to unbounded $U$ (contradicting $V \\leq V_0$). Thus, $\\dot{e}_u = 0$.  - If $\\dot{e}_u = 0$, then $\\dot{\\Delta} = 0$ (from $\\dot{\\Delta} = k_2 M_{ua}\\dot{e}_u$), $\\dot{\\rho}_a = 0$ (from $k_1\\rho_a\\dot{\\rho}_a = 0$ and $k_1 > 0$), and $\\dot{\\rho}_c = 0$ (from $k_3\\rho_c\\dot{\\rho}_c = 0$ and $k_3 > 0$). For invariance, $\\dot{e}_a = 0$ (otherwise $\\ddot{e}_a \\neq 0$, implying $G_u \\neq 0$ and $e_u \\neq 0$ by Concept 2), so $e_a = 0$ (from $\\dot{e}_a = -\\rho_a^2 e_a + \\rho_c^2\\Delta$) and $\\Delta = 0$. Finally, $\\dot{e}_u = 0$ and $G_u = 0$ (from bounded $U$) imply $e_u = 0$ (Concept 2). Thus, $M = \\{x \\mid e_u = 0, \\dot{e}_u = 0, e_a = 0, \\dot{e}_a = 0\\}$. By LaSalle’s Theorem, all solutions starting in $\\Omega$ approach $M$ as $t \\to \\infty$, proving $\\dot{e}_a \\to 0, e_a \\to 0, \\dot{e}_u \\to 0, e_u \\to 0$. The conclusion is therefore true."
        ],
        "step_count": 4
    },
    "recuVuXTqRyi9f": {
        "reasoning_steps": [
            "The overall goal is to ensure the final error of the aggregated predictor is at most $\\epsilon$. Due to the error metric's convexity (Concept_5), this final error is bounded by the average of the excess losses of the individual predictors from each iteration. Thus, our goal is to bound this average excess loss.",
            "For any step $t$, the excess loss is upper-bounded by the instantaneous PGD regret (Concept_1).",
            "We then apply the standard PGD analysis (Concept_2) to this regret term, yielding an upper bound on the single-step excess loss: $ExcessLoss_t \\le \\frac{1}{2\\eta}(\\|w_{t}-w\\|_{2}^{2}-\\|w_{t+1}-w\\|_{2}^{2})+\\frac{\\eta}{2}\\|\\nabla_{w}f_t(w_t)\\|_{2}^{2}$.",
            "We sum this inequality over all $T$ steps. The term $\\sum_{t=0}^{T-1}(\\|w_{t}-w\\|_{2}^{2}-\\|w_{t+1}-w\\|_{2}^{2})$ forms a telescoping sum. Using the initial conditions and weight space constraints from Concept_4 ($w_0=0, \\|w\\| \\le R$), this sum is bounded by $\\|w_0 - w\\|_2^2 \\le R^2$.",
            "The sum of the second term is bounded using the uniform gradient bound from Concept_3: $\\sum_{t=0}^{T-1} \\frac{\\eta}{2}\\|\\nabla_{w}f_t(w_t)\\|_{2}^{2} \\le \\frac{T\\eta L^2}{2}$.",
            "Combining these results gives a bound on the sum of excess losses: $\\sum_{t=0}^{T-1} ExcessLoss_t \\le \\frac{R^2}{2\\eta} + \\frac{T\\eta L^2}{2}$. The average excess loss is therefore bounded by $\\frac{1}{T}\\sum ExcessLoss_t \\le \\frac{R^2}{2\\eta T} + \\frac{\\eta L^2}{2}$.",
            "This upper bound on the average excess loss is minimized by choosing the learning rate $\\eta = \\frac{R}{L\\sqrt{T}}$. Substituting this choice of $\\eta$ gives an average excess loss of at most $\\frac{LR}{\\sqrt{T}}$.",
            "To guarantee the final error is at most $\\epsilon$, we set this bound to be less than or equal to $\\epsilon$: $\\frac{LR}{\\sqrt{T}} \\le \\epsilon$.",
            "Solving the inequality for $T$ provides the sufficient number of iterations: $T \\ge \\frac{L^2 R^2}{\\epsilon^2}$."
        ],
        "step_count": 9
    },
    "recuVvsvxc5qz7": {
        "reasoning_steps": [
            "According to the concept 1, the Pd species exist in the form of Pd0 and Pd2+. Therefore, the higher Pd2+ proportion means the lower Pd0 proportion. Pd4.5Se has the higher Pd2+ proportion in comparison with Pd7Se2, suggesting the Pd7Se2 possess the higher Pd0 proportion. Moreover, the (3-11) crystal face of Pd7Se2 is conducive to the formation of Pd0 species.",
            "According to the concept 2, the higher proportion of Pd0 leads to more Pd active sites lowering the reaction barrier, thus the higher proportion of Pd0 leads to the higher activity. Therefore, the Pd7Se2 with the higher Pd0 proportion provides the highest activity for ethylene glycol electrooxidation reaction.",
            "According to the concept 3, the lower d-bands center suggests the weaker adsorption of intermediates species. Thus, the Pd7Se2 with the lower d-bands center exhibits the weaker adsorption of intermediates species. This is favorable of the oxidation of CO and the release of CO2, thereby facilitating ethylene glycol electrooxidation reaction"
        ],
        "step_count": 3
    },
    "recuVxqriUzjYe": {
        "reasoning_steps": [
            "Provide the vertical momentum equation for a single particle based on theorem_1 (particle vertical momentum equation) \\frac{\\mathrm{d}w_p}{\\mathrm{d}t} = \\frac{w_f - w_p}{\\tau_p} - g. Here, w_f is the instantaneous fluid velocity at the particle's position, \\tau_p is the particle relaxation time, and g is the gravitational acceleration. This equation ignores the added mass, Basset force, and lift force.",
            "Define the instantaneous slip velocity according to concept_1 (definition of slip velocity) w_s \\equiv w_f - w_p, Rewrite the vertical momentum equation of a single particle as \\frac{\\mathrm{d}w_p}{\\mathrm{d}t} = \\frac{w_s}{\\tau_p} - g. Explicitly take the slip velocity w_s as the direct driving force for the drag force on the particles.",
            "According to concept_2 (conservation of the second moment of the particle phase space PDF) P(z,w,t) = \\langle \\delta(z_p - z)\\,\\delta(w_p - w) \\rangle, Multiply it by w^2 and integrate all w to obtain the conservation equation of the second moment of the vertical height \\frac{\\mathrm{d}}{\\mathrm{d}z}\\bigl[\\varrho\\langle w_p^3 \\rangle_z\\bigr]-2\\varrho\\langle a_p w_p \\rangle_z = 0, Among them, \\varrho is the local particle number density, and a_p is the conditional acceleration.",
            "Substitute the vertical momentum equation of a single particle into the second-order moment conservation equation and obtain it using the instantaneous slip velocity. \\frac{\\mathrm{d}}{\\mathrm{d}z}\\bigl[\\varrho\\langle w_p^3 \\rangle_z\\bigr]-\\frac{2\\varrho}{\\tau_p}\\langle w_s w_p \\rangle_z+2\\varrho g\\langle w_p \\rangle_z = 0, Couple the third-order moment of particles with the slip-particle velocity correlation function.",
            "Organize to obtain the equation for the variance of the slip velocity: \\langle w_s^{\\prime 2}\\rangle_z = \\bigl(\\langle w_f^{\\prime 2}\\rangle_z - \\langle w_p^{\\prime 2}\\rangle_z\\bigr) +R_t + R_g",
            "According to concept_3 (local homogeneous approximation) \\bigl|R_t\\bigr| + \\bigl|R_g\\bigr| \\ll \\bigl|\\langle w_f^{\\prime 2}\\rangle_z - \\langle w_p^{\\prime 2}\\rangle_z\\bigr|, Therefore, the higher-order terms R_t,R_g can be regarded as small quantities.",
            "Final Conclusion: \\langle w_s^{\\prime 2}\\rangle_z \\approx \\langle w_f^{\\prime 2}\\rangle_z - \\langle w_p^{\\prime 2}\\rangle_z. This indicates that within the logarithmic layer, the variance of the slip velocity is mainly determined by the difference between the variance of the local \"observed\" fluid fluctuating velocity and the variance of the particle fluctuating velocity."
        ],
        "step_count": 7
    },
    "recuVyZggWBICl": {
        "reasoning_steps": [
            "First, according to theorem_1, we start from the action of nonlinear electrodynamics (NLED), take the invariant \\(\\mathcal F=\\tfrac14 F_{\\mu\\nu}F^{\\mu\\nu}\\) and define \\(L_{\\mathcal F}\\equiv \\partial L/\\partial \\mathcal F\\), thereby fixing the subsequent dependence of the potentials on the charge parameters \\((q,p)\\).",
            "Then, based on theorem_2 (Wald formalism), we obtain the first-law–like relation \\[dM=\\Phi_e\\,dQ_e+\\Phi_m\\,dQ_m .\\]",
            "Next, using concept_1, we write the electric and magnetic potentials as radial integrals and obtain the partial-derivative identities: \\[\\Phi_e=\\int_{r_+}^{\\infty}\\frac{q}{r^2\\,L_{\\mathcal F}}\\,dr,\\qquad \\Phi_m=\\int_{r_+}^{\\infty}\\frac{p\\,L_{\\mathcal F}}{r^2}\\,dr,\\qquad \\frac{\\partial M}{\\partial Q_e}=\\Phi_e,\\ \\ \\frac{\\partial M}{\\partial Q_m}=\\Phi_m .\\]",
            "Finally, for regular black holes, following concept_2 we drop the horizon lower limit (\\(r_+\\!\\to\\!0\\)), integrate the exact differential \\(dM\\) over the charge space (taking the uncharged reference \\(M(0,0)=0\\)), and obtain two equivalent charge-space expressions for the mass: \\[\\boxed{\\text{Electric: }\\; M(Q_e,0)=\\int_{0}^{Q_e}\\!\\left(\\int_{0}^{\\infty}\\frac{q}{r^2\\,L_{\\mathcal F}(q,0;\\,r)}\\,dr\\right)\\,dq}\\] \\[\\boxed{\\text{Magnetic: }\\; M(0,Q_m)=\\int_{0}^{Q_m}\\!\\left(\\int_{0}^{\\infty}\\frac{p\\,L_{\\mathcal F}(0,p;\\,r)}{r^2}\\,dr\\right)\\,dp}\\]"
        ],
        "step_count": 4
    },
    "recuVB5yu84DBb": {
        "reasoning_steps": [
            "Our goal is to derive an upper bound for $H(X|Y)$ by decomposing it into two parts, as specified by the instruction. We introduce an auxiliary binary random variable $W = 1\\{X \\neq Y\\}$ to represent the error event.",
            "In the information-event diagram framework, the conditional entropy $H(X|Y)$ corresponds to the measure of the set difference $\\tilde{G}(X) \\setminus \\tilde{G}(Y)$. We can decompose this region into two disjoint parts: a part that intersects with the error information $\\tilde{G}(W)$, and a part that intersects with the event of an error occurring, $G(X \\neq Y)$. Let's call their measures $a$ and $d$ respectively, so that $H(X|Y) = a + d$.",
            "First Component (Entropy of the error event): The measure $a$ is part of the total measure of $\\tilde{G}(W)$, which equals $H(W)$. The full region for $\\tilde{G}(W)$ also includes regions corresponding to the mutual information $I(Y;W)$. Using Concept_5, we know $I(Y;W) \\ge 0$, which leads to the inequality $a \\le H(W)$.",
            "We now use Concept_3, which states that the entropy of the binary indicator variable $W$ is the binary entropy function of its underlying event's probability. This gives us $H(W) = H_b(\\mathbb{P}(X \\neq Y))$. Therefore, the bound on our first component is $a \\le H_b(\\mathbb{P}(X \\neq Y))$.",
            "Second Component (Expected entropy given an error): The measure $d$ represents the information in $X$ not shared with $Y$, conditioned on the event $\\{X \\neq Y\\}$. Using Concept_4, we can write its measure precisely as $d = \\mathbb{P}(X \\neq Y) H(X|Y, X \\neq Y)$.",
            "To bound this second component, we apply Concept_6. Given that $Y=y$ and an error has occurred ($X \\neq Y$), the random variable $X$ can take any of its $|\\mathcal{X}|$ values except for $y$. Thus, its support size is $|\\mathcal{X}|-1$, and its entropy is upper-bounded by $\\log(|\\mathcal{X}|-1)$.",
            "This provides the bound on our second component: $d \\le \\mathbb{P}(X \\neq Y) \\log(|\\mathcal{X}|-1)$.",
            "Combining the Components: Finally, we sum the upper bounds for the two components to get the total upper bound for the conditional entropy: $H(X|Y) = a + d \\le H_b(\\mathbb{P}(X \\neq Y)) + \\mathbb{P}(X \\neq Y) \\log(|\\mathcal{X}|-1)$."
        ],
        "step_count": 8
    },
    "recuVCUpwdtUxX": {
        "reasoning_steps": [
            "Direct computation shows that the quiver Hilbert series is \\frac{(1-t^{8/3})}{(1-t^{2/4})(1-t^{4/3})^3}.",
            "From concept 2, a weighted homogeneous affine hypersurface singularity in C^4 with weights 1/2, 1/2, 1/2, 1/4 has a graded coordinate ring with this Hilbert series.",
            "A possible choice is x^2+y^2+z^2+w^4=0 (Not unique)"
        ],
        "step_count": 3
    },
    "recuVD1HLTDcnu": {
        "reasoning_steps": [
            "First, according to concept_1, require that the scalar curvature \\(R(r)\\) be expandable at \\(r=0\\) with no negative powers: \\[R(r)=\\sum_{n=2}^{\\infty} C_n\\, r^{\\,n-2}.\\]",
            "Substituting the relation in theorem_2, \\[R(r)=\\frac{2 r\\, m''(r)+4 m'(r)}{r^2},\\] and integrating twice with respect to \\(r\\), one obtains the general solution \\[m(r)= M - \\frac{Q^2}{2r} + \\frac12 \\sum_{n=2}^{\\infty} \\frac{C_n\\, r^{\\,n+1}}{(n+1)(n+2)},\\] where \\(M\\) and \\(Q\\) are integration constants corresponding to the exterior Schwarzschild and Reissner–Nordström–type “singular” contributions, respectively.",
            "Next, using theorem_1, \\[ds^2=-f(r)\\,dt^2+\\frac{dr^2}{f(r)}+r^2 d\\Omega^2,\\qquad f(r)=1-\\frac{2\\,m(r)}{r},\\] plugging the above \\(m(r)\\) in gives \\[f(r)=1-\\frac{2M}{r}+\\frac{Q^2}{r^2} -\\frac12\\sum_{n=2}^{\\infty}\\frac{C_n\\,r^{\\,n}}{(n+1)(n+2)}.\\] Since the summation starts at \\(n=2\\) (i.e., \\(\\mathcal O(r^2)\\)), it remains finite as \\(r\\to0\\); whereas \\[\\lim_{r\\to0}\\frac{Q^2}{r^2}=+\\infty,\\qquad \\lim_{r\\to0}\\frac{M}{r}=+\\infty,\\] introduce \\(r^{-2}\\) and \\(r^{-1}\\) divergences in \\(g_{tt}=-f(r)\\) and \\(g_{rr}=1/f(r)\\), respectively. To ensure a regular center (finite metric components at \\(r=0\\)), these divergent terms must be removed, which forces \\(Q=0\\) and \\(M=0\\).",
            "Therefore, under the regularity constraint of concept_1, together with theorem_2 and theorem_1 and the limiting analysis, we obtain \\[\\boxed{Q=0,\\; M=0}.\\]"
        ],
        "step_count": 4
    },
    "recuVDosjnzWmq": {
        "reasoning_steps": [
            "Goal: write down the expression for the binary-collision contribution to the initial local entropy density stextbin(x,y,etas)s_{\\text{bin}}(x,y,\\eta_s) in a (3+1)D hydrodynamic initialization, to be stated in one sentence.",
            "Target identification (Concept_1): identify stextbin(x,y,etas)s_{\\text{bin}}(x,y,\\eta_s) as the binary-collision contribution to the initial local entropy density.",
            "Factorization & scaling (Concept_2–3): adopt a factorization ansatz into a longitudinal envelope and a transverse profile with overall scaling K/tau0K/\\tau_0 to ensure correct normalization/dimensions at formation time tau0\\tau_0.",
            "Transverse profile (Concept_4): build from binary-collision points with Gaussian smearing width sigmar\\sigma_r: \\\\(\\displaystyle \\tilde s_{\\text{bin}}(x,y)=\\sum_{i\\in \\text{bin}}\\frac{1}{2\\pi\\sigma_r^2}\\exp\\!\\Big[-\\frac{(x-x_i)^2+(y-y_i)^2}{2\\sigma_r^2}\\Big].\\\\)",
            "Longitudinal envelope (Concept_5): choose a Gaussian centered at \\\\eta_w with width \\\\sigma_w and support enforced by a step function: \\\\(\\displaystyle H_s^{\\text{bin}}(\\eta_s)=\\exp\\!\\Big[-\\frac{(\\eta_s-\\eta_w)^2}{2\\sigma_w^2}\\Big],\\theta(|\\eta_s|-\\eta_w).\\\\)",
            "Assembly (Concept_7): combine the longitudinal envelope and transverse profile with the scale factor to obtain the one-line result \\\\(s_{\\text{bin}}(x,y,\\eta_s)=\\dfrac{K}{\\tau_0}\\,H_s^{\\text{bin}}(\\eta_s)\\,\\tilde s_{\\text{bin}}(x,y).\\\\)"
        ],
        "step_count": 6
    },
    "recuVDZy1iiliU": {
        "reasoning_steps": [
            "estimate $\\left\\langle \\nabla\\nu(\\nabla_\\Gamma u), \\nabla_\\Gamma u\\right\\rangle$. By theorem 1, we may assume WLOG that $\\nabla\\nu$ is a diagonal matrix, so theorem 2 implies that \\[\\min \\left\\{\\kappa_1,\\dots,\\kappa_{N-1}\\right\\}|\\nabla_\\Gamma u|^2\\leq\\left\\langle \\nabla\\nu(\\nabla_\\Gamma u), \\nabla_\\Gamma u\\right\\rangle\\leq \\max \\left\\{\\kappa_1,\\dots,\\kappa_{N-1}\\right\\}|\\nabla_\\Gamma u|^2.\\] Since $\\Omega$ is a $C^2$ domain, each $\\kappa_i$ can be controlled by the radius of the uniform interior ball and exterior ball, thus $\\left|\\left\\langle \\nabla\\nu (\\nabla_\\Gamma u),\\nabla_\\Gamma u\\right\\rangle\\right|\\leq C(r_i, r_e)\\left|\\nabla_\\Gamma u\\right|^2$.",
            "estimate $\\left\\| h_\\nu \\right\\|_{L^2(\\Gamma)}$. Apply comparison theorem to $\\overline{u}-u$ and $f$ gives $\\overline{u}-u\\geq -f$ in $\\Omega$, so by theorem 3 \\[\\left\\| h_\\nu \\right\\|_{L^2(\\Gamma)}\\leq C \\left\\| (-f)\\nabla^2h \\right\\|_{L^2(\\Gamma)}\\leq C \\left\\| (\\overline{u}-u)\\nabla^2h \\right\\|_{L^2(\\Gamma)}.\\]",
            "combine the estimates obtained in Step 2 and Step 3, we get \\[\\left\\| (\\overline{u}-u)\\nabla^2h \\right\\|_{L^2(\\Gamma)}^2\\leq C \\left(\\left\\| \\overline{u}-u \\right\\|_{1,2,\\tau,\\Gamma}^2+\\left\\| \\overline{u}-u \\right\\|_{1,2,\\tau,\\Gamma} \\left\\| (\\overline{u}-u)\\nabla^2h \\right\\|_{L^2(\\Gamma)}\\right).\\] View it as a quadratic polynomial of $\\left\\| (\\overline{u}-u)\\nabla^2h \\right\\|_{L^2(\\Gamma)}$, we obtain \\[\\left\\| (\\overline{u}-u)\\nabla^2h \\right\\|_{L^2(\\Gamma)}\\leq C \\left\\| \\overline{u}-u \\right\\|_{1,2,\\tau,\\Gamma}.\\]",
            "relate LHS of the inequality derived in Step 3 to $\\rho_e-\\rho_i$: take $p=2$, $\\alpha=1/2$ in theorem 4, apply theorem 4 to $\\nabla h$, by the interpolation inequality we have \\[\\rho_e-\\rho_i\\leq C \\left\\| \\overline{u}-u \\right\\|_{1,2,\\tau,\\Gamma}^{p/N}\\text{ for any }1\\leq p\\leq \\min \\left\\{\\frac{2N}{N-1}, N\\right\\}.\\] To obtain optimal exponent $\\tau_N$, we need to maximize $p$, since $N\\geq 100$, we have $2N/(N-1)\\leq N$, thus the maximum choice of $p$ is $2N/(N-1)$, which also gives $\\tau_N=2/(N-1)$."
        ],
        "step_count": 4
    },
    "recuVH3SEhT5ga": {
        "reasoning_steps": [
            "The core of the proof relies on a constructive theorem (Theorem_1), which states that a hierarchy solution of level `d = t/(1+k/β)` can be built if two conditions are met: the CSP must be \"extensible\" and its instance graph must be \"confined\".",
            "The first condition is satisfied by the problem's premise. A key result in the paper (related to Concept_2) is that any `l`-null-constraining `k`-CSP is proven to be BW-fully-extensible. This property is essential as it guarantees that local solutions can be consistently glued together to form a global, albeit relaxed, solution.",
            "The second condition relates to the structure of a typical instance. A random CSP instance corresponds to a random `k`-uniform hypergraph. According to standard probabilistic arguments (Theorem_2), such a random hypergraph is, with uniformly positive probability, `(2t, γ/l)`-confined for `t = \\Omega(n)` (Concept_3). This confinement ensures that the local variable and constraint dependencies required to define the solution at any given point remain small.",
            "With both conditions of Theorem_1 met, we can conclude that a BW hierarchy solution exists for a random instance of an `l`-null-constraining `k`-CSP.",
            "The level `d` of this constructed solution is given by the formula `d = t/(1+k/β)`. Substituting the value `t = \\Omega(n)` from the properties of random graphs (Theorem_2), we find that `d = \\Omega(n) / (1+k/(γ/l)) = \\Omega(n)`, since `k`, `γ`, and `l` are constants.",
            "Therefore, the hierarchy can be \"fooled\" up to a level linear in the number of variables `n`, establishing a lower bound of `\\Omega(n)` on the level required to solve the problem."
        ],
        "step_count": 6
    },
    "recuVHnPzNyQ92": {
        "reasoning_steps": [
            "Based on the concept 1, the oxygen species mainly consists of lattice oxygen species and the surface oxygen species on the oxygen vacancies. Thus, the higher lattice oxygen proportion means the lower oxygen vacancy proportion. In comparison with t-ZrO2, the m-ZrO2 with the lower lattice oxygen proportion has the higher oxygen vacancy proportion.",
            "In comparison with lattice oxygen, the adsorbed oxygen species on oxygen vacancies shows the weaker Zr-O bond, which is vital in H2S dissociation.",
            "The m-ZrO2 with more oxygen vacancies exhibits the higher adsorption strength of CO2 and CH4 as well as the lower reaction energy barriers. Therefore, the m-ZrO2 provides the higher activity for plasma-catalyzed conversion of H2S-CO2 to syngas."
        ],
        "step_count": 3
    },
    "recuVHogjyBVj5": {
        "reasoning_steps": [
            "Reformulate the causality constraint of the diffusive mode as a moment problem. Express this via the positive semidefiniteness (PSD) of Hankel matrices constructed from the spectral measure \\(\\mu(x) = 1 - \\sum_{n=1}^\\infty r^{2n-1} \\beta_{2n} (1-x^2)^{-1/2} T_{2n}(x)\\), ensuring microscopic causality (concept1) is satisfied at all orders.",
            "For the lowest-order analysis N=2, compute the relevant moments of \\(\\mu(x)\\): \\(a_0 = 1\\) (normalization condition) \\(a_1 = 1/3 - (\\pi/8) r \\beta_2\\) (linear relation to \\(\\beta_2\\) at first order) These moments encode the spectral weight of the diffusive mode and serve as inputs for Hankel matrix construction.",
            "Apply the PSD conditions from concept2 to these Hankel matrices: \\(H_2^1 = a_1 \\ge 0 \\implies 1/3 - (\\pi/8) r \\beta_2 \\ge 0 \\implies r \\beta_2 \\le 8/(3\\pi)\\) (upper bound from first moment) \\(H_1^0 - H_2^1 = a_0 - a_1 \\ge 0 \\implies 1 - (1/3 - (\\pi/8) r \\beta_2) \\ge 0 \\implies 2/3 + (\\pi/8) r \\beta_2 \\ge 0 \\implies r \\beta_2 \\ge -16/(3\\pi)\\) (lower bound from Hankel difference) These inequalities illustrate how the lower and upper bounds on \\(\\beta_2\\) arise naturally from matrix positivity and the structure of the moments.",
            "Take the physical limit \\(r \\to R\\) to obtain the tightest causality bounds on the dimensionless combination: \\(-16/(3\\pi) \\le R \\beta_2 \\le 0\\) This step ensures that the bounds reflect the largest physically relevant length scale while preserving causality constraints.",
            "Conclude that the causality-imposed lower bound on the diffusive mode is: \\(R \\beta_2 \\ge -16/(3\\pi)\\) This result encapsulates the interplay between microscopic causality, spectral moment positivity, and the Hankel matrix framework, providing a rigorous constraint on transport coefficients in relativistic hydrodynamics."
        ],
        "step_count": 5
    },
    "recuVIAg8GQrkq": {
        "reasoning_steps": [
            "First, clarify the original forms of the three key theorems (critical for avoiding dimensional errors in subsequent steps):   ### Van Trees Inequality (Theorem 1) For $L_2$ loss, the mean squared error lower bound is:  $$\\mathbb{E}\\left[\\|\\theta-\\hat{\\theta}\\|_2^2\\right] \\geq \\frac{d}{\\det\\left(I_X(\\theta)+J(\\pi)\\right)^{1/d}}$$  ### Efroimovich’s Inequality (Theorem 2) Relates conditional entropy $h(\\theta \\mid X)$ to total Fisher information:  $$\\frac{1}{2\\pi e} e^{\\frac{2}{d} h(\\theta \\mid X)} \\geq \\frac{1}{\\det\\left(I_X(\\theta)+J(\\pi)\\right)^{1/d}}$$  ### Maximum Entropy Under $L_q$ Moment Constraints (Theorem 3) For any random variable $Z$, its differential entropy is upper-bounded by:  $$h(Z) \\leq \\log\\left(C_{ME}(q) \\cdot \\mathbb{E}[|Z|^q]^{1/q}\\right)$$",
            "Define Estimation Error & Use Entropy Translation Invariance Estimation Error: Let $$Z = \\hat{\\theta}(X) - \\theta$$ (d-dimensional vector, where $$Z_i$$ = $i$-th component of $Z$). Our goal is to bound $\\mathbb{E}_{\\theta,X}\\left[\\|Z\\|_q^q\\right] = \\mathbb{E}_{\\theta,X}\\left[\\sum_{i=1}^d |Z_i|^q\\right]$.  Entropy Invariance: For fixed $X$, $$\\theta = \\hat{\\theta}(X) - Z_i$$ (a translation of $Z$). Since differential entropy is invariant to translation:   $$h(\\theta \\mid X) = h(Z \\mid X)$$",
            "Derive Lower Bound for $$h(\\theta \\mid X)$$ (via Efroimovich’s Inequality) We use Theorem 2 to find a lower bound for $h(\\theta \\mid X)$:  Start with Efroimovich’s Inequality:   $$\\frac{1}{2\\pi e} e^{\\frac{2}{d} h(\\theta \\mid X)} \\geq \\frac{1}{\\det\\left(I_X(\\theta)+J(\\pi)\\right)^{1/d}}$$ Take the natural logarithm of both sides (preserves inequality, as $$\\ln(\\cdot)$$ is increasing):   $$\\ln\\left(\\frac{1}{2\\pi e}\\right) + \\frac{2}{d} h(\\theta \\mid X) \\geq -\\frac{1}{d} \\ln\\left(\\det\\left(I_X(\\theta)+J(\\pi)\\right)\\right)$$ Rearrange terms to solve for $h(\\theta \\mid X)$:   $$h(\\theta \\mid X) \\geq \\frac{d}{2} \\ln(2\\pi e) - \\frac{1}{2} \\ln\\left(\\det\\left(I_X(\\theta)+J(\\pi)\\right)\\right) \\tag{1}$$",
            "We use Theorem 3 to find an upper bound for $h(Z \\mid X)$ (the entropy of the $d$-dimensional error $Z$): Split $d$-dimensional entropy into component-wise entropies using subadditivity:  For a $d$-dimensional random vector $Z$, its total conditional entropy satisfies the subadditivity property (no independence assumption is required):  $$h(Z \\mid X) \\leq \\sum_{i=1}^d h(Z_i \\mid X)$$ Apply Theorem 3 to each 1-dimensional component $Z_i \\mid X$:  For each $Z_i$, the maximum entropy principle gives:  $$h(Z_i \\mid X) \\leq \\log\\left(C_{ME}(q) \\cdot \\mathbb{E}[|Z_i|^q \\mid X]^{1/q}\\right)$$ Sum over all components to get the upper bound for $h(Z \\mid X)$:  $$h(Z \\mid X) \\leq \\sum_{i=1}^d \\log\\left(C_{ME}(q) \\cdot \\mathbb{E}[|Z_i|^q \\mid X]^{1/q}\\right) \\tag{2}$$",
            "Combine Entropy Bounds (Link Entropy to Error) From Step 2, $h(\\theta \\mid X) = h(Z \\mid X)$. Substitute (1) and (2) to 联立 (link entropy to the error $Z$):  $$\\frac{d}{2} \\ln(2\\pi e) - \\frac{1}{2} \\ln\\left(\\det\\left(I_X(\\theta)+J(\\pi)\\right)\\right) \\leq \\sum_{i=1}^d \\log\\left(C_{ME}(q) \\cdot \\mathbb{E}[|Z_i|^q \\mid X]^{1/q}\\right)$$ Simplify the right-hand side (RHS) using logarithmic properties ($\\sum \\log a_i = \\log\\left(\\prod a_i\\right)$):  $$\\text{RHS} = \\log\\left(\\prod_{i=1}^d \\left[C_{ME}(q) \\cdot \\mathbb{E}[|Z_i|^q \\mid X]^{1/q}\\right]\\right) = \\log\\left(C_{ME}(q)^d \\cdot \\prod_{i=1}^d \\mathbb{E}[|Z_i|^q \\mid X]^{1/q}\\right)$$",
            "Apply AM-GM Inequality to Error Moments To connect the product of component moments to the total $L_q$ error, use the **Arithmetic Mean-Geometric Mean (AM-GM) Inequality**:  For non-negative terms $\\mathbb{E}[|Z_i|^q \\mid X]$ ($i=1,...,d$), AM-GM states:  $$\\frac{1}{d} \\sum_{i=1}^d \\mathbb{E}[|Z_i|^q \\mid X] \\geq \\left(\\prod_{i=1}^d \\mathbb{E}[|Z_i|^q \\mid X]\\right)^{1/d}$$ Take the $1/q$-th power of both sides (preserves inequality, as $x^{1/q}$ is increasing for $x \\geq 0$):  $$\\left(\\frac{1}{d} \\sum_{i=1}^d \\mathbb{E}[|Z_i|^q \\mid X]\\right)^{1/q} \\geq \\left(\\prod_{i=1}^d \\mathbb{E}[|Z_i|^q \\mid X]\\right)^{1/(dq)}$$ Note that $\\sum_{i=1}^d \\mathbb{E}[|Z_i|^q \\mid X] = \\mathbb{E}[\\|Z\\|_q^q \\mid X]$. Rearrange to bound the product term:  $$\\prod_{i=1}^d \\mathbb{E}[|Z_i|^q \\mid X]^{1/q} \\leq d^{d/q} \\cdot \\mathbb{E}[\\|Z\\|_q^q \\mid X]^{d/q} \\tag{3}$$",
            "Solve for Lower Bound of Conditional $L_q$ Error Substitute (3) into the combined entropy inequality (from Step 5) and solve for $\\mathbb{E}[||Z||_q^q \\mid X]$:  Substitute (3) into the RHS:   $$\\frac{d}{2} \\ln(2\\pi e) - \\frac{1}{2} \\ln\\left(\\det\\left(I_X(\\theta)+J(\\pi)\\right)\\right) \\leq \\log\\left(C_{ME}(q)^d \\cdot d^{-d/q} \\cdot \\mathbb{E}[||Z||_q^q \\mid X]^{d/q}\\right)$$ Exponentiate both sides (preserves inequality, as $$\\exp(\\cdot)$$ is increasing):   $$(2\\pi e)^{d/2} \\cdot \\det\\left(I_X(\\theta)+J(\\pi)\\right)^{-1/2} \\leq C_{ME}(q)^d \\cdot d^{-d/q} \\cdot \\mathbb{E}[||Z||_q^q \\mid X]^{d/q}$$ Rearrange terms to solve for $\\mathbb{E}[||Z||_q^q \\mid X]$:   $$\\mathbb{E}[||Z||_q^q \\mid X] \\geq \\frac{(2\\pi e)^{q/2} \\cdot d}{\\det\\left(I_X(\\theta)+J(\\pi)\\right)^{q/(2d)} \\cdot C_{ME}(q)^q} \\tag{4}$$",
            "Take Expectation Over $\\theta$ and (Final Bound) The inequality (4) holds for all $X$. To get the unconditional lower bound, take the expectation over $\\theta$ and (expectation is monotonic, so the lower bound is preserved): \\n$$\\mathbb{E}_{\\theta,X}\\left[||Z||_q^q\\right] \\geq \\frac{(2\\pi e)^{q/2} \\cdot d}{\\det\\left(I_X(\\theta)+J(\\pi)\\right)^{q/(2d)} \\cdot C_{ME}(q)^q}$$\\nRewrite into the standard form by grouping constants: \\n$$\\boxed{\\mathbb{E}_{\\theta, X}\\left[\\|\\hat{\\theta}(X)-\\theta\\|_q^q\\right] \\geq\\left(\\frac{\\sqrt{2 \\pi e}}{C_{M E}(q)}\\right)^q \\frac{d}{\\operatorname{det}\\left(I_X(\\theta)+J(\\pi)\\right)^{\\frac{q}{2 d}}}}$$",
            "Validate Consistency with Van Trees Inequality (Special Case $q=2$) To confirm correctness, substitute $$q=2$$ (where $C_{ME}(2)=\\sqrt{2\\pi e}$): \\n$$\\left(\\frac{\\sqrt{2\\pi e}}{\\sqrt{2\\pi e}}\\right)^2 \\cdot \\frac{d}{\\det\\left(I_X(\\theta)+J(\\pi)\\right)^{2/(2d)}} = \\frac{d}{\\det\\left(I_X(\\theta)+J(\\pi)\\right)^{1/d}}$$\\nThis matches the Van Trees Inequality (Theorem 1), verifying the derivation."
        ],
        "step_count": 9
    },
    "recuVISOGHiBy5": {
        "reasoning_steps": [
            "Identify potential sources of uncertainty L: According to the problem, there are three types of possible sources of uncertainty: 1. Different climate forcing scenarios (such as Shared Socioeconomic Pathways). 2. Model differences. 3. Initial conditions (for example, the phase of centennial oscillations).",
            "Starting from Concept 1 (the existence of the Southern Ocean Centennial Oscillation) ：Concept 1 points out that the deep circulation, sea ice extent, and circulation intensity of the Southern Ocean undergo spontaneous oscillations on a centennial scale, and these oscillations can significantly affect the global climate. This indicates that even under the same climate scenarios and model settings, the internal system can trigger completely different evolutionary paths due to differences in initial conditions (phase differences).",
            "Incorporating Concept 2 (Centennial-scale oscillations simulated by HadGEM3-GC3.1-LL)：In the control experiments of CMIP6, the simulation results of HadGEM3-GC3.1-LL show that centennial-scale oscillations of sea ice and deep currents naturally emerge around 500 years. These oscillations originate from the energy imbalance (TOA flux imbalance) existing under the initial conditions of the model. Different phases can cause the system to evolve in different directions. This indicates that under the same climate scenario, simply changing the initial conditions can trigger completely different trends in the circulation changes of the Southern Ocean.",
            "Introduce Concept 3 (Uncertainties in Antarctic Bottom Water Transport): Concept 3 emphasizes that the formation and transport of Antarctic Bottom Water (AABW) are highly sensitive to different research settings: some studies suggest that significant mixing occurs before transoceanic transport, while others argue that the water masses in the source regions remain independent. This sensitivity exacerbates the system's internal dependence on the setting of initial conditions: different initial states may amplify or weaken the role of AABW, thereby enhancing the dominance of centennial oscillations in future predictions.",
            "Compare the importance of the three sources of uncertainty: 1. Differences in climate forcing scenarios: They determine long-term trends (such as greenhouse gas levels), but their impacts on the Southern Ocean circulation at the regional scale will be masked or delayed by internal oscillations. 2. Model differences: They do lead to different simulation results, but centennial oscillations are widespread in multi-model experiments, indicating that they are not the product of a single model. 3. Initial conditions (phase of centennial oscillations): They directly determine whether the system is in the 'rising phase' or 'declining phase' of the oscillation, and have an overwhelming impact on short-to-medium-term predictions (on the scale of decades to a century). Therefore, it can be deduced that the largest source of uncertainty is not scenario differences or model differences, but initial conditions (the phase of centennial oscillations). This is because these oscillations can occur spontaneously under the same scenario and with the same model, leading to greater discrepancies in the results."
        ],
        "step_count": 5
    },
    "recuVKl5fbSct3": {
        "reasoning_steps": [
            "Any 2-Prover-1-Round game with alphabet size q has value at least 1/q under random assignment, since each constraint is satisfied with probability 1/q when assignments are chosen uniformly at random.",
            "The question asks for the best (smallest) soundness error that can be proven NP-hard to achieve, meaning the smallest value in the \"NO\" case that maintains computational hardness.",
            "Since random assignment achieves value 1/q, it is information-theoretically impossible to have soundness error less than 1/q, because every instance has value at least 1/q.",
            "Theorem 1.3 establishes that for any ε > 0, it is NP-hard to distinguish between value ≥ 1-δ versus value ≤ 1/q^(1-ε). Since 1/q^(1-ε) = q^ε/q, this soundness error is actually greater than 1/q when ε > 0.",
            "The soundness error 1/q^(1-ε) approaches the theoretical minimum 1/q from above as ε → 0, since (1-ε) approaches 1 and thus q^ε approaches 1.",
            "The paper states that the best one can hope for is γ = 1 - o(1) in the exponent, meaning soundness error 1/q^(1-o(1)), which represents near-optimal hardness approaching the information-theoretic barrier.",
            "Since 1/q^(1-ε) approaches 1/q from above as ε → 0, and 1/q is the information-theoretic lower bound, this represents the theoretical limit achievable while maintaining NP-hardness."
        ],
        "step_count": 7
    },
    "recuVKmbDduAgB": {
        "reasoning_steps": [
            "Goal: derive the unique two-dimensional angular yield per trigger jet that equals 1 at $\\Delta\\eta=\\Delta\\varphi=0$ and has $S(\\Delta\\eta,\\Delta\\varphi)$ in the numerator, to be stated in one sentence.",
            "Observable definition (Concept_1): identify the per-trigger-jet differential yield $\\mathcal{Y}(\\Delta\\eta,\\Delta\\varphi)=\\displaystyle\\frac{1}{N_{\\text{jet}}}\\frac{\\mathrm{d}^2N}{\\mathrm{d}\\Delta\\eta\\,\\mathrm{d}\\Delta\\varphi}$.",
            "Mixed-event normalization (Concept_2): define $\\text{ME}(\\Delta\\eta,\\Delta\\varphi)=\\displaystyle\\frac{N_{\\text{mixed}}(\\Delta\\eta,\\Delta\\varphi)}{N_{\\text{mixed}}^{\\text{tot}}}$ to flatten detector acceptance.",
            "Efficiency correction (Concept_3): multiply by the tracking-efficiency factor $S(\\Delta\\eta,\\Delta\\varphi)$ to correct for reconstruction losses.",
            "Ansatz construction (Concept_4): postulate $\\mathcal{Y}(\\Delta\\eta,\\Delta\\varphi)=\\displaystyle\\frac{\\text{ME}(0,0)\\,S(\\Delta\\eta,\\Delta\\varphi)}{\\text{ME}(\\Delta\\eta,\\Delta\\varphi)\\,S(0,0)}$, explicitly placing $S(\\Delta\\eta,\\Delta\\varphi)$ in the numerator.",
            "Boundary condition (Concept_5): verify $\\mathcal{Y}(0,0)=1$.",
            "Efficiency normalization (Concept_6): note that $S(\\Delta\\eta,\\Delta\\varphi)$ is conventionally normalized to $S(0,0)=1$ at the reference point $\\Delta\\eta=\\Delta\\varphi=0$."
        ],
        "step_count": 7
    },
    "recuVKxGnLijDz": {
        "reasoning_steps": [
            "First, according to \\concept_1, perform a power series expansion of the two shape function metrics as $r\\to0$: \\[ A(r)=A_0+A_1 r + A_2 r^2 + \\cdots,\\qquad f(r)=B_0+B_1 r + B_2 r^2 + \\cdots . \\]",
            "Next, using concept_2, examine the leading singular term of the Ricci scalar: \\[ R=\\frac{2\\bigl(1 - f(r)/A(r)^2\\bigr)}{r^{2}} + O(1). \\] Substituting the expansions shows that if $A_0\\neq B_0$, a $1/r^2$ divergence occurs. Therefore, to ensure $R$ is finite, the following must hold: \\[ A_0=B_0 . \\]",
            "Then, in accordance with concept_3, eliminate all $1/r$ terms using the invariants composed of the Ricci tensor from the Zakhary\\textendash McIntosh (ZM) set (which is equivalent to examining the singular parts of $R_{tt}$ and $R_{rr}$). From \\[ R_{tt}=\\frac{B_0 B_1}{A_0}\\,\\frac{1}{r}+O(1) \\] under the condition $A_0=B_0$, it is deduced that $B_1=0$; further, from \\[ R_{rr}=\\Bigl(\\frac{A_1}{A_0}-\\frac{B_1}{B_0}\\Bigr)\\frac{1}{r}+O(1) \\] under the conditions $A_0=B_0$ and $B_1=0$, it is obtained that $A_1=0$."
        ],
        "step_count": 3
    },
    "recuVMql7D1CY0": {
        "reasoning_steps": [
            "The instruction asks for the 'fundamental limit on the asymptotic transformation ratio,' $r(P \\rightarrow Q)$, which is a formal way of asking for the optimal rate. The transformation itself is defined by majorization (Concept_1).",
            "The general method to find this limit is to compute the minimum ratio of certain information-theoretic quantities known as monotones (Concept_2, Concept_3).",
            "The core of the problem is to identify the correct family of monotones for the given support condition: $supp~p^{(1)}$ is a strict subset of $supp~p^{(2)}$ (Concept_5).",
            "Concept_4 introduces the mathematical definition of a crucial family of monotones without naming them 'Rényi divergences.'",
            "Theorem_1 provides the critical link: it states that for the specific strict-subset support condition, the set of all necessary monotones is precisely the family defined in Concept_4, evaluated for $(p^{(1)}, p^{(2)})$, over the parameter range $\\alpha \\in [0, \\infty]$.",
            "We can now apply the general formula from Concept_2. We replace the abstract 'monotone $D$' with the specific functional form from Concept_4 (which we recognize as Rényi divergence), and we perform the minimization over the parameter $\\alpha \\in [0, \\infty]$ as specified by Theorem_1.",
            "This procedure directly yields the final answer, expressing the rate as the minimum ratio of these information measures."
        ],
        "step_count": 7
    },
    "recuVMBduqXvjK": {
        "reasoning_steps": [
            "The objective is to determine the highest possible misclassification exponent ($E^{EST}$) for any sequential test that adheres to the Expected Stopping Time Universality Constraint in an 'exactly one outlier' setting (Concept_1, Concept_2, Concept_3). This requires finding a bound that is both achievable by some test and serves as an upper limit for all possible tests.",
            "To establish achievability, a specific sequential test, denoted $\\Phi^{EST}$, is constructed. This test employs a minimum scoring function rule (Concept_4). The score for each hypothesis $H_i$, $S_i(x^k)$, is calculated using a function based on the sum of KL-divergences, $G_i$, which measures the similarity of the empirical distributions of all sequences except the i-th one (Concept_5). The test stops when any score drops below a predefined, time-varying threshold, and the decision corresponds to the hypothesis with the minimum score.",
            "The performance of this test, specifically its misclassification exponent, is analyzed using large deviations theory. An error occurs under hypothesis $H_i$ if the test wrongly decides $H_j$ for some $j \\neq i$. The probability of this event is bounded. The analysis shows that the exponential decay rate of this probability is determined by a minimization problem over all possible empirical distributions, which has the form $\\min_{Q\\in\\mathcal{P}(X)} D(Q||P_{A})+(M-2)D(Q||P_{N})$.",
            "This specific optimization problem is then connected to a standard information measure. By applying the variational form of the Rényi divergence (Theorem_1) with the parameter $\\alpha = M-2$, the expression $\\min_{Q\\in\\mathcal{P}(X)} (M-2)D(Q||P_{N}) + D(Q||P_{A})$ is shown to be exactly equal to the Rényi divergence of order $\\frac{M-2}{M-1}$, i.e., $D_{\\frac{M-2}{M-1}}(P_{N}||P_{A})$.",
            "This result demonstrates that the proposed test $\\Phi^{EST}$ can achieve an exponent of at least $D_{\\frac{M-2}{M-1}}(P_{N}||P_{A})$, thus establishing the achievability part of the tight bound (Theorem_2).",
            "To prove that this bound is tight, a converse is required. The converse proof (Theorem_3) uses the data processing inequality for KL-divergence to show that for *any* sequential test satisfying the expected stopping time universality constraint, the misclassification exponent cannot exceed a certain value. By optimizing over auxiliary distributions, this upper bound is shown to be precisely $D_{\\frac{M-2}{M-1}}(P_{N}||P_{A})$.",
            "Since the achievability result shows a test can attain this exponent and the converse result shows no test can exceed it, the value $D_{\\frac{M-2}{M-1}}(P_{N}||P_{A})$ is established as the tight bound on the misclassification exponent."
        ],
        "step_count": 7
    },
    "recuVMGhp0EmSB": {
        "reasoning_steps": [
            "The primary objective is to formulate the expression for the error exponent, which, according to the Performance Limit Principle (Concept_3), is equivalent to the average information gained per sample.",
            "First, we must quantify the information provided by a single sample. According to the Information Measure (Concept_2), the information gained from a sample from a specific source $j$ for distinguishing the distribution $P_{m,j}$ (under hypothesis $m$) from $P_{\\theta,j}$ (under hypothesis $\\theta$) is the Kullback-Leibler (KL) divergence, $D(P_{m,j}||P_{\\theta,j})$.",
            "Next, we consider the agent's behavior. The agent does not stick to a single source but chooses among them according to a stationary strategy. As defined by Agent Strategy (Concept_1), this strategy is a probability distribution over the $n$ sources, denoted by the vector $\\beta^m = (\\beta_1^m, \\dots, \\beta_n^m)$, where $\\beta_j^m$ is the probability of selecting source $j$.",
            "To find the *average* information gain across all sources, we must average the information from each source, weighted by the probability of selecting it. This is a direct application of the Law of Total Expectation (Concept_4).",
            "Applying this law, the overall expected information per sample is the sum of the conditional expectations for each source (the KL divergence $D(P_{m,j}||P_{\\theta,j})$) multiplied by the probability of choosing that source ($\\beta_j^m$).",
            "This calculation yields the final mathematical expression: $\\sum_{j=1}^{n}\\beta_{j}^{m}D(P_{m,j}||P_{\\theta,j})$. This expression represents the average rate of information accumulation, which, by Concept_3, is the error exponent limit."
        ],
        "step_count": 6
    },
    "recuVNmsHLOjyY": {
        "reasoning_steps": [
            "Use the method of multiple scales in Concept_1 by expanding time as T_0=t,\\;T_1=\\varepsilon t, and expand displacement as u_j=u_j^{(0)}+\\varepsilon u_j^{(1)}+\\dots. Time derivatives become D_0=\\partial/\\partial T_0,\\;D_1=\\partial/\\partial T_1. The zeroth-order equation contains only D_0^2 and yields the linear eigenwave problem; hence, the zeroth-order solution must be taken as the linear Bloch-wave basis so higher orders can introduce amplitude-dependent corrections.",
            "In the linear left half (j<0) take the Bloch-wave form including incident right-going and reflected left-going parts: \\frac{1}{2}A e^{i(\\omega T_0-\\mu j)}+\\frac{1}{2}A_{1h}^- e^{i(\\omega T_0+\\mu j)}+\\text{c.c.}. This directly uses Concept_2 and ensures compliance with the linear dispersion \\omega(\\mu)=\\sqrt{\\frac{2k_1}{m}(1-\\cos\\mu)} at zeroth order.",
            "In the right half (j\\ge0) take the zeroth-order solution as a transmitted right-going component \\frac{1}{2}A_{1h}^+ e^{i(\\omega T_0-\\mu_0 j)}+\\text{c.c.}. Although the right side is weakly nonlinear, the zeroth-order waveform is still linear, but its wavenumber \\mu_0 may differ from the left-side \\mu, reflecting the potential amplitude-dependent dispersion of Concept_3.",
            "Represent the frequency with a slow correction \\omega=\\omega_0+\\varepsilon\\sigma (i.e. \\omega_1=\\sigma) and let amplitudes depend on slow time T_1: A=A(T_1),\\;A_{1h}^{\\pm}=A_{1h}^{\\pm}(T_1). This follows Concept_1 and sets up the first-order problem that will produce amplitude/phase modulation and nonlinear frequency shift (Concept_3).",
            "Impose matching at the interface j=-1,0 by substituting the zeroth-order left and right expressions into the zeroth-order interface equations; these matching conditions link A,\\,A_{1h}^{\\pm},\\;\\mu,\\;\\mu_0,\\;\\sigma. The actual nonlinear amplitude-dependent corrections (e.g. shift of \\mu_0 or \\sigma) are determined at first order—this ordering respects the multiple-scales logic and keeps usage of Concepts 1–3 mathematically consistent.",
            "Therefore, the zeroth-order solutions are (matching the provided answer): For j<0: u_j^{(0)}=\\tfrac{1}{2}A\\,e^{i(\\omega T_0-\\mu j)}+\\tfrac{1}{2}A_{1h}^{-}\\,e^{i(\\omega T_0+\\mu j)}+\\text{c.c.}. For j\\ge0: u_j^{(0)}=\\tfrac{1}{2}A_{1h}^{+}\\,e^{i(\\omega T_0-\\mu_0 j)}+\\text{c.c.}, with \\omega=\\omega_0+\\varepsilon\\sigma and \\omega_0(\\mu_0)=\\sqrt{\\tfrac{2k_1}{m}(1-\\cos\\mu_0)}. This formulation explicitly invokes Concept_1, Concept_2 and Concept_3; their usages are consistent and logically valid in the multiple-scales perturbation framework."
        ],
        "step_count": 6
    },
    "recuVOAMbAyzVh": {
        "reasoning_steps": [
            "The objective is to find the average partial weight spectrum, denoted $B^{(l)}(X)$, which corresponds to the set of codewords $\\mathbb{G}_l$ from the random SGMC ensemble (Concept_3, Concept_5).",
            "A random SGMC is defined by its staircase generator matrix, specified by a profile $w$, with random elements below the staircase (Concepts 1 & 2).",
            "The set of codewords $\\mathbb{G}_l$ is generated by the $2^l$ information sequences in the subset $\\mathcal{U}_l$, whose rightmost '1' is at position $l$ (Concept_4).",
            "We first analyze the structure of a single codeword $c$ generated by an input $u \\in \\mathcal{U}_l$. Due to the structure of the random generator matrix, any such codeword is a random vector with a specific three-part structure (Theorem_1).",
            "The weight enumerator of such a codeword can be determined by analyzing its segments: * The all-zero segment of length $n-n_l$ contributes a factor of $X^0=1$. * The all-one segment of length $w_l$ has a fixed weight of $w_l$, contributing a factor of $X^{w_l}$. * The first segment of length $n_l - w_l$ is totally random. The weight enumerator of a totally random binary vector of length $N$ is $(1/2 + 1/2X)^N$. Thus, this segment contributes a factor of $(1/2 + 1/2X)^{n_l - w_l}$.",
            "The average weight enumerator for a single codeword in $\\mathbb{G}_l$ is the product of the enumerators of its independent segments: $X^{w_l}(1/2 + 1/2X)^{n_l - w_l}$.",
            "The set $\\mathbb{G}_l$ contains $2^l$ such codewords, each generated by an information vector in $\\mathcal{U}_l$. Since each codeword in $\\mathbb{G}_l$ follows the same random distribution, the average partial weight spectrum for the entire set is $2^l$ times the average weight enumerator of a single codeword.",
            "Therefore, the final expression for the average partial weight spectrum is $B^{(l)}(X) = 2^l X^{w_l} (1/2 + 1/2X)^{n_l - w_l}$."
        ],
        "step_count": 8
    },
    "recuVTejQQ3Buw": {
        "reasoning_steps": [
            "find $C(N, R)$ through a special case. When $\\Omega$ is a ball centered at $0$, the Poisson equation admits the unique solution $u(z)=\\frac{1}{2}(|x|^2-R^2)$, so the boundary condition $u=0$ implies that $\\Omega=B_R$. Direct computation shows that $\\int_{\\Omega}^{}(-u)dx=\\frac{\\omega_N R^{N+2}}{N+2}$ and $\\Omega=\\omega_NR^N$, where $\\omega_N=|B_1|$. In this special case $H=H_0$, thus $\\int_{\\Omega}^{}(-u)dx=C(N, R)|\\Omega|$, which gives $C(N,R)=\\frac{R^2}{N+2}$.",
            "estimate $\\left|\\int_{\\Omega}^{}(-u)dx-\\frac{R^2}{N+2}|\\Omega|\\right|$ in terms of $\\left\\| u_\\nu-R \\right\\|_{L^2(\\partial\\Omega)}$. Apply theorem 2 to $u$, since $\\Delta u=N$ and $u=0$ on $\\partial\\Omega$, we get \\[\\int_{\\Omega}^{}(-u)dx=\\frac{1}{N}\\int_{\\Omega}^{}|\\nabla u|^2 dx.\\] Apply theorem 1, we find \\[\\int_{\\Omega}^{}(-u)dx=\\frac{1}{N(N+2)}\\int_{\\partial\\Omega}^{}u_\\nu^2 \\left\\langle x-z,\\nu\\right\\rangle dS.\\] So \\begin{align*} \\left|\\int_{\\Omega}^{}(-u)-\\frac{R^2}{N+2}|\\Omega|\\right|=&~\\left|\\frac{1}{N(N+2)}\\int_{\\partial\\Omega}^{}(u_\\nu^2-R^2)\\left\\langle x-z,\\nu\\right\\rangle\\right| \\\\ \\leq&~ \\frac{1}{N(N+2)}(R+G)d_\\Omega\\int_{\\partial\\Omega}^{}|u_\\nu-R|dx \\\\ \\leq&~ \\frac{1}{N(N+2)}(R+G)d_\\Omega |\\partial\\Omega|^{1/2}\\left\\| u_\\nu-R \\right\\|_{L^2(\\partial\\Omega)} \\\\ \\leq&~2G d_\\Omega |\\partial\\Omega|^{1/2} \\left\\| u_\\nu-R \\right\\|_{L^2(\\partial\\Omega)},\\end{align*} where in the second inequality we use the Cauchy-Schwarz inequality, theorem 4, and in the last inequality we use the fact that \\[R=\\frac{1}{|\\partial\\Omega|}\\int_{\\partial\\Omega}^{}u_\\nu \\leq G.\\]",
            "relate $|\\partial\\Omega|^{1/2}\\left\\| u_\\nu-R \\right\\|_{L^2(\\partial\\Omega)}$ to $\\delta$. By theorem 3, since $|\\nabla u|^2-\\frac{(\\Delta u)^2}{N}\\geq 0$, we have \\[\\frac{1}{R}\\int_{\\partial\\Omega}^{}(u_\\nu-R)^2\\leq G^2\\delta.\\] Recall the definition of $R$: \\[R=\\frac{1}{\\partial\\Omega}\\int_{\\partial\\Omega}^{}u_\\nu=\\frac{1}{|\\partial\\Omega|}\\int_{\\Omega}^{}\\Delta u=\\frac{N|\\Omega|}{|\\partial\\Omega|}.\\] Thus $|\\partial\\Omega|^{1/2}\\left\\| u_\\nu-R \\right\\|_{L^2(\\partial\\Omega)}\\leq G (N|\\Omega|)^{1/2}\\delta^{1/2}$.",
            "find $\\tau_N$. Combine all the estimates obtained in step 2 and step 3 we have \\[\\left|\\int_{\\Omega}^{}(-u)-\\frac{R^2}{N+2}|\\Omega|\\right|\\leq C(N, d_\\Omega, |\\Omega|)G^2\\delta^{1/2}\\leq C(N, d_\\Omega)G^2 \\delta^{1/2},\\] which gives $\\tau_N=\\frac{1}{2}$."
        ],
        "step_count": 4
    },
    "recuVTKPQPtHAh": {
        "reasoning_steps": [
            "The objective is to determine the pessimistic slightly superexponential capacity, \\(\\dot{C}_{DI}(W)\\). This requires finding the maximum number of codewords `N` for a DI code (Concept_1) and relating its asymptotic scaling to the block length `n` via the capacity definition (Concept_2).",
            "We start with an arbitrary \\((n, N, \\lambda_1, \\lambda_2)\\)-DI code. The reliability condition (Concept_1) implies that the output distributions for any two distinct codewords must be highly distinguishable. Using the first part of Theorem_1, this high distinguishability requires the corresponding sequences of spherised vectors to be separated by a minimum Euclidean distance. This transforms the coding problem into a geometric packing problem.",
            "To limit the size `N` of this packing, we use a covering argument. We cover the single-letter spherised output set \\(\\sqrt{\\tilde{\\mathcal{X}}}\\) (Concept_4) with a finite number of small balls. The total number of distinct codeword sequences can be no more than \\(|\\text{covering set}|^n\\). By relating the size of this covering to the lower Minkowski dimension `d` of \\(\\sqrt{\\tilde{\\mathcal{X}}}\\) (Concept_3), we can bound `N`. Taking the logarithm and applying the definition of \\(\\dot{C}_{DI}(W)\\) yields the upper bound: \\(\\dot{C}_{DI}(W) \\le \\frac{1}{2}d\\).",
            "To find the lower bound, we construct a code. The goal is to find a large set of codewords whose output distributions are highly distinguishable. We begin by creating a geometric packing of the single-letter spherised output set \\(\\sqrt{\\tilde{\\mathcal{X}}}\\) with a specific minimum separation. The number of points in this packing is lower-bounded by the lower Minkowski dimension `d` (Concept_3).",
            "We use the points from this single-letter packing as an alphabet to construct `n`-length codeword sequences. We choose all possible sequences formed from this alphabet, creating a codebook of size \\(N = |\\text{packing set}|^n\\).",
            "For any two distinct codeword sequences from this construction, we can lower-bound the sum of squared Euclidean distances between them. Using the second part of Theorem_1, this geometric separation is strong enough to guarantee that the total variation distance between the corresponding product output distributions is close to 1, fulfilling the reliability requirement for a DI code (Theorem_2).",
            "The size of this constructible code `N` is determined by the size of the initial single-letter packing. By relating the packing size to the lower Minkowski dimension `d` and applying the capacity definition (Concept_2), we find that a rate of at least \\(\\frac{1}{2}d\\) is achievable. Thus, the lower bound is \\(\\dot{C}_{DI}(W) \\ge \\frac{1}{2}d\\).",
            "Since the upper bound from the converse (Step 3) is \\(\\dot{C}_{DI}(W) \\le \\frac{1}{2}d\\) and the lower bound from achievability (Step 7) is \\(\\dot{C}_{DI}(W) \\ge \\frac{1}{2}d\\), the bounds are tight. Therefore, the capacity is exactly equal to \\(\\frac{1}{2}d\\)."
        ],
        "step_count": 8
    },
    "recuVOcYTWds4t": {
        "reasoning_steps": [
            "Establish the Contradiction Hypothesis We proceed by contradiction. The goal is to find the minimum feedback required to achieve an error resilience of \\alpha > 1/4. We begin by assuming the conclusion is false. That is, we hypothesize that there exists a coding scheme, based on the principles of error-correcting codes with feedback, that can achieve an error resilience of \\alpha = 1/4 + \\delta (for some constant \\delta > 0) while using a sub-logarithmic number of feedback bits, specifically \\zeta = o(\\log k) bits for a k-bit message. This assumption directly challenges the no-feedback limit, which states that without feedback, an error resilience greater than 1/4 is impossible.",
            "Construct the Message Distinguishability Graph and Model Feedback as Coloring Based on the assumption in Step 1, we construct a complete graph G where the vertex set V consists of all K=2^k possible messages Alice might send. For the coding scheme to be successful, the protocol must provide a way to distinguish between any two distinct messages, say v_i and v_j, even in the presence of errors. This means every edge (v_i, v_j) in the graph G must be accounted for. The feedback mechanism is the tool used to ensure this distinguishability. Since we assumed a total of \\zeta feedback bits, there are l = 2^\\zeta possible feedback strings that Bob can send. We model the protocol's strategy as an edge-coloring of the graph G with l colors. We assign a color to the edge (v_i, v_j) corresponding to the specific feedback string that the protocol would rely on to resolve the ambiguity between messages v_i and v_j. For the protocol to be valid, every edge in the complete graph must be colored.",
            "Apply Ramsey Theory to Reach a Contradiction We now have a complete graph on K=2^k vertices, edge-colored with l=2^\\zeta colors. We apply Ramsey's Theorem. This theorem states that if the number of vertices K is sufficiently large compared to the number of colors l, the graph must contain a large monochromatic clique. A monochromatic clique in this context represents a large subset of messages for which the protocol relies on the exact same feedback string to distinguish between any pair of them. For this subset of messages, the feedback is fixed and provides no new information. The problem of distinguishing between them is therefore reduced to a standard coding problem without feedback. This sub-code must adhere to the constraints of the no-feedback limit. However, the Plotkin bound puts a strict upper limit on the number of messages that can exist in a code while maintaining the distance required for an error resilience of 1/4 + \\delta. A sufficiently large monochromatic clique, whose existence is guaranteed by Ramsey's Theorem for our assumed small number of colors l, would imply a sub-code that violates this very bound. This is a contradiction.",
            "Conclusion and Underlying Logic The contradiction forces us to reject our initial assumption from Step 1. Therefore, no scheme can achieve an error resilience greater than 1/4 with only o(\\log k) feedback bits. The cornerstone of this final quantitative step is the bound on multicolor Ramsey numbers. This theorem provides the explicit relationship between the number of vertices (K), the size of the monochromatic clique to be avoided (s, a constant that depends on \\delta), and the necessary number of colors (l). The bound states that to avoid such a clique, the number of colors l must grow at least as fast as a logarithmic function of K. Formally: l = \\Omega(\\log K) Since the number of feedback bits is \\zeta = \\log_2 l and the number of vertices is K=2^k, we can substitute these into the bound: \\zeta = \\log_2 l \\ge \\log_2(\\Omega(\\log K)) = \\log_2(\\Omega(\\log 2^k)) = \\Omega(\\log k) This proves that \\Omega(\\log k) feedback bits are necessary. This entire investigation is motivated by Berlekamp's thesis, which showed that with unlimited feedback, resilience can be improved to 1/3, establishing that feedback is a valuable resource and making the question of its minimal cost a critical one."
        ],
        "step_count": 4
    },
    "recuW0yr54nXwK": {
        "reasoning_steps": [
            "Let \\(Q\\) be the unit square, which we can define as \\(Q = [0, 1] \\times [0, 1]\\) without loss of generality. Its area is \\(|Q|=1\\). Let \\(\\Omega\\) be any convex domain such that \\(Q \\subset \\Omega\\). We want to prove that \\(\\mu_1(\\Omega) \\le \\mu_1(Q)\\). If \\(\\Omega=Q\\), the equality is trivial. Therefore, we will focus on the case where \\(\\Omega\\) strictly contains \\(Q\\).",
            "By elementary calculations, it is easy to see that \\(|\\Omega| \\ge w_\\Omega\\). For example, we can prove it as follows. Because \\(\\Omega\\) is a convex set strictly containing the unit square \\(Q\\), we can identify points N, W, S, E on the boundary of \\(\\Omega\\) that lie, respectively, to the North, West, South, and East of the square. Let \\(h_N, h_W, h_S, h_E\\) be the minimum distances from these points to the corresponding sides of the square \\(Q\\). Since \\(\\Omega\\) strictly contains \\(Q\\), at least one of these distances is positive. The width of \\(\\Omega\\) in the vertical direction is at least \\(1 + h_N + h_S\\), and its width in the horizontal direction is at least \\(1 + h_W + h_E\\). The minimum width \\(w_\\Omega\\) must be less than or equal to the width in any direction, so we have:  \\(w_\\Omega \\le 1 + h_N + h_S \\quad \\text{and} \\quad w_\\Omega \\le 1 + h_W + h_E\\) By convexity, \\(\\Omega\\) must contain the four triangles formed by connecting each point (N, W, S, E) to the two corresponding vertices of the square. The areas of these triangles are \\(h_N/2, h_W/2, h_S/2, h_E/2\\), respectively. This gives us a lower bound for the area of \\(\\Omega\\):  \\(|\\Omega| \\ge |Q| + \\frac{h_N+h_W+h_S+h_E}{2} = 1 + \\frac{1}{2}(h_N+h_W+h_S+h_E)\\) From the width inequalities, we know that \\(2w_\\Omega \\le (1+h_N+h_S) + (1+h_W+h_E)\\), which implies \\(\\frac{1}{2}(h_N+h_W+h_S+h_E) \\ge w_\\Omega - 1\\). Substituting this into the area inequality yields the crucial geometric relationship:  \\(|\\Omega| \\ge 1 + (w_\\Omega - 1) \\implies |\\Omega| \\ge w_\\Omega\\)",
            "From **Concept 1**, we have the inequality for the first non-zero Neumann eigenvalue:  \\(\\mu_1(\\Omega) \\le \\frac{\\pi^2 w_\\Omega^2}{|\\Omega|^2}\\) Using our geometric result \\(|\\Omega| \\ge w_\\Omega\\), which implies \\(\\frac{w_\\Omega}{|\\Omega|} \\le 1\\), we can bound the term on the right:  \\(\\frac{\\pi^2 w_\\Omega^2}{|\\Omega|^2} = \\pi^2 \\left(\\frac{w_\\Omega}{|\\Omega|}\\right)^2 \\le \\pi^2\\) Combining these gives us:  \\(\\mu_1(\\Omega) \\le \\pi^2\\) From **Concept 2**, we know the value for the unit square:  \\(\\mu_1(Q) = \\pi^2\\) Therefore, we arrive at the final inequality:  \\(\\mu_1(\\Omega) \\le \\mu_1(Q)\\)"
        ],
        "step_count": 3
    },
    "recuW1AcCDUHSh": {
        "reasoning_steps": [
            "estimate $u^{-1}(I)\\cap \\left\\{|\\nabla u|>\\sigma\\right\\}$. Take $g=|\\nabla u|^{-1}$, $U=u^{-1}(I)\\cap\\left\\{|\\nabla u|>\\sigma\\right\\}$ in theorem 1 and apply theorem 3, we get \\begin{align*} \\left|u^{-1}(I)\\cap \\left\\{|\\nabla u|>\\sigma\\right\\}\\right|=&~\\int_{I}^{}dt \\int_{\\left\\{u=t\\right\\}\\cap \\left\\{|\\nabla u|>\\sigma\\right\\}}^{}\\frac{dS}{|\\nabla u|} \\\\ \\leq &~ \\sigma^{-1} \\int_{I}^{}\\mathcal{H}^{N-1}\\left(\\left\\{u=t\\right\\}\\cap \\left\\{|\\nabla u|>\\sigma\\right\\}\\right)dt, \\end{align*} where $\\mathcal{H}^{N-1}$ denotes the $(N-1)$-dimensional Hausdorff measure. By theorem 3 again \\begin{align*} \\mathcal{H}^{N-1}\\left(\\left\\{u=t\\right\\}\\cap \\left\\{|\\nabla u|>\\sigma\\right\\}\\right)\\leq&~ \\sigma^{1-p}\\int_{\\left\\{u=t\\right\\}}^{}|\\nabla u|^{p-1}dS \\\\ =&~\\sigma^{1-p}\\int_{\\left\\{u>t\\right\\}}^{}f(u)dx \\\\ \\leq&~ \\left\\| f \\right\\|_{L^\\infty([0,M])}|\\Omega|\\sigma^{1-p}, \\end{align*} where we have used theorem 2 in the second equality, so combine these estimates we derive \\[\\left|u^{-1}(I)\\cap \\left\\{|\\nabla u|>\\sigma\\right\\}\\right|\\leq C \\sigma^{-p}.\\] This gives $\\beta=p$.",
            "estimate $\\left\\{|\\nabla u|\\leq\\sigma, \\mathrm{dist}(x, \\partial\\Omega)>\\sigma^\\theta\\right\\}$. Denote $E=\\left\\{x\\in\\Omega: \\mathrm{dist}(x, \\partial\\Omega)>\\sigma^\\theta\\right\\}$, by theorem 3 and the integral inequality, note that $\\delta_-(E)=\\sigma^\\theta$ and $\\delta_+(E)\\leq d_\\Omega$, we have for $r=\\frac{1}{2}$ \\begin{align*} \\left|\\left\\{|\\nabla u|\\leq\\sigma, \\mathrm{dist}(x, \\partial\\Omega)>\\sigma^\\theta\\right\\}\\right|\\leq&~\\int_{E}^{}\\frac{\\sigma^{(p-1)r}}{|\\nabla u|^{(p-1)r}}dS \\\\ \\leq&~ C \\left(\\sigma^{(p-1)r-\\theta}+\\sigma^{(p-1)r-2\\theta}+\\sigma^{(p-1)r}\\right). \\end{align*} Since $\\theta>0$, the dominated term is $\\sigma^{(p-1)r-2\\theta}$.",
            "estimate $\\left\\{|\\nabla u|\\leq\\sigma, \\mathrm{dist}(x, \\partial\\Omega)\\leq\\sigma^\\theta\\right\\}$. By theorem 4 \\[\\left|\\left\\{|\\nabla u|\\leq\\sigma, \\mathrm{dist}(x, \\partial\\Omega)\\leq\\sigma^\\theta\\right\\}\\right|\\leq \\left|\\mathrm{dist}(x,\\partial\\Omega)\\leq\\sigma^\\theta\\right|\\leq C\\sigma^\\theta.\\] In order to make the two parts having same decay rate, we need $(p-1)r-2\\theta=\\theta$, which gives $\\theta=\\frac{(p-1)r}{3}=\\frac{p-1}{6}$, and thus $\\alpha=\\theta=\\frac{p-1}{6}$."
        ],
        "step_count": 3
    },
    "recuW2aBSTw4ST": {
        "reasoning_steps": [
            "We assume that $u_2$ is not a Morse function. This implies the existence of at least one degenerate critical point $p \\in M$ for $u_2$. By definition, this means that the Hessian of $u_2$ vanishes at $p$.",
            "Define an auxiliary function $w := u_2 - u_2(p)$. Clearly, $w$ is harmonic ($\\Delta w = 0$), and $p$ is a zero of $w$ such that both its gradient and Hessian vanish, i.e., $\\nabla w(p)=0$ and $D^2 w(p)=0$. According to the local theory of harmonic functions, at a zero that is also a degenerate critical point, the function's nodal set (zero level set) consists of at least three curves intersecting at that point with equal angles. These nodal curves divide a small geodesic disk $D$ around $p$ into $2k$ ($k \\ge 3$) disjoint regions, $D_i$ ($i=1, ..., 2k$).",
            "To arrive at a contradiction, we first assume that $u_2$ (and thus $w$) has at most 3 nodal domains in M. Since there are at least 6 regions $D_i$ with alternating signs around $p$, the pigeonhole principle implies that at least three non-adjacent regions (let us call them $D_1, D_2, D_3$) must belong to the same nodal domain, $\\Omega$. We can construct a simple closed curve $\\gamma_{12}$ that passes through $p$, remains within the nodal domain $\\Omega$, and connects the regions $D_1$ and $D_2$. Because $D_1$ and $D_2$ are non-adjacent, there must be a region between them that belongs to a different nodal domain, $\\Omega'$. **Crucially, because the manifold M is of genus 0, the curve $\\gamma_{12}$ separates M, and therefore the nodal domain $\\Omega'$ must be contained in the bounded region enclosed by $\\gamma_{12}$.** By repeating this argument for the pairs of regions $(D_1, D_3)$ and $(D_2, D_3)$, we can construct two other closed curves, $\\gamma_{13}$ and $\\gamma_{23}$, which respectively enclose two other distinct nodal domains, $\\Omega''$ and $\\Omega'''$. We have thus shown that the function $w$ must have at least four nodal domains ($\\Omega, \\Omega', \\Omega'', \\Omega'''$). This contradicts our starting assumption of at most 3 nodal domains, thereby proving that $w$ must have at least 4 interior nodal domains.",
            "Let $\\Omega_i$ ($i=1, ..., m$, with $m \\ge 4$) be the interior nodal domains of $w$. We construct a test function $\\phi := \\sum_{i=1}^m a_i w_i$, where $w_i$ is the restriction of $w$ to $\\Omega_i$ (and zero outside). Since $m \\ge 4$, it is always possible to find a non-trivial set of real coefficients $\\{a_i\\}$ that simultaneously solve the following two linear systems: (a) $\\int_{\\partial M} \\phi = \\sum_{i=1}^m a_i \\int_{\\partial \\Omega_i} w_i = 0$ (b) $\\sum_{i=1}^m a_i^2 \\int_{\\partial \\Omega_i} w_i = 0$. Condition (a) ensures that $\\phi$ is a valid test function for the second eigenvalue $\\sigma_2$. By the min-max principle, we have $\\sigma_2 \\le \\frac{\\int_M |\\nabla \\phi|^2}{\\int_{\\partial M} \\phi^2}$. We now compute the Rayleigh quotient for $\\phi$. The denominator is $\\int_{\\partial M} \\phi^2 = \\sum_{i=1}^m a_i^2 \\int_{\\partial \\Omega_i} w^2$. For the numerator, using Green's identity and the Steklov condition, we have $\\int_{\\Omega_i} |\\nabla w|^2 = \\sigma_2 \\int_{\\partial \\Omega_i} w^2 + \\sigma_2 u_2(p) \\int_{\\partial \\Omega_i} w$. Therefore, $\\sum_{i=1}^m a_i^2 \\int_{\\Omega_i} |\\nabla w|^2 = \\sigma_2 \\sum_{i=1}^m a_i^2 \\int_{\\partial \\Omega_i} w^2 + \\sigma_2 u_2(p) \\sum_{i=1}^m a_i^2 \\int_{\\partial \\Omega_i} w$. By our choice of coefficients $a_i$ (Condition b), the second term on the right-hand side vanishes. The numerator is thus exactly equal to $\\sigma_2 \\sum a_i^2 \\int_{\\partial \\Omega_i} w^2$. This means the Rayleigh quotient for $\\phi$ is precisely $\\sigma_2$. It follows that $\\phi$ is also a second Steklov eigenfunction. However, by its construction, $\\phi$ has at least four nodal domains, which contradicts Courant's Nodal Domain Theorem (any second eigenfunction can have at most two nodal domains)."
        ],
        "step_count": 4
    },
    "recuW2wddZto9U": {
        "reasoning_steps": [
            "Theorem 1 focuses on **aggressive driving behavior**, which is defined by a concave speed-density relationship in the free-flow regime (i.e., \\(v^{\\prime \\prime}(k) \\leq 0\\) for densities \\(k \\leq k_{crit}\\), where \\(k_{crit}\\) is the density that maximizes traffic flow). The core goal is to prove that the path formed by the expected travel time \\(\\mathbb{E}[\tau(t)]\\) and travel time variance \\(Var[\tau(t)]\\) (for vehicles entering the corridor at different times \\(t\\) during rush hour) forms a **counterclockwise loop** in the mean-variance phase plane.",
            "Lemma 3.1 provides the critical criterion for variance dominance, which is essential to proving the counterclockwise loop. Its core content is: - Consider two vehicles (A and B) entering the corridor at times \\(t_1 < t_2\\) with identical expected travel times (\\(\\mathbb{E}[\tau_1] = \\mathbb{E}[\tau_2]\\)). - Let \\(\\Delta \tau = \tau(t_2) - \tau(t_1)\\) (the travel time difference between B and A) be a function of peak flow \\(q_p\\). If there exists an \\(x_0\\) such that: 1. \\(\\Delta \tau(x) < 0\\) when \\(x < x_0\\), 2. \\(\\Delta \tau(x) > 0\\) when \\(x > x_0\\) (i.e., \\(\\Delta \tau\\) undergoes exactly one sign change from negative to positive), 3. \\(\\tau_1\\) (travel time of vehicle A) is an increasing function of \\(q_p\\), - Then, \\(Var[\tau_2] \\geq Var[\tau_1]\\) holds. This lemma is foundational because a counterclockwise loop in the mean-variance plane requires variance to increase as the expected travel time evolves (consistent with \\(Var[\tau_2] \\geq Var[\tau_1]\\) when \\(\\mathbb{E}[\tau_1] = \\mathbb{E}[\tau_2]\\)).",
            "In the scenario of Theorem 1 (rush hour congestion on a single corridor with stochastic peak flow \\(q_p\\)): - For **low peak flow \\(q_p < x_0\\)** (where \\(x_0\\) is a threshold peak flow): Vehicle A (entering earlier at \\(t_1\\)) experiences less congestion because the upstream flow is lower. Thus, \\(\\tau_1 < \\tau_2\\), leading to \\(\\Delta \tau = \\tau_2 - \\tau_1 < 0\\). - For **high peak flow \\(q_p > x_0\\)** : As \\(q_p\\) increases, the upstream flow intensifies, and congestion accumulates. Vehicle B (entering later at \\(t_2\\)) is more affected by this increased congestion—its travel time \\(\\tau_2\\) rises faster than \\(\\tau_1\\) (since A entered before the congestion peaked). Eventually, \\(\\tau_2 > \\tau_1\\), leading to \\(\\Delta \tau = \\tau_2 - \\tau_1 > 0\\). This means \\(\\Delta \\tau\\) has exactly one sign change (from negative to positive) at \\(x_0\\), satisfying the first two conditions of Lemma 3.1.",
            "In the context of Theorem 1: - Peak flow \\(q_p\\) directly determines the severity of rush hour congestion: a larger \\(q_p\\) means more vehicles enter the corridor, increasing traffic density \\(k\\) and reducing speed \\(v\\) (per the speed-density relationship). - Vehicle A’s travel time \\(\\tau_1\\) depends on the congestion level during its trip. As \\(q_p\\) increases, congestion becomes more severe, so \\(\\tau_1\\) (the time A takes to traverse the corridor) increases. Thus, \\(\\tau_1\\) is an increasing function of \\(q_p\\), satisfying the third condition of Lemma 3.1.",
            "Combining the above steps with the unimodality of \\(\\mathbb{E}[\tau(t)]\\) (a key property stated in the paper: \\(\\mathbb{E}[\tau(t)]\\) is a unimodal function of departure time, approaching the free-flow travel time \\(\\tau_{free}\\) for very early/late departures and peaking during rush hour): - When \\(\\mathbb{E}[\tau(t)]\\) **increases** (from early to peak departure times): For any two vehicles A (earlier) and B (later) with \\(\\mathbb{E}[\tau_1] = \\mathbb{E}[\tau_2]\\), Lemma 3.1 gives \\(Var[\tau_2] \\geq Var[\tau_1]\\)—so variance rises with the expected travel time. - When \\(\\mathbb{E}[\tau(t)]\\) **decreases** (from peak to late departure times): For another pair of vehicles (A’ and B’) with \\(\\mathbb{E}[\tau_{A’}] = \\mathbb{E}[\tau_{B’}]\\), the same logic applies: \\(Var[\tau_{B’}] \\geq Var[\tau_{A’}]\\). Variance remains relatively high even as the expected travel time falls. This trajectory—\\(\\mathbb{E}[\tau]\\) first rising with increasing \\(Var[\tau]\\), then falling with \\(Var[\tau]\\) still elevated—forms a counterclockwise loop in the mean-variance plane, completing the proof of Theorem 1."
        ],
        "step_count": 5
    },
    "recuVmZdC6WvW1": {
        "reasoning_steps": [
            "The energy required for water molecules to transition from the liquid phase to the vapor phase is characterized by the evaporation enthalpy, ΔH_evap. A lower ΔH_evap facilitates faster evaporation due to reduced energy barriers for molecular escape. A higher ΔH_evap makes evaporation more difficult by stabilizing the water structure through stronger hydrogen bonding. Thus, any factor that modifies the hydrogen-bond (HB) network or the local molecular environment can substantially alter evaporation performance.",
            "During interfacial solar evaporation, ions in seawater redistribute near the photothermal interface depending on their hydration properties and affinities for surface functional groups. Divalent ions such as Mg²⁺ and Ca²⁺ can undergo spontaneous ion exchange with the interface material, selectively accumulating at the interface and disrupting the HB network of interfacial water molecules. This process reduces local ΔH_evap and promotes more efficient molecular transport across the interface, leading to enhanced evaporation dynamics.",
            "Natural seawater contains multiple dissolved ions at significantly different concentrations. Because Na⁺ is the most abundant and strongly hydrated, it dominates bulk thermodynamic behavior and tends to increase ΔH_evap, suppressing evaporation. Conversely, although Mg²⁺ and Ca²⁺ are lower in concentration, their preferential accumulation at the interface allows them to significantly influence local HB networks, reducing interfacial energy barriers and thus locally enhancing evaporation rates. K⁺, despite its low concentration, has minimal affinity for interfacial sites and consequently has a relatively minor impact on overall evaporation.",
            "Combining these mechanisms and observations: Mg²⁺ & Ca²⁺ locally promote seawater evaporation, while Na⁺ & K⁺ tend to suppress it."
        ],
        "step_count": 4
    },
    "recuVZRfUNCp3n": {
        "reasoning_steps": [
            "Start from the heat-transfer channels (Concept 1). Across the air layer, heat can pass by conduction, convection, and radiation.",
            "Eliminate convection at low airflow (Concept 2). Under low-velocity (quiescent) conditions, convection contributes little, so the effective heat transfer reduces to a conduction + radiation problem.",
            "Thickness penalizes conduction (basic scaling). For a given temperature difference, the conductive path through air behaves like a thermal resistance that increases with layer thickness; thus, as the air layer gets thicker, the conductive heat flux weakens.",
            "Surface temperatures set radiation (basic scaling). Radiative exchange depends mainly on the surface temperatures (∝ T4 difference) and only weakly on the gap thickness; when the gap is thicker (and conduction is weaker), the hot surface must run hotter to pass the imposed heat, which boosts the radiative share.",
            "Thin vs. thick regimes. Thin gap: conduction is strong, surfaces need only modest temperature rise → conduction dominates, radiation is secondary. Thick gap: conduction is suppressed, surfaces run hotter → radiation dominates, conduction is secondary.",
            "Conclusion. As the air-layer thickness increases, the dominant heat-transfer contribution shifts from conduction to radiation (with convection negligible under low-velocity conditions)."
        ],
        "step_count": 6
    },
    "recuVUrfCLyKJh": {
        "reasoning_steps": [
            "Let $R$ be the symmetric key rate, such that $R = H(K_1) = H(K_2)$. We aim to find the maximum possible value of $R$. Let $X_{In(v)}$ denote the information received by any node $v$.",
            "From the network topology and the entropy bound (Concept_1), the information received at nodes `x` and `z` is constrained. The total information entering `x` from the source is bounded by the link capacity, $H(X_{In(x)}) \\le 2$. The joint entropy of information at `x` and `z` can be expressed using the chain rule (Theorem_1) as $H(X_{In(x)}, X_{In(z)}) = H(X_{In(x)}) + H(X_{In(z)}|X_{In(x)})$. This leads to the inequality $H(X_{In(x)}, X_{In(z)}) \\le 2 + 0.5H(X_{In(z)})$.",
            "According to the decodability requirement (Concept_2), key $K_1$ must be a function of the information received at terminal $d_{11}$, which comes from nodes `x` and `z`.",
            "Applying the security constraint (Concept_3), the information from node `z` is independent of $K_1$. This allows us to establish a lower bound on the entropy of the information received by terminal $d_{11}$, which is shown to be at least $2H(K_1)$.",
            "The total information available at nodes `x` and `z`, $H(X_{In(x)}, X_{In(z)})$, must be sufficient to generate the signals for terminal $d_{11}$ and must also be independent of key $K_2$ due to the security constraint (Concept_3) and key independence (Concept_4). Using the independence property (Theorem_2), this implies $H(X_{In(x)}, X_{In(z)}) \\ge H(\\text{info for } d_{11}) + H(K_2)$.",
            "Combining the results from steps 4 and 5, we derive a lower bound on the joint entropy in terms of the key rates: $H(X_{In(x)}, X_{In(z)}) \\ge 2H(K_1) + H(K_2) = 3R$.",
            "We can also derive another bound using the security at node `z`. Since $X_{In(z)}$ is independent of $K_1$ (Concept_3), we have $H(X_{In(x)}, X_{In(z)}) \\ge H(X_{In(z)}, K_1) = H(X_{In(z)}) + H(K_1) = H(X_{In(z)}) + R$.",
            "Finally, combining the lower bound from step 6 with the upper bound from step 9 gives the final constraint on the rate: $3R \\le H(X_{In(x)}, X_{In(z)}) \\le 4 - R$. This resolves to $4R \\le 4$, which means $R \\le 1$. Since the thesis also provides a constructive scheme that achieves a rate of 1 for this configuration, the maximum achievable rate is exactly 1."
        ],
        "step_count": 8
    },
    "recuVOH2xdRMfC": {
        "reasoning_steps": [
            "Rule out the \"interface excitation\" hypothesis: According to Concept 1 (mechanical oscillator effect), internal waves may indeed be excited at the top of the convection due to the up-and-down oscillation of air masses. However, this mechanism usually generates high-frequency waves only in the upper atmosphere under strong stable stratification and is not universal. Meanwhile, Concept 2 (narrowband spectrum vs. broadband interface) points out that the internal wave spectrum emitted by plumes/fountains is narrowband, while the interface turbulent/non-turbulent oscillations are broadband. The inconsistency between the two indicates that the observed internal waves do not originate from interface fluctuations. Conclusion: The interface is not the main excitation region.",
            "Confirm that the \"turbulent region inside the fountain\" is the main excitation source: Concept 3 (Reynolds stress excitation) provides the physical mechanism: when the convective zone interacts with the stable layer, internal waves are excited by the Reynolds stress within the convection/turbulence, which is different from the interface mechanism. Concept 4 (viscous internal wave model) further verifies: if a virtual source is set in the turbulent layer and extrapolated, the model can better reconstruct the far-field internal wave spectrum, indicating that the source region of internal waves is within the turbulence. Conclusion: Internal waves mainly originate from the turbulent region inside the fountain."
        ],
        "step_count": 2
    },
    "recuVJksNKr5dI": {
        "reasoning_steps": [
            "For isotropic linear elastic material, the micropotential $w(\\xi)$ of peridynamic is $w(\\xi)=\\frac{1}{2} c s^2|\\xi|$",
            "According to concept_1, the strain energy density of peridynamic becomes: $W=\\frac{1}{2} \\int_{\\mathcal{H}_x} \\frac{1}{2} c s^2|\\xi| d V_{\\xi}=\\frac{c}{4} \\int_{\\mathcal{H}_x} s^2|\\xi| d V_{\\xi}$",
            "The bond stretch $s$ is equal to strain $\\varepsilon$",
            "Calculated the strain energy density of peridynamic and obtained: $W_{pd}=\\frac{c \\varepsilon^2}{4} \\cdot \\pi \\delta^4=\\frac{c \\pi \\delta^4}{4} \\varepsilon^2$",
            "Let the strain energy density of peridynamic equal to the strain energy density of classical continuum mechanics (concept_2)",
            "Obtain the solution $\\frac{6 E}{\\pi \\delta^4(1-2 \\nu)}$"
        ],
        "step_count": 6
    },
    "recuUwZ7AtDDsQ": {
        "reasoning_steps": [
            "Introduce the variable-curvature FLRW metric Based on concept_1, a curvature term \\kappa(z)varying with redshift is introduced into the standard FLRW metric, treating the spatial curvature of the Universe as a time-evolving function. This step provides the mathematical framework of the model, in which cosmic dynamics no longer rely on an additional dark-energy component but instead allow curvature variation to affect the expansion rate.",
            "Analyze the role of curvature in the acceleration equation Following concept_2, \\kappa(z)and its time derivative \\dot{\\kappa}(z) are substituted into the FLRW acceleration equation. It is found that even in the absence of dark energy, the variation of curvature introduces additional terms, enabling the emergence of cosmic acceleration at low redshift. This ensures theoretical feasibility, i.e., curvature evolution itself can drive acceleration.",
            "Parametrize curvature evolution while maintaining smoothness According to concept_3, a smooth step-like curvature function is chosen, implementing a transition at low redshift (z∼0.5–0.6) from nearly flat to slightly nonzero curvature. Meanwhile, the condition \\dot{\\kappa} \\approx 0 is imposed, ensuring that the cosmological principle is only mildly violated without introducing discontinuities. This step guarantees that the model remains physically consistent while producing the required acceleration effect.",
            "Constrain the model with observational data Following concept_4, observational data from cosmic chronometers (CC) and Type Ia supernovae (SNIa) are used in MCMC fitting to optimize the parameters of the curvature function, and to reconstruct H(z), q(z), and w_{\\mathrm{eff}}(z). Through this fitting procedure, the model is tested against whether it can successfully reproduce the observed history of accelerated expansion.",
            "Verify consistency with ΛCDM Based on concept_5, the fitting results show that the “effective phase transition” of acceleration occurs at z∼0.5–0.6, with Ωm comparable to that in the ΛCDM model. Moreover, the model predicts a slight deceleration at z=0, consistent with several dark-energy parameterizations. This step confirms that the model can not only reproduce acceleration but also maintain a matter density consistent with standard cosmology, thereby addressing the original instruction."
        ],
        "step_count": 5
    },
    "recuUzEke6Xwzy": {
        "reasoning_steps": [
            "Use the eigenvalues \\(\\mu_n\\) and eigenfunctions \\((\\varphi_n, \\psi_n)\\) of the operator \\(\\mathcal{O}_{\\Delta, R}\\) to construct a bad sequence. Let \\(\\lambda_n = \\mu_n\\), \\(U_n = \\left( \\frac{\\varphi_n}{i\\mu_n}, \\varphi_n, \\frac{\\psi_n}{i\\mu_n}, \\psi_n, -\\frac{1}{i\\mu_n}C\\gamma(\\varphi_n) \\right)\\), and \\(F_n = \\left( 0, 0, 0, 0, -\\frac{i}{\\mu_n}BC\\gamma(\\varphi_n) \\right)\\).",
            "Verify that this sequence satisfies \\((i\\mu_n I - A)U_n = F_n\\).",
            "Use the multiplier method to derive that \\(\\|U_n\\|_H^2 \\geq 1\\).",
            "According to the boundary trace theorem, show that \\(\\|F_n\\|_H^2 \\to 0\\).",
            "Then \\(\\lim_{n \\to \\infty} \\|(i\\lambda I - A)^{-1}\\| = \\lim_{\\mu_n \\to \\infty} \\frac{\\|U_n\\|_H}{\\|F_n\\|_H} = \\infty\\), which means that the resolvent of operator \\(A\\) is not uniformly bounded on the imaginary axis, thus proving that the system is not exponentially stable."
        ],
        "step_count": 5
    },
    "recuVZfQqvTo2l": {
        "reasoning_steps": [
            "Compute each term in $\\langle A(X-Y),\\,X-Y\\rangle$.",
            "Use the Lipschitz continuity of $f(u)$ together with Young’s inequality to estimate $\\int_\\Omega \\big(f(u_1)-f(u_2)\\big)\\,(\\bar u_1-\\bar u_2)\\,dx$.",
            "Apply the Cauchy inequality to estimate the cross term between $z$ and $\\bar u$.",
            "Using commutativity and the relation $z(\\cdot,0)=\\bar u$, estimate $\\frac{\\xi\\|a\\|_\\infty}{2}\\int_0^1\\int_\\Omega \\big(z_1(x)-z_2(x)\\big)\\big(\\partial_\\rho z_1(x)-\\partial_\\rho z_2(x)\\big)dxd\\rho$.",
            "Perform analogous estimates for $v$ and $w$.",
            "Add together all the resulting inequalities, and, using the assumptions on $\\mu_1,\\mu_2,\\xi$, derive the final result."
        ],
        "step_count": 6
    },
    "recuV0Ekx58aPd": {
        "reasoning_steps": [
            "Clarify the quantitative indicator of rate enhancement: According to the definition of rate enhancement in the paper, the rate enhancement of NESS diffusion-controlled reactions relative to equilibrium is quantified by the combination of the logarithmic ratio of reactive frequencies and transition path times. Specifically, it is expressed as \\( \\ln\\frac{k_\\xi \\tau_0^c}{k_0 \\tau_\\xi^c} \\), where \\( k_\\xi \\) and \\( k_0 \\) are the reactive frequencies under NESS (external field \\( \\xi \\)) and equilibrium, respectively, and \\( \\tau_\\xi^c \\) and \\( \\tau_0^c \\) are the transition path times under NESS and equilibrium, respectively. This indicator is the core object for subsequent derivation of the upper limit.",
            "Use Girsanov Transform to link trajectory probability ratio with reactive correlation function ratio: Apply Theorem 1 (Girsanov Transform), which is a tool to calculate the logarithmic ratio of trajectory probabilities under equilibrium (\\( \\xi=0 \\)) and NESS (external field \\( \\xi \\)). First, derive the logarithmic ratio of a trajectory’s probability under \\( \\xi \\) (\\( P_\\xi \\)) to that under equilibrium (\\( P_0 \\)) as \\( \\Gamma' = \\ln\\frac{P_\\xi[\\{X_t\\}|r_0]}{P_0[\\{X_t\\}|r_0]} = -\\frac{\\beta D}{4}\\int_0^\\tau dt\\left[\\xi^2 - 2\\xi \\hat{z}\\cdot\\left(\\beta^{-1}D^{-1}\\dot{r} - F\\right)\\right] \\). Then, introduce the auxiliary function \\( \\Gamma = \\beta \\Gamma' + \\ln\\frac{\\rho_\\xi(r_0)}{\\rho_0(r_0)} \\) (where \\( \\rho_\\xi/\\rho_0 \\) is the ratio of steady-state probability densities at the initial position \\( r_0 \\) under NESS and equilibrium). Through probability normalization, link the ratio of reactive correlation functions (\\( \\ln\\frac{h_\\xi(\\tau)}{h_0(\\tau)} \\), \\( h_\\xi(\\tau) \\) and \\( h_0(\\tau) \\) are the probabilities of observing AB-reactive trajectories within time \\( \\tau \\) under NESS and equilibrium, respectively) to the conditional average of the auxiliary function, resulting in \\( \\ln\\frac{h_\\xi(\\tau)}{h_0(\\tau)} = -\\ln\\langle e^{-\\Gamma} \\rangle_{AB,\\xi} \\) (where \\( \\langle \\cdot \\rangle_{AB,\\xi} \\) is the conditional average over AB-reactive trajectories).",
            "Apply Jensen’s Inequality to obtain the upper limit of the correlation function ratio: Use Jensen’s Inequality for convex functions. Since the exponential function \\( e^x \\) is convex, the inequality \\( -\\ln\\langle e^{-\\Gamma} \\rangle_{AB,\\xi} \\leq \\langle \\Gamma \\rangle_{AB,\\xi} \\) holds. Combining the result from Step 2 (\\( \\ln\\frac{h_\\xi(\\tau)}{h_0(\\tau)} = -\\ln\\langle e^{-\\Gamma} \\rangle_{AB,\\xi} \\)) and the rate enhancement indicator from Step 1 (\\( \\ln\\frac{k_\\xi \\tau_0^c}{k_0 \\tau_\\xi^c} = \\ln\\frac{h_\\xi(\\tau)}{h_0(\\tau)} \\)), we get the preliminary upper limit relationship: \\( \\ln\\frac{k_\\xi \\tau_0^c}{k_0 \\tau_\\xi^c} \\leq \\langle \\Gamma \\rangle_{AB,\\xi} \\).",
            "Expand \\( \\langle \\Gamma \\rangle_{AB,\\xi} \\) and associate it with the work done by the external field: According to Concept 5, the auxiliary function \\( \\Gamma \\) expands to \\( \\langle \\Gamma \\rangle_{AB,\\xi} = \\beta \\langle \\Gamma' \\rangle_{AB,\\xi} + \\langle \\ln\\frac{\\rho_\\xi}{\\rho_0} \\rangle_{AB,\\xi} \\) under the conditional average \\( \\langle \\cdot \\rangle_{AB,\\xi} \\). Then, simplify \\( \\beta \\langle \\Gamma' \\rangle_{AB,\\xi} \\): neglect the weak ion-ion conservative force term \\( F \\) (since the ion-paired state boundary \\( \\partial B \\) is near the Bjerrum length \\( l_B \\), \\( F \\) is weak, making \\( \\int_0^\\tau dt \\cdot \\hat{z}\\cdot F \\approx 0 \\)), and substitute the definitions of the two components of the work done by the external field from Concept 4. For the \\( \\xi^2 \\) term, it corresponds to the work on free ions \\( W_f = \\beta D \\xi^2 \\tau \\), so \\( -\\frac{\\beta^2 D \\xi^2 \\tau}{4} = -\\frac{\\beta}{4}W_f \\); for the \\( \\xi \\hat{z}\\cdot\\dot{r} \\) term, it corresponds to the mean work on AB-reactive trajectories \\( \\langle W \\rangle_{AB,\\xi} = \\langle \\xi \\int_0^\\tau dt \\hat{z}\\cdot\\dot{r} \\rangle_{AB,\\xi} \\), so \\( \\frac{\\beta^2 D \\xi}{2}\\int_0^\\tau dt \\cdot \\hat{z}\\cdot\\left(\\beta^{-1}D^{-1}\\dot{r}\\right) = \\frac{\\beta}{2}\\langle W \\rangle_{AB,\\xi} \\). Thus, \\( \\beta \\langle \\Gamma' \\rangle_{AB,\\xi} = \\frac{\\beta}{2}\\langle W \\rangle_{AB,\\xi} - \\frac{\\beta}{4}W_f \\).",
            "Derive the final upper limit formula of rate enhancement: Substitute \\( \\beta \\langle \\Gamma' \\rangle_{AB,\\xi} = \\frac{\\beta}{2}\\langle W \\rangle_{AB,\\xi} - \\frac{\\beta}{4}W_f \\) into the preliminary upper limit relationship from Step 3. The result is the upper limit formula of rate enhancement for NESS diffusion-controlled reactions: \\( \\ln\\frac{k_\\xi \\tau_0^c}{k_0 \\tau_\\xi^c} \\leq \\frac{\\beta}{2}\\langle W \\rangle_{AB,\\xi} - \\frac{\\beta}{4}W_f + \\langle \\ln\\frac{\\rho_\\xi}{\\rho_0} \\rangle_{AB,\\xi} \\). Among the terms on the right, \\( \\frac{\\beta}{2}\\langle W \\rangle_{AB,\\xi} \\) is the positive contribution of the work on AB-reactive trajectories, \\( -\\frac{\\beta}{4}W_f \\) is the negative contribution of the work on free ions, and \\( \\langle \\ln\\frac{\\rho_\\xi}{\\rho_0} \\rangle_{AB,\\xi} \\) is the minor contribution of the steady-state probability density ratio."
        ],
        "step_count": 5
    },
    "recuVCQfkTKMZY": {
        "reasoning_steps": [
            "Based on Concept 2 (Barus' Law), clarify the core relationship of pressure-induced viscosity: pressure-induced viscosity satisfies Barus' Law, that is, the fluid viscosity increases exponentially with pressure, and the formula is: \\eta = \\eta_0 e^{\\alpha p}, where \\eta_0 is the viscosity at the reference pressure, \\alpha is the pressure-viscosity coefficient, and p is the pressure.",
            "Combine Concept 1 (lubrication approximation method) with Concept 4 (asymptotic behavior at small velocities) to simplify the equation: According to Concept 1, under the lubrication approximation, the flow equation can be simplified to a two-dimensional thickness evolution equation; further, based on Concept 4, under the small velocity limit (satisfying \\delta = \\frac{3C}{\\theta_{\\text{eq}}^3} \\ll 1, where \\theta_{\\text{eq}} is a characteristic quantity such as the equilibrium contact angle, and C is a relevant coefficient), the lubrication equation can retain the low-order term approximate expansion to capture the characteristics of thin film thickness evolution and contact line movement at low velocities. At this point, combined with the viscosity expression from Barus's law, perform a low-order expansion and simplification on the flow equation.",
            "Relate to the slip length in Concept 3 (contact line singularity paradox): As known from Concept 3, the viscous resistance to the movement of the contact line in the classical model is deeply bound to the \"slip length\" at the solid-liquid interface in the sliding model. At low velocities, the influence of pressure-induced viscosity changes in the pressure-induced viscosity model on the velocity gradient near the contact line can be equivalent to the effect of adjusting the slip length with flow conditions in the sliding model.",
            "Equivalence derivation: By performing an asymptotic expansion of the lubrication equation related to pressure-induced viscosity at small velocities (combining Concept 1, Concept 2, and Concept 4), and then incorporating the critical role of the \"slip length\" in resistance calculation from Concept 3, it can be found that the description of the contact line motion by the pressure-induced viscosity model is completely consistent with the way the velocity-dependent slip model characterizes the sliding effect at the solid-liquid interface through the slip length, thereby achieving equivalence."
        ],
        "step_count": 4
    },
    "recuUTLZzkmYnd": {
        "reasoning_steps": [
            "Clarify the task: map applied potential (U, vs RHE) to surface state of Pd/Pt and then to dominant reaction pathways and products.",
            "Invoke electrochemical thermodynamics: use bulk and surface Pourbaix reasoning to determine stable phases under operating U. For Pd: metallic Pd at lower U transitions to PdO as U increases; for Pt: PtO2 is stable across the relevant U.",
            "Identify reconstructed active centers: at low U, Pd(111) with ~1/3 ML O*; at higher U, PdO(101) with ~1/2 ML OH*; for Pt, PtO2(110) with ~1/2 ML OH* over the whole window.",
            "Set up the reaction network on each active center: enumerate elementary steps leading to acrolein, acetone, propylene oxide (PO), and propylene glycol (PG), including O*/OH* coupling and electrochemical dehydrogenation (PCET).",
            "Evaluate step thermodynamics at potential using CHE: ΔG(U)=ΔG°−n e U for PCET steps; constant-potential GC-DFT provides ΔG and barriers for chemical steps (C–O coupling, intramolecular H transfer).",
            "Link potential to kinetics: for electrochemical steps use Butler–Volmer-type barrier dependence ΔG‡(U)=ΔG‡(0)−α e U; combine with Arrhenius k=ν·exp(−ΔG‡/kBT).",
            "Construct microkinetic steady-state: write rate expressions for formation of each product on each surface; solve steady-state coverages and rates; compute TOF_i(U) for each product and surface.",
            "Incorporate surface-phase coexistence for Pd: as U increases, weight rates from Pd(111) and PdO(101) by their potential-dependent fractions (reflecting gradual oxidation), yielding smooth transitions in selectivity.",
            "Analyze selectivity control: use Degree of Rate Control (DRC) and Degree of Selectivity Control (DSC) to identify RDS/SS points—e.g., on PtO2, CH2CHOCH3*→PO* controls PO rate; on PdO, conversion of CH2CH(OH)CH3* biases PG vs acetone with U.",
            "Deduce Pd selectivity vs U: at 0.7–0.9 VRHE on O*-Pd(111), low barrier path via allylic dehydrogenation + O* coupling yields acrolein dominance; as U→1.0–1.4 VRHE and PdO/OH* grows, CH2CH(OH)CH3* channels favor acetone and PG (PG gains at higher U as its C–O coupling barrier drops faster).",
            "Deduce Pt selectivity vs U: on OH*-PtO2(110) across 1.2–1.6 VRHE, formation of CH2CH(OH)CH3* then branches with similar barriers; slight preference and lowering of barrier to epoxidation maintains PO along with acetone; PG remains minor due to higher access barrier to the dihydroxylated route.",
            "Synthesize to final answer: attribute product maps directly to potential-driven reconstruction (phase + coverage) rather than only barrier shifts; report main products and U ranges for Pd and Pt consistent with the mechanism."
        ],
        "step_count": 12
    },
    "recuW73ODRO295": {
        "reasoning_steps": [
            "Under the premise of 'wet/dry bistability + WTG availability', regard the evolution of humidity as a type of Allen–Cahn reaction-diffusion system. The uniform RCE state will fall into two stable fixed points q₊ and q₋ under perturbation, forming a coexistence of wet and dry phases and entering the stage of scale coarsening. Universal conclusions of the Allen–Cahn system: interface thickness ~ (κ/μ)^(1/2), small patches merge into large patches, interface curvature is smoothed out, and the typical scale increases monotonically.",
            "Incorporate rotation fff, friction α\\alphaα, and thermal damping λ\\lambdaλ, and define the dynamic effective scale LdynL_{\\mathrm{dyn}}Ldyn of WTG. When the coarsening scale approaches LdynL_{\\mathrm{dyn}}Ldyn, large-scale equilibrium and damping inhibit further coarsening. Although the domain scale is 'much larger than' LdynL_{\\mathrm{dyn}}Ldyn, sustainable coarsening will be clamped around LdynL_{\\mathrm{dyn}}Ldyn, and the system enters a dynamically constrained 'post-coarsening' stage (no longer unifying towards the domain scale).",
            "Determine the long-term area fraction A+ of moist regions using the balance of Ghq(q;Fh(q)) (Eqs.8-10). The system adjusts Fh(q) to a stable value Cs, where the potential areas V+(Cs)=V−(Cs) and the interface speed cRD→0. The area ratio A+/A− is determined by initial conditions and the balance of divergence (moist regions: convergence; dry regions: divergence), and remains stable without systematic contraction or expansion of moist regions. This leads to Conclusion-1: 'The area ratio of moist to dry regions stabilizes at a value determined by global heating balance (Fh(q)=Cs), with no systematic contraction or expansion relative to the post-initial-separation state.'",
            "Introduce a moderate-intensity nonlinear horizontal advection term ϵ(u ⁣⋅ ⁣∇q) and consider the flow field response when f≠0. Advection stretches the contour lines into anisotropic structures; as ϵ increases from small to moderate, the steady state (or quasi-steady state) transitions from zonal to cross-shaped more easily. This leads to Conclusion-2 ('The morphology of the wet region is anisotropic, and may form zonal/cross shapes'). When f≠0, the rotational flow stretches moist contours into filamentous/spiral shapes; only when f=0 do banded/cross-shaped structures form. This leads to Conclusion-2: 'Moist region morphology is anisotropic, with banded/cross shapes (f=0) or filamentous/spiral shapes (f≠0).'",
            "After clamping L_{\\mathrm{dyn}}, the system is not strictly stationary: the interface is still (finely adjusted) at an extremely slow speed due to the local residual and curvature terms of c_{RD}, as well as weak advection traction, showing residual dynamics. Therefore, in long-term statistics, it is a quasi-steady state, not a strictly steady state; this gives Answer-3. In Regime I, residual evolution at the interface is extremely weak, approaching a steady state; only in Regime II does slow residual evolution persist. This leads to Conclusion-3: 'The main coarsening process ends, and the steady state depends on the regime—nearly static (Regime I) or with slow residual evolution (Regime II).'"
        ],
        "step_count": 5
    },
    "recuWmZtmofWbT": {
        "reasoning_steps": [
            "First, under the given spherically symmetric static metric \\[ds^2=-g(r)\\,dt^2+\\frac{dr^2}{g(r)}+r^2 d\\Omega^2\\] adopt the purely electric gauge potential \\(A_\\mu=h(r)\\,\\delta_\\mu^{\\,t}\\).",
            "In accordance with \\textbf{concept_1} (taking \\(\\mathcal{P}=0\\) for the purely electric case) and the ModMax Lagrangian from \\textbf{theorem_1} \\[\\mathcal{L}=\\frac12\\!\\left(\\mathcal{S}\\cosh\\gamma-\\sqrt{\\mathcal{S}^2+\\mathcal{P}^2}\\,\\sinh\\gamma\\right),\\qquad \\mathcal{S}=\\frac{F}{2},\\ \\ \\mathcal{P}=0,\\] it can be derived that the derivative with respect to \\(F\\) is a constant: \\[\\mathcal{L}_F\\equiv\\frac{\\partial\\mathcal{L}}{\\partial F}=\\frac{\\cosh\\gamma+\\sinh\\gamma}{4}.\\]",
            "As a result, the source-free field equation \\[\\nabla_\\mu\\!\\left(\\mathcal{L}_F F^{\\mu\\nu}\\right)=0\\] reduces to the standard form \\(\\nabla_\\mu F^{\\mu\\nu}=0\\).",
            "Using \\(\\sqrt{-g}=r^2\\sin\\theta\\) and noting that only the \\(tr\\)-component is non-vanishing, setting \\(\\nu=t\\) gives \\[0=\\nabla_\\mu F^{\\mu t} =\\frac{1}{\\sqrt{-g}}\\partial_\\mu\\!\\left(\\sqrt{-g}\\,F^{\\mu t}\\right) =\\frac{1}{r^2\\sin\\theta}\\partial_r\\!\\left(r^2\\sin\\theta\\,F^{rt}\\right) \\ \\Rightarrow\\ \\partial_r\\!\\left(r^2 F^{rt}\\right)=0,\\] thus leading to \\[r^2 F^{rt}=-Q,\\] where the constant is denoted by the charge \\(Q\\).",
            "Furthermore, since \\[F_{tr}=-\\partial_r A_t=-h'(r),\\qquad F^{tr}=g^{tt}g^{rr}F_{tr}=h'(r)\\ \\Rightarrow\\ F^{rt}=-h'(r),\\] substituting back yields \\[h'(r)=\\frac{Q}{r^2}.\\]",
            "In natural units and with the gauge condition \\(h(\\infty)=0\\), integration results in \\[\\boxed{\\,h(r)=-\\frac{Q}{r}\\,}.\\]"
        ],
        "step_count": 6
    },
    "recuW6lRxSkllV": {
        "reasoning_steps": [
            "Thermodynamic constraints: Use Concept 1 (Clausius-Clapeyron relationship): The saturated water vapor pressure decreases rapidly with temperature; the lower the temperature, the less water vapor the air can hold. Use Concept 2 (the role of the cold-point tropopause): When an air mass rises past the point of the lowest temperature, it is \"freeze-dried,\" and the water vapor concentration is fixed at a very low value. It is concluded that the minimum temperature of the air mass determines the upper limit of the saturated water vapor pressure entering the stratosphere. This corresponds to option A.",
            "Dynamic transport: Using Concept 3 (Zero-radiation heating altitude and large-scale ascent): Air rises slowly above the zero-radiation heating altitude and is transported to the stratosphere, thereby ensuring that the water vapor concentration set at the minimum temperature point can be retained and transported on a long-term, large-scale basis. Deduce the conclusion: The slow large-scale ascent above the zero-radiation heating altitude is a necessary process for maintaining and transferring the minimum temperature constraint. This corresponds to option C.",
            "Verification and Exclusion: Using Concept 4 (Atmospheric Recorder Effect): Observations show that stratospheric water vapor exhibits stripes over time and altitude, which verifies that the minimum temperature signal is transmitted with fidelity, supporting the correctness of A and C. Using Concept 5 (Finiteness of Tropopause Penetration): Penetration events can indeed inject water vapor, but their impact is local and short-lived, and they cannot serve as the dominant factor for long-term averages. Therefore, B and D are excluded."
        ],
        "step_count": 3
    },
    "recuVoimWGmeWz": {
        "reasoning_steps": [
            "Under neutral bicarbonate (weakly alkaline) conditions, the CO2RR follows the 'CO pathway.' The fundamental stepwise process is: CO₂ + H* → COOH; COOH + H → CO + H₂O. Protons in the system are supplied by water dissociation (H₂O → H + OH*), coupling proton transfer with product formation.",
            "The active site is an asymmetric diatomic site centered on M–Nₓ and adjacent to the main-group element Te (represented by Te–Cu M–E DAC). This topology breaks the single-center limitation structurally and electronically, enabling division of labor and cooperation.",
            "The Te site preferentially adsorbs/activates CO₂, while the Cu site promotes H₂O dissociation to supply H* in situ. This synergy advances the continuous 'activation–proton supply–conversion' steps within the same microenvironment.",
            "In this pathway, the *COOH → CO step involves simultaneous protonation of COOH and CO generation/desorption. It is most directly influenced by local proton supply capacity and interfacial kinetics. Since protons in the weakly alkaline system primarily originate from water dissociation, this step inherently tends to become the kinetic bottleneck.",
            "The cooperative design (Te for CO₂ activation, Cu for H₂O dissociation) aims to accelerate proton transfer kinetics and alleviate limitations in preceding steps: Te handles initial CO₂ adsorption/activation, while Cu enhances in-situ H accessibility. Nevertheless, within this framework, the protonation/generation step converting 'activated *COOH' to 'CO' remains rate-determining for the overall process."
        ],
        "step_count": 5
    },
    "recuVrxkfasBGy": {
        "reasoning_steps": [
            "Decompose total mass transfer resistance. According to Concept_1, the total CO₂ mass transfer resistance of the gas diffusion electrode (GDE) can be expressed as: Rtotal=RGDL+RMPL+RCLR Independent measurement of RGDLR and RMPL allows isolating the catalyst layer (CL) resistance.",
            "Design comparative experiments to isolate ionomer effects. Two catalyst layers (Target CL and Blank CL) with identical morphology, thickness, and catalyst loading are prepared, differing only in ionomer content or type. This ensures that variations in resistance arise solely from ionomer properties.",
            "Measure and calculate resistances. Using the limiting current method, obtain the total resistance Rtotal for each GDE and calculate the CL resistance: RCL,total=Rtotal−RGDL−RMPL The ionomer-induced local resistance is then determined as: RCL,ionomer=RCL,total−RCL,pore",
            "Apply Fick’s diffusion law to quantify ionomer effects. According to Concept_2, the effective CO₂ diffusion coefficient is linked to the CL resistance."
        ],
        "step_count": 4
    },
    "recuVTLxQnuWDZ": {
        "reasoning_steps": [
            "The problem is modeled using a complete graph $G$ where the $K=2^k$ vertices represent all possible messages Alice can send. This is based on Concept_1.",
            "For the code to be resilient to $\\frac{1}{4}+\\delta$ errors, for any pair of distinct messages $(v_i, v_j)$, there must exist at least one feedback string $f$ from Bob that causes Alice's transmissions, $FC(v_i; f)$ and $FC(v_j; f)$, to have a large Hamming distance. If no such $f$ exists for a pair, an adversary can corrupt the communication so that Bob cannot distinguish between $v_i$ and $v_j$, as described in Concept_2.",
            "We can represent the feedback strings as colors. Using Concept_3, we color the edge $(v_i, v_j)$ in the graph $G$ with the first feedback string $f$ that ensures the transmissions $FC(v_i;f)$ and $FC(v_j;f)$ are sufficiently separated. Let the total number of feedback bits be $\\zeta(FC)$, which means there are $l = 2^{\\zeta(FC)}$ possible feedback strings, or colors.",
            "For a fixed feedback string $f$ (a single color), the set of Alice's possible transmissions $\\{FC(v;f) | v \\in \\{0,1\\}^k\\}$ forms a code. To distinguish any two messages with error resilience $\\frac{1}{4}+\\delta$, their transmissions must have a relative distance of at least $2(\\frac{1}{4}+\\delta) = \\frac{1}{2}+2\\delta$. According to Theorem_1 (the Plotkin Bound), any subset of messages where all pairs satisfy this distance condition must be small. Specifically, a monochromatic clique of size $t = \\lceil10/\\delta\\rceil$ is shown to be impossible.",
            "Since every edge in the graph $G$ must be colored to ensure distinguishability, we have a complete graph $K_K$ (where $K=2^k$) whose edges are colored with $l$ colors, and there is no monochromatic clique of size $t = \\lceil10/\\delta\\rceil$.",
            "This structure allows us to apply Ramsey theory. According to Theorem_2, the number of vertices $K$ must be less than the Ramsey number $r(t; l)$. Using the bound from the theorem, we have $K \\le r(t;l) \\le l^{lt}$.",
            "Substituting $K=2^k$ and $t = \\lceil10/\\delta\\rceil$, we get $2^k \\le l^{l \\cdot \\lceil10/\\delta\\rceil}$. Taking the logarithm of both sides gives $k \\le l \\cdot \\lceil10/\\delta\\rceil \\cdot \\log l$. For this inequality to hold as $k$ grows, the number of colors $l$ must also grow. This relationship implies that $l$ must be at least $\\Omega_{\\delta}(\\frac{\\log K}{\\log\\log K})$, and therefore the number of feedback bits, $\\zeta(FC) = \\log l$, must be at least $\\Omega_{\\delta}(\\log K) = \\Omega_{\\delta}(k)$. *Correction based on the source's derivation which simplifies this relationship*: The final step in the paper's proof leads to $\\zeta(FC) = \\log l = \\Omega_{\\delta}(\\log k)$."
        ],
        "step_count": 7
    },
    "recuVVfazYiowy": {
        "reasoning_steps": [
            "The objective is to determine the upper bound on the Griesmer defect for a specific affine Solomon-Stiffler code. Using Concept 2, the defect is given by the formula g(C) = n - g_q(k,d), where g_q(k,d) = ∑_{i=0}^{k-1}⌈d/q^i⌉.",
            "We first establish the parameters of the code based on the given conditions. From Concept 3 and the condition u_1 = u_2 = ... = u_h, the code's length is n = (q^k-1) - h(q^{u_1}-1). For the calculation of the Griesmer bound, we use the minimum distance lower bound from Concept 4, setting d = (q-1)(q^{k-1} - hq^{u_1-1}).",
            "The Griesmer defect arises when the argument of the ceiling function in the summation for g_q(k,d) is not an integer, causing the sum to be smaller than n. We analyze the terms of the sum ∑_{i=0}^{k-1}⌈(q-1)(q^{k-1} - hq^{u_1-1})/q^i⌉ to find the source of this deviation.",
            "The deviation is most significant when the denominator q^i is close to the terms in the numerator. Let's analyze the term at the index i = u_1. The term is ⌈(q-1)(q^{k-1} - hq^{u_1-1})/q^{u_1}⌉.",
            "We simplify this expression: ⌈(q-1)(q^{k-1-u_1} - hq^{-1})⌉ = ⌈(q-1)q^{k-1-u_1} - h(q-1)/q⌉. Since (q-1)q^{k-1-u_1} is an integer, we can rewrite this as (q-1)q^{k-1-u_1} - ⌊h(q-1)/q⌋.",
            "Now, we evaluate the floor function using the given condition h ≤ q. We have ⌊h(q-1)/q⌋ = ⌊hq - h/q⌋ = ⌊h - h/q⌋. Since 0 < h/q ≤ 1, the value of the floor function is exactly h-1.",
            "This shows that the term at i=u_1 is smaller by h-1 than what would be expected from a simple distribution of terms. For other indices i > u_1, the term ⌊h(q-1)/q^{i-u_1+1}⌋ is zero because h < q ≤ q^{i-u_1+1}. Therefore, the primary contribution to the total defect comes from the term at i=u_1."
        ],
        "step_count": 7
    },
    "recuVVCTL6bNEt": {
        "reasoning_steps": [
            "Define Objective and Identify Core Thesis: Deconstruct the prompt to confirm the required output: an Instruction, a mathematical Answer, and foundational Concepts. Perform a high-level skim of the paper's title, abstract, and introduction to identify its central thesis, which is establishing a stability criterion for a complex AMP algorithm.",
            "Formulate the Central \"Instruction\": Synthesize the paper's primary contribution into a single, direct question that defines the problem it solves: \"What is the necessary and sufficient condition for the dynamical stability and convergence of AMP with non-separable multivariate nonlinearities?\"",
            "Locate the Mathematical \"Answer\": Find the explicit, concise answer to the Instruction within the paper's conclusion and supporting theorems. The direct answer is the condition: ρAT < 1.",
            "Trace the Logical Chain to Identify Key Concepts: Identify the essential building blocks required to understand how the answer is derived: 1. AMP Algorithm Dynamics, 2. State Evolution, 3. Decoupling Principle, 4. AT Stability Condition (Theorem 1).",
            "Synthesize the Structured Output: Assemble the extracted components into the requested formal structure, presenting the Instruction, followed by the Answer, and then the list of key Concepts/Theorems."
        ],
        "step_count": 5
    },
    "recuVTSjBuSSDV": {
        "reasoning_steps": [
            "We begin with the standard definition of graph capacity, $\\Theta(G)$, as per **Concept_1**. An equivalent perspective, using **Concept_2** and **Concept_3**, is to define it as the maximum growth rate over all distinguishable languages.",
            "We introduce a restriction on the type of languages considered, focusing on those recognized by reversible DFAs, as defined in **Concept_4**. This leads to the definition of reversible capacity, $\\Theta_{REV}(G)$, as outlined in **Concept_5**.",
            "To establish a lower bound on $\\Theta_{REV}(G)$, we construct a specific reversible language. We start with a maximum independent set of $G^n$, which contains $M=\\alpha(G^n)$ strings of length $n$, as described in **Concept_6**.",
            "A simple DFA can recognize these $M$ strings, but it is not generally reversible. To enforce reversibility, we employ the strategy from **Concept_7**. After recognizing one of the $M$ strings, we append a suffix that uniquely identifies which of the $M$ strings was seen.",
            "The suffix is constructed using the vertices of $G$ as an alphabet, forming a base-$|G|$ representation of the index of the string (from 1 to $M$). The length of this suffix will be $\\lceil \\log_{|G|}(M) \\rceil$. This process makes the overall language reversible because the unique suffix ensures a unique path back to the initial state.",
            "We analyze this growth rate as $n \\to \\infty$. We know that for large $n$, $M = \\alpha(G^n) \\approx \\Theta(G)^n$. The total length becomes $n' \\approx n + \\log_{|G|}(\\Theta(G)^n) = n(1 + \\frac{\\log(\\Theta(G))}{\\log(|G|)})$.",
            "Substituting these approximations into the growth rate expression gives: $\\Theta_{REV}(G) \\ge ( \\Theta(G)^n )^{1 / (n(1 + \\frac{\\log(\\Theta(G))}{\\log(|G|)}))}$ $\\Theta_{REV}(G) \\ge \\Theta(G)^{1 / (1 + \\frac{\\log(\\Theta(G))}{\\log(|G|)})}$ $\\Theta_{REV}(G) \\ge \\Theta(G)^{\\log(|G|)/(log(|G|)+log(\\Theta(G)))}$ This yields the final lower bound for the reversible capacity."
        ],
        "step_count": 7
    },
    "recuVriIMUUa4C": {
        "reasoning_steps": [
            "The goal is to find the tightest function $B_m$ such that the polynomial $p(v) = B_m^m ||v||_2^m - \\mathbb{E}_{X \\sim P}[\\langle X, v \\rangle^m]$ is a Sum-of-Squares (SoS) for any s-subgaussian distribution P, as defined by its moment properties in Concept_1.",
            "Instead of directly constructing an SoS proof, we employ Theorem_1 (the SoS-Duality principle). This powerful theorem allows us to reframe the problem: we must show that for any valid Concept_2 (pseudoexpectation) $\\tilde{\\mathbb{B}}$, the inequality $\\tilde{\\mathbb{B}}[p(v)] \\ge 0$ holds. This is equivalent to proving that $max_{\\tilde{\\mathbb{B}}} \\frac{\\tilde{\\mathbb{B}}[\\mathbb{E}[\\langle X, v \\rangle^m]]}{\\tilde{\\mathbb{B}}[||v||_2^m]} \\le B_m^m$.",
            "We consider potential proof strategies. Theorem_2 (the KLS conjecture) offers strong certifiable bounds, but it applies specifically to distributions with strong geometric properties like log-concavity. The instruction explicitly states that the class of s-subgaussian distributions is broader and lacks this uniform geometric guarantee. Therefore, the KLS conjecture is not a suitable tool for this general problem and represents a distinct, inapplicable line of reasoning.",
            "We proceed with a different strategy. The expression involving the population expectation $\\mathbb{E}[\\cdot]$ is difficult to handle directly within the pseudoexpectation framework. We first replace it with an empirical average over many i.i.d. samples from P. This results in a high-degree polynomial of the random samples, which is a non-linear stochastic process. To make this tractable, we apply Theorem_4 (the variational identity), which leverages a form of Hölder's inequality to transform the non-linear moment expression into the supremum of a new process that is now *linear* in the samples.",
            "After linearization, we are left with the task of bounding the supremum of a *linear* process driven by *subgaussian* random variables. This is the precise setting where Theorem_3 (Talagrand's majorizing measures theorem) is applicable. This theorem lets us bound the expected supremum of our subgaussian process by the expected supremum of an equivalent process driven by standard Gaussian variables, introducing a scaling factor of $O(s\\sqrt{m})$.",
            "The final step is to bound the new expression involving Gaussian variables. It is a known fact that Gaussian distributions themselves are certifiably subgaussian, and their corresponding bound is on the order of $(C'\\sqrt{m})^m$. By combining this base case with the scaling factor from Step_5, we arrive at an overall bound of $(O(s\\sqrt{m}))^m$. This implies that $B_m^m = (Cs\\sqrt{m})^m$, which yields the final answer $B_m = Cs\\sqrt{m}$."
        ],
        "step_count": 6
    },
    "recuVUWWSTPcqx": {
        "reasoning_steps": [
            "The problem asks for the minimal length of a minimal rank metric code, which is equivalent to finding $\\varpi_{\\mathbb{B}/\\mathbb{P}}(k, 1)$ according to **Theorem 4:**.",
            "We first establish a lower bound. **Theorem 3** provides a direct lower bound for the case where $m=3$, stating that $\\varpi_{\\mathbb{B}/\\mathbb{P}}(k, 1) \\ge 2k$.",
            "Now, we must find an upper bound. The existence of a k-dimensional minimal code of a certain length `n` implies that $\\varpi_{\\mathbb{B}/\\mathbb{P}}(k, 1) \\le n$. The existence of r-minimal codes can be established using combinatorial arguments based on the frameworks described in **Theorem 1** and **Theorem 2**. These proofs, when applied to the specific case of rank metric codes, result in an upper bound on minimal length.",
            "The paper's authors demonstrate that applying this general framework and the explicit bounds derived in subsequent sections leads to the specific formula for the minimal length of a k-dimensional minimal rank metric code when $m=3$. By combining the concepts and properties of r-minimal codes, and particularly the property of 1-minimality, one can demonstrate that an upper bound of $2k$ can be achieved.",
            "The final conclusion that $\\varpi_{\\mathbb{B}/\\mathbb{P}}(k,1) = 2k$ is reached by showing that the derived upper bound is equal to the lower bound from **Theorem 3**. The **Concept 1** helps to understand the relationship between r-minimality and s-minimality, which is a key property used in the general framework. The model must synthesize this information to conclude the exact value."
        ],
        "step_count": 5
    },
    "recuVGjEuxqSEc": {
        "reasoning_steps": [
            "The objective is to determine the upper bound on the covert capacity, defined as $C_{covert} = \\lim_{n\\rightarrow\\infty}\\frac{log~M}{\\sqrt{n~log~L_{n}}}$, for an AWGN channel under the specified conditions. This requires finding the maximum number of messages, $M$, that can be sent reliably and covertly.",
            "We begin by considering the covertness requirement. According to the **Covertness Constraint on Codeword Power (Concept_1)**, for a code to be undetectable, its codewords must not have excessively high power. This establishes a critical power threshold that any covert codebook must adhere to.",
            "Leveraging this constraint, the principle of **Existence of a Dominant Low-Power Sub-codebook (Concept_2)** asserts that any valid covert codebook must contain a substantial fraction of codewords with power below this threshold. We can, therefore, analyze the capacity of this low-power sub-codebook, $\\mathcal{C}^{(l)}$, as its size is proportional to the total number of messages $M$.",
            "The **Information Rate Bound for Reliable Communication (Theorem_1)** is applied to this sub-codebook $\\mathcal{C}^{(l)}$. This provides an upper bound on the number of codewords it can contain: $log|\\mathcal{C}^{(l)}| \\le n\\mathbb{I}(\\overline{X};\\overline{Y}) + \\text{vanishing error terms}$. As $log M$ scales with $log|\\mathcal{C}^{(l)}|$, we can bound $log M$ by bounding $n\\mathbb{I}(\\overline{X};\\overline{Y})$.",
            "To bound the mutual information term, we use the **Mutual Information for an AWGN Channel (Theorem_2)**, which gives $\\mathbb{I}(\\overline{X};\\overline{Y}) \\le \\frac{1}{2}log(1+\\frac{\\overline{R}}{\\sigma_{b}^{2}})$, where $\\overline{R}$ is the average power per symbol for the codewords in $\\mathcal{C}^{(l)}$.",
            "The average power per symbol, $\\overline{R}$, is determined by the power constraint on the sub-codebook from Concept_2. Specifically, $\\overline{R}$ is bounded by the maximum allowed power per symbol: $\\overline{R} \\le \\frac{1}{n} \\max_{x \\in C^{(l)}} ||x||_2^2 \\le \\frac{1}{n}\\sqrt{4\\sigma_{w}^{4}n\\log L_{n}} = 2\\sigma_w^2 \\sqrt{\\frac{\\log L_n}{n}}$. As $n \\to \\infty$, $\\overline{R} \\to 0$.",
            "Since $\\overline{R}$ is small, we can apply **The Inequality $log(1+x) \\le x$ (Concept_3)** to simplify the mutual information bound: $\\mathbb{I}(\\overline{X};\\overline{Y}) \\le \\frac{\\overline{R}}{2\\sigma_{b}^{2}}$.",
            "Substituting the bound on $\\overline{R}$ from Step_6 into the simplified mutual information bound from Step_7 yields: $\\mathbb{I}(\\overline{X};\\overline{Y}) \\le \\frac{1}{2\\sigma_{b}^{2}} \\left( 2\\sigma_w^2 \\sqrt{\\frac{\\log L_{n}}{n}} \\right) = \\frac{\\sigma_{w}^{2}}{\\sigma_{b}^{2}}\\sqrt{\\frac{log~L_{n}}{n}}$.",
            "Finally, we combine this result with the rate bound from Step_4: $log M \\lesssim n \\cdot \\mathbb{I}(\\overline{X};\\overline{Y}) \\le n \\cdot \\left( \\frac{\\sigma_{w}^{2}}{\\sigma_{b}^{2}}\\sqrt{\\frac{log~L_{n}}{n}} \\right) = \\frac{\\sigma_{w}^{2}}{\\sigma_{b}^{2}}\\sqrt{n~log~L_{n}}$. By dividing by the normalization factor $\\sqrt{n \\log L_n}$ and taking the limit, we arrive at the upper bound for the covert capacity: $C_{covert} \\le \\frac{\\sigma_{w}^{2}}{\\sigma_{b}^{2}}$."
        ],
        "step_count": 9
    },
    "recuVI2Ioe8GAr": {
        "reasoning_steps": [
            "The total complexity is the number of test vectors multiplied by the complexity of the IRF for a single test vector. From concept_4, the number of test vectors is $2^{\\eta}$.",
            "The per-vector IRF complexity has two main parts: finding error locations and finding error values (corrected symbols).",
            "To find error locations, we use the error locator polynomial (concept_1). According to concept_2, this polynomial has a degree of $O(n-k)$ and must be evaluated at $n$ distinct points. The complexity of this step is the number of points times the cost per evaluation, resulting in $O(n \\cdot (n-k))$.",
            "To find the corrected symbols at these locations, we turn to concept_4. There are $O(n-k)$ error positions (from theorem_1), and the computation for each costs $O(n)$. This also results in a complexity of $O((n-k) \\cdot n)$.",
            "The total complexity for one test vector is the sum of the complexities from Step_3 and Step_4: $O(n(n-k)) + O(n(n-k)) = O(n(n-k))$.",
            "Finally, we multiply the per-vector complexity by the total number of vectors to get the final asymptotic complexity: $2^{\\eta} \\times O(n(n-k)) = O(2^{\\eta}n(n-k))$."
        ],
        "step_count": 6
    },
    "recuVQm8cfkMt6": {
        "reasoning_steps": [
            "Goal: state the unique closed-form expression for the second-order cumulant \\(v_n\\{2\\}\\) in one sentence.",
            "Ensemble definition (Concept_1): treat each event as a member of an ensemble and define the event-average \\[\\displaystyle \\langle\\mathcal{O}\\rangle=\\frac{1}{N_{\\text{ev}}}\\sum_{\\text{ev}}\\mathcal{O}.\\]",
            "Reference windows (Concept_2): identify two symmetric, disjoint pseudorapidity bins \\[\\displaystyle \\eta_{\\text{ref}}^{A}\\quad\\text{and}\\quad\\eta_{\\text{ref}}^{B}.\\]",
            "Flow-weighted integral (Concept_3): form the complex harmonic-flow integral \\[\\displaystyle Q_{n}(\\eta_{\\text{ref}})=\\sum_{k\\in\\eta_{\\text{ref}}}w_{k}\\,e^{in\\phi_{k}}.\\]",
            "Multiplicity integral (Concept_4): form the scalar multiplicity integral \\[\\displaystyle Q_{0}(\\eta_{\\text{ref}})=\\sum_{k\\in\\eta_{\\text{ref}}}w_{k}.\\]",
            "Cumulant definition (Concept_5): invoke the two-particle cumulant \\[\\displaystyle v_n\\{2\\}\\equiv\\sqrt{\\langle\\!\\langle e^{in(\\phi_{1}-\\phi_{2})}\\rangle\\!\\rangle}.\\]",
            "Cross-window correlation (Concept_6): express the two-particle average via cross-window correlations \\[\\displaystyle \\langle\\!\\langle e^{in(\\phi_{1}-\\phi_{2})}\\rangle\\!\\rangle =\\frac{\\langle Q_n(\\eta_{\\text{ref}}^{A})\\,Q_n^{*}(\\eta_{\\text{ref}}^{B})\\rangle}{\\langle Q_0(\\eta_{\\text{ref}}^{A})\\,Q_0^{*}(\\eta_{\\text{ref}}^{B})\\rangle}.\\]"
        ],
        "step_count": 7
    },
    "recuVUm9f6jWTB": {
        "reasoning_steps": [
            "The objective is to determine the overall complexity of a refined Guruswami-Sudan list decoding algorithm. The algorithm consists of an interpolation step and a root-finding step (Concept_1). The primary improvement is made to the interpolation step.",
            "The interpolation step is modeled as finding a small element in the interpolant module \\(\\mathcal{M}_{s,l,r}\\) (Concept_2). This is accomplished by first constructing a basis for the module and then performing a basis reduction.",
            "A novel basis for \\(\\mathcal{M}_{s,l,r}\\) is chosen. This basis relies on specific polynomials \\(g_t(z)\\) that yield a special structure (Concept_3), which is instrumental for reducing computational complexity.",
            "This special basis is represented as a block matrix \\(M_{s,l,r}\\) where all complex polynomial entries are concentrated in the first \\(\\mu s\\) columns (Concept_4). This structure is key to achieving a low average column degree.",
            "A critical part of constructing this matrix is computing the submatrix \\(\\bar{R}\\) without letting its polynomial degrees grow excessively. This is achieved via an efficient iterative procedure involving matrix remainders (Concept_6), ensuring that the degree of \\(\\bar{R}\\) remains in \\(\\mathcal{O}(s(n+g)/\\mu)\\).",
            "The resulting matrix \\(M_{s,l,r}\\) has a total column degree sum of \\(\\mathcal{O}(s^2(n+g))\\). The search for the desired low-degree interpolant polynomial is performed by computing the shifted Popov form of this matrix (Concept_5).",
            "The computational cost for this shifted Popov form calculation directly yields the complexity of the improved interpolation step. Given the structure and degree properties of \\(M_{s,l,r}\\), this cost is \\(\\tilde{\\mathcal{O}}(s^{2}l^{\\omega-1}\\mu^{\\omega-1}(n+g)+l^{\\omega}\\mu^{\\omega})\\).",
            "To establish the overall decoder complexity, the cost of the root-finding step is analyzed. It is shown to be \\(\\tilde{\\mathcal{O}}(sl\\mu^{\\omega-1}(n+g))\\) (Theorem_1).",
            "Comparing the complexities of the two steps, the cost of the refined interpolation step is dominant. Therefore, the overall complexity of the faster list decoder is determined by the cost of the Popov form computation, which is \\(\\tilde{\\mathcal{O}}(s^{2}l^{\\omega-1}\\mu^{\\omega-1}(n+g)+l^{\\omega}\\mu^{\\omega})\\)."
        ],
        "step_count": 9
    },
    "recuVIxwNr9GRp": {
        "reasoning_steps": [
            "The Two-Layer Decryption Workflow: The decryption process is structured as a sequential two-layer workflow. Outer Layer (Exp-ElGamal Threshold Decryption): This layer first decrypts a small encryption key, denoted as \\(e_k\\), using the Exp-ElGamal threshold decryption protocol. This protocol is inherently efficient for distributed computation. Inner Layer (MeSA Lattice Decryption): Subsequently, the recovered key \\(e_k\\) is used to decrypt the primary lattice-based MeSA (Message-Space Adapter) ciphertext, \\(c t_{\\text{mesa}}\\), which contains the original large message.",
            "Complexity Analysis of the Outer Layer: The computational cost of the outer layer is derived from two main operations. Modular Exponentiation: The Exp-ElGamal decryption requires modular exponentiation, which has a complexity of \\(O(\\log q)\\) for a modulus \\(q\\). In the threshold setting with \\(N\\) parties, this scales to \\(O(N \\log q)\\). Shamir's Secret Sharing (SSS) Reconstruction: The protocol employs SSS for private key distribution. Reconstructing the secret from a threshold of shares involves polynomial interpolation, which, when optimized with the Fast Fourier Transform (FFT), contributes a complexity of \\(O(N \\log N)\\). The total complexity for this layer is thus dominated by SSS reconstruction, resulting in \\(O(N \\log N)\\).",
            "Complexity Analysis of the Inner Layer: The inner layer's decryption complexity is associated with the lattice-based MeSA scheme. While decryption for lattice schemes like Ring-LWE is generally proportional to the lattice dimension, the MeSA is specifically designed to operate on a small key. This optimization reduces the complexity of lattice operations significantly. Consequently, the computational cost of recovering \\(e_k\\) from the lattice ciphertext is negligible compared to the outer layer, amounting to \\(O(\\log \\lambda)\\) for a security parameter \\(\\lambda\\), which is effectively constant for fixed parameters.",
            "Combined Complexity and Architectural Innovations: The total decryption complexity is the sum of the costs of the outer and inner layers. Since the outer layer's complexity of \\(O(N \\log N)\\) is asymptotically dominant, the combined complexity is \\(O(N \\log N)\\). This efficiency is enabled by two key innovations: Message-Space Adapter (MeSA): This component decouples large-message encryption (handled by the lattice scheme) from small-key encryption (handled by Exp-ElGamal), thus preserving Exp-ElGamal's efficiency without being constrained by its small-message limitation. Dynamic Participant Sets: The scheme accommodates arbitrary thresholds and dynamic participant groups without incurring additional computational overhead.",
            "Security Considerations and Practical Validation: The scheme presents a hybrid security model. The lattice-based MeSA offers post-quantum security, while the Exp-ElGamal component's security relies on the Computational Diffie-Hellman (CDH) assumption, which is vulnerable to quantum attacks. However, the architecture allows for upgrading the Exp-ElGamal component to a quantum-resistant alternative. The theoretical complexity analysis has been empirically verified through a prototype implementation. For \\(N=100\\) parties, the scheme demonstrated the predicted quasi-linear scaling and was approximately four times faster than a comparable threshold Paillier implementation."
        ],
        "step_count": 5
    },
    "recuVMzq9clILU": {
        "reasoning_steps": [
            "The instruction asks to determine the minimum possible sub-packetization level, denoted as $l$, for a specific category of codes: linear Minimum Storage Regenerating (MSR) codes with the help-by-transfer property.",
            "First, we define the key terms using the provided concepts. An MSR code is a type of MDS array code that is optimized to minimize the amount of data downloaded when repairing a single failed node (concept_1). The sub-packetization level, $l$, is the size of the vector stored at each node (concept_2).",
            "The instruction adds a crucial constraint: the code must have the 'help-by-transfer' property. This means that during repair, a helper node only needs to read the exact symbols it is going to transmit, without performing any computation on other stored data (concept_3). This property is highly desirable for practical systems but places significant constraints on the code's internal structure.",
            "The problem of finding the 'lowest possible' sub-packetization level for codes with these properties translates to finding the theoretical lower bound on $l$. This bound represents the best-case scenario that any code construction meeting these criteria can achieve.",
            "A key result in the field, as stated in theorem_1, provides this exact lower bound. The theorem establishes that for any linear MSR code satisfying the help-by-transfer property, the sub-packetization level $l$ must be at least $s^{\\lceil n/r \\rceil}$, where $s=d-k+1$ and $r=n-k$.",
            "Therefore, the lowest possible sub-packetization level is given by this bound, which is $l = s^{\\lceil n/r \\rceil}$."
        ],
        "step_count": 6
    },
    "recuVIEEFtpBQb": {
        "reasoning_steps": [
            "Clearly, by their definitions, we have $\\lim_{\\epsilon\\to 0}\\limsup_{n\\to\\infty}-\\frac{1}{n}\\log(\\mu(B(x,n,e^{-n\\epsilon})))\\geq\\lim_{\\epsilon\\to 0}\\limsup_{n\\to\\infty}-\\frac{1}{n}\\log(\\mu(B(x,n,\\epsilon)))$. Thus, we will prove the other direction in the following.",
            "We proceed by contradiction and assume that there exists $ \\lambda\\in(0,\\min\\{\\chi^3,\\frac{1}{2}\\})$ s.t $\\E_\\mu\\geq \\h_\\mu+2\\lambda$ $\\mu$-a.e by the ergodicity of $\\mu$.",
            "There exists $0<r\\leq \\frac{\\min\\{1,\\chi_0\\}}{3d}\\frac{\\lambda^4}{4}$ s.t $\\mu(A_1)\\geq 1-\\lambda^3$ where $A_1:=\\{x\\in A:\\limsup\\frac{-1}{n}\\log\\mu(B(x,n,e^{-rn}))>h_\\mu^\\mathrm{BK}(x)+\\frac{7\\lambda}{8}\\}$. We can then choose $0<\\tau\\leq\\min\\{\\frac{r}{400}\\}$ and $\\ell\\in\\mathbb{N}$ s.t $\\mu(A_2)\\geq 1-2\\lambda^4$ where $$A_2:=\\{x\\in A_1\\text{ Lyapunov reg.}: x\\in \\Lambda_\\ell^{(\\underline{\\chi}(x),\\tau)}\\}.$",
            "$\\mu(\\{x:\\mu_x(A_2)\\geq 1-\\sqrt{2\\lambda^4}\\})\\geq 1-\\sqrt{2\\lambda^4}$. Then $\\mu(\\{x\\in A_2: \\mu_x(A_2)\\geq 1-\\sqrt{2}\\lambda^2\\})\\geq 1-2\\lambda^2$. So by the ergodic theorem $\\exists n_0\\in\\mathbb{N}$ s.t $\\mu(A_3)\\geq 1-3\\lambda^2$ where $$A_3:=\\left\\{x\\in A_2: \\forall n\\geq n_0 \\exists j\\in[n(1+4\\lambda^2),n(1+8\\lambda^2)]\\text{ s.t }f^j(x)\\in A_2 \\right\\}.$",
            "Let $K_\\tau\\subseteq A_3$ as in Lemma 3 s.t $\\mu(K_\\tau)>0$. Note that the restrictions on $K_\\tau$ in Lemma 3 are given by Lemma 2, which in turn are merely Brin-Katok estimates; which are inherited by subsets. Let $N\\geq n_0$ large, then for all $x\\in K_\\tau$ set $$n_x^N:=\\min\\{n\\geq N: \\frac{-1}{n}\\log\\mu(B(x,n,e^{-rn}))>h+\\frac{6\\lambda}{8}\\}.$$ For all $n\\geq N$, set $K_n:=\\{x\\in K_\\tau:n_x^N=n\\}$.",
            "By Lemma 3, we can cover $K_n$ with a cover whose cardinality is less or equal to $e^{n(1+ 8\\lambda^2)(h+ \\frac{\\lambda}{8}+3dr)}$, of exponential Bowen balls of the form $B(x,n,e^{-nr})$, $x\\in K_n$. Hence, we have $\\mu(K_n)\\leq e^{n(1+8\\lambda^2)(h+\\frac{\\lambda}{8}+3dr)} \\cdot e^{-n(h+\\frac{6\\lambda}{8})}$. Then, we conclude $$0<\\mu(K_\\tau)\\leq \\sum_{n\\geq N} e^{-n\\frac{\\lambda}{8}}\\xrightarrow[]{N\\to\\infty}0$$, a contradiction! Hence, we get $\\lim_{\\epsilon\\to 0}\\limsup_{n\\to\\infty}\\frac{1}{n}-\\log(\\mu(B(x,n,e^{-n\\epsilon})))\\leq\\lim_{\\epsilon\\to 0}\\limsup_{n\\to\\infty}\\frac{1}{n}\\log(\\mu(B(x,n,\\epsilon)))$. We finish the proof."
        ],
        "step_count": 6
    },
    "recuVQ2NkU1rrx": {
        "reasoning_steps": [
            "Suppose that the density property fails. Then there exists a constant $\\lambda>0$ and a subset $A_0\\subset A$ of positive measure such that for every $x\\in A_0$, the subexponential decay fails with rate at least $\\lambda$.",
            "Using Pesin theory, we can choose a suitable Lyapunov block $\\Lambda^{(\\underline{\\chi},\\tau)}$ (with $\\tau$ sufficiently small compared to $\\lambda$ and $r$) so that a positive portion of $A_0$ is contained in a Pesin chart. Call this intersection $A^{(1)}$.",
            "For points $x\\in A^{(1)}$, define the first time $n_x^N$ at which the failure occurs with rate at least $\\tfrac{3}{4}\\lambda$. This allows us to partition $A^{(1)}$ into measurable subsets $A_n^{(1)}$ according to this “bad return time.”",
            "Applying the covering lemma from \\cite{ORH23}, we can cover each $A_n^{(1)}$ by $r$-neutralized Bowen balls with bounded multiplicity. By assumption, the measure of $A_n^{(1)}$ decays at least like $e^{-(5/8)\\lambda n}$. Summing over $n$, we find that $\\mu(A^{(1)})=0$, contradicting the choice of $A^{(1)}$ with positive measure. Hence, the contradiction shows that the density lemma with exponential error must hold."
        ],
        "step_count": 4
    },
    "recuWngldoBusH": {
        "reasoning_steps": [
            "Governing equation. Start with the NRCH model: \\partial_t \\phi = \\nabla^2\\!\\left[\\,(-1+i\\alpha)\\phi + |\\phi|^2\\phi - \\nabla^2\\phi\\,\\right], \\qquad \\alpha>0.",
            "Plane-wave branch. Admissible far-field states are: \\phi(\\mathbf r,t)=R\\,e^{\\,i(\\mathbf k\\cdot\\mathbf r-\\omega t)}, \\qquad R=\\sqrt{1-k^2}, \\quad \\omega=\\alpha k^2, \\quad k<1.",
            "Defect ansatz. Axisymmetric defects are written as: \\phi(r,t)=R(r)\\,e^{\\,i[m\\theta+Z(r)-\\omega t]}, \\qquad m\\in\\{0,\\pm1\\}, \\quad k(r)=Z'(r).",
            "Far-field matching. Require convergence to a plane-wave solution: k(r)\\to k_\\infty, \\quad R(r)\\to R_\\infty=\\sqrt{1-k_\\infty^2}, \\quad \\omega=\\alpha k_\\infty^2.",
            "Selection constant 𝐶. By integrating the radial ODEs with core boundary data and matching to far-field asymptotics, the selected scaling is obtained: k_\\infty = C\\sqrt{\\alpha}, \\qquad R_\\infty=\\sqrt{1-C^2\\alpha}. with numerical values C\\approx0.76 \\; (\\text{spirals}), \\quad C\\approx0.70 \\; (\\text{targets}).",
            "Existence and stability thresholds. The amplitude vanishes at \\alpha_\\times = \\frac{1}{C^2}. Eckhaus stability requires k_\\infty < \\tfrac{1}{\\sqrt{3}} \\;\\;\\Rightarrow\\;\\; \\alpha_\\times^{(\\mathrm{Eckhaus})} = \\frac{1}{3C^2}."
        ],
        "step_count": 6
    },
    "recuVV2Em25B7J": {
        "reasoning_steps": [
            "The objective is to find the optimal quantizer resolution $\\Delta(k)$ that minimizes the worst-case radius of the feasible state set, $\\rho(k) = \\text{rad}(\\Xi(k|k))$, for a first-order system (Concept_1) with bounded disturbances (Concept_2). This is formulated as a min-max optimization problem (Concept_6).",
            "The quantizer parameters at time $k$ are adapted based on the predicted feasible state set $\\Xi(k|k-1)$. This set is an interval whose radius is known to be $a\\rho(k-1) + \\delta_w$ (Concept_5).",
            "To solve the min-max problem, the thresholds of the uniform quantizer (Concept_3) must be positioned optimally relative to the predicted set $\\Xi(k|k-1)$. A geometric argument leads to the conclusion that the quantizer must be centered on this predicted interval. Therefore, the optimal quantizer center $\\tau_c(k)$ is set to the center of $\\Xi(k|k-1)$ (Theorem_1).",
            "With $\\tau_c(k)$ fixed, the radius of the updated feasible set, $\\rho(k)$, is calculated for every possible quantized output $y(k) \\in \\{0, 1, ..., d\\}$. The analysis shows that $\\rho(k)$ is a function of the resolution $\\Delta(k)$, the known bounds $\\delta_w, \\delta_v$, the predicted radius $a\\rho(k-1) + \\delta_w$, and the specific output $y(k)$.",
            "The next step is to find the supremum of $\\rho(k)$ over all possible outcomes of $y(k)$. This \"worst-case radius\" is a piecewise function of $\\Delta(k)$. The analysis in the thesis shows this function is composed of a decreasing part and an increasing part with respect to $\\Delta(k)$.",
            "The optimal resolution $\\Delta(k)$ is the value that minimizes this worst-case radius. The minimum of the piecewise function is achieved at the point where the decreasing and increasing components are equal.",
            "Solving for this intersection point yields the optimal resolution: $\\Delta(k) = \\frac{2}{d+1}(a\\rho(k-1) + \\delta_w - \\delta_v)$, which is the final answer."
        ],
        "step_count": 7
    },
    "recuVqIrSxkqgn": {
        "reasoning_steps": [
            "The goal is to determine the sample complexity lower bound for the worst-case scenario of the Generalized Trace Reconstruction problem.",
            "The proof strategy relies on constructing two specific strings, S_e and S_o, that are designed to be difficult to distinguish after the deletion process (Concept_2).",
            "The distinguishability of these strings is measured by the total variation distance between their trace distributions. This distance is found by analyzing a complex alternating sum (Concept_4). While it is known that the average-case version of this problem is easy and requires only polynomial traces (Concept_3), this fact is not relevant for the worst-case analysis.",
            "The key insight is to treat the alternating sum as a Fourier transform evaluated at a specific point, π (Concept_4).",
            "To prove that this value is exponentially small, a technical argument is employed that bounds the decay of the Fourier transform. This is done by analyzing its moment generating function, which allows for tight tail bounds (Theorem_1).",
            "This analysis reveals that the statistical distance between the trace distributions for S_e and S_o is e^{-Ω(√n)}.",
            "The number of samples (traces) required to distinguish two hypotheses is inversely proportional to their statistical distance. Therefore, the sample complexity has a lower bound of e^{Ω(√n)}."
        ],
        "step_count": 7
    },
    "recuVGpJupEwDa": {
        "reasoning_steps": [
            "We first establish a limit on the best possible performance. For any coding scheme, the error probability $P_e$ must be greater than the probability of an \"outage\" event where the decoder receives ambiguous information. This event is modeled by observing too few distinct molecules.",
            "Using Concept_1, we relate the error probability to $p(N, M, \\delta M)$ for any $\\delta < R_0$. We then apply the lower bound from Concept_2, which states that $P_e \\ge (1-o(1))\\delta^N$. This bound arises from considering the simple scenario where all N sampled molecules happen to fall within a small, pre-defined subset of size $\\delta M$.",
            "By taking the logarithm, normalizing by N, and taking the limit, the inequality from Step 2 becomes $-\\frac{1}{N}\\log P_e \\le \\log\\frac{1}{\\delta} + o(1)$. Since this must hold for any $\\delta$ arbitrarily close to $R_0$, we conclude that the error exponent is upper bounded by $\\log\\frac{1}{R_0}$.",
            "Next, we show that this exponent is achievable. We use a specific code construction and decoding rule. The existence of a suitable codebook with limited overlap between codewords is guaranteed by Theorem_1.",
            "The success of the decoding process is guaranteed if a set of sufficient conditions are met. An error can only occur if at least one of these conditions fails. The analysis then focuses on bounding the probability of these failure events.",
            "The dominant failure event is having a large number of \"undersampled\" molecules. The probability of this event is bounded using the Chernoff bound (Theorem_2). The bound shows that this failure probability decays exponentially in N.",
            "The exponent of this decay is determined by the KL divergence (Concept_3). By carefully taking the limits of the parameters in the sufficient condition (letting $\\epsilon \\to 0$ and $\\eta \\to 0$), the exponent converges to $D(0 || 1-R_0)$, which equals $\\log\\frac{1}{R_0}$.",
            "The achievability analysis demonstrates that an error exponent of $\\log\\frac{1}{R_0}$ is possible. Since the converse bound from Step 3 shows that no scheme can perform better, we conclude that the exact error exponent is $\\log\\frac{1}{R_0}$."
        ],
        "step_count": 8
    },
    "recuWgmGHIRbTi": {
        "reasoning_steps": [
            "According to concept 1, after periodic intermittent fasting, the body’s metabolic changes include reduced glucose metabolism along with increased fatty acid and ketone body metabolism.",
            "Furthermore, concept 2 shows that excessive fatty acid metabolism in hair follicle stem cells enhances inflammatory responses, thereby causing stem cell damage.",
            "Concept 3 demonstrates the relationship between metabolism and the hair follicle stem cell cycle, revealing that glycogen accumulation prolongs the anagen phase, whereas glycogen depletion promotes the onset of the catagen phase.",
            "The decline in glycogen metabolism and the rise in fatty acid metabolism are consistent with the effects of periodic intermittent fasting, thus accelerating the transition of hair follicle stem cells into catagen and shortening the anagen phase.",
            "Combining the inflammatory effects of fatty acid metabolism with the role of glycogen concentration in regulating the hair follicle growth cycle, it can be concluded that intermittent fasting exerts an inhibitory effect on hair growth.",
            "Since glycogen is depleted under intermittent fasting conditions, the key metabolic change in this context is the increase in fatty acids."
        ],
        "step_count": 6
    },
    "recuUXYfgAFH3y": {
        "reasoning_steps": [
            "The shock wave, characterized by its high Lorentz factor $\\gamma_p$, compresses the background magnetic field $B_0$ due to the conservation of magnetic flux. This compression amplifies the magnetic field strength in the shock region, leading to $B_z \\propto \\gamma_p^2 B_0$. This establishes the initial $\\gamma_p^2$ dependence, providing the strong magnetic environment necessary for acceleration.",
            "When we transform to the rest frame of the shock wave, the compressed magnetic field $B_z$ (which scales as $\\gamma_p^2$) induces a transverse electric field due to relativistic effects. The transformation yields $E_y = \\gamma_p v_p B_z \\approx \\gamma_p c B_z$ (since $v_p \\approx c$). Thus, $E_y \\propto \\gamma_p B_z \\propto \\gamma_p^3$. This electric field $E_y$ is the direct driving force that accelerates the protons.",
            "Using Hamiltonian analysis, we solve the equations of motion for a proton in the combined $E_y$ and $B_z$ fields. This analysis reveals that the gain in transverse momentum $p_y$ scales as $p_y \\propto \\gamma_p^2$. Since energy is related to momentum, this implies that the energy gain from the acceleration process also scales with $\\gamma_p^2$.",
            "The acceleration process terminates when the proton's transverse momentum $p_y$ first returns to zero ($p_y = 0$). Applying this condition to the solutions from the Hamiltonian analysis allows us to find the maximum energy $E_{\\text{max}}$. The combination of the $\\gamma_p^2$ scaling from the momentum gain and the dependence on the initial energy $\\gamma_0$ yields the final scaling relation: $E_{\\text{max}} \\propto \\gamma_p^2 \\gamma_0$."
        ],
        "step_count": 4
    },
    "recuVeVPPxlALG": {
        "reasoning_steps": [
            "The goal is to determine the complexity of a deterministic Δ-coloring algorithm on constant-degree graphs using ruling subgraphs. The overall strategy is to identify easily colorable subgraphs, select a non-interfering subset of them, color the rest of the graph, and finally color the selected subgraphs.",
            "First, the algorithm identifies a set of candidate subgraphs. Based on theorem_1, every node in the graph can find a Nice Locally Δ-extendable Component (NLAEC) within a distance of $O(log_{\\Delta}n)$. This ensures that the initial collection of NLAECs, denoted as family $\\mathcal{R}$, is dense enough to cover the entire graph.",
            "The critical step is to select a subset of these NLAECs that do not interfere with each other. Instead of using a slow ruling set computation on a power graph, the algorithm employs the new concept of a Ruling Subgraph Family (concept_1). Using theorem_2, it computes a subfamily $\\mathcal{R}' \\subseteq \\mathcal{R}$ in $O(log~n~log^{*}n)$ rounds. This family $\\mathcal{R}'$ has the property that its members are far apart, yet every node in the graph is still relatively close to some member of $\\mathcal{R}'$.",
            "With the ruling subgraph family $\\mathcal{R}'$ established, the nodes outside of these subgraphs ($V \\setminus \\cup_{H \\in \\mathcal{R}'} H$) are colored. These nodes are partitioned into layers based on their distance to the nearest subgraph in $\\mathcal{R}'$. The number of layers is bounded by the maximum distance of any node to $\\mathcal{R}'$, which is $O(log~n~log^{*}n)$ for constant Δ. Using a Layered Graph Coloring algorithm (concept_3), this step takes $O(log~n~log^{*}n)$ rounds.",
            "After coloring the exterior, the algorithm colors the nodes within the NLAECs of $\\mathcal{R}'$. For each NLAEC, a Flexible Node (concept_4) is created, which is guaranteed to be possible for an NLAEC (concept_2). This step ensures that every node within the NLAEC has a valid color available from the set $\\{1, ..., \\Delta\\}$.",
            "The remaining uncolored nodes inside the NLAECs are then colored using another layered coloring approach (theorem_3), this time with layers defined by the distance to the nearest flexible node. Since each NLAEC has a size of $O(log_{\\Delta}n)$, this step is very fast, taking $O(log^2_{\\Delta}n \\cdot log~n)$ rounds, which is dominated by the previous step.",
            "By summing the complexities, the dominant step is the layered coloring of the nodes outside the ruling subgraphs, which takes $O(log~n~log^{*}n)$ rounds. The initial discovery of NLAECs and the computation of the ruling subgraph family also fit within this complexity. Therefore, the total time complexity is $O(log~n~log^{*}n)$."
        ],
        "step_count": 7
    },
    "recuVOq4L169uf": {
        "reasoning_steps": [
            "The code is MDS if and only if no polynomial of a specific form divides $t^{10}-1$ (Concepts 1 & 2). The non-MDS case hinges on finding a solution to a system of polynomial equations.",
            "For $n=10$, this system is reduced to two polynomial equations in a single variable $y$: $f_8(y)=0$ and $f_9(y)=0$ (Concept_3).",
            "The paper explicitly calculates these polynomials for $n=10$: $f_8(y) = y+3$ and $f_9(y) = 4y+1$.",
            "To determine if these two polynomials have a common non-zero root over any field $\\mathbb{F}_p$ (where $p$ is a prime not dividing the resultant), we can compute their Sylvester resultant, an integer value independent of the field.",
            "The Sylvester resultant is the determinant of the Sylvester matrix formed by the coefficients of the two polynomials (Concept_4). For the linear polynomials $f_8(y) = y+3$ and $f_9(y) = 4y+1$, the Sylvester matrix is a $2 \\times 2$ matrix.",
            "The Sylvester matrix is constructed as follows: $\\begin{pmatrix} 1 & 3 \\\\ 4 & 1 \\end{pmatrix}$",
            "This value, -11, is the Sylvester resultant. Since it is non-zero, the polynomials $f_8(y)$ and $f_9(y)$ have no common root in any field whose characteristic does not divide 11. This confirms the code is MDS under these conditions."
        ],
        "step_count": 7
    },
    "recuW6hTg9Womb": {
        "reasoning_steps": [
            "Clarify the zero pressure gradient condition, indicating that the pressure is uniform and constant within the boundary layer;",
            "Use the ideal gas law to relate pressure, density, and temperature;",
            "Apply the adiabatic Crocco-Busemann equation to relate temperature and velocity."
        ],
        "step_count": 3
    },
    "recuWotjZ4PaiN": {
        "reasoning_steps": [
            "Non-state character (Concept 1). Since entransy is not a thermodynamic state function, its balance for open/cyclic systems cannot, in general, collapse to a simple “in–out = dissipation” identity the way a conserved extensive state might; path/process features must appear explicitly.",
            "Heat↔work conversion at different temperatures (Concept 2). An absorption cycle performs two conversions at distinct temperature levels (generator vs. absorber). The general entransy balance must therefore include a conversion-entransy term Gconv that accounts for the temperature-level dependence of the heat↔chemical-potential conversion.",
            "Balance structure. For the cycle (control volume), Gin−Gout = Gdiss + Gconv + dGsysdt. Because Gconv≠0 for absorption cycles (two conversions at different T), the in–out difference cannot equal Gdiss in general.",
            "Steady vs. unsteady. Steady state (dGsys/dt=0): Gin−Gout=Gdiss+Gconv≠Gdiss. Unsteady: the storage term dGsys/dt further prevents equality.",
            "Edge case (why the confusion arises). Only in a pure heat-transfer element at one temperature level (no conversion, no storage) does Gconv=0 and dGsys/dt=0, giving equality. Absorption cycles do not satisfy this, so the equality does not hold."
        ],
        "step_count": 5
    },
    "recuWsQBjkpOEZ": {
        "reasoning_steps": [
            "Map load split → strong-solution circulation (Concept 1). Increasing the high-pressure (EV2) load requires more refrigerant at the high-pressure level, which drives up the strong-solution flow/circulation ratio through the generator–absorber loop. Higher circulation magnifies mixing and finite-ΔT heat-transfer irreversibilities in the generator and absorber, pushing total exergy destruction upward.",
            "Condenser variation is constrained (Concept 2). With terminal temperatures fixed, the condenser’s exergy destruction is largely bounded by the temperature approach; changes with load are modest and, in absorption cycles, typically remain below generator/absorber losses. Thus, shifts in condenser irreversibility cannot compensate for large increases on the solution loop.",
            "Apply to the three EV1–EV2 cases (EV1 = low-pressure, EV2 = high-pressure). (5,10 kW): EV2 dominates → circulation highest → generator/absorber irreversibilities largest → total exergy destruction maximized. (10,10 kW): loads balanced → circulation and generator/absorber irreversibilities intermediate → total exergy medium. (10,5 kW): EV2 lowest → circulation lowest → generator/absorber irreversibilities smallest → total exergy minimum.",
            "Conclusion: (5,10) kW > (10,10) kW > (10,5) kW for total irreversibility, consistent with Concept 1 dominance and the bounded condenser effect in Concept 2."
        ],
        "step_count": 4
    },
    "recuWtd4nRKVpn": {
        "reasoning_steps": [
            "Goal: derive, in one sentence, the per-tpbt_{pb}, per-recoil-energy event-rate formula with an explicit Heaviside step function enforcing the kinematic cutoff.",
            "Interaction–rate theorem (Concept_1): start from N˙=L σ⇒dRdtpb=NT ⁣∫dEν dψ(Eν)dtpb σ(Eν)\\dot N = \\mathcal{L}\\,\\sigma \\Rightarrow \\frac{dR}{dt_{pb}} = N_T \\!\\int dE_\\nu\\, \\frac{d\\psi(E_\\nu)}{dt_{pb}}\\, \\sigma(E_\\nu), identifying NTN_T targets and the spectral flux dψ/dtpbd\\psi/dt_{pb}.",
            "Spectral convolution with energy bounds (Concept_3): restrict the integral to the physically relevant source window ∫Eνmin⁡Eνmax⁡dEν\\int_{E_\\nu^{\\min}}^{E_\\nu^{\\max}} dE_\\nu.",
            "Differential in recoil energy (Concept_2): use dσ=(dσν(X,p)/dEr) dErd\\sigma = \\bigl(d\\sigma_\\nu(X,p)/dE_r\\bigr)\\, dE_r to obtain the spectrum-level rate dRdtpb dEr=NT ⁣∫Eνmin⁡Eνmax⁡dEν dψ(Eν)dtpb dσν(X,p)dEr\\frac{dR}{dt_{pb}\\, dE_r} = N_T \\!\\int_{E_\\nu^{\\min}}^{E_\\nu^{\\max}} dE_\\nu\\, \\frac{d\\psi(E_\\nu)}{dt_{pb}}\\, \\frac{d\\sigma_\\nu(X,p)}{dE_r}.",
            "Kinematic recoil bound (Concept_4): from two-body kinematics define a global upper bound Ermax⁡≡max⁡Eν∈[Eνmin⁡,Eνmax⁡]Ermax⁡(Eν)E_r^{\\max} \\equiv \\max_{E_\\nu\\in[E_\\nu^{\\min},E_\\nu^{\\max}]} E_r^{\\max}(E_\\nu) to uniformly constrain the integrated spectrum.",
            "Heaviside indicator (Concept_5): enforce the allowed kinematic domain by multiplying the integrand by Θ ⁣(Ermax⁡−Er)\\Theta\\!\\big(E_r^{\\max}-E_r\\big); with a global bound this factor can be written outside the EνE_\\nu integral.",
            "Efficiency factorization (Concept_6): include detector acceptance/efficiency via a multiplicative factor ε(Er)\\varepsilon(E_r), which depends only on ErE_r and thus factors outside the EνE_\\nu integral.",
            "Thin-target & no dead-time assumptions (Concept_7): single-scattering and negligible dead-time justify linear dependence on σ\\sigma without higher-order pileup or live-time corrections.",
            "Final one-sentence result: \\displaystyle \\frac{dR_{\\rm det}}{dt_{pb}\\, dE_r} = N_T\\,\\varepsilon(E_r)\\! \\int_{E_\\nu^{\\min}}^{E_\\nu^{\\max}} \\! dE_\\nu\\; \\frac{d\\sigma_\\nu(X,p)}{dE_r}\\; \\frac{d\\psi(E_\\nu)}{dt_{pb}}\\; \\Theta\\!\\big(E_r^{\\max}-E_r\\big)."
        ],
        "step_count": 9
    },
    "recuWsWpkWrdHJ": {
        "reasoning_steps": [
            "Hydrodynamic entropy definition: SH = -∑ pk log2 pk, where pk = E(k) / ∑ E(k) for isotropic systems, using shell spectrum E(k).",
            "Kolmogorov energy spectrum: E(k) = KKo ε^{2/3} k^{-5/3}, applied to forced turbulence at high Re.",
            "Logarithmic binning model: Shell radii kn = k0 λ^{n}, with λ=1.3; energy per shell En ≈ ε^{2/3} k_n^{-2/3} using given approximation (3/2) KKo (1 - λ^{-2/3}) ≈1.",
            "Probability and total energy: pn = En / E, with E ≈ ∑ En (geometric series, infinite sum approximation for large N).",
            "Analytical SH derivation: SH ≈ (log2 λ / ln λ) - log2 (λ^{2/3} ln(λ^{2/3})) ≈3.71 for λ=1.3 (large-scale terms cancel).",
            "Numerical simulation setup: 512^3 grid, ν=10^{-3}, ε=0.1, random forcing at low k, Re≈5580, k_max η≈1.25.",
            "Time evolution: SH increases initially as E(k) grows at high k, then saturates for t>7.5 eddy turnover times.",
            "Final numerical solution: SH converges to ≈3.8 (simulation value, near analytical for λ=1.3, independent of grid size)."
        ],
        "step_count": 8
    },
    "recuWtHy1LrYOI": {
        "reasoning_steps": [
            "Goal: under the Marshak boundary condition, obtain in one sentence a self-similar expression for the radiation flux F(x,t) with its temporal power-law exponent and constant prefactors.",
            "Boundary driver (Concept_1): impose the Marshak boundary at x=0 so that the surface/bath condition fixes the time dependence of the driving temperature T_s(t) ∝ t^τ and hence sets the overall time scale that the similarity solution must match.",
            "Material closures (Concept_2): adopt κ(T)=k_0 T^{-α} for the Rosseland opacity and an internal-energy/heat-capacity law characterized by α′, which determine how diffusion and storage scale with temperature in the similarity reduction.",
            "Similarity ansatz (Concept_3): write the radiation energy density as E(x,t)=aT^4=E_0 t^{4τ} f(ξ) with similarity variable ξ≡x/L(t) and let T^α=(E/a)^{α/4} ∝ E_0^{α/4} t^{ατ} g^{α/4}(ξ) (with g identified by the E↔T relation), so spatial derivatives factor as ∂_xE = (E_0 t^{4τ}/L(t)) f′(ξ).",
            "Time-scale balance for the similarity length (Concept_5): balance diffusion and storage under the Marshak driving to determine the similarity length L(t) ∝ t^β with β = ατ + ½(1 − α/α′), which encodes how the front spreads given the material (α,α′) and boundary exponent τ.",
            "Diffusion-form flux in similarity variables (Concept_4): use F = −(c/3κ) ∂_x(aT^4) with κ=k_0 T^{−α} to obtain F ∝ −[T^α][∂_xE] ∝ − t^{ατ} · (t^{4τ}/L) · E_0^{1+α/4} g^{α/4}(ξ) f′(ξ), so the time exponent becomes 4τ + ατ − β = 4τ + ½(α/α′ − 1).",
            "Constant prefactors (Concept_6): collect constitutive constants from a, c, k_0 and the chosen scale E_0 into a dimensionally consistent group K^{1/2} E_0^{1+α/8}, which multiplies the similarity shapes g^{α/4}(ξ) f′(ξ).",
            "Flux direction (Concept_7): retain the minus sign to reflect outward transport down the radiation-energy gradient in the diffusion limit.",
            "Assemble result: F(x,t) = − t^{4τ + ½(α/α′ − 1)} K^{1/2} E_0^{1+α/8} g^{α/4}(ξ) f′(ξ)."
        ],
        "step_count": 9
    },
    "recuWutxnisvpA": {
        "reasoning_steps": [
            "First, assume a monochromatic power spectrum for the Gaussian source field \\zeta_g (\\textbf{concept_1}), \\mathcal{P}_g(k)=A_g\\,\\delta\\!\\bigl(\\ln k-\\ln k_\\*\\bigr).",
            "By the definitions in \\textbf{concept_2}, this immediately gives \\sigma_n^2=\\int\\frac{dk}{k}\\,k^{2n}\\mathcal{P}_g(k)=A_g\\,k_\\*^{2n},\\qquad \\psi_n(r)=\\frac{1}{\\sigma_n^2}\\int\\frac{dk}{k}\\,k^{2n}\\frac{\\sin(kr)}{kr}\\,\\mathcal{P}_g(k) =\\frac{\\sin(k_\\* r)}{k_\\* r}.",
            "Moreover, with R_g=\\sqrt{3}\\,\\sigma_1/\\sigma_2 we obtain R_g^2=3/k_\\*^2. Using the spherical identity \\Delta\\!\\left[\\frac{\\sin(k_\\* r)}{k_\\* r}\\right]=-k_\\*^2\\,\\frac{\\sin(k_\\* r)}{k_\\* r}\\equiv -k_\\*^2\\,\\psi_1(r), insert these into \\textbf{theorem_2}. In the monochromatic limit (\\mu_3=1), the first bracket \\bigl[\\psi_1(r)+(1/3)R_g^2\\Delta\\psi_1(r)\\bigr]=\\psi_1-\\psi_1=0 vanishes, while the second bracket reduces to \\gamma_3^2\\psi_1+(1/3)R_g^2\\Delta\\psi_1=(\\gamma_3^2-1)\\psi_1=-(1-\\gamma_3^2)\\psi_1.",
            "Hence the overall (1-\\gamma_3^2) cancels and we are left with the simplified Gaussian peak profile \\hat{\\zeta}_g(r)=\\mu_2\\,\\psi_1(r)=\\mu_2\\,\\frac{\\sin(k_\\* r)}{k_\\* r}.",
            "Finally, substituting this into the logarithmic mapping of \\textbf{theorem_1}, \\hat{\\zeta}(r)=-\\frac{1}{\\gamma}\\ln\\!\\bigl(1-\\gamma\\,\\hat{\\zeta}_g(r)\\bigr), yields the non-Gaussian curvature peak profile \\boxed{\\;\\hat{\\zeta}(r)=-\\frac{1}{\\gamma}\\,\\ln\\!\\left(1-\\gamma\\,\\mu_2\\,\\frac{\\sin(k_\\* r)}{k_\\* r}\\right)\\;}."
        ],
        "step_count": 5
    },
    "recuWuHuxQe6rJ": {
        "reasoning_steps": [
            "Mass integral (Concept_1): For spherical symmetry, use the enclosed-mass relation $M(<r)=4\\pi\\int_{0}^{r}\\rho(s)\\,s^{2}\\,ds$.",
            "Domain normalization (Concept_2): Impose $\\int_{4M_{\\rm BH}}^{r_c}4\\pi r^{2}\\rho(r)\\,dr = M_{\\rm Halo}/M_{\\rm BH}^{2}$ to fix the amplitude solely on the fitting interval.",
            "Linearity (Concept_3): With $\\rho(r)=C\\,\\bar\\rho(r)$ and linearity of integration, solve $C=\\big(M_{\\rm Halo}/M_{\\rm BH}^{2}\\big)\\Big/\\!\\left(\\int_{4M_{\\rm BH}}^{r_c}4\\pi r^{2}\\bar\\rho(r)\\,dr\\right)$.",
            "Scale–shape decomposition (Concept_4): Treat $\\bar\\rho(r)$ as a positive, integrable, dimensionless smooth broken–power-law shape, carrying only geometry while the overall scale sits in $C$.",
            "Physical guidance for the shape (Concept_5): Adiabatic black-hole growth maps an initial $\\rho\\propto r^{-\\gamma}$ to a spike with $\\gamma_{\\rm sp}=(9-2\\gamma)/(4-\\gamma)$, motivating Hernquist-like inner/outer behavior while keeping $\\bar\\rho$ symbolic.",
            "Assemble: Substitute $C$ back to obtain the standard answer $\\rho(r)=\\dfrac{M_{\\rm Halo}}{M_{\\rm BH}^{2}}\\left(\\int_{4M_{\\rm BH}}^{r_c}4\\pi r^{2}\\bar\\rho(r)\\,dr\\right)^{-1}\\bar\\rho(r)$."
        ],
        "step_count": 6
    },
    "recuWv2H0E4VNr": {
        "reasoning_steps": [
            "Goal: Derive the critical energy density \\rho_c at which the universe transitions from acceleration to deceleration, in the cosmological model where Hayward regular primordial black holes act as 'Swiss cheese' voids, using the given theorems and concept.",
            "Transition condition for scale factor acceleration: The universe transitions from acceleration to deceleration when the acceleration of the scale factor a (denoted \\ddot{a}) is zero, i.e., \\ddot{a} = 0.",
            "Relate \\ddot{a} to Hayward metric's F(R) via Theorem 1: Theorem 1 states R = a r_\\Sigma (where r_\\Sigma is a constant) and \\frac{d^2 R}{du^2} = -\\frac{1}{2} F'(R). Differentiate R = a r_\\Sigma twice with respect to u: \\frac{d^2 R}{du^2} = r_\\Sigma \\ddot{a}. Substitute into the second relation of Theorem 1: r_\\Sigma \\ddot{a} = -\\frac{1}{2} F'(R). At transition (\\ddot{a} = 0), this simplifies to F'(R) = 0 (since r_\\Sigma \\neq 0).",
            "Recall Hayward's F(R) and M(R) from Theorem 2: Theorem 2 gives F(R) = 1 - \\frac{2 G_N M(R)}{R} and M(R) = \\frac{m R^3}{R^3 + 2 G_N m L^2}, where G_N is Newton's gravitational constant, m is the mass of the Hayward black hole, and L is the regularization length scale.",
            "Compute F'(R) using the quotient rule: Differentiate F(R) = 1 - \\frac{2 G_N M(R)}{R} with respect to R. Using the quotient rule \\frac{d}{dR}\\left(\\frac{M(R)}{R}\\right) = \\frac{M'(R) R - M(R)}{R^2}, we get F'(R) = -2 G_N \\cdot \\frac{M'(R) R - M(R)}{R^2}. From Step 3 (F'(R) = 0), the numerator must vanish: M'(R) R - M(R) = 0 \\implies M'(R) = \\frac{M(R)}{R}.",
            "Calculate M'(R) from Hayward's M(R): Let C = 2 G_N m L^2 (for simplification), so M(R) = \\frac{m R^3}{R^3 + C}. Using the quotient rule \\frac{d}{dR}\\left(\\frac{A}{B}\\right) = \\frac{A'B - AB'}{B^2}, we compute M'(R) = m \\cdot \\frac{3 R^2 (R^3 + C) - R^3 \\cdot 3 R^2}{(R^3 + C)^2} = \\frac{3 m C R^2}{(R^3 + C)^2}.",
            "Apply M'(R) = \\frac{M(R)}{R} to solve for R^3: Substitute M'(R) and M(R) into the condition: \\frac{3 m C R^2}{(R^3 + C)^2} = \\frac{1}{R} \\cdot \\frac{m R^3}{R^3 + C}. Cancel non-zero terms (m, R^2, R^3 + C) and simplify: 3 C = R^3 + C \\implies R^3 = 2 C.",
            "Substitute C = 2 G_N m L^2 back into R^3 = 2 C: This gives R^3 = 2 \\cdot 2 G_N m L^2 = 4 G_N m L^2. Rearrange to solve for m: m = \\frac{R^3}{4 G_N L^2}.",
            "Relate m, \\rho_c, and R via Concept 1: Concept 1 states the cosmic mean density \\rho = \\frac{3 m}{4 \\pi R^3} (since \\rho equals the average density outside each vacuole). At transition, \\rho = \\rho_c, so rearrange to m = \\frac{4 \\pi}{3} \\rho_c R^3.",
            "Equate the two expressions for m and solve for \\rho_c: Set \\frac{R^3}{4 G_N L^2} (from Step 8) equal to \\frac{4 \\pi}{3} \\rho_c R^3 (from Step 9). Cancel R^3 (non-zero) and rearrange: \\rho_c = \\frac{3}{16 \\pi G_N L^2}."
        ],
        "step_count": 10
    },
    "recuWvaDJUTkdZ": {
        "reasoning_steps": [
            "Given the Born-Infeld Lagrangian (concept_1): $L_{\\text{BI}} = b^2 \\left(1 - \\sqrt{1 + \\frac{S}{b^2} - \\frac{P^2}{4b^4}}\\right)$, where $S = \\frac{1}{2}F_{\\mu\\nu}F^{\\mu\\nu}$ is the electromagnetic invariant, $P = \\frac{1}{2}F_{\\mu\\nu}(\\star F)^{\\mu\\nu}$, and $b > 0$ controls the nonlinearity strength. Simplification: The problem specifies that the theory depends only on the electromagnetic invariant $S$ (i.e., $P = 0$), and under the pure electric field condition in flat spacetime, $S = -E^2$ (given). Substituting these, we obtain: $L_{\\text{BI}} = b^2 \\left(1 - \\sqrt{1 + \\frac{S}{b^2}}\\right) = b^2 \\left(1 - \\sqrt{1 - \\frac{E^2}{b^2}}\\right)$.",
            "Derive the constitutive relation $D_{\\mu\\nu} = 2(L_S F_{\\mu\\nu} + L_P \\star F_{\\mu\\nu})$ using theorem_2 (electromagnetic duality invariance). Simplification: The theory depends only on $S$ (i.e., $L_P = 0$), so: $D_{\\mu\\nu} = 2L_S F_{\\mu\\nu}$. Compute $L_S = \\frac{\\partial L_{\\text{BI}}}{\\partial S} = \\frac{\\partial}{\\partial S}\\left[b^2 \\left(1 - \\left(1 + \\frac{S}{b^2}\\right)^{1/2}\\right)\\right] = -\\frac{1}{2}\\left(1 + \\frac{S}{b^2}\\right)^{-1/2}$. Substitute $S = -E^2$: $L_S = -\\frac{1}{2}\\left(1 - \\frac{E^2}{b^2}\\right)^{-1/2}$.",
            "Expression for the physical field $D$ under electrostatic spherical symmetry: $D_{01} = 2L_S F_{01} = 2\\left(-\\frac{1}{2}\\left(1 - \\frac{E^2}{b^2}\\right)^{-1/2}\\right)(-E) = \\frac{E}{\\sqrt{1 - \\frac{E^2}{b^2}}}$. The radial component (scalar) of the physical field $D$ is defined as: $D \\equiv |D| = \\sqrt{-g_{00}g_{11}}\\,|D_{01}| = |D_{01}| \\quad \\text{(flat spacetime: } g_{00} = -1, g_{11} = 1\\text{)}$. Thus: $D = \\frac{E}{\\sqrt{1 - \\frac{E^2}{b^2}}}$.",
            "Using the Gaussian unit normalization condition: $\\oint_{S^2} \\mathbf{D} \\cdot d\\mathbf{a} = 4\\pi Q$. Under spherical symmetry, $D$ is a radial vector with magnitude depending only on $r$: $D \\cdot 4\\pi r^2 = 4\\pi Q \\implies D = \\frac{Q}{r^2}$. Combining with the constitutive relation: $\\frac{Q}{r^2} = \\frac{E}{\\sqrt{1 - \\frac{E^2}{b^2}}}$.",
            "Solve for the electric field $E(r)$: From the equation: $\\frac{Q}{r^2} = \\frac{E}{\\sqrt{1 - \\frac{E^2}{b^2}}}$, we obtain: $E = \\frac{\\frac{Q}{r^2}}{\\sqrt{1 + \\frac{1}{b^2}\\left(\\frac{Q}{r^2}\\right)^2}} = \\frac{Q}{r^2}\\left(1 + \\frac{Q^2}{b^2 r^4}\\right)^{-1/2}$.",
            "Leading asymptotic expression in the weak-field limit ($b \\to \\infty$): Taylor expansion: When $b \\to \\infty$, $\\frac{Q^2}{b^2 r^4} \\to 0$, so expand $\\left(1 + \\frac{Q^2}{b^2 r^4}\\right)^{-1/2}$: $\\left(1 + \\frac{Q^2}{b^2 r^4}\\right)^{-1/2} = 1 - \\frac{1}{2}\\frac{Q^2}{b^2 r^4} + O\\left(\\frac{1}{b^4}\\right)$. Substituting back: $E(r) = \\frac{Q}{r^2}\\left(1 - \\frac{1}{2}\\frac{Q^2}{b^2 r^4} + O\\left(\\frac{1}{b^4}\\right)\\right)$. The leading term is: $E(r) = \\frac{Q}{r^2} + O\\left(\\frac{1}{b^2}\\right)$.",
            "Theorem_1 states: For $L = L(S)$-type theories, the light cones of the two photon propagation modes must coincide. The Born-Infeld theory satisfies this condition, and in the weak-field limit $b \\to \\infty$, the theory reduces to Maxwell theory (no birefringence), consistent with the result."
        ],
        "step_count": 7
    },
    "recuWy6viXW0dl": {
        "reasoning_steps": [
            "From c1∥x∥2≤V≤c2∥x∥2 and ∂xV fav≤−c3∥x∥2, replace ∥x∥2 by V/c2 to get the averaged decay ∂xV fav ≤ −c3c2 V = −0.8 V.",
            "For y˙=fav(y,us)+εg with ∥g∥≤δ1∥y∥+δ2∥y∥ ∥u˙s∥ and ∥∂xV∥≤c5∥y∥, use ∥y∥2≤V/c1 to bound V˙ ≤ −c3c2V + εc5δ1c1V + (εc5δ2c1+c4c1)V ∥u˙s∥. Numerically, the flow coefficients are c5δ1c1=3.2, c5δ2c1=0.8, and c4c1=1.2.",
            "At jump times, with ∥y+−y−∥≤εϕ∥Δus∥ ∥y−∥, ∥∂xV∥≤c5∥y∥, ∥∂usV∥≤c4∥y∥2, V≥c1∥y∥2, there exist κ1,κ2>0 such that V+−V−V− ≤ (κ1ε+κ2) ∥Δus∥. Taking the standard (tight) choices consistent with the given bounds yields the ε–linear jump weight c5ϕc1=1.2 and the constant jump weight c4c1=1.2.",
            "With ∇V LVL_V–Lipschitz and κ=LV/2, V(y+Δy)≤V(y)+∇V(y)⊤Δy+κ∥Δy∥2. Using ∥y+−y−∥≤εϕ∥Δus∥ ∥y−∥, ∥Δus∥≤diam(Γ), and V≥c1∥y∥2, the quadratic jump coefficient is ℓ2=LVϕ2 diam(Γ)2c1=2.5⋅0.09⋅1.62⋅1.0=0.18, so the ε²–term over an interval contributes A=μ ℓ2=0.5⋅0.18=0.09.",
            "With ∫t1t2∥dus∥≤μ(t2−t1)+α, merge flow and jump effects into a single TV factor using the supremum ε–linear weight kTV=max{c5δ2c1,c5ϕc1}=max{0.8,1.2}=1.2. Thus the ε–linear coefficient becomes B=c5c1δ1+μ kTV=3.2+0.5⋅1.2=3.8, and the constant term is C=−c3c2+μ c4c1=−0.8+0.6=−0.2.",
            "The net exponential rate over any interval has the quadratic form λ(ε)=Aε2+Bε+C=0.09 ε2+3.8 ε−0.2. Exponential convergence requires λ(ε)<0, so the admissible ε lie below the positive root of λ(ε)=0.",
            "Solve 0.09 ε2+3.8 ε−0.2=0: ε∗=−3.8+3.82−4⋅0.09⋅(−0.2)2⋅0.09=−3.8+14.5120.18≈0.052566… Round to four decimals: ε∗=0.0526."
        ],
        "step_count": 7
    },
    "recuWzqIVaMlkD": {
        "reasoning_steps": [
            "From Concept 2, the estimation errors \\tilde a_dis and \\tilde tau_dis converge to zero in finite time; these are then used as compensation terms in the outer- and inner-loop control laws.",
            "For the inner loop, by constructing a Lyapunov function based on the SE(3) error function, it follows that e_R and e_Omega are bounded and decay exponentially once the estimation errors vanish.",
            "For the outer loop, by employing an energy-type Lyapunov function V_t together with saturated feedback tanh, one obtains varsigma, dot varsigma -> 0, which further implies theta_x, theta_y -> 0 and e_gamma -> 0.",
            "After restoring the coupling term and applying the growth-bounded property |f_Delta| <= |f_gamma| * |e_R| along with the linear growth bound of |f_gamma(e_O)|, the cascade stability and LaSalle’s invariance principle ensure that the closed-loop system is invariant only at the zero-error point, thus all errors converge to zero."
        ],
        "step_count": 4
    },
    "recuWzAXY7qF8u": {
        "reasoning_steps": [
            "Goal: Derive the closed-form radial metric function e^{-2\\beta(r)}{\\mathrm{Hayward}} for a static, spherically symmetric compact star sourced by an isotropic perfect fluid and a Hayward-type nonlinear electromagnetic (NED) magnetic monopole.",
            "Field equations (Theorem_1): For ds^{2}=-e^{2\\alpha}c^{2}dt^{2}+e^{2\\beta}dr^{2}+r^{2}d\\Omega^{2} and T^{\\text{m}}{\\mu\\nu}=(\\rho c^{2}+p)U_{\\mu}U_{\\nu}+pg_{\\mu\\nu}, the tt–equation can be written as a total derivative: \\; \\dfrac{d}{dr}\\big[r(1-e^{-2\\beta})\\big]=\\dfrac{8\\pi G}{c^{4}}\\, r^{2}\\big(T^{\\text{NED}}{00}e^{-2\\alpha}+\\rho c^{2}\\big).",
            "Radial integration & mass function (Concept_1): Integrate Step_2 from 0 to r and impose regularity at the center to obtain 1-e^{-2\\beta(r)}=\\dfrac{8\\pi G}{c^{4}}\\,\\dfrac{1}{r}\\int{0}^{r}x^{2}\\big(T^{\\text{NED}}{00}e^{-2\\alpha}+\\rho c^{2}\\big)dx. Define the matter part by m(r)=4\\pi\\int{0}^{r}\\rho(x)x^{2}dx so that \\dfrac{8\\pi G}{c^{4}}\\,\\dfrac{1}{r}\\int_{0}^{r}x^{2}\\rho c^{2}dx=\\dfrac{2Gm(r)}{c^{2}r}.",
            "Hayward Lagrangian (Theorem_2): Use L_{\\mathrm{Hayward}}(F)=-\\dfrac{3}{2}\\dfrac{1}{s q^{2}}\\dfrac{(2q^{2}FC_{d})^{3/2}}{\\big[1+(2q^{2}FC_{d})^{3/4}\\big]^{2}} and the magnetic monopole potential A=q\\cos\\theta\\, d\\phi (so the configuration is purely magnetic).",
            "Electromagnetic invariant for the monopole: With F_{\\theta\\phi}=-q\\sin\\theta and metric inverses g^{\\theta\\theta}=1/r^{2}, g^{\\phi\\phi}=1/(r^{2}\\sin^{2}\\theta), one gets F\\equiv \\tfrac14F_{\\mu\\nu}F^{\\mu\\nu}=q^{2}/(2r^{4}). Hence (2q^{2}FC_{d})^{3/4}=C_{d}^{3/4}q^{3}/r^{3} and (2q^{2}FC_{d})^{3/2}=C_{d}^{3/2}q^{6}/r^{6}.",
            "NED energy density entering G_{tt}: For a purely magnetic field F_{0\\lambda}=0, so T^{\\text{NED}}{00}=g{00}L and therefore T^{\\text{NED}}{00}e^{-2\\alpha}=-L{\\mathrm{Hayward}}. Substituting Step_5 into L_{\\mathrm{Hayward}} gives -L_{\\mathrm{H}}=\\dfrac{3}{2s}\\,\\dfrac{C_{d}^{3/2}q^{4}}{r^{6}}\\,\\dfrac{1}{\\big(1+\\dfrac{C_{d}^{3/4}q^{3}}{r^{3}}\\big)^{2}}.",
            "Evaluate the NED integral in Step_3: Let a\\equiv C_{d}^{3/4}q^{3}. Then \\int_{0}^{r}\\!x^{2}(-L_{\\mathrm{H}})\\,dx=\\dfrac{3}{2s}C_{d}^{3/2}q^{4}\\int_{0}^{r}\\!\\dfrac{x^{-4}}{\\big(1+a/x^{3}\\big)^{2}}dx=\\dfrac{C_{d}^{3/4}q}{2s}\\,\\dfrac{r^{3}}{r^{3}+a}.",
            "Assemble the result: Insert the matter piece from Step_3 and the NED integral from Step_7 into Step_2’s integrated form to obtain e^{-2\\beta(r)}=1-\\dfrac{2Gm(r)}{c^{2}r}-\\dfrac{8\\pi G}{c^{4}}\\,\\dfrac{1}{r}\\Big[\\dfrac{C_{d}^{3/4}q}{2s}\\,\\dfrac{r^{3}}{r^{3}+C_{d}^{3/4}q^{3}}\\Big]."
        ],
        "step_count": 8
    },
    "recuWzHXCQY4JW": {
        "reasoning_steps": [
            "Link COP to irreversibilities. With inputs fixed, COP improves when dominant exergy destruction (mainly in the generator/absorber) is reduced (Concept 1).",
            "HregH_{\\mathrm{reg}}Hreg — regenerator height (→ Positive). A taller regenerator strengthens desorption effectiveness, yielding a more concentrated strong solution at the same duty. That lets the cycle meet the vapor load with a lower strong-solution circulation ratio, which cuts mixing and finite-ΔT losses in generator/absorber → lower destruction → higher COP (Concept 1).",
            "HabsH_{\\mathrm{abs}}Habs — absorber height (→ Positive). A taller absorber boosts absorption effectiveness, so the same vapor load is handled with less solution circulation and smaller temperature driving requirements on the solution loop; generator/absorber irreversibilities fall → COP rises (Concept 1).",
            "mw,con,im_{w,\\mathrm{con,i}}mw,con,i — condenser cooling-water flow (→ Negative). At fixed terminal temperatures, condenser exergy destruction is constrained by the temperature approach and varies only modestly with flow (Concept 2). Increasing mw,con,im_{w,\\mathrm{con,i}}mw,con,i therefore does not meaningfully reduce system-limiting destruction in generator/absorber and does not raise useful output, so the net effect on COP is non-positive; we classify it as negative under “all else equal.”"
        ],
        "step_count": 4
    },
    "recuWA2L4YdoEi": {
        "reasoning_steps": [
            "By Concept~1, a traveling-wave branch with selected \\( q_{0}>0 \\) bifurcates at onset. Write the physical wavenumber as \\( q=q_{0}+\\delta q \\) with small detuning \\( \\delta q \\). The linear growth rate near onset can be parameterized as \\[ \\mu \\equiv \\mu(q) \\;\\approx\\; \\mu_{0} \\;-\\; c_{\\mathrm{E}}\\,\\bigl(q^{2}-q_{0}^{2}\\bigr), \\] where \\( \\mu_{0} \\) is the control parameter distance to onset and \\( c_{\\mathrm{E}}>0 \\) is a system-dependent constant.",
            "By Concept~2, the conserved Eckhaus boundary is attained at \\[ q \\;=\\; \\frac{q_{0}}{\\sqrt{3}} \\quad\\Longleftrightarrow\\quad q^{2}-q_{0}^{2} \\;=\\; -\\,\\frac{2}{3}\\,q_{0}^{2}. \\] Hence the effective linear drive at the Eckhaus edge becomes \\[ \\mu_{\\mathrm{E}} \\;\\equiv\\; \\mu(q_{0}/\\sqrt{3}) \\;=\\; \\mu_{0} \\;+\\; \\frac{2}{3}\\,c_{\\mathrm{E}}\\,q_{0}^{2}. \\]",
            "By Concept~3, in a neighborhood of onset the envelope \\(A\\) obeys a universal cubic amplitude equation of the form \\[ \\partial_{T} A \\;=\\; \\mu_{\\mathrm{E}}\\,A \\;-\\; g\\,|A|^{2}A \\;+\\; \\xi\\,\\partial_{X}^{2}A, \\qquad g>0,\\;\\xi>0, \\] where the cubic coefficient \\(g\\) is inherited from the microscopic nonlinearities of the NRCH reduction.",
            "Impose the Instruction condition that the microscopic nonreciprocity is amplitude-dependent \\( \\alpha(|\\varphi|)=\\alpha_{0}-\\alpha_{1}|\\varphi|^{2} \\). A localized state marginal at coexistence requires two balances: \\[ \\text{(i) linear--nonlinear balance:}\\quad |A|^{2} \\;=\\; \\frac{\\mu_{\\mathrm{E}}}{g}, \\qquad \\text{(ii) nonreciprocity balance:}\\quad \\alpha(|\\varphi|)=0 \\Rightarrow\\n|\\varphi|^{2} \\;=\\; \\rho_{0}^{2} \\;=\\; \\frac{\\alpha_{0}}{\\alpha_{1}}. \\] Near onset the physical amplitude scales linearly with the envelope, \\( |\\varphi|^{2} \\sim \\kappa\\,|A|^{2} \\) with some \\( \\kappa>0 \\) (constant matching factor).",
            "At the threshold for coexistence (plane wave at Eckhaus edge and marginal localized state), insert the balances into the scaling relation to obtain \\[ \\kappa\\,\\frac{\\mu_{\\mathrm{E}}}{g} \\;=\\; \\frac{\\alpha_{0}}{\\alpha_{1}} \\quad\\Longleftrightarrow\\quad \\alpha_{1}\\,\\mu_{\\mathrm{E}} \\;=\\; \\frac{g}{\\kappa}\\,\\alpha_{0}. \\] Using the conserved Eckhaus value of \\( \\mu_{\\mathrm{E}} \\) from Step~2 and the NRCH amplitude reduction (Concept~3) that fixes the positive constants \\(g,\\kappa,c_{\\mathrm{E}}\\), the marginal coexistence condition yields the inequality \\[ 3\\,\\alpha_{0} \\;>\\; \\alpha_{1}, \\] which is the claimed threshold in the \\((\\alpha_{0},\\alpha_{1})\\)-plane."
        ],
        "step_count": 5
    },
    "recuWAdoh5QPal": {
        "reasoning_steps": [
            "According to Concepts 1 and 2, the special grid cells do not exhibit a typical grid-like firing pattern in the standard autocorrelation map. However, if spikes are plotted against positions at a specific distance along the movement trajectory, a grid map with a improved gridness score emerges. This indicates that this type of grid cell encodes a spatial location at a certain distance from the current position, meaning its grid map has undergone a shift. Yet, due to the periodic nature of grid cell firing fields, it remains undetermined whether they represent a future location at a specific distance or a past location at a specific distance.",
            "At this point, according to Concept 3, a difference in the average firing rate of the cells across the 0–1 position range was observed (specifically, the special grid cells showed a higher average firing rate in the 0–0.5 segment and a lower rate in the 0.5–1 segment). This suggests that the firing of the grid cells is confined within the two-dimensional open field and that their firing fields have shifted opposite to the direction of movement. This implies that if a special grid cell representing a particular location fires *before* the animal reaches that location, it indicates that the grid cells discovered by the researchers encode a location on the future path.",
            "According to Concept 4, when the mouse moves into the firing field of a grid cell, the typical grid cells fire primarily at the 0° phase of the theta wave, at which point they encode the animal's precise current location. In contrast, at the 180° phase of the theta wave, it is primarily the special grid cells discovered by the researchers that fire, suggesting they likely encode a location on the animal's future path.",
            "If the mouse's position is reconstructed using a Bayesian decoding algorithm based on the firing patterns of grid cells, the decoding at the 0° theta phase relies mainly on typical grid cells representing the current position, while the decoding at the 180° phase relies mainly on the special grid cells representing a future position (because the decoding uses the grid maps of the shifted cells for coding and decoding). Therefore, the position decoded during the 0° to 180° phase of the theta wave would gradually shift from the current position towards a future position."
        ],
        "step_count": 4
    },
    "recuWAmC9kxRIF": {
        "reasoning_steps": [
            "NFW profile (Concept_1): adopt the standard NFW density \\(\\rho(r)=\\dfrac{\\rho_{s}}{(r/r_{s})(1+r/r_{s})^{2}}\\) with characteristic density \\(\\rho_s\\) and scale radius \\(r_s\\).",
            "Spherical mass integral (Concept_2): use the spherical symmetry theorem \\( M_{\\text{D}}(r)=4\\pi\\int_{0}^{r}\\rho(r')\\,r'^{2}\\,\\mathrm{d}r' \\) to accumulate mass from the centre out to \\(r\\).",
            "Exact integral (Concept_3): insert the NFW form and evaluate to obtain the exact enclosed mass \\( M_{\\text{D, exact}}(r)=4\\pi\\rho_s r_s^3\\left[\\ln\\left(1+\\frac{r}{r_s}\\right)-\\frac{r}{r_s + r}\\right] \\), then switch to the small parameter \\(x=r_s/r\\) for the outer halo.",
            "Outer-halo expansion (Concept_4): in the limit \\(r\\gg r_s\\) (\\(x\\ll 1\\)) apply Taylor expansions \\(\\ln(1+1/x)\\approx -\\ln x + x\\) and \\(\\frac{1}{1+x}\\approx 1-x\\) to show that the bracketed factor asymptotes to \\(2/\\!\\sqrt{1+x}\\), yielding the compact approximation.",
            "Final expression (Concept_5): combine the prefactor with the asymptotic bracket to give the quoted one-line formula \\( M_{\\text{D}}(r)=\\dfrac{8\\pi\\rho_{s}r_{s}^{3}}{\\sqrt{1+r_{s}/r}} \\)."
        ],
        "step_count": 5
    },
    "recuWAFhvt3rIw": {
        "reasoning_steps": [
            "Model & symmetry (Concept_1 + Concept_5): in the SU(2)_L×SU(2)R parity-doublet (mirror) assignment the two opposite-parity nucleon fields mix via a chirally invariant mass m0 and couple to the isoscalar-scalar σ and isovector-scalar a; on an isospin eigenstate τ3|j⟩=j|j⟩ the mean-field combination becomes Δ ≡ (σ − j a).",
            "Mean-field quasiparticles (Concept_2): replacing meson fields by uniform expectation values gives ω{α j} = √(p^2 + m{α j}^2), so the problem reduces to finding the effective mass eigenvalues of the parity-mixed nucleon sector.",
            "Mass matrix with mirror signs (Concept_3 + Concept_4): the parity partners couple with opposite signs to the scalar mean fields, yielding a real symmetric (Hermitian) 2×2 mass matrix 𝓜 = [[A, m0], [m0, D]] with A = g1 Δ and D = − g2 Δ.",
            "Spectral theorem & eigenvalues (Concept_6): a Hermitian 2×2 matrix is unitarily diagonalizable with real eigenvalues λ± = (A + D)/2 ± (1/2) √((A − D)^2 + 4 m0^2).",
            "Substitution & branch labeling (Concept_3 + Concept_6): insert A = g1 Δ and D = −g2 Δ to obtain m_{α j} = 1/2 [ α (g1 − g2) Δ + √( (g1 + g2)^2 Δ^2 + 4 m0^2 ) ], where α = ±1 labels the two parity branches.",
            "Consistency (Concept_7): in the chiral restoration limit σ, a → 0 (thus Δ→0) both branches become degenerate at m0, confirming the formula."
        ],
        "step_count": 6
    },
    "recuWB0CjLI11i": {
        "reasoning_steps": [
            "Goal: under a static, spherically symmetric background with b_\\mu = (b_t(r), b_r(r), 0, 0), derive \\xi R_{rr} b_r = 0.",
            "Geometry & field setup (Theorem_1): Use ds^2 = −e^{2\\nu(r)} dt^2 + e^{2\\mu(r)} dr^2 + r^2(d\\theta^2 + sin^2\\theta d\\phi^2), with b_\\mu = (b_t(r), b_r(r), 0, 0). Since all fields depend only on r, the field strength b_{\\mu\\nu} = ∂_\\mu b_\\nu − ∂_\\nu b_\\mu has only b_{tr} = −∂_r b_t(r) potentially nonzero; all other components vanish.",
            "Action & vector-field equation (Theorem_2 + Concept_1): From the action, the bumblebee equation of motion is E^{\\mu} ≡ ∇_\\nu b^{\\mu\\nu} − (\\xi/\\kappa) R^{\\mu\\nu} b_\\nu + 2 b^{\\mu} dV/d(B^2) = 0.",
            "Vacuum-minimum condition: In the stated 'vacuum background', take the potential at its minimum so V' ≡ dV/d(B^2) = 0; staticity also implies ∂_t(…) = 0.",
            "Pick the \\mu = r component: 0 = E^{r} = ∇_\\nu b^{r\\nu} − (\\xi/\\kappa) R^{r\\nu} b_\\nu + 2 b^{r} V' ⇒ with V' = 0 this reduces to ∇_\\nu b^{r\\nu} − (\\xi/\\kappa) R^{r\\nu} b_\\nu = 0.",
            "Compute the divergence: For any antisymmetric tensor A^{\\mu\\nu}, ∇_\\nu A^{\\mu\\nu} = (1/√−g) ∂_\\nu(√−g A^{\\mu\\nu}). With A^{\\mu\\nu} = b^{\\mu\\nu} and the ansatz b^{rr} = b^{r\\theta} = b^{r\\phi} = 0 and b^{rt} = b^{rt}(r), we get ∇_\\nu b^{r\\nu} = (1/√−g)[∂_t(√−g b^{rt}) + ∂_r(√−g b^{rr}) + ∂_\\theta(√−g b^{r\\theta}) + ∂_\\phi(√−g b^{r\\phi})] = 0.",
            "From Steps 5–6 it follows that R^{r\\nu} b_\\nu = 0.",
            "Use static spherical symmetry (Theorem_1): The Ricci tensor has no off-diagonal components, so R^{tr} = 0. Hence R^{r\\nu} b_\\nu = R^{rr} b_r = 0.",
            "Lower indices to match the requested form: R^{rr} b_r = 0 ⇒ g_{rr} R^{rr} b_r = R_{rr} b_r = 0.",
            "Conclusion: Multiplying by \\xi leaves zero unchanged, yielding the result \\xi R_{rr} b_r = 0 (agreeing with the given answer)."
        ],
        "step_count": 10
    },
    "recuWB6MQKVuYy": {
        "reasoning_steps": [
            "Goal: derive the explicit zero-temperature (T=0) energy-density equation for a fixed quark flavor f and color c in CSQY matter, in one sentence.",
            "Statistical setup (Concept_1): work in natural units (ℏ=c=k_B=1) with the phase-space measure ∫ d^3k/(2π)^3; at T→0 use f(E;μ,T)=[e^{(E−μ)/T}+1]^{-1} \\xrightarrow[T\\to0]{} \\theta(μ−E), so occupied states are selected by n_\\mathbf{k}=\\theta(μ−\\varepsilon_\\mathbf{k}), and the energy density takes the generic form ε=∫ (d^3k/(2π)^3) f(k) 𝔈(k).",
            "Quasiparticle spectrum (Concept_2): in the paired phase (Nambu–Gor’kov/BCS) there are two branches E_\\mathbf{k}^{±}=\\sqrt{(\\varepsilon_\\mathbf{k}±μ)^2+Δ^2} (nonrelativistic limit E_\\mathbf{k}=\\sqrt{\\xi_\\mathbf{k}^2+Δ^2}, with \\xi_\\mathbf{k}=\\varepsilon_\\mathbf{k}−μ).",
            "Coherence factors and weights (Concept_3): the Bogoliubov factors satisfy u_{\\mathbf{k},±}^2=\\tfrac12(1+\\xi_{\\mathbf{k},±}/E_{\\mathbf{k}}^{±}), v_{\\mathbf{k},±}^2=\\tfrac12(1−\\xi_{\\mathbf{k},±}/E_{\\mathbf{k}}^{±}), hence u_{\\mathbf{k},±}^2−v_{\\mathbf{k},±}^2=\\xi_{\\mathbf{k},±}/E_{\\mathbf{k}}^{±}=(μ±\\varepsilon_\\mathbf{k})/|E_{\\mathbf{k}}^{±}|, which provides the required weight; sum explicitly over the two branches ∑_{±}.",
            "Energy accounting and vacuum subtraction (Concept_4): use the unshifted single-particle energy 𝔈_{±}^{unshifted}=E_{\\mathbf{k}}^{±}∓μ, and implement vacuum subtraction (no-sea/normal ordering), which at T=0 leaves only the Fermi-sphere contribution enforced by \\theta(μ−\\varepsilon_\\mathbf{k}).",
            "Assembly to the fixed-(f,c) integrand: multiply the occupancy selector \\theta(μ−\\varepsilon_\\mathbf{k}) by the Bogoliubov weight (μ±\\varepsilon_\\mathbf{k})/|E_{\\mathbf{k}}^{±}| and by the unshifted energy (E_{\\mathbf{k}}^{±}∓μ), integrate over k with ∫ d^3k/(2π)^3, and sum over ±.",
            "Notation alignment and total density: identify μ≡\\varepsilon_{fc}, \\varepsilon_\\mathbf{k}≡\\varepsilon_{fk}, E_{\\mathbf{k}}^{±}≡\\varepsilon_{fck}^{±}; the total quark energy density is obtained by additionally summing the fixed-(f,c) result over flavors f and colors c."
        ],
        "step_count": 7
    },
    "recuWB9pwqtUNp": {
        "reasoning_steps": [
            "Using PBMCs (peripheral blood mononuclear cells) from patients P1 and P2 as experimental materials: Flow cytometry showed elevated STAT1 phosphorylation, and intracellular staining revealed marked increases in IFN-α, IL-1β, IL-6, and IL-8. The proportion of CD14⁺ monocytes increased, while that of CD19⁺ B cells decreased in PBMCs. These results indicate that the type I IFN signaling pathway is strongly activated in the cells of patients P1 and P2.",
            "Activation of TLR7/9 and the downstream type I IFN pathway was most prominent in dendritic cells (DCs), suggesting that PLD4 deficiency in DCs triggers systemic inflammation and autoimmunity in the patients.",
            "The endogenous exonuclease activity of PLD4 was evaluated in PBMCs from patient P1 and healthy controls. After 1, 2, and 4 hours of incubation, more residual substrate was detected in P1 PBMCs compared with controls, indicating that the endogenous PLD4 exonuclease activity in patient P1 was severely impaired.",
            "qPCR and Western blot analyses showed that knockdown of STING restored type I IFN pathway activation, while NF-κB signaling was only partially recovered. These findings suggest that the cGAS–STING signaling axis plays a critical role in mediating immune dysregulation caused by PLD4 deficiency.",
            "Compared with wild-type mice, Pld4⁻/⁻ mice exhibited marked expansion of immune cell populations. Key cell types implicated in SLE pathogenesis—including pDCs and plasma cells—were significantly increased. Both CD4⁺ effector T cells and CD8⁺ effector T cells were elevated, while age-associated B cells and myeloid DCs (mDCs) remained unchanged. These results indicate that PLD4 deficiency leads to tissue-specific consequences, with renal tissue damage being particularly prominent.",
            "The beneficial effects of baricitinib indicate that the type I IFN pathway is a key mediator of autoimmune and inflammatory phenotypes resulting from PLD4 deficiency."
        ],
        "step_count": 6
    },
    "recuWBdJMtzn62": {
        "reasoning_steps": [
            "Goal — Reduce the continuity equation ∇ν T_m^{μν}=0 in a static, spherically symmetric perfect-fluid background to an explicit radial hydrostatic balance expression.",
            "Perfect-fluid stress–energy (Concept_1) — Use T^{μν}=(ε+P)u^{μ}u^{ν}+P g^{μν}; in the fluid rest frame T^{μ}{}{ν}=diag(−ε,P,P,P).",
            "Metric & mass function (Concept_2) — Take ds^{2}=−e^{2Φ(r)}dt^{2}+e^{2δ(r)}(1−2m(r)/r)^{−1}dr^{2}+r^{2}dΩ^{2}, with the mass function m(r) defined via g_{rr}.",
            "Fluid 4-velocity (Concept_3) — In static equilibrium u^{μ}=(e^{−Φ},0,0,0), so only the gravitational potentials Φ(r) and δ(r) enter the balance.",
            "Covariant conservation → hydrostatic balance (Concept_4) — From ∇_ν T^{μν}=0, the radial component yields P'(r)=−(ε+P)Φ'(r).",
            "Field equations for Φ' (Concept_5) — Einstein’s equations give Φ'(r)=[r m'(r)−m(r)]/[r(r−2m(r))]+δ'(r).",
            "Combine Steps 5 & 6 — Substituting yields the explicit hydrostatic balance: P'(r)=−(ε+P)\\left(\\frac{r m'(r)−m(r)}{r[r−2m(r)]}+δ'(r)\\right).",
            "Use m'(r)=4π r^{2}ε(r) (if expressed in terms of the matter energy density) — Then P'(r)=−(ε+P)\\left(\\frac{4π r^{3}ε−m}{r[r−2m]}+δ'\\right); an equivalent rearrangement is (P+ε)\\left(\\frac{m−r m'}{2 m r−r^{2}}+δ'\\right) with the overall minus sign understood."
        ],
        "step_count": 8
    },
    "recuWFdnj57XXA": {
        "reasoning_steps": [
            "Goal: write, in one sentence, the 2×2 overlap matrix up to and including O(e^2) for rows (O1, O2) and columns (|ρ⟩, |πγ⟩).",
            "Spectral representation (Concept_1): introduce the Euclidean two-point function C_ij(t)=∑n V{i,n} V_{j,n} e^{-E_n t}+… to motivate the role of the overlaps V_{i,n}.",
            "Overlap definition (Concept_2): define V_{i,n}≡⟨0|O_i|n⟩ so each matrix element of the overlap matrix is a vacuum–state matrix element between operator O_i and state |n⟩.",
            "Small-e expansion (Concept_3): expand each overlap as V_{i,n}=∑{k≥0} e^k V^{(k)}{i,n}, making the explicit e-power counting manifest entry-wise.",
            "Truncation to O(e^2) (Concept_4): keep only terms with k≤2 in V_{i,n}=∑k e^k V^{(k)}{i,n} and drop k≥3, so the final one-sentence answer displays explicit e-powers and coefficients V^{(k)}_{i,n} without replacing entries by big-O notation."
        ],
        "step_count": 5
    },
    "recuWHuOJGqcsf": {
        "reasoning_steps": [
            "The instruction requires the formulation of the second-stage objective function, $Y^s$, which quantifies the total \"operational failure cost\" for a specific scenario $s$.",
            "The problem is framed within a two-stage stochastic model. The instruction specifies that the first-stage decision ($\\alpha_{t,r,f}$) is already fixed, and we are now in the second stage, evaluating a single scenario of realized random events (Concept_1).",
            "The core of the \"operational failure cost\" is the penalty associated with delayed rescues (Concept_2).",
            "The instruction states the cost must be based on the \"positive time difference\" between arrival and a deadline. For any single rescue task (fleet $f$ to accident $w$), this time difference is $(\\mu_{w,f}^{s,t} - l_w^s)$. The \"positive\" requirement is mathematically handled by the positive part function, $(x)^+$ (Concept_3), using the specific model variables for arrival time and latest rescue time (Concept_4).",
            "This time delay must be converted into a monetary value. The instruction calls for a \"scenario-specific unit cost,\" which corresponds to the parameter $c_w^s$. Multiplying the time delay by this unit cost gives the penalty for a single event: $c_w^s (\\mu_{w,f}^{s,t} - l_w^s)^+$ (Concept_4).",
            "Finally, the instruction requires this cost to be aggregated \"over the entire set of vehicle fleets, accident sites, and time periods.\" This translates directly to applying a summation over the sets $F$, $W$, and $T$ (Concept_4).",
            "Combining these components yields the complete expression for the total cost. The model's goal is to minimize this cost by optimizing the second-stage decisions (like arrival times), hence the `Min` operator is prefixed to the summation. This results in the final formula: $Y^s = \\text{Min} \\sum_{f \\in F} \\sum_{t \\in T} \\sum_{w \\in W} c_w^s (\\mu_{w,f}^{s,t} - l_w^s)^+$."
        ],
        "step_count": 7
    },
    "recuWKPwEMwWdZ": {
        "reasoning_steps": [
            "Link adjacency matrices to the invariant subspace (Concept 1). From the RDPG framework, each population adjacency matrix satisfies P^{(t)} = U B^{(t)} U^\\top, where U \\in \\mathbb{R}^{n \\times d} is the common invariant subspace. Thus, the column space of P^{(t)} is exactly \\text{span}(U) for all t. The goal reduces to recovering this subspace from the observed \\{A^{(t)}\\}.",
            "Correct structural distortions (Concept 2). Naively using A^{(t)} (A^{(t)})^\\top introduces bias from diagonal dominance and sampling noise. Lei \\& Rinaldo (2015) show that hollowing (removing diagonals) and subtracting diagonal bias yields an unbiased estimator: \\tilde{C}^{(t)} = H(A^{(t)} (A^{(t)})^\\top) - \\hat{M}^{(t)}, with expectation \\mathbb{E}[\\tilde{C}^{(t)}] = U (B^{(t)} B^{(t)\\top}) U^\\top. Averaging across layers suppresses layer-specific noise: \\bar{C} = \\frac{1}{m} \\sum_{t=1}^m \\tilde{C}^{(t)} \\approx U \\Big( \\frac{1}{m} \\sum_{t=1}^m B^{(t)} B^{(t)\\top} \\Big) U^\\top.",
            "Extract U via spectral decomposition (Concept 3). Since \\bar{C} is symmetric and low-rank with column space \\text{span}(U), its top-d eigenvectors span U. Let \\hat{U} = [v_1, v_2, \\dots, v_d], where v_i are the leading eigenvectors of \\bar{C}. By the Davis–Kahan theorem, if \\delta = \\lambda_d - \\lambda_{d+1} > 0 is the eigengap of \\mathbb{E}[\\bar{C}], then \\|\\sin \\Theta(\\hat{U}, U)\\| \\leq \\frac{\\|\\bar{C} - \\mathbb{E}[\\bar{C}]\\|}{\\delta} \\to 0 \\quad \\text{as } n,m \\to \\infty, ensuring that \\hat{U} consistently estimates U."
        ],
        "step_count": 3
    },
    "recuWMOFoF2xXe": {
        "reasoning_steps": [
            "Establish the Foundational Friction Framework with the Linear Slip-Weakening (LSW) Law. The analysis begins by adopting the **Linear Slip-Weakening Friction Law** (Eq. 22: \\(\\mu\\left(S, D_{c}\\right):=\\mu_{s}-\\frac{\\mu_{s}-\\mu_{d}}{D_{c}} min \\left(S, D_{c}\\right)\\)), where \\(D_c\\) (critical slip-weakening distance) is a core parameter. This law defines how fault friction decreases from the static friction coefficient (\\(\\mu_s\\)) to the dynamic friction coefficient (\\(\\mu_d\\)) as slip (\\(S\\)) accumulates, forming the fundamental basis for modeling dynamic rupture behavior and constraining \\(D_c\\).",
            "Generate Long-Term Lithospheric Context via 3D Visco-Plastic Thermo-Mechanical Modeling (pTatin3D). To obtain physically realistic initial conditions for dynamic rupture simulations, the thesis uses **pTatin3D** to perform 3D visco-plastic thermo-mechanical geodynamic modeling. This tool simulates long-term lithospheric deformation by solving three core conservation equations: momentum conservation (Eq. 1), mass conservation (Eq. 2), and thermal energy conservation (Eq. 4). These simulations capture the slow, large-scale processes that shape the fault system’s pre-rupture state.",
            "Extract Critical Initial Conditions from pTatin3D Outputs. The pTatin3D models produce three key outputs required for dynamic rupture modeling: (1) a 3D stress field (which drives rupture initiation and propagation), (2) fault zone rheological properties (which influence friction evolution), and (3) volumetric representations of shear zones (the precursors to discrete fault surfaces). These outputs are essential for grounding dynamic rupture simulations in geophysically plausible conditions.",
            "Refine Fault Geometry Using the Medial Axis Transform (MAT). Accurate fault geometry is a prerequisite for reliable dynamic rupture modeling, as rupture propagates along 2D fault surfaces (not 3D volumes). The thesis applies the **Medial Axis Transform** (Section 3), a geometric method that converts the 3D volumetric shear zones (from pTatin3D) into discrete, high-resolution 2D fault surfaces. This step ensures the dynamic rupture model uses a realistic representation of the fault’s spatial extent and orientation.",
            "Set Up 3D Dynamic Rupture Simulations with SeisSol. Using the refined inputs (pTatin3D-derived stress/rheology and MAT-derived fault geometry), the thesis employs **SeisSol** to run 3D dynamic rupture and seismic wave propagation simulations. SeisSol solves the governing equations for dynamic rupture: momentum conservation (Eq. 17) and the constitutive relationship for fault friction (Eq. 18), which incorporates the LSW law (and thus the \\(D_c\\) parameter).",
            "Design a Parameter-Sweeping Experiment for \\(D_c\\). To constrain \\(D_c\\), the thesis conducts **9 constrained SeisSol simulations** by systematically varying \\(D_c\\) across a range of potential values. Each simulation tests a unique \\(D_c\\) while holding all other parameters (stress, rheology, fault geometry) constant. This controlled experiment isolates the effect of \\(D_c\\) on rupture dynamics, enabling direct comparison of results.",
            "Analyze Rupture Dynamic Metrics to Evaluate Physical Plausibility. For each SeisSol simulation (with a distinct \\(D_c\\)), the thesis analyzes key metrics of rupture behavior to assess physical consistency: - **Fault slip distribution**: Whether slip magnitudes and patterns align with expected behavior for strike-slip faults (e.g., concentrated slip in fault cores, gradual tapering at edges). - **Rupture velocity**: Whether the velocity stays below the shear wave speed (a physical constraint; super-shear velocities are only plausible in specific contexts, which are evaluated here). - **Seismic magnitude**: Whether the simulated magnitude matches typical values for the studied strike-slip fault system (if observational data is available) or falls within geophysically reasonable bounds.",
            "Filter Unphysical Simulations to Retain Plausible \\(D_c\\) Values. Simulations with \\(D_c\\) values that produce unphysical behavior (e.g., unrealistic slip concentrations, super-shear rupture where not justified, or magnitudes outside expected ranges) are discarded. Only simulations where the rupture metrics (slip, velocity, magnitude) are geophysically consistent are retained, narrowing down the set of candidate \\(D_c\\) values.",
            "Consolidate Retained Values to Define the Plausible \\(D_c\\) Range. The final step involves compiling the \\(D_c\\) values from the physically plausible SeisSol simulations. This compilation defines the **plausible range for \\(D_c\\)**—a set of values that, when integrated with the LSW law, geodynamic initial conditions, and refined fault geometry, consistently reproduce realistic strike-slip fault rupture behavior."
        ],
        "step_count": 9
    },
    "recuWPQMdNIolp": {
        "reasoning_steps": [
            "Establish system dynamics equations: Using the concept: Concept 1 (Rate-State Friction Law). Express the frictional force of the fault as: \\tau = \\sigma_n \\big[\\mu_{\\rm ref}+a\\ln(V/V_{\\rm ref})+b\\ln(\\theta V_{\\rm ref}/D_{RS})\\big] and combine it with the state evolution equation \\theta=1-\\theta V/D_{RS} to obtain a dynamic system describing the evolution of slip rate V and state variable \\theta.",
            "Analyze the uniform steady state and its stability. The concept used here is Concept 2 (critical stiffness criterion): first, find the steady-state condition \\(\\theta_{\\rm ss} = D_{RS}/V\\), then linearize the system, obtain the characteristic equation, and solve for the critical stiffness: \\(k_c = \\frac{\\sigma_n (b-a)}{D_{RS}}\\): when \\(k < k_c\\), the uniform steady state becomes unstable, and the system enters the self-excited sliding region.",
            "Determine the type of event sequence: The concept used is Concept 3 (Chaotic Attractor Theory): Low stiffness and velocity-weakening conditions cause the system trajectory to fall into a chaotic attractor, manifesting as an irregular and aperiodic event sequence. Therefore, the event sequence type is determined to be chaotic.",
            "Analyze the evolution trend of time intervals: The concept used is Concept 3 (Chaotic Attractor Theory). On a chaotic attractor, although the event intervals have short-term fluctuations, the long-term statistics will converge to the average recurrence time of the attractor. Therefore, the interval trend is determined to converge to the statistical average value.",
            "Analyze the evolution of the rupture space range: The concept used is Concept 4 (correlation between pre-event state and spatial rupture). By searching for the optimal pre-event state on the attractor, it can be seen that its rupture range gradually approaches the maximum rupturable area of the system, showing a trend of slow spatial expansion. Therefore, the rupture range is determined to expand slowly until it covers the rupturable area."
        ],
        "step_count": 5
    },
    "recuWPFjHL0bXk": {
        "reasoning_steps": [
            "Write the earthquake magnitude distribution (using concept_1): According to the frequency-magnitude relationship by Gutenberg & Richter: \\log_{10} N(M \\ge M_w) = a - b M_w, the conditional cumulative distribution function of magnitude can be obtained: F(M_w \\mid M_c,b)=P(M \\le M_w)=1-10^{-b(M_w-M_c)}, \\quad M_w \\ge M_c",
            "Write out the maximum magnitude distribution (using concept_3): According to the extreme value statistical results by Cornell., if events are independent and identically distributed, the probability that the maximum magnitude is less than the threshold is P(M_{\\max}<M_w \\mid N) = [F(M_w)]^{N}",
            "Write the exceedance probability formula (using concept_4: According to Shapiro's definition of exceedance: P_{\\mathrm{ex}} = 1 - P(M_{\\max}<M_w \\mid N) = 1 - [F(M_w)]^{N}",
            "Analyze the impact of an increase in M_c on F(M_w) (using concept_2): According to Ogata's (1988) definition of the completeness magnitude, increasing M_c is equivalent to shifting the lower limit of the distribution to the right. Under the condition that N remains unchanged, the exponential term F(M_w) = 1 - 10^{-b(M_w - M_c)} in the CDF formula increases as M_c increases, meaning that events are more concentrated in higher magnitudes.",
            "Determine the monotonicity of the exceedance probability and derive the answer: Substitute the result of the increase in F(M_w) into the formula from Step 3: P_{ex} = 1 - [F(M_w)]^N. Since 0 < F(M_w) < 1, its power term [F(M_w)]^N increases as F(M_w) increases, causing 1 - [F(M_w)]^N to increase accordingly. Therefore, the exceedance probability increases as M_c increases."
        ],
        "step_count": 5
    },
    "recuWY8O3pa7iE": {
        "reasoning_steps": [
            "Start with the earthquake catalog data, ensuring it covers a magnitude range where M_min ≤ M ≤ M_max and M_max - M_min ≥ 3 to satisfy the condition for simplifying the probability density function f(M) = b ln(10) 10^{-b (M - M_min)}, as derived from the Gutenberg-Richter law log N(M) = a - b M (concept1 and concept2). This ensures the distribution is suitable for exponential fitting without truncation effects.",
            "Exclude short-term aftershock incompleteness (STAI) periods following major earthquakes to avoid bias in the magnitude-frequency distribution (MFD), using predefined durations like +2 days for the Norcia mainshock and +0.8, +0.6, +0.4 days for others (from Herrmann et al., 2022). Filter the catalog to include only events above potential incompleteness thresholds.",
            "Determine M_min (magnitude of completeness, McLilliefors) using the Lilliefors test, a modification of the Kolmogorov-Smirnov (KS) test to check if magnitudes are exponentially distributed (null hypothesis). Perform the test as a function of candidate M_min values: for each, add random noise to binned magnitudes (ΔM=0.01) to treat them as continuous, run multiple initializations (e.g., many iterations), and compute p-values per magnitude bin. Set significance level to 0.1; McLilliefors is the lowest M_min where p-value > 0.1 for at least five consecutive bins (first exceedance).",
            "Once McLilliefors (M_min) is identified, select all events in the catalog with M ≥ M_min. Compute the sample mean magnitude ⟨M⟩ of these events, which represents the average of magnitudes larger than M_min.",
            "Apply the maximum likelihood estimation (MLE) formula for b (theorem1), accounting for discrete magnitudes: b = 1 / [ln(10) (⟨M⟩ - (M_min - ΔM/2))], where ΔM=0.01 is the fixed binning resolution, introducing the ΔM/2 offset to correct for the discrete nature (Utsu et al., 1966). This yields the b-value estimate.",
            "Validate the result by confirming the MFD follows the Gutenberg-Richter law above M_min and compute 95% confidence intervals using Shi and Bolt (1982) if needed. Repeat for subsets (e.g., lithologies or fault volumes) to ensure robustness, with N events ≥ McLilliefors ideally >1000 for stable estimates."
        ],
        "step_count": 6
    },
    "recuWZ9kPN0LB5": {
        "reasoning_steps": [
            "Define test functions and multiply by the equilibrium equations Let \\((n, p)\\) be a positive solution of the equilibrium system (21): \\[\\left\\{\\begin{array}{l}\\left(\\frac{\\mu}{1+\\rho p}-n-\\frac{(1-\\eta) \\Phi p}{1+\\sigma(1-\\eta) n+\\xi p}-\\delta\\right) n+\\gamma_{1} \\Delta n=0, \\\\ \\left(1-\\frac{\\theta}{(1-\\eta) n+\\nu}\\right) p+\\gamma_{2} \\Delta p=0.\\end{array}\\right.\\] Multiply the first equation of (21) by \\(\\frac{n - \\bar{n}}{n}\\) (where \\(\\bar{n} = \\frac{1}{|\\Omega|}\\int_{\\Omega} n \\, d\\Omega\\) denotes the spatial average of \\(n\\)) and the second equation by \\(\\beta \\frac{p - \\bar{p}}{p}\\) (where \\(\\bar{p} = \\frac{1}{|\\Omega|}\\int_{\\Omega} p \\, d\\Omega\\) is the spatial average of \\(p\\), and \\(\\beta\\) is a positive constant to be determined later). Integrate both resulting equations over the domain \\(\\Omega\\) and sum the two integrals.",
            "Apply the divergence theorem and boundary conditions By the **divergence theorem** and the homogeneous Neumann boundary conditions (\\(\\nabla n \\cdot \\mathbf{n} = 0\\) and \\(\\nabla p \\cdot \\mathbf{n} = 0\\) on \\(\\partial\\Omega \\times \\mathbb{R}^+\\), where \\(\\mathbf{n}\\) is the outward unit normal to \\(\\partial\\Omega\\)), the diffusion terms (involving \\(\\Delta n\\) and \\(\\Delta p\\)) simplify to positive terms involving gradients. The left-hand side (LHS) of the summed integral becomes: \\[\\gamma_{1}\\int _{\\Omega }\\bar{n}\\frac{(\\nabla n)^{2}}{n^{2}}d\\Omega +\\beta \\gamma_{2}\\int _{\\Omega }\\bar{p} \\frac{(\\nabla p)^{2}}{p^{2}}d\\Omega.\\] The right-hand side (RHS) expands into a sum of integrals involving products of \\((n - \\bar{n})\\) and \\((p - \\bar{p})\\), accounting for the reaction terms in (21). These include terms with coefficients like \\(\\mu\\rho\\), \\((1-\\eta)\\Phi\\), \\(\\sigma(1-\\eta)^2\\Phi\\), and \\(\\theta\\beta(1-\\eta)\\), each associated with denominators derived from the original model’s functional response and growth terms.",
            "Use Harnack inequality and Poincaré inequality for lower bounds From **Theorem 3**, the Harnack inequality gives: \\[\\frac{\\max_{\\overline{\\Omega}} n(x)}{\\min_{\\overline{\\Omega}} n(x)} \\leq C_1(\\gamma), \\quad \\frac{\\max_{\\overline{\\Omega}} p(x)}{\\min_{\\overline{\\Omega}} p(x)} \\leq C_2(\\gamma),\\] where \\(C_1(\\gamma), C_2(\\gamma)\\) are positive constants depending on \\(\\gamma = \\min\\{\\gamma_1, \\gamma_2\\}\\) and the model parameters. By the **Poincaré inequality**, for the smallest positive eigenvalue \\(\\alpha_1\\) of the spectral problem (23): \\[\\left\\{\\begin{array}{l}\\Delta \\varphi = -\\alpha \\varphi \\quad \\text{in } \\Omega, \\\\ \\nabla \\varphi \\cdot \\mathbf{n} = 0 \\quad \\text{on } \\partial\\Omega \\times \\mathbb{R}^+,\\end{array}\\right.\\] we obtain lower bounds for the gradient integrals on the LHS: \\[\\gamma_{1} \\int_{\\Omega} \\bar{n} \\frac{(\\nabla n)^{2}}{n^{2}} d\\Omega+\\beta \\gamma_{2} \\int_{\\Omega} \\bar{p} \\frac{(\\nabla p)^{2}}{p^{2}} d\\Omega \\geq \\frac{\\alpha_{1} \\gamma_{1}}{C_{1}(\\gamma)}\\| n-\\bar{n}\\| ^{2}+\\frac{\\alpha_{1} \\beta \\gamma_{2}}{C_{2}(\\gamma)}\\| p-\\bar{p}\\| ^{2}.\\]",
            "Substitute \\(\\beta\\) and rearrange the inequality Choose \\(\\beta = \\frac{2\\nu^2}{\\theta(1-\\eta)}\\) (a value selected to simplify the RHS terms). Using the upper bounds of \\(n\\) and \\(p\\) (\\(\\max_{\\overline{\\Omega}} n \\leq M_1\\), \\(\\max_{\\overline{\\Omega}} p \\leq M_2\\) from Theorem 1), bound the RHS integrals by linear combinations of \\(\\|n - \\bar{n}\\|^2\\) and \\(\\|p - \\bar{p}\\|^2\\). Rearranging the inequality gives: \\[\\frac{\\alpha_{1} \\gamma_{1}}{C_{1}(\\gamma)}\\| n-\\bar{n}\\| ^{2}+\\frac{\\alpha_{1} \\beta \\gamma_{2}}{C_{2}(\\gamma)}\\| p-\\bar{p}\\| ^{2} \\leq \\left[K_1\\right] \\frac{\\|n - \\bar{n}\\|^2}{2} + \\left[K_2\\right] \\frac{\\|p - \\bar{p}\\|^2}{2},\\] where \\(K_1, K_2\\) are constants composed of model parameters (e.g., \\(\\mu\\rho\\), \\((1-\\eta)\\Phi\\), \\(\\sigma(1-\\eta)^2\\Phi M_1\\)).",
            "Derive a contradiction from Theorem 4’s conditions If the conditions of Theorem 4 hold: \\[\\left\\{\\begin{array}{l}\\gamma_{1} \\geq \\frac{C_{1}(\\gamma)}{2 \\alpha_{1}}\\left\\{\\mu \\rho+(1-\\eta) \\Phi+\\sigma(1-\\eta)^{2} \\Phi\\left[M_{1}+M_{2}\\right]\\right\\}, \\\\ \\gamma_{2} \\geq \\frac{C_{2}(\\gamma) \\theta(1-\\eta)}{4 \\mu^{2} \\alpha_{1}}\\left\\{\\mu \\rho+(1-\\eta) \\Phi+\\sigma(1-\\eta)^{2} \\Phi M_{1}+1\\right\\},\\end{array}\\right.\\] the LHS of the inequality in Step 4 becomes strictly larger than the RHS. This is a contradiction because the LHS is a sum of non-negative gradient terms, while the RHS cannot exceed the LHS under the given conditions. Thus, no positive non-constant solution \\((n, p)\\) to system (21) exists."
        ],
        "step_count": 5
    },
    "recuX1cNCRcOnW": {
        "reasoning_steps": [
            "Substitute plane wave into the stabilized equations: Laplacian to k^2, hence Δ^α → k^{2α}.",
            "Each equation’s added γ term contributes γ k^{2α} times the field amplitude.",
            "Form the coupled algebraic system; eliminating a field yields a quadratic in ω with the unperturbed solution ω = c0 k.",
            "Treat γ k^{2α} as small: first-order perturbation ⇒ δω ∝ γ k^{2α}.",
            "Convert to phase speed error: Δc = δω / k ∝ γ k^{2α−1}.",
            "Insert α=2: Δc ∝ γ k^{3}.",
            "Replace γ by c h^{4}; omit constants and c ⇒ Δc = O(h^{4} k^{3})."
        ],
        "step_count": 7
    },
    "recuX1u2XnFJW0": {
        "reasoning_steps": [
            "Using: Concept 1 + Concept 2. Local stoichiometry shows that the RP plane is equivalent to the A₂BX₄ termination layer. The RP defect plane has a local composition of Cs₂PbI₄ (A₂BX₄), meaning that the B–X corner-sharing connections across this plane are broken. Both sides retain their respective \"surface-like\" termination layers, with only a weakly interacting gap in between. This is completely isomorphic to placing two free surfaces side by side inside a crystal. Fingerprint of surface-type structural relaxation: octahedral tilting is weakened. On free surfaces, due to the decrease in coordination number and reduced constraints, a relaxation mode where the octahedral tilt angle shrinks/tends to zero is common. A local decrease in the tilt angle is observed adjacent to the RP defect, which is consistent with the characteristics of surface relaxation. The structural response supports the idea that \"defect gap = two opposing embedded free surfaces\". In terms of both stoichiometry (A₂BX₄ termination) and structural relaxation (tilting suppression), the RP gap is equivalent to the free surface in terms of microscopic configuration.",
            "Use: Concept 3 + Concept 4. The strain energy minimization path is \"opening a slit into a surface\" rather than point/line defects. The tensile strain and shear strain at the (001)/(110) grain boundaries can be released by forming interstitials/stacking faults; this is equivalent to generating two surfaces with low interaction, which saves more energy than retaining high-energy mismatches in the bulk. Mechanics drive the selection of the \"surface-like defect interstitial\" morphology. No deep energy levels = surface states rather than bulk defect states. DFT results show that RP defects do not introduce significant deep energy levels, being closer to clean or weakly reconstructed surface states (which may only change band edge degeneracy/slight band bending). The electronic signature is consistent with a free surface, rather than typical bulk defects (the latter commonly have deep traps). Intermediate conclusion B: From the perspective of **energetics (strain release) and electronic structure (no deep traps)**, the most stable and observation-consistent explanation for RP interstitials is also \"free surface pairs\"."
        ],
        "step_count": 2
    },
    "recuXaAsJbdUGy": {
        "reasoning_steps": [
            "Expand \\mathbb{E}[u_k z_{k+i+1}] via conditional expectation and independence Since \\{u_k\\} is independent of \\{s_k\\}, we use Lemmas 2.1 and 2.2 to decompose the expectation via conditional expectation: \\begin{align*} \\mathbb{E}[u_k z_{k+i+1}] &= \\mathbb{E}\\left[\\mathbb{E}[u_k z_{k+i+1} \\mid y_{k+i+1}, s_{k+i+1}]\\right] \\\\ &= \\mathbb{E}\\left[z_{k+i+1} \\mathbb{E}[u_k \\mid y_{k+i+1}, s_{k+i+1}]\\right] \\\\ &= \\mathbb{E}\\left[z_{k+i+1} \\mathbb{E}[u_k \\mid y_{k+i+1}]\\right] \\\\ &= \\frac{h_i}{\\sigma_{y,k+i+1}^2} \\mathbb{E}\\left[z_{k+i+1} y_{k+i+1}\\right] \\\\ &= \\frac{h_i}{\\sigma_{y,k+i+1}^2} \\mathbb{E}\\left[y_{k+i+1} I_{[y_{k+i+1} \\geq z_{k+i+1}]}\\right], \\end{align*} where I_{[\\cdot]} is the indicator function.",
            "Compute \\mathbb{E}[y_{k+i+1} I_{[y_{k+i+1} \\geq z_{k+i+1}]} using probability density functions (pdfs) Let \\varphi_k(\\cdot) denote the pdf of y_k for k \\geq 1, and let \\varphi_0(\\cdot) denote the pdf of the standard normal distribution. Since y_{k+i+1} and s_{k+i+1} are independent, we evaluate the expectation as a double integral. Using the substitution y = \\sigma_{y,k+i+1} x (to normalize y_{k+i+1} to a standard normal variable): \\begin{align*} \\mathbb{E}\\left[y_{k+i+1} I_{[y_{k+i+1} \\geq z_{k+i+1}]}\\right] &= \\frac{1}{b} \\int_0^b \\int_s^\\infty y \\varphi_{k+i+1}(y) \\, dy \\, ds \\\\ &= \\frac{1}{b} \\int_0^b \\int_{\\frac{s}{\\sigma_{y,k+i+1}}}^\\infty \\sigma_{y,k+i+1} x \\varphi_0(x) \\, dx \\, ds. \\end{align*}",
            "Combine results and take the limit as k \\to \\infty Substitute the integral from Step 2 into the expression for \\mathbb{E}[u_k z_{k+i+1}] from Step 1: \\mathbb{E}[u_k z_{k+i+1}] = \\frac{1}{b} \\cdot \\frac{h_i}{\\sigma_{y,k+i+1}} \\int_0^b \\int_{\\frac{s}{\\sigma_{y,k+i+1}}}^\\infty x \\varphi_0(x) \\, dx \\, ds. As k \\to \\infty, \\sigma_{y,k+i+1} \\to \\sigma_y (converging to the scale of the standard normal distribution). Thus: \\mathbb{E}[u_k z_{k+i+1}] \\xrightarrow[k \\to \\infty]{} \\frac{h_i}{b \\sigma_y} \\int_0^b \\int_{\\frac{s}{\\sigma_y}}^\\infty x \\varphi_0(x) \\, dx \\, ds, \\quad \\forall i \\geq 0.",
            "Evaluate the integral using the standard normal distribution function \\Phi_0 Recall \\Phi_0(\\cdot) is the standard normal distribution function. Evaluate the integral \\frac{1}{\\sigma_y} \\int_0^b \\int_{\\frac{s}{\\sigma_y}}^\\infty x \\varphi_0(x) \\, dx \\, ds: \\begin{align*} \\frac{1}{\\sigma_y} \\int_0^b \\int_{\\frac{s}{\\sigma_y}}^\\infty x \\varphi_0(x) \\, dx \\, ds &= \\int_0^b \\frac{1}{\\sqrt{2\\pi} \\sigma_y} e^{-\\frac{s^2}{2\\sigma_y^2}} \\, ds \\\\ &= \\Phi_0\\left(\\frac{b}{\\sigma_y}\\right) - \\frac{1}{2}. \\end{align*} By definition, \\rho \\triangleq \\frac{1}{b} \\left( \\Phi_0\\left( \\frac{b}{\\sigma_y} \\right) - \\frac{1}{2} \\right) > 0. Substituting this into the limit from Step 3 gives: \\mathbb{E}[u_k z_{k+i+1}] \\xrightarrow[k \\to \\infty]{} \\rho h_i, \\quad \\forall i \\geq 0, completing the proof."
        ],
        "step_count": 4
    },
    "recuViPtBcNeiY": {
        "reasoning_steps": [
            "In two PDAC cell lines, a whole-genome CRISPR-Cas9 screen was conducted, with changes in cell survival after gene knockout as the analysis metric. It was found that A3C and A3D were the two strongest \"sensitizing genes.\" Their knockout significantly enhanced gemcitabine toxicity to cells, with no such effect observed in untransformed cells. This suggests that PDAC cells are specifically sensitive to the loss of A3C or A3D, while untransformed cells do not exhibit this vulnerability.",
            "Nab-paclitaxel made PDAC cells with A3C and A3D knockouts more sensitive to treatment, and combining nab-paclitaxel with gemcitabine further increased cell death. This suggests that A3C and A3D contribute to the development of resistance to the current standard treatment for PDAC (gemcitabine-paclitaxel).",
            "Analysis of APOBEC3 family gene expression revealed a stronger association between A3C and A3D, with the expression of A3B requiring A3C or A3D. This suggests that A3B, A3C, and A3D may belong to the same expression regulatory network.",
            "Treatment of PDAC cells with immune factors showed that gemcitabine could trigger an innate immune response, but the induction effect on A3C and A3D was weak. This indicates that the induction of A3C and A3D mRNA expression by gemcitabine is coordinated, both dependent on and independent of the innate immune response. It is stronger in PDAC cells compared to untransformed cells and generally specific to gemcitabine.",
            "Replication stress causes DNA release into the cytoplasm. In A3C and A3D knockout cells, no change in cytoplasmic ssDNA content was observed after gemcitabine treatment, while in wild-type cells, cytoplasmic ssDNA levels increased. This suggests that A3C and A3D do not participate in clearing cytoplasmic ssDNA. Furthermore, there were no significant differences in immune response activation between wild-type and A3C, A3D-deficient cells, suggesting that A3C and A3D do not suppress innate immune signaling by removing cytoplasmic DNA to protect PDAC cells from gemcitabine.",
            "A3A has high activity on gemcitabine bases in ssDNA, while A3C and A3D show no significant activity on dFdC. Inactivation of A3C or A3D does not alter the levels of deoxycytidine and deoxyuridine metabolites. This suggests that although A3C and A3D contribute to gemcitabine resistance, A3A’s transcriptional activation of gemcitabine is not linked to resistance. However, gemcitabine nucleosides incorporated into the genome are not extensively deaminated by A3C or A3D.",
            "In A3C and A3D-deficient PDAC cells, the number of cells in S-phase was reduced, and compared to parental cells, DNA synthesis was decreased, replication fork speed was slower, and the proportion of stalled forks was higher. This indicates that A3C and A3D play a critical role in maintaining DNA synthesis under gemcitabine treatment conditions.",
            "Gemcitabine not only directly blocks DNA replication forks but also depletes dNTPs by inhibiting ribonucleotide reductase. However, in A3C and A3D knockout cells, no significant effect on intracellular dCTP levels was observed. This suggests that ssDNA exposed by gemcitabine-induced replication stress provides substrates for A3C and A3D cytidine deamination. Cytidine deamination facilitates DNA synthesis and the recovery of gemcitabine-stalled replication forks, thereby improving PDAC cell survival.",
            "After gemcitabine treatment, UNG-deficient cells were more sensitive, indicating that deaminated products are removed by UNG, resulting in abasic sites. However, knockout of other BER pathway genes did not increase gemcitabine sensitivity, suggesting that abasic sites are not repaired through the classical BER pathway but instead recruit other mechanisms as a signaling process.",
            "After gemcitabine treatment, RAD51 foci increased in S-phase cells. When A3C and A3D were knocked out, RAD51 foci were reduced. Using DNA double-strand break inducers (such as mitomycin C) did not affect RAD51 foci formation in A3C or A3D-deficient cells. This suggests that RAD51 is specifically recruited to the abasic sites generated by deamination and is not dependent on DNA double-strand breaks, but instead helps restart replication through fork reversal mechanisms.",
            "In CRISPR screens, POLH knockout caused hypersensitivity to gemcitabine. Replication fork restart experiments in POLH-deficient cells showed increased stalled fork proportions, similar to A3C and A3D knockouts. Using TLS inhibitors such as JH-RE-06 also led to replication fork restart failure. This suggests that abasic sites recruit POLH-mediated TLS, bypassing the gemcitabine-incorporated DNA strand and completing replication fork restart."
        ],
        "step_count": 11
    },
    "recuVeRamrdQcG": {
        "reasoning_steps": [
            "Per concept_6, the total degree of any vertex in $Z$ is the sum of its intra-cloud degree (edges within its cloud) and its inter-cloud degree (edges to other clouds).",
            "According to concept_1 and concept_3, the cloud $C_u$ is a copy of graph $H$, and its internal connections are inherited from $H$. Since $H$ is a $k$-regular graph (concept_5), the intra-cloud degree of vertex $(u, a)$ is exactly $k$.",
            "The revised concept_4 explicitly states that the number of inter-cloud edges for any vertex $(u,a)$ is equal to the degree of graph $H$.",
            "Since $H$ is a $k$-regular graph (concept_5), its degree is $k$. Therefore, the inter-cloud degree of vertex $(u, a)$ is also $k$.",
            "Finally, we sum the two components to find the total degree: Total Degree = (Intra-cloud Degree) + (Inter-cloud Degree) = $k + k = 2k$."
        ],
        "step_count": 5
    },
    "recuVRLVbnLj4r": {
        "reasoning_steps": [
            "When tracking pulse-labeled fluorescent HaloTag (Janelia Fluor (JF) 549) with a non-fluorescent blocker, the level of nuclear MCM4-Halo gradually decreases, but remains stable during continuous labeling. This phenomenon is also observed when immunoblotting with HaloTag ligands or MCM4 antibodies. This indicates that newly synthesized MCM proteins replenish the MCM pool.",
            "By recycling the parental subunits and isolating the newly synthesized subunits from chromatin, the mother cell passes on two distinct MCM pools to its daughter cells: both pools have the potential to initiate replication at origins, but differ in characteristics due to their previous involvement in pre-replication complexes (pre-RC).",
            "Compared to parental MCM, daughter cells have twice as much newly synthesized MCM; however, the ratio of CMG formed by newly synthesized MCM and parental MCM is 1:1. This indicates that the efficiency of parental MCM converting to active replisomes is twice that of daughter MCM. Similar results were obtained by inhibiting ATR to enhance replication origin activation, suggesting that even with enhanced DNA replication, parental MCM is more likely to form active CMG than newly synthesized MCM.",
            "In soluble components, the interaction between MCM subunits and MCMBP is very prominent. This suggests that MCMBP regulates a specific MCM pool distinct from the pre-RC or active CMG pools.",
            "In the absence of MCMBP, the reduction in newly synthesized MCM levels weakens the formation of pre-RCs in daughter cells. This highlights the role of MCMBP in maintaining the flow of newly synthesized MCM subunits into the entire potentially active MCM pool.",
            "By disrupting the Walker B motif required for MCMBP-MCM binding, mislocalization of MCM3-7 to the cytoplasm was observed. This suggests that MCMBP-mediated nuclear translocation of MCM3-7 depends on direct physical interaction.",
            "Compared to MCMBP gene knockout cells, MCM3-7 in the cytoplasm remains stable in cells reconstituted with NLS-deficient MCMBP mutants (MCMBP(ΔNLS)). However, the stability of MCM2 is affected in cells containing MCMBP(ΔNLS). This suggests that the barrier to the nuclear translocation of newly synthesized MCM3-7 disrupts the stability of unused MCM2.",
            "In cells lacking MCMBP or with MCMBP(ΔNLS), the number of licensed pre-RCs is significantly reduced. On one hand, this does not affect the regulation of active replisome factors binding to chromatin, such as PCNA, but on the other hand, it increases the incidence of micronuclei and reduces colony formation.",
            "When cells are treated with HU or Claspin deletion to forcibly enhance the activity of silent replication origins, LFD (local fork density) is increased in both wild-type and MCMBP knockout cells. This suggests that even though the number of pre-RCs is significantly reduced, MCMBP knockout cells still retain enough \"backup\" origins that can be converted into CMG.",
            "In wild-type and MCMBP knockout cells, altering replisome activity by depleting TIMELESS does not affect LFD. This indicates that replication initiation remains largely unchanged under conditions of severely reduced newly synthesized MCM.",
            "Combining partial CDC6 degradation with MCMBP degradation does not produce a cumulative effect, suggesting that pre-RC chromatin occupancy negatively affects replication fork speed.",
            "Slow-moving replication forks caused by excessive replication origin activation after CHK1 inhibition remain sensitive to the acceleration of replication forks due to MCMBP deficiency, while chromatin-bound TIMELESS levels remain unchanged. This suggests that the abnormally fast replication forks observed in the absence of MCMBP are due to reduced chromatin-bound MCMs and are independent of active replisomes."
        ],
        "step_count": 12
    },
    "recuW7NMoIjU5l": {
        "reasoning_steps": [
            "Start from the standard defect formation energy formula ΔH_f = ΔE_tot + Σ_i n_i μ_i + q(E_VBM + E_F). Note that ΔE_tot and E_VBM shift with supercell size L due to spurious interactions.",
            "Reformulate formation energy with a common reference potential: by subtracting the unphysical interaction energy E_int^q, the jellium reference shifts from V̄_D to V̄_B. The qΔV̄ alignment term then vanishes automatically.",
            "Write down the MP multipole expansion for the unphysical interaction energy: (E_{int}^q(L) = -\\frac{q\\alpha}{L}\\tilde q + \\frac{4\\pi q}{3L^3}\\tilde Q_{loc} + O(L^{-5})), where (\\tilde q) and (\\tilde Q_{loc}) are the screened monopole and quadrupole moments. The dielectric constant ε enters later via linear response, not in the original MP form.",
            "Define the charge density difference (Eq. 14): (\\Delta\\rho(r)= -\\Delta\\eta(r) + \\Delta q_I\\delta(r) - \\frac{q}{\\Omega}), where (\\Delta\\eta=\\eta_D-\\eta_B), Δq_I is the ionic charge difference, and (q/\\Omega) is the compensating jellium. Before screening (\\Delta\\rho_{bs}=\\rho_c(r)-\\frac{q}{\\Omega}); after linear response (\\Delta\\rho=\\frac{1}{\\varepsilon}(\\rho_c-\\frac{q}{\\Omega})).",
            "From Δρ and Δη=η_D-η_B, derive the screened quadrupole: \\tilde Q_{loc} = -∫_Ω r^2 Δη(r) d^3r - (q/Ω) ∫_Ω r^2 (1 - 1/ε) d^3r. This leads to the quadrupole correction E_corr^(3) = (4π q / 3 ε L^3) \\tilde Q_{loc}.",
            "For the challenging case V²⁺ in diamond, using ε_bulk leaves residual errors; DFPT shows ε(L) strongly depends on L. Using ε(L) reduces even 64-atom cells to within 0.09 eV of convergence.",
            "The defect contribution to dielectric constant scales as ε_D ∝ 1/Ω (Eq. 17). For V²⁺ in diamond, ΔE_D,p ≈ E_D – E_VBM ∝ 1/L² controls magnitude, giving overall ε(L) ≈ ε_bulk + C/L.",
            "Summarize: the correct workflow is (i) adopt a common reference energy; (ii) subtract MP monopole and quadrupole corrections; (iii) use ε(L) where necessary. Potential alignment correction is thus unnecessary."
        ],
        "step_count": 8
    },
    "recuWgjsDOM6aH": {
        "reasoning_steps": [
            "The primary goal is to ensure the asymptotic consensus of the networked hyperbolic system. This is defined as all agent states converging to an unknown, agreed-upon state over time and space. We are considering a specific case where the communication topology is a fixed undirected and connected graph.",
            "The document establishes that the asymptotic consensus of the original system is equivalent to the asymptotic stability of a derived error system. The analysis for this stability is performed using the Lyapunov approach (Theorem 1), which involves constructing a Lyapunov function and analyzing its time derivative.",
            "The theoretical foundation for achieving consensus under this fixed, undirected topology is provided in Lemma 3 (Theorem 3). It states that consensus is achieved if a set of low-dimensional matrix inequalities holds for all non-zero Laplacian eigenvalues $\\lambda_i$ (where $i=2, \\dots, N$).",
            "Recognizing that checking a separate inequality for each non-zero eigenvalue becomes difficult for a large number of agents ($N$), the document introduces a single, more tractable condition. This is the modified algebraic Riccati inequality, which is given as $A^TQA-Q-(1-\\alpha^2)A^TQB(B^TQB)^{-1}B^TQA<0$. This single inequality is designed to guarantee that the condition in Theorem 3 is satisfied for all relevant eigenvalues.",
            "The connection between the two inequalities is established in Theorem 1. For the modified algebraic Riccati inequality to guarantee the set of conditions in Lemma 3, it's necessary to select the parameter $\\alpha$ and the gain $\\mu$ such that $\\lambda_i^2\\mu^2-2\\lambda_i\\mu \\le -(1-\\alpha^2)$ for all $i=2, \\dots, N$.",
            "The paper states that for this to be true, the parameter $\\alpha$ must be chosen to satisfy a specific range related to the eigenvalues of the Laplacian matrix. This condition is $\\frac{\\lambda_N - \\lambda_2}{\\lambda_N + \\lambda_2} \\le \\alpha < 1$. The lower bound ensures a non-empty range for the gain $\\mu$ and, consequently, the satisfaction of the conditions for all non-zero eigenvalues. The upper bound of $\\alpha < 1$ is required for the inequality to hold and for consensus to be achieved."
        ],
        "step_count": 6
    },
    "recuW7nNtQVHYh": {
        "reasoning_steps": [
            "Model & solvability. Adopt the integrable 1D Hubbard lattice with hopping t and on-site interaction U. Use Bethe-ansatz quantization for N spin-up fermions plus one spin-down impurity: sin k_j − Λ = u cot(k_j L/2), with u = U/(4t), total momentum Q = (∑_{j=1}^{N+1} k_j) mod 2π, and classify regular states into real-k and k−Λ branches.",
            "Construct exact many-body wavefunctions. Write the normalized regular Bethe state in Edwards’ determinant form (Eq. (2)) with single-particle χ_j(y), enabling evaluation for both real-k and k−Λ solutions.",
            "Compute overlaps (form factors) with the noninteracting bath × impurity plane wave. Use the Slater-determinant identity to get the (N+1)×(N+1) determinant expression for the form factor F{N+1} and its normalization C_Ψ; define the residue Z_{N+1}=|F_{N+1}|^2.",
            "Assemble the spectral function. Sum over all (regular + irregular) many-body states with a small broadening δ ≡ 4t/L: A(Q,ω)=−(1/π) Im ∑ Z_{N+1}/(ω−E_{N+1}+E_{FS,N}+iδ). This gives a numerically exact, finite-L spectrum.",
            "Include irregular states to ensure completeness and capture dominant residues at large Q. Construct the spin-flip (ζ†) and η-pairing (η†) states explicitly; their energies relative to the Fermi sea are E_ζ−E_FS=−2t cos Q and E_η−E_FS=−2t cos Q+U, with overlaps F_ζ=1/√(N+1), F_η=1/√(L−N+1). Enforce the sum rule ρ_s=F_ζ^2+F_η^2+∑ Z_{N+1}=1 as a check.",
            "Diagnose low-momentum physics (baseline). At Q≈0 the spectral function exhibits conventional Fermi-edge singularities at two thresholds, as expected in 1D. The bare impurity dispersion ε_Q=−2t cos Q benchmarks one threshold location.",
            "Predict critical-exponent behavior from phase shifts. Use the known 1D relation α = 1 − 2(δ′_F/π)^2, with δ′_F = −arctan(π U n_F/2), n_F = 1/(2π t sin k_F), k_F = ν π. For U=4t and ν=0.5 the analytic α=7/8≈0.875 matches finite-size fits, validating the singularity identification.",
            "Increase momentum and track residues. For Q > k_F the largest residues shift from the lowest-energy state to irregular branches, turning the single-sided Fermi-edge singularity into a two-sided one; at Q=π the high-energy side vanishes because the spin-flip or η state carries the largest residue, leaving an anomalous low-energy–tailed singularity.",
            "Identify a distinct broad peak at large Q ≈ π and ν≈1/2. Inspect residues: a bundle of many-body states around ω≈3t with small but numerous residues Z≈10^{-3} produces a visible, finite peak—this is the polaron quasiparticle (collective origin), distinct from any power-law singularity. These polaron states are characterized by nearly zero quasi-momentum Λ, indicating a symmetric distribution in the quantum numbers {s_j}.",
            "Disentangle singularities vs polaron via scaling. Perform finite-size scaling with δ=4t/L: A_max ∝ δ^{−α} (0<α<1) for singularities, but A_max ≈ const (α≈0) for a genuine polaron peak. Data give α≈0.50 and 0.89 for the first/third peaks (singularities) and a much smaller α≈0.31→0.17 as δ→0 for the middle polaron peak at ω≈3t, confirming its quasiparticle character. The small but non-zero α for the polaron peak is due to finite-size overlap with singularities, and it approaches zero as δ→0.",
            "Locate the coexistence window and its mechanism. Coexistence occurs near quarter filling ν=1/2 when 2k_F≈π≈Q favors backward scattering; it is absent in dilute ν=0.2 (no polaron clustering) and suppressed near half filling ν=0.8 (residues too small). The picture is robust for attractive U with an overall energy shift.",
            "Synthesize the spectral signatures. Combining steps 6–11 yields the answer: at large Q and ν≈1/2, A(Q,ω) shows (i) anomalous Fermi singularities with low-energy oriented power-law tails and (ii) a finite, broad polaron peak at ω~3t from collective many-body weight—two features that coexist in 1D lattices but lack analogues in 2D/3D."
        ],
        "step_count": 12
    },
    "recuWsqhrJCG2F": {
        "reasoning_steps": [
            "The number of γH2A.X foci significantly increases in cells after RAD21 depletion, indicating that RAD21 deficiency leads to extensive DNA damage in human K562 cells.",
            "Compared to WT cells, the incidence of chromosomal breaks is higher (about three times) after RAD21 gene depletion, suggesting that RAD21 also maintains genome stability in mES cells.",
            "RAD21 deletion leads to an increase in genome-wide DSBs, and ectopic expression of wild-type RAD21 restores the RAD21 deletion phenotype. This indicates that complete or partial loss of RAD21 causes DNA damage at the genomic level.",
            "In these five cancers (endometrial cancer, melanoma, colorectal cancer, non-small cell lung cancer, and glioma), 77.6%-98.3% of hotspot genes have a higher mutation frequency when cohesin mutations are present than when they are absent. This suggests that RAD21 deletion promotes recurrent DNA damage in cancer-related genes.",
            "In the absence of RAD21, the translocation frequency around the loop anchor sites remains unchanged, suggesting that changes in loop boundary stability may not be substantively related to DSB formation after cohesin protein dysfunction.",
            "RAD21 deletion only induces transcriptional changes in a limited number of genes (1.0% of the total genes), and there is no correlation with the translocation hotspots observed after RAD21 deletion.",
            "The association between DSBs and translocation junctions with OK-seq signals after RAD21 deletion indicates that the DSBs caused by RAD21 depletion may result from DNA replication. RAD21 depletion leads to a significant reduction in replication speed and an increased frequency of replication fork stalling, suggesting that RAD21 removal may cause replication stress, thereby inducing DNA damage.",
            "After RAD21 depletion, more genomic regions replicate earlier in S-phase, indicating that the genomic instability induced by RAD21 loss may be due to the additional early replication. Replication initiation factors ORC, MCM, and H2A.Z are enriched at new ERIZs in RAD21-depleted cells, and a sharp conversion of OK-seq signals is detected at the centers of new ERIZs, indicating replication initiation. This suggests that new ERIZs display the typical features of DNA replication initiation.",
            "After RAD21 degradation, the replication time of genomic regions containing increased and new ERIZs is significantly advanced, indicating that in RAD21-depleted cells, new ERIZs originate from the premature activation of dormant replication origins.",
            "Rad21-depleted mES cells start replication in more regions than WT cells, indicating that RAD21 suppresses the premature use of dormant replication origins to maintain replication timing.",
            "In CTCF-deleted cells, the replication signal profile on the identified ERIZs is similar to that in WT cells, indicating that CTCF is dispensable for suppressing the activation of dormant replication origins. Cohesin, independent of CTCF's loop extrusion function, may be involved in coordinating DNA replication initiation.",
            "Compared to WT cells, in cells where cohesin is removed, MCM accumulates more in the internal regions of looped domains, suggesting that cohesin may suppress the abnormal activation of dormant replication origins at a distance from loop boundaries."
        ],
        "step_count": 12
    },
    "recuWsPrBESIAx": {
        "reasoning_steps": [
            "Goal: In an MD (matter-dominated) Universe, compute the late-time limit x ≫ 1 of the kernel I_MD^2(u, v, x) for scalar-induced gravitational waves.",
            "Definition (theorem_1): Introduce the kernel I(u,v,x) ≡ ∫_0^x [a(η̄)/a(η)] k G_k(η,η̄) f(u,v,kη̄) d(kη̄), with x ≡ kη, where G_k is the Green’s function of the tensor mode and f is the source kernel built from the Bardeen-potential transfer function.",
            "MD background (concept_1): For w = 0, the scale factor is a(η) ∝ η^2, hence 𝓗 ≡ a'/a = 2/η and a''/a = 2/η^2.",
            "Tensor Green’s equation: The tensor mode satisfies G_k'' + (k^2 − a''/a) G_k = δ(η − η̄); using Step_3 gives G_k'' + (k^2 − 2/η^2) G_k = δ(η − η̄).",
            "Transfer function and source (concept_2): The Bardeen-potential transfer function obeys T_φ'' + (6/η) T_φ' = 0, whose normalized, regular solution is T_φ = 1; therefore the MD source kernel is a constant f_MD(u, v, kη̄) = 6/5.",
            "Time integral with constant source: Insert f_MD = 6/5 and the standard Green’s function for G_k into the definition in Step_2, perform the η̄-integration to obtain I_MD(u,v,x) = (6/5) · [x^3 + 3x cos x − 3 sin x] / x^3. Because f_MD is constant, this result is independent of u and v.",
            "Late-time limit: As x → ∞, I_MD(x) tends to the constant 6/5 and does not oscillate at late times in MD.",
            "Answer: Therefore I_MD^2(u,v,x ≫ 1) = (6/5)^2 = 36/25."
        ],
        "step_count": 8
    },
    "recuWtlfNPnBB3": {
        "reasoning_steps": [
            "By theorem_1 and concept_1, varying the $f(R)$ action with respect to $g^{\\mu u}$ yields the $f(R)$ field equations.",
            "Move the purely geometric (non-matter) terms to the right-hand side and define the curvature-induced effective energy--momentum tensor $T^{\\,f(R)\\,\\mu}{}_{ u}$ as in concept_2. This lets us treat the extra curvature degrees of freedom as an “effective fluid”.",
            "Choose a flat FLRW background, use conformal time $\\eta$, set $\\mathcal H \\equiv a'/a$ and $F\\equiv df/dR$ (prime $' \\equiv d/d\\eta$), and use the standard FLRW background relations for $R^{0}{}_{0}$, $R$, and $(\\delta^{0}{}_{0}\\Box-\\nabla^{0}\\nabla_{0})F$ given in concept_2.",
            "Substitute those background relations into the definition of $T^{\\,f(R)\\,\\mu}{}_{\\nu}$, keep only the $0$–$0$ component, and then define the effective-fluid energy density by $\\tilde{\\rho}_{f(R)} \\equiv -\\,T^{\\,f(R)\\,0}{}_{0}/(8\\pi G)$.",
            "Collect the $\\mathcal H^2$, $F\\,\\mathcal H'$, $\\mathcal H F'$, and $f$ terms to obtain the final one-line expression."
        ],
        "step_count": 5
    },
    "recuWuZLzNBpJ1": {
        "reasoning_steps": [
            "Goal: express the mass m^2 of the new degree of freedom in terms of f(R) and its derivatives, adopting the Klein–Gordon sign convention (\\Box - m^2)\\,\\delta\\phi=0, and give the result in one sentence.",
            "Scalar–tensor mapping (Concept_1): introduce an auxiliary field \\chi, define \\phi \\equiv f_R(\\chi) and V(\\phi)=\\chi(\\phi)\\,\\phi - f(\\chi(\\phi)); with \\omega=0 this is dynamically equivalent to a Brans–Dicke theory for the scalar \\phi.",
            "Trace equation (Concept_2): vary the action and take the trace to obtain 3\\,\\Box f_R + f_R R - 2f = 8\\pi G\\,T; in vacuum or about a fixed background set T=0 and evaluate near R_0.",
            "Linearization (Concept_3): expand R=R_0+\\delta R and \\phi=\\phi_0+\\delta\\phi with the identity \\delta\\phi = f_{RR}(R_0)\\,\\delta R (since \\phi=f_R), then keep terms linear in perturbations in the traced field equation.",
            "Klein–Gordon identification: cast the linearized trace equation into (\\Box - m^2)\\,\\delta\\phi=0 under the stated sign convention, which fixes the mass as m^2 = (f_R - R f_{RR})/(3 f_{RR}) evaluated at R_0.",
            "One-sentence answer: \\(\boxed{\\,m^{2}=\\dfrac{f_{R}-R f_{RR}}{3 f_{RR}}\\,}\\) (evaluated at the background R_0)."
        ],
        "step_count": 6
    },
    "recuWCay6yq5mw": {
        "reasoning_steps": [
            "1: Clarify the System and Partition Function Basics Identify system properties: This model is a 1D ring road traffic flow system, mathematically equivalent to the 1D Ising model (Concept 1). It contains \\(N\\) lattice sites corresponding to vehicle positions, where each site has a state \\(S_i = \\pm 1\\) — \\(S_i = +1\\) represents an occupied site (with a vehicle), and \\(S_i = -1\\) represents an empty site. The periodic boundary condition \\(S_{N+1} = S_1\\) holds (since it is a ring road).  Confirm the form of the partition function: The original fine-grained partition function is given by  \\(Z = \\sum_S \\prod_{i \\in N} \\exp\\left\\{ K S_i S_{i+1} + B S_i \\right\\}\\),  where \\(K\\) is the interaction coefficient between adjacent lattice sites (describing the correlation between neighboring vehicles), and \\(B\\) is the field coefficient biasing forward movement (favoring occupied or empty states).",
            "2: Define the Goal and Core Operation of Renormalization Core operation: Perform the renormalization group (RG) transformation specified — **decimate all even-numbered sites**. This means summing over the states of all even-indexed spins (\\(S_2, S_4, \\dots, S_N\\)) and only retaining the odd-indexed spins (\\(S_1, S_3, \\dots, S_{N-1}\\)), reducing the system’s degrees of freedom from \\(N\\) to \\(N/2\\) (assuming \\(N\\) is even).  Key principle: The coarse-grained partition function \\(Z'\\) after decimation must be **equivalent to the original partition function** (they describe the same physical system). Thus, \\(Z'\\) must maintain the same functional form as \\(Z\\), i.e.,  \\(Z' = \\sum_{\\{S_{\\text{odd}}\\}} \\prod_{j} \\exp\\left( K' S_{2j-1} S_{2j+1} + B' S_{2j-1} \\right)\\),  where \\(K'\\) is the renormalized interaction coefficient (between non-adjacent original odd sites, now adjacent in the coarse-grained system) and \\(B'\\) is the renormalized field coefficient. A spin-independent constant factor \\(f(K,B)\\) (from the partial sum over even spins) is allowed in the equivalence.",
            "3: Extract Key Matching Conditions (Based on Neighboring Coarse Spin Configurations) When decimating even sites, each even spin \\(S_i\\) (even-indexed) is surrounded by two odd spins: \\(S_{i-1}\\) and \\(S_{i+1}\\) (odd-indexed). The partial sum over \\(S_i = \\pm 1\\) for each \"odd-even-odd\" block must match the corresponding term in the coarse-grained partition function. There are 3 distinct configurations of the outer odd spins, leading to 3 core equations:  1. **Case 1: Outer odd spins are both \\(+1\\) (\\(S_{i-1} = S_{i+1} = +1\\))**   Sum over \\(S_i = \\pm 1\\):   \\(\\exp\\left\\{ K \\cdot (+1) \\cdot S_i + K \\cdot S_i \\cdot (+1) + B\\left( \\frac{+1}{2} + S_i + \\frac{+1}{2} \\right) \\right\\}\\) (from the problem’s given partial sum exponent)   Simplifies to summing over \\(S_i = \\pm 1\\): \\(\\exp(2K S_i + B(1 + S_i)) = \\exp(B) \\cdot \\exp((2K + B)S_i)\\).   Evaluating for \\(S_i = +1\\) and \\(S_i = -1\\), the sum becomes:   \\(\\exp(2K + 2B) + \\exp(-2K) = f(K,B) \\cdot \\exp(K' + B')\\)   (matches the coarse-grained term for adjacent coarse spins \\(+1, +1\\)).  2. **Case 2: Outer odd spins are both \\(-1\\) (\\(S_{i-1} = S_{i+1} = -1\\))**   Similarly, sum over \\(S_i = \\pm 1\\):   \\(\\exp\\left\\{ K \\cdot (-1) \\cdot S_i + K \\cdot S_i \\cdot (-1) + B\\left( \\frac{-1}{2} + S_i + \\frac{-1}{2} \\right) \\right\\} = \\exp(-B) \\cdot \\exp((-2K + B)S_i)\\).   Evaluating for \\(S_i = +1\\) and \\(S_i = -1\\), the sum becomes:   \\(\\exp(-2K) + \\exp(2K - 2B) = f(K,B) \\cdot \\exp(K' - B')\\)   (matches the coarse-grained term for adjacent coarse spins \\(-1, -1\\)).  3. **Case 3: Outer odd spins have opposite signs (\\(S_{i-1} = -S_{i+1} = \\pm 1\\))**   Let \\(S_{i-1} = +1\\) and \\(S_{i+1} = -1\\) (the result is symmetric for \\(S_{i-1} = -1, S_{i+1} = +1\\)). Sum over \\(S_i = \\pm 1\\):   \\(\\exp\\left\\{ K \\cdot (+1) \\cdot S_i + K \\cdot S_i \\cdot (-1) + B\\left( \\frac{+1}{2} + S_i + \\frac{-1}{2} \\right) \\right\\} = \\exp(B S_i)\\).   Evaluating for \\(S_i = +1\\) and \\(S_i = -1\\), the sum becomes:   \\(\\exp(B) + \\exp(-B) = f(K,B) \\cdot \\exp(-K')\\)   (matches the coarse-grained term for adjacent coarse spins with opposite signs).",
            "4: Eliminate the Constant Factor \\(f(K,B)\\) to Establish Equations for \\(K'\\) and \\(B'\\) First, solve for \\(f(K,B)\\) from Case 3 (it only involves \\(K'\\) and \\(B\\), simplifying elimination):  From Case 3: \\(f(K,B) = \\frac{\\exp(B) + \\exp(-B)}{\\exp(-K')} = (\\exp(B) + \\exp(-B)) \\cdot \\exp(K')\\) — denoted as Equation (3a).  Substitute Equation (3a) into Case 1 and Case 2 to eliminate \\(f(K,B)\\):  - For Case 1:   \\(\\exp(2K + 2B) + \\exp(-2K) = (\\exp(B) + \\exp(-B)) \\cdot \\exp(K') \\cdot \\exp(K' + B')\\)   Simplifies to: \\(\\frac{\\exp(2K + 2B) + \\exp(-2K)}{\\exp(B) + \\exp(-B)} = \\exp(2K' + B')\\) — denoted as Equation (1a).   - For Case 2:   \\(\\exp(2K - 2B) + \\exp(-2K) = (\\exp(B) + \\exp(-B)) \\cdot \\exp(K') \\cdot \\exp(K' - B')\\)   Simplifies to: \\(\\frac{\\exp(2K - 2B) + \\exp(-2K)}{\\exp(B) + \\exp(-B)} = \\exp(2K' - B')\\) — denoted as Equation (2a).",
            "5: Solve for the Renormalized Interaction Coefficient \\(K'\\) To eliminate \\(B'\\), multiply Equation (1a) by Equation (2a):  - Left-hand side (LHS):   \\(\\frac{[\\exp(2K + 2B) + \\exp(-2K)] \\cdot [\\exp(2K - 2B) + \\exp(-2K)]}{(\\exp(B) + \\exp(-B))^2}\\)   Expand the numerator using \\((A+B)(C+D) = AC + AD + BC + BD\\):   \\(\\exp((2K+2B)+(2K-2B)) + \\exp((2K+2B)+(-2K)) + \\exp((-2K)+(2K-2B)) + \\exp((-2K)+(-2K))\\)   Simplifies to: \\(\\exp(4K) + \\exp(2B) + \\exp(-2B) + \\exp(-4K)\\).   - Right-hand side (RHS):   \\(\\exp(2K' + B') \\cdot \\exp(2K' - B') = \\exp(4K')\\) (since exponents add, and \\(B'\\) terms cancel).   Equate LHS and RHS:  \\(\\frac{\\exp(4K) + \\exp(2B) + \\exp(-2B) + \\exp(-4K)}{(\\exp(B) + \\exp(-B))^2} = \\exp(4K')\\)   Take the natural logarithm of both sides and divide by 4 to solve for \\(K'\\):  \\(K' = \\frac{1}{4} \\ln\\left( \\frac{\\exp(4K) + \\exp(2B) + \\exp(-2B) + \\exp(-4K)}{(\\exp(B) + \\exp(-B))^2} \\right)\\)",
            "6: Solve for the Renormalized Field Coefficient \\(B'\\) To eliminate \\(K'\\) and \\(f(K,B)\\), divide Equation (1a) by Equation (2a):  - Left-hand side (LHS):   \\(\\frac{\\exp(2K + 2B) + \\exp(-2K)}{\\exp(2K - 2B) + \\exp(-2K)}\\)   - Right-hand side (RHS):   \\(\\frac{\\exp(2K' + B')}{\\exp(2K' - B')} = \\exp(2B')\\) (since exponents subtract, and \\(K'\\) terms cancel).   Equate LHS and RHS:  \\(\\frac{\\exp(2K + 2B) + \\exp(-2K)}{\\exp(2K - 2B) + \\exp(-2K)} = \\exp(2B')\\)   Take the natural logarithm of both sides and divide by 2 to solve for \\(B'\\):  \\(B' = \\frac{1}{2} \\ln\\left( \\frac{\\exp(2K + 2B) + \\exp(-2K)}{\\exp(2K - 2B) + \\exp(-2K)} \\right)\\)",
            "7: Final Results The renormalized coefficients, containing only exponential functions (no hyperbolic cosines), are:  1. Renormalized interaction coefficient:  \\(K' = \\frac{1}{4} \\ln\\left( \\frac{\\exp(4K) + \\exp(2B) + \\exp(-2B) + \\exp(-4K)}{\\exp(2B) + 2 + \\exp(-2B)} \\right)\\)  (Note: \\((\\exp(B) + \\exp(-B))^2 = \\exp(2B) + 2 + \\exp(-2B)\\), so the denominator can be written in this simplified form.)  2. Renormalized field coefficient:  \\(B' = \\frac{1}{2} \\ln\\left( \\frac{\\exp(2K + 2B) + \\exp(-2K)}{\\exp(2K - 2B) + \\exp(-2K)} \\right)\\)"
        ],
        "step_count": 7
    },
    "recuWFIUj74GY9": {
        "reasoning_steps": [
            "Goal: In curved spacetime, for a high-frequency (ω ≫ 1/ℓ_geom) circularly polarized electromagnetic wave starting from W = \\tfrac{1}{8} \\int F^2 \\sqrt{-g} \\, d^4x, derive the field-strength invariant F^2 \\equiv F_{\\mu\\nu} \\bar F^{\\mu\\nu}.",
            "High-frequency ansatz (Concept_1): Take the complex potential A_\\mu = a \\, M_\\mu \\, e^{i\\omega S}, with real amplitude a and phase S, and k_\\mu \\equiv S_{,\\mu}; the complex null vector M^\\mu satisfies M_\\mu M^\\mu = \\bar M_\\mu \\bar M^\\mu = 0, M_\\mu \\bar M^\\mu = 1, and M^\\mu k_\\mu = 0 (circular polarization encoded by M).",
            "Order expansion of the field strength (Concept_2): F_{\\mu\\nu} = A_{\\nu;\\mu} - A_{\\mu;\\nu} = e^{i\\omega S}(H_{\\mu\\nu} + G_{\\mu\\nu}), with H_{\\mu\\nu} = i\\omega a (k_{\\mu} M_{\\nu} - k_{\\nu} M_{\\mu}) = \\mathcal{O}(\\omega) and G_{\\mu\\nu} = (a M_{\\nu})_{;\\mu} - (a M_{\\mu})_{;\\nu} = \\mathcal{O}(1). Hence F^2 = H_{\\mu\\nu} \\bar H^{\\mu\\nu} + (H_{\\mu\\nu} \\bar G^{\\mu\\nu} + G_{\\mu\\nu} \\bar H^{\\mu\\nu}) + G_{\\mu\\nu} \\bar G^{\\mu\\nu}.",
            "Power counting (Concept_2,3): Keep \\mathcal{O}(\\omega^2) and \\mathcal{O}(\\omega) terms—i.e., H\\bar H (leading) and the cross term H\\bar G + G\\bar H; drop G\\bar G = \\mathcal{O}(1) as subleading in the high-frequency expansion.",
            "Compute H\\bar H via the bivector inner-product identity and constraints M·\\bar M = 1, M·k = 0: writing H_{\\mu\\nu} = i\\omega a (k \\wedge M)_{\\mu\\nu} gives H_{\\mu\\nu} \\bar H^{\\mu\\nu} = 2 \\omega^2 a^2 (k \\cdot k) = 2 \\omega^2 a^2 (\\nabla S)^2.",
            "Prepare the cross term (Concept_3): Use G_{\\mu\\nu} = a(M_{\\nu;\\mu} - M_{\\mu;\\nu}) + (a_{;\\mu} M_{\\nu} - a_{;\\nu} M_{\\mu}) and the identity \\bar M_\\nu M^\\nu = 1 \\Rightarrow (\\bar M_{\\nu;\\mu} M^\\nu + \\bar M_\\nu M^{\\nu}{}_{;\\mu}) = 0.",
            "Introduce the spin-connection one-form B_\\mu \\equiv i \\, \\bar M_\\nu M^{\\nu}{}_{;\\mu} and simplify: (H_{\\mu\\nu} \\bar G^{\\mu\\nu} + G_{\\mu\\nu} \\bar H^{\\mu\\nu}) = -4 \\omega a^2 B_\\mu k^\\mu + (\\text{total derivative}) + \\mathcal{O}(\\omega^0) (Concept_4).",
            "Assemble terms and drop boundary/subleading pieces: F^2 \\simeq H\\bar H + (H\\bar G + G\\bar H) = 2 \\omega^2 a^2 (k \\cdot k) - 4 \\omega a^2 B_\\mu k^\\mu.",
            "Rewrite with k_\\mu = S_{,\\mu} and (\\nabla S)^2 \\equiv k_\\mu k^\\mu; factor out 4\\omega^2 a^2.",
            "Final result: F^2 = 4 \\, \\omega^2 a^2 \\!\\left[ \\tfrac{1}{2} (\\nabla S)^2 - \\frac{1}{\\omega} B^\\mu S_{,\\mu} \\right], which is the desired high-frequency, polarization-corrected field-strength invariant."
        ],
        "step_count": 10
    },
    "recuWOoknAO2mx": {
        "reasoning_steps": [
            "First, a baseline viscosity is established that accounts for the increase in pressure with depth. This is represented by a depth-dependent prefactor, $\\eta_r$ (Concept_2). This forms the base of the viscosity calculation.",
            "Next, the strong influence of temperature on mantle rheology is incorporated. This is modeled as an exponential relationship where viscosity decreases with increasing temperature, governed by the activation coefficient $E$ and the non-dimensional temperature $T$. This term, $exp[E(0.5-T)]$, is multiplied by the base viscosity (Concept_1).",
            "The model must also account for the significant change in material properties due to the bridgmanite (Bdg) to post-perovskite (pPv) phase transition in the deep mantle (Concept_3).",
            "To quantify this effect, a phase function, $\\Gamma$, is introduced. This function smoothly varies from 0 (representing pure Bdg) to 1 (representing pure pPv), indicating the fraction of the material that has transformed into the pPv phase at a given location (Concept_4).",
            "The presence of the pPv phase alters the viscosity by a specific multiplicative factor, $\\eta_{ppv}$ (Concept_5). The effect of this factor is scaled by the fraction of pPv present, which is mathematically expressed as $(\\eta_{ppv})^\\Gamma$.",
            "Finally, all these components are combined into a single equation. The depth-dependent prefactor $\\eta_r$ is multiplied by the temperature-dependent term and the phase-dependent term. The two exponential effects are combined into a single exponent, using the mathematical identity that $exp(a) \\cdot (\\eta_{ppv})^\\Gamma = exp(a) \\cdot exp[\\Gamma~ln(\\eta_{ppv})] = exp[a + \\Gamma~ln(\\eta_{ppv})]$. This yields the final comprehensive formula: $\\eta=\\eta_{r}exp[E(0.5-T)+\\Gamma~ln(\\eta_{ppv})]$."
        ],
        "step_count": 6
    },
    "recuWRd7d8pLsA": {
        "reasoning_steps": [
            "To obtain the dominant orientation before and after the mainshock, we follow the spatiotemporal sampling strategy. Specifically, we extract focal mechanisms within the centroid zone (defined in Fig. 1 of the paper) and apply separate time windows for the pre- and post-mainshock periods: Spatial window: centroid zone region around the Ridgecrest Mw 7.1 mainshock centroid (same as in Atterholt et al. 2025). Temporal windows: Pre-mainshock window: seismicity prior to July 4, 2019 (up to foreshock sequence). Post-mainshock window: early post-seismic period within 0–0.1 years after mainshock (as used in Fig. 2 of the paper). Event selection criteria: Minimum number of events per window: N≥150 focal mechanisms (consistent with SMS temporal sampling). Minimum event quality: only focal mechanisms with strike, dip, rake uncertainty < 7.5° are used (as stated in Section \"Moment Tensor Catalogue Construction\"). Only double-couple mechanisms retained (removes pathological solutions; consistent with authors’ filtering). Then the normalized moment tensors in each time window are stacked using SMS: Sij=∑k=1NP^ij(k),P^ij(k)=Pij(k)M0(k). The P-axis azimuth of Sij defines θpre and θpost.",
            "Using Concept 1, the dominant average orientations before and after the mainshock are represented by θpre and θpost, obtained via P-axis azimuths from SMS. Because the orientation is axis-symmetric (equivalent under +180°), the rotation is computed using the corrected axis-angle expression: Δθ=mod(θpost−θpre+90∘,180∘)−90∘. The result yields Δθ<0, indicating a counterclockwise rotation, and ∣Δθ∣≈20∘, consistent with moderate rotational amplitude.",
            "Referring to the post-earthquake stress field adjustment mechanism (Concept 3) and combining with observational results, the rotation angle in the main slip zone of the main earthquake remains stable for many years after the earthquake, indicating that this counterclockwise rotation reflects the permanent adjustment of the stress field caused by the main earthquake."
        ],
        "step_count": 3
    },
    "recuWYZyxhQFaj": {
        "reasoning_steps": [
            "Goal: derive the one-line expression for the third-order differential probability density d3Px,yds d2r\\frac{\\mathrm{d}^3 P_{x,y}}{\\mathrm{d}s\\,\\mathrm{d}^2\\mathbf{r}} in a small-xx geometric model of pppp collisions.",
            "Concept_1: identify the target quantity as the momentum–space–impact-parameter differential probability d3Px,yds d2r\\frac{\\mathrm{d}^3 P_{x,y}}{\\mathrm{d}s\\,\\mathrm{d}^2\\mathbf{r}}.",
            "Concept_2: invoke the factorised collinear+impact-parameter picture: d3P=[longitudinal factor]×[transverse-overlap factor]×[normalisation]\\mathrm{d}^3 P = [\\text{longitudinal factor}] \\times [\\text{transverse-overlap factor}] \\times [\\text{normalisation}].",
            "Concept_3: adopt the leading-order small-xx unintegrated gluon probability",
            "Concept_4: model the transverse overlap as the coherent sum of a universal background ∣w0∣2\\lvert w_{0} \\rvert^{2} and a collision-specific term ∣wx,y∣2\\lvert w_{x,y} \\rvert^{2}.",
            "Concept_5: impose the saturation-inspired weight s28(1−s)\\frac{s^{2}}{8(1-s)} on the background term to reproduce the GBW saturation correction to the gluon form-factor",
            "Concept_6: fix the overall normalisation to the leading-order colour-singlet eikonal cross-section yielding α(2πη)2\\frac{\\alpha}{(2\\pi\\eta)^{2}}.",
            "Assemble the three factors to obtain the unique one-line expression: d3Px,yds d2r=[α(2πη)2]⋅[s1−s]⋅[s2∣w0∣28(1−s)+∣wx,y∣2]\\frac{\\mathrm{d}^3 P_{x,y}}{\\mathrm{d}s\\,\\mathrm{d}^2\\mathbf{r}} = \\left[\\frac{\\alpha}{(2\\pi\\eta)^{2}}\\right] \\cdot \\left[\\frac{s}{1-s}\\right] \\cdot \\left[\\frac{s^{2}\\lvert w_{0}\\rvert^{2}}{8(1-s)} + \\lvert w_{x,y} \\rvert^{2}\\right]."
        ],
        "step_count": 8
    },
    "recuVPuZERWrr4": {
        "reasoning_steps": [
            "Goal: derive, in one sentence, the large-r asymptotic form of the scattering-state wave function under a short-range potential at low energy (s-wave dominance).",
            "Short-range potential (Concept_1): for r ≫ range(V), the interaction vanishes and ψ obeys the free Helmholtz equation (∇² + k²)ψ = 0 in the asymptotic region.",
            "Low-energy regime (Concept_2): assume kR ≪ 1 with R the potential range; retain only the s-wave (ℓ = 0) in the partial-wave series since higher ℓ are suppressed.",
            "Incoming regular solution (Concept_3): the ℓ = 0 free regular solution is the spherical Bessel j₀(kr) = sin(kr)/(kr), which we write as the incident component.",
            "Outgoing scattered solution (Concept_4 & Concept_5): the asymptotic outgoing solution is the spherical Hankel h₀^{(+)}(kr) ∼ e^{ikr}/(ikr); imposing the Sommerfeld radiation condition selects a purely outgoing term ∝ e^{ikr}/r.",
            "Scattering amplitude (Concept_6): in the s-wave limit the scattering amplitude is angle-independent; denote it by f(k), absorbing constant 1/ik factors and phase conventions into f(k).",
            "Normalization of the incident piece (Concept_7): represent the incident s-wave as N · sin(kr)/(kr), where N is a normalization constant fixed by the chosen convention (e.g., matching to an s-wave component of a plane wave).",
            "Linear superposition (Concept_8): by linearity in the asymptotic region, write ψ_asy as the sum of the incoming regular solution and the outgoing scattered wave.",
            "Optional parametrization (Concept_9): at low k, relate f(k) to the s-wave phase shift via the effective-range expansion, f(k) = [k cot δ₀(k) − i k]^{-1}, if one needs an explicit low-energy model for f(k).",
            "Units/conventions (Concept_10): set ℏ = c = 1 if desired; residual constants and overall phases are absorbed into N and f(k) without changing the asymptotic structure.",
            "Domain of validity (Concept_11): result holds for r → ∞ and kR ≪ 1 with s-wave dominance; angular dependence is neglected consistently.",
            "Assemble the result (Answer): ψ_{asy}(\\mathbf r,\\mathbf k) = N\\,\\dfrac{\\sin(kr)}{kr} + f(k)\\,\\dfrac{e^{ikr}}{r}, \\quad (r \\to \\infty)."
        ],
        "step_count": 12
    },
    "recuTLFdUIZ0ty": {
        "reasoning_steps": [
            "From Concept 2, the K-theoretic vertex function satisfies a q-difference equation, and thus we can use the connection matrix for the q-difference equation to describe the monodromy of the K-theoretic vertex function",
            "From Theorem 1, the monodromy of the K-theoretic vertex function is described by the elliptic stable envelopes in the equivariant elliptic cohomology.",
            "From the Fact 1, we can identify the equivariant quantum K-theory of Higgs branch as the corresponding A-twisted Q_A-cohomology of a 3d N=4 quiver gauge theory, and identify the equivariant elliptic cohomology by the ordinary Q-cohomology of the 3d N=4 quiver gauge theory.",
            "Using the Fact 3, we know that we can identify the elliptic stable envelopes as the Janus interface and it is expected to be true for arbitrary Higgs branch.",
            "By the definition of the Janus interface in the Fact 2, we can now say that the monodromy for the K-theoretic vertex function for the Higgs branch in the context of 3d N=4 quantum field theory is the Janus interface."
        ],
        "step_count": 5
    },
    "recuUc16ttR11G": {
        "reasoning_steps": [
            "Gut microbiota proliferates after invading the intestinal tract and induces local intestinal inflammation, at which time microbiota-specific CD4 T cells and other immune cells such as neutrophils and macrophages are involved in the formation of local intestinal inflammation",
            "After activation in the intestines, T cells migrate to the circulatory system, where most T cells home to lymph nodes to guide B cell differentiation, and some T cells migrate to the blood-brain barrier",
            "Due to the selective function of the blood-brain barrier, only activated T cells enter the nervous system as immune surveillance components",
            "Due to molecular mimicry, there is similarity between gut microbiota-specific peptides and some peptides of cells in the body",
            "Similar peptides activate T cells and induce their local proliferation, thereby forming a sufficient cell population to produce cytokines such as IL-17A, IL-17F, and IL-22",
            "Inflammatory factors further inhibit the blood-brain barrier and selectively recruit more immune cells, ultimately leading to neuroinflammation"
        ],
        "step_count": 6
    },
    "recuTRbG79flPN": {
        "reasoning_steps": [
            "According to concept_1, a polynomial f for 'computing square roots' must satisfy: for all non-zero squares a ∈ ℤ_p, f(a)^2 = a, that is, f(X)^2 - X is zero at all non-zero squares. Combined with Hint 1, the factorization f(X)^2 = A(X)X^{(p-1)/2} + B(X) can be obtained, where deg A = deg B = 2d (d = deg f) minus (p-1)/2, and A ≠ 0, B ≠ 0.",
            "Hint 2 indicates that if we assume d < (p-1)/3, simplifying the factorization modulo X^{(p-3)/2} can derive 2f'/f = B'/B, and further obtain f^2 = λB (λ is a constant). Substituting into the factorization leads to A = 0, which contradicts the premise A ≠ 0. Thus, d ≮ (p-1)/3, i.e., d ≥ (p-1)/3.",
            "Combined with theorem_1, for f^2 = A X^{(p-1)/2} + B, where deg f = d and deg B = 2d - (p-1)/2, the condition of 'degree below a specific threshold' is satisfied. Therefore, d + deg B ≥ (p-1)/2. Substituting deg B and simplifying gives 3d ≥ p - 1, i.e., d ≥ (p-1)/3.",
            "In summary, the minimum degree of any polynomial for 'computing square roots' over ℤ_p (when p ≡ 1 mod 4) is (p-1)/3."
        ],
        "step_count": 4
    },
    "recuUkH37v8PLg": {
        "reasoning_steps": [
            "Curve Γ is represented by the level set φ, and its interior (foreground) is denoted as Ω_Γ={x:φ(x)≤0}. The paper adopts the convention: the foreground corresponds to g_F and the background corresponds to g_B. The image functional is I(Γ)=∫_{ℝ²} g(x)dx, and the Heaviside/characteristic function is used to express g as two parts: foreground and background, i.e., I(Γ)=∫_{ℝ²} [g_F(x)(1-H(φ(x)))+g_B(x)H(φ(x))]dx (paper (6),(28)).",
            "To compute the shape derivative, expand g into foreground/background and Heaviside, then compute the Gateaux derivative for φ_s=φ+sψ (definition of directional derivative in Equation (4)), substitute H'(φ)=δ(φ) to obtain dI(Γ)(ψ)=∫_{ℝ²} (-g_Fδ(φ)ψ + g_Bδ(φ)ψ)dx = ∫_{ℝ²} (g_F - g_B)(-ψ/|∇φ|)δ(φ)|∇φ|dx = ∫_Γ (g_F-g_B)v dσ (paper (2d_derivation)), where the normal velocity v=-ψ/|∇φ|, and the integrand of the shape derivative is g_F-g_B (core quantity for subsequent TD derivation).",
            "The definition of topological derivative: introduce a closed disk B_ε(x) at point x, the topological derivative is defined as D_τ(x,Ω_Γ) ≜ lim_{ε→0}[I(Ω_Γ\\overline{B_ε(x)}) - I(Ω_Γ)]/V(B_ε) (paper (9)), and the standard asymptotic expansion is I(Ω_Γ\\overline{B_ε}) = I(Ω_Γ) + V(B_ε)D_τ(x,Ω_Γ) + o(V(B_ε)) (paper (10)).",
            "Select the velocity field v: the paper constructs a normal velocity field v such that v=0 on the original unperturbed Γ, v=1 on the new boundary ∂B_ε (perimeter of the 'hole'), with a smooth transition between them; v represents a directional field that 'expands the perimeter of the hole outward'.",
            "Express topological difference using shape derivative: use the Gateaux (shape) derivative to write the derivative of the perturbed shape, note that the shape-derivative is 0 on the original Γ (since v=0 on Γ, Equation (11)), so the TD is rewritten as D_τ(x,Ω_Γ)=lim_{ε→0} [1/V'(B_ε)]dI(Ω_Γ\\overline{B_ε})(v) = lim_{ε→0} [1/V'(B_ε)]∫_{Γ∪∂B_ε} (g_F-g_B)v dσ (paper (12)), where V'(B_ε)=d/dε V(B_ε)=2πε (since V(B_ε)=πε²).",
            "Only the contribution of ∂B_ε remains and take the limit: since v=0 on the original Γ, only ∂B_ε contributes; substitute v=1 on ∂B_ε and simplify to get D_τ(x,Ω_Γ)=lim_{ε→0} [1/(2πε)]∫_{∂B_ε} (g_F-g_B)dσ = lim_{ε→0} (circular average of g_F-g_B on ∂B_ε) = g_F(x)-g_B(x) (paper (13)), using the continuity of g_F and g_B at point x (local average → point value), finally obtaining D_τI(Γ)(x)=g_F(x)-g_B(x) for x∈Ω_Γ."
        ],
        "step_count": 6
    }
}